{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from transformers import (AutoModelForCausalLM, AutoTokenizer, BertForSequenceClassification, BertTokenizer, BertModel,\n",
    " RobertaForSequenceClassification, RobertaTokenizer, RobertaModel, TrainingArguments, Trainer, pipeline, RobertaConfig)\n",
    "\n",
    "\n",
    "\n",
    "from datasets import load_dataset, load_from_disk, Dataset, DatasetDict, concatenate_datasets\n",
    "import evaluate\n",
    "import wandb\n",
    "import os\n",
    "\n",
    "os.environ[\"WANDB_PROJECT\"] = \"gen_detector\"  # name your W&B project\n",
    "os.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint\"  # log all model checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init Generator and Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GEN_PATH = \"microsoft/phi-2\"\n",
    "GEN_PATH = \"openai-community/gpt2\"\n",
    "#GEN_PATH = \"Qwen/Qwen1.5-0.5B-Chat\"\n",
    "#BERT_PATH = \"bert-base-uncased\"\n",
    "BERT_PATH = \"openai-community/roberta-base-openai-detector\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "class LLMGenerator(nn.Module):\n",
    "  def __init__(self, gpt_model, tokenizer, device=None, gen_params=None):\n",
    "    super().__init__()\n",
    "\n",
    "    # gpt should already be trained\n",
    "    self.gpt = gpt_model\n",
    "    self.tokenizer = tokenizer\n",
    "    if device is None:\n",
    "      self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    else:\n",
    "      self.device = device\n",
    "\n",
    "    self.default_gen_params = {\n",
    "        \"max_length\": 150,\n",
    "        #\"max_new_tokens\": 100,\n",
    "        \"temperature\": 0.8,\n",
    "        \"top_p\": 0.8,\n",
    "        \"repetition_penalty\": 1\n",
    "    }\n",
    "    self.gen_params = gen_params if gen_params is not None else self.default_gen_params\n",
    "       \n",
    "\n",
    "  def forward_old(self, text, max_length=512, max_new_tokens=100, temperature=1, top_k=50, top_p=0.9, repetition_penalty=1, skip_special_tokens=True):\n",
    "\n",
    "    # tokenize text using the tokenizer\n",
    "    input_ids = self.tokenizer.encode(text, return_tensors=\"pt\").to(self.device)\n",
    "    # generate text using the gpt model\n",
    "    #output_ids = self.gpt.generate(input_ids, max_length=max_length, max_new_tokens=max_new_tokens, temperature=temperature, top_k=top_k, top_p=top_p, repetition_penalty=repetition_penalty)\n",
    "    with torch.no_grad():\n",
    "      output_ids = self.gpt.generate(input_ids, max_new_tokens=max_new_tokens, temperature=temperature, top_k=top_k, top_p=top_p, repetition_penalty=repetition_penalty)\n",
    "\n",
    "    # optional, remove input_ids from output_ids\n",
    "    #output_ids = [output_id[len(input_ids):] for input_id, output_id in zip(input_ids, output_ids)]\n",
    "\n",
    "    # decode the generated text\n",
    "    decoded_output = self.tokenizer.batch_decode(output_ids, skip_special_tokens=skip_special_tokens)[0]\n",
    "\n",
    "    # remove the input text from the generated text\n",
    "    decoded_output = decoded_output.replace(text, \"\").strip()\n",
    "\n",
    "    # remove special tokens from the generated text\n",
    "    special_tokens = self.tokenizer.additional_special_tokens\n",
    "\n",
    "    for special_token in special_tokens:\n",
    "      decoded_output = decoded_output.replace(special_token, \"\")\n",
    "\n",
    "    #decoded_output = decoded_output.replace(text, \"\")\n",
    "    return decoded_output\n",
    "  \n",
    "\n",
    "  def forward(self, samples, max_new_tokens=100):\n",
    "\n",
    "    max_length = self.gen_params[\"max_length\"]\n",
    "    #self.gen_params[\"max_new_tokens\"] = max_new_tokens\n",
    "    encoding = self.tokenizer.batch_encode_plus(samples, return_tensors='pt', padding=True, truncation=True, max_length=max_length)\n",
    "    input_ids = encoding['input_ids'].to(self.device)\n",
    "\n",
    "    # generate text using the gpt model\n",
    "    with torch.no_grad():\n",
    "      #output_ids = self.gpt.generate(input_ids, max_new_tokens=max_new_tokens, temperature=temperature, top_p=top_p, repetition_penalty=repetition_penalty)\n",
    "      output_ids = self.gpt.generate(input_ids, pad_token_id = self.tokenizer.pad_token_id, **self.gen_params)\n",
    "\n",
    "    # decode the generated text\n",
    "    #decoded_outputs = self.tokenizer.batch_decode(output_ids, skip_special_tokens=skip_special_tokens)\n",
    "    decoded_outputs = self.tokenizer.batch_decode(output_ids[:, input_ids.shape[1]:])\n",
    "        \n",
    "    # remove special tokens from the generated text\n",
    "    special_tokens = self.tokenizer.additional_special_tokens + [self.tokenizer.pad_token] + [self.tokenizer.eos_token]\n",
    "    for i, sample in enumerate(samples):\n",
    "        decoded_output = decoded_outputs[i]\n",
    "        for special_token in special_tokens:\n",
    "            decoded_output = decoded_output.replace(special_token, \"\")\n",
    "        decoded_outputs[i] = decoded_output\n",
    "        \n",
    "    return decoded_outputs\n",
    "  \n",
    "\n",
    "class LLMDetector(nn.Module):\n",
    "  def __init__(self, bert_model, tokenizer, num_classes):\n",
    "    super().__init__()\n",
    "\n",
    "    self.tokenizer = tokenizer\n",
    "\n",
    "    # bert should already be trained\n",
    "    self.bert = bert_model\n",
    "\n",
    "    # set num_classes\n",
    "    self.num_classes = num_classes\n",
    "\n",
    "  def forward(self, text):\n",
    "\n",
    "    # tokenize text using the tokenizer\n",
    "    output = self.tokenizer(text, return_tensors=\"pt\")\n",
    "    input_ids = output[\"input_ids\"].to(device)\n",
    "    logits = self.bert(input_ids)[\"logits\"]\n",
    "\n",
    "    # apply sigmoid to get probabilities of each class\n",
    "    output = torch.sigmoid(logits)\n",
    "    return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list = []\n",
    "\n",
    "if  test_list:\n",
    "    print(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " return [0, 0] for i in range (3) do try: # Get prime numbers from the list of digits if len(primples[i]) > 5 then s = [] while not sorted.empty() or None : tuple=dict([1..9], dict((j-s), strlen(tuple))).split().join(' ','') end break elseif k >= 2 then j = lambda x: vy += 3 * np.zeros(-k + y)*2/4 except KeyboardError as e: raise ExitIteration EndIterator def get_routine():... >>> int64(\"%c\", \"\").putln(lambda r: f32+f34)+\"|\".format(predictor()) - pprintl(*{'x':''})....}..... python\n"
     ]
    }
   ],
   "source": [
    "# this causes an issue with Trainer\n",
    "#torch.set_default_device(\"cuda\")\n",
    "\n",
    "gen_model = AutoModelForCausalLM.from_pretrained(GEN_PATH, torch_dtype=\"auto\").to(device)\n",
    "gen_tokenizer = AutoTokenizer.from_pretrained(GEN_PATH, trust_remote_code=True)\n",
    "\n",
    "gen_tokenizer.pad_token = gen_tokenizer.eos_token\n",
    "\n",
    "gen_params = {\n",
    "        \"max_length\": 200,\n",
    "        \"max_new_tokens\": None,\n",
    "        \"temperature\": 0.8,\n",
    "        \"top_p\": 0.8,\n",
    "        \"repetition_penalty\": 1,\n",
    "        \"do_sample\": True\n",
    "    }\n",
    "\n",
    "gen_params[\"repetition_penalty\"] = 1.1\n",
    "generator = LLMGenerator(gen_model, gen_tokenizer, gen_params=gen_params, device=device)\n",
    "\n",
    "\n",
    "\n",
    "text_input = ['''def print_prime(n):\n",
    "   \"\"\"\n",
    "   Print all primes between 1 and n\n",
    "   \"\"\"''']\n",
    "with torch.no_grad():\n",
    "   output = generator(text_input)\n",
    "print(output[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at openai-community/roberta-base-openai-detector were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "detector_model = RobertaForSequenceClassification.from_pretrained(BERT_PATH).to(device)\n",
    "bert_tokenizer = RobertaTokenizer.from_pretrained(BERT_PATH)\n",
    "detector = LLMDetector(detector_model, bert_tokenizer, 2)\n",
    "\n",
    "text = \"def print_prime(n):\\n   \\\"\\\"\\\"\\n   Print all primes between 1 and n\\n   \\\"\\\"\\\"\"\n",
    "\n",
    "logits = detector(text)\n",
    "fake = logits.argmax().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detector_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "detector_model.classifier = nn.Sequential(\n",
    "        nn.Linear(detector_model.config.hidden_size, detector_model.config.hidden_size),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.Linear(detector_model.config.hidden_size, detector_model.config.hidden_size),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.Linear(detector_model.config.hidden_size, num_classes)\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.1, inplace=False)\n",
       "    (3): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.1, inplace=False)\n",
       "    (6): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detector_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>After weeks\n"
     ]
    }
   ],
   "source": [
    "test_squence =  [0, 4993,  688]\n",
    "decoded = bert_tokenizer.decode(test_squence)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The women are too afraid and ashamed to show their faces. They have been raped, abused and left for dead by men they trusted. Now they are'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_str = \"The women are too afraid and ashamed to show their faces.  They have been raped, abused and left for dead by men they trusted.  Now they are\"\n",
    "test_str = test_str.replace(\".  \", \". \")\n",
    "test_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detector_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifier.dense.weight\n",
      "classifier.dense.bias\n",
      "classifier.out_proj.weight\n",
      "classifier.out_proj.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in detector_model.named_parameters():\n",
    "    if name.startswith(\"classifier\"):\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This sentence. Has double spaces. And is not a complete sentence'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_text_mistral = \"This sentence.  Has double spaces.  And is not a complete sentence\"\n",
    "fake_text_mistral = fake_text_mistral.replace(\".  \", \". \")\n",
    "fake_text_mistral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset of instructions and output with gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['instruction', 'context', 'response', 'category'],\n",
       "        num_rows: 15011\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_path = \"databricks/databricks-dolly-15k\"\n",
    "dataset = load_dataset(dataset_path)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'When did Virgin Australia start operating?',\n",
       " 'context': \"Virgin Australia, the trading name of Virgin Australia Airlines Pty Ltd, is an Australian-based airline. It is the largest airline by fleet size to use the Virgin brand. It commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route. It suddenly found itself as a major airline in Australia's domestic market after the collapse of Ansett Australia in September 2001. The airline has since grown to directly serve 32 cities in Australia, from hubs in Brisbane, Melbourne and Sydney.\",\n",
       " 'response': 'Virgin Australia commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route.',\n",
       " 'category': 'closed_qa'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.dataset_dict.DatasetDict"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.arrow_dataset.Dataset"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length of responses: 358.10419026047566\n"
     ]
    }
   ],
   "source": [
    "# compute mean length of the responses\n",
    "lengths_response = [len(x) for x in dataset[\"train\"][\"response\"]]\n",
    "print(\"Average length of responses:\", np.mean(lengths_response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length of instructions: 71.83938445140231\n"
     ]
    }
   ],
   "source": [
    "lengts_instruction = [len(x) for x in dataset[\"train\"][\"instruction\"]]\n",
    "print(\"Average length of instructions:\", np.mean(lengts_instruction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent of data discarded: 26.29%\n"
     ]
    }
   ],
   "source": [
    "# discard instructions that are more than max_nb_tokens_input tokens\n",
    "max_nb_tokens_input = 100\n",
    "\n",
    "# tokenize the instructions\n",
    "dataset = dataset.map(lambda x: {\"tokenized_instruction\": gen_tokenizer(x[\"instruction\"])})\n",
    "dataset = dataset.map(lambda x: {\"tokenized_context\": gen_tokenizer(x[\"context\"])})\n",
    "dataset_before_len = len(dataset[\"train\"])\n",
    "dataset = dataset.filter(lambda x: len(x[\"tokenized_instruction\"][\"input_ids\"]) + len(x[\"tokenized_context\"][\"input_ids\"]) <= max_nb_tokens_input)\n",
    "dataset_after_len = len(dataset[\"train\"])\n",
    "print(f\"Percent of data discarded: {100*(1 - dataset_after_len/dataset_before_len):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['instruction', 'context', 'response', 'category', 'tokenized_instruction', 'tokenized_context'],\n",
       "        num_rows: 11065\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'Which is a species of fish? Tope or Rope',\n",
       " 'context': '',\n",
       " 'response': 'Tope',\n",
       " 'category': 'classification',\n",
       " 'tokenized_instruction': {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "  'input_ids': [23085, 374, 264, 9419, 315, 7640, 30, 2014, 375, 476, 97896]},\n",
       " 'tokenized_context': {'attention_mask': [], 'input_ids': []}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test output with first instruction\n",
    "text = dataset[\"train\"][0]\n",
    "text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  Context:  \n",
      " Question: Which is a species of fish? Tope or Rope\n",
      "Generated answer:  Context:  \n",
      " Question: Which is a species of fish? Tope or Rope\n",
      "Real human answer:  Tope\n"
     ]
    }
   ],
   "source": [
    "text_instruction = f\"Context: {text[\"context\"]} \\n Question: {text[\"instruction\"]}\"\n",
    "output = generator(text_instruction)\n",
    "print(\"Question: \", text_instruction)\n",
    "#print()\n",
    "print(\"Generated answer: \", output)\n",
    "#print()\n",
    "print(\"Real human answer: \", text[\"response\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the capital of France?'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = generator(\"What is the capital of France?\")\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n",
    "]\n",
    "text = gen_tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system\n",
      "You are a helpful assistant.\n",
      "user\n",
      "What is the capital of France?\n",
      "assistant\n",
      "Paris\n"
     ]
    }
   ],
   "source": [
    "output = generator(text)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system\n",
      "You are a helpful assistant.\n",
      "user\n",
      "Context:  \n",
      " Question: Which is a species of fish? Tope or Rope\n",
      "assistant\n",
      "Tope is a species of fish.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": f\"{text_instruction}\"},\n",
    "]\n",
    "text = gen_tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "output = generator(text)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate fake dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see Open AI GPT-2 generated dataset: https://github.com/openai/gpt-2-output-dataset?tab=readme-ov-file\n",
    "and https://github.com/openai/gpt-2-output-dataset/tree/master/detector for their roberta detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fake_responses(generator, dataset):\n",
    "    \"\"\"\n",
    "    Traverse dataset and generate responses for each instruction\n",
    "    \"\"\"\n",
    "\n",
    "    fake_responses = []\n",
    "    for data in dataset:\n",
    "        # Create query in the format that the generator expects\n",
    "        text_instruction = f\"Context: {data['context']} \\n {data['instruction']}\"\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"{text_instruction}\"},\n",
    "        ]\n",
    "        text = gen_tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "\n",
    "        # Generate response\n",
    "        output = generator(text, skip_special_tokens=False)\n",
    "        \n",
    "        fake_responses.append(output)\n",
    "    return fake_responses\n",
    "\n",
    "def create_random_subset(dataset, n=10):\n",
    "    \"\"\"\n",
    "    Create a random subset of the dataset\n",
    "    \"\"\"\n",
    "    if n > len(dataset):\n",
    "        n = len(dataset)\n",
    "    indices = np.random.choice(len(dataset), n, replace=False)\n",
    "    subset = dataset.select(indices)\n",
    "    return subset\n",
    "\n",
    "def filter_instruction(sample):\n",
    "    \"\"\"\n",
    "    Note: only works if special tokens are not removed\n",
    "    \"\"\"\n",
    "\n",
    "    text_instruction = f\"Context: {sample['context']} \\n {sample['instruction']}\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"{text_instruction}\"},\n",
    "    ]\n",
    "    text_template = gen_tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    generated_response = sample[\"generated_response\"]\n",
    "\n",
    "    response_without_instruction = generated_response.replace(text_template, \"\")\n",
    "    return {\"generated_response\": response_without_instruction}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d380059993e945a3b7612f30a8775f48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# take a random subset of the dataset\n",
    "subset_size = 5\n",
    "train_subset = create_random_subset(dataset[\"train\"], n=subset_size)\n",
    "#test_subset = create_random_subset(dataset[\"test\"], n=10)\n",
    "#eval_subset = create_random_subset(dataset[\"validation\"], n=10)\n",
    "\n",
    "# generate fake responses for the subsets\n",
    "fake_responses_train = generate_fake_responses(generator, train_subset)\n",
    "#fake_responses_test = generate_fake_responses(generator, test_subset)\n",
    "#fake_responses_eval = generate_fake_responses(generator, eval_subset)\n",
    "\n",
    "#fake_dataset = Dataset.from_dict({\"train\": fake_responses_train, \"test\": fake_responses_test, \"validation\": fake_responses_eval})\n",
    "\n",
    "fake_responses_train = Dataset.from_dict({\"generated_response\": fake_responses_train, \"instruction\": train_subset[\"instruction\"],\n",
    "    \"context\": train_subset[\"context\"], \"true_response\": train_subset[\"response\"], \"category\": train_subset[\"category\"]})\n",
    "\n",
    "fake_dataset = DatasetDict()\n",
    "fake_dataset[\"train\"] = fake_responses_train\n",
    "\n",
    "# save fake dataset\n",
    "fake_dataset.save_to_disk(\"fake_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['generated_response', 'instruction', 'context', 'true_response', 'category'],\n",
       "        num_rows: 5\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_tokenizer = AutoTokenizer.from_pretrained(GEN_PATH, trust_remote_code=True)\n",
    "\n",
    "# load fake dataset\n",
    "fake_dataset = load_from_disk(\"fake_dataset\")\n",
    "fake_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Where was the 11th BRICS held?\n"
     ]
    }
   ],
   "source": [
    "print(fake_dataset[\"train\"][0][\"instruction\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brasila, the capital of Brazil\n"
     ]
    }
   ],
   "source": [
    "print(fake_dataset[\"train\"][0][\"true_response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Context:  \n",
      " Where was the 11th BRICS held?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "The 11th BRICS held in Brazil took place at the National Stadium in Brasília.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "print(fake_dataset[\"train\"][0][\"generated_response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7430ceb13b20445fa4975dbd75ba9e90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fake_dataset = fake_dataset.map(filter_instruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 11th BRICS held in Brazil took place at the National Stadium in Brasília.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "print(fake_dataset[\"train\"][0][\"generated_response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_dataset = dataset.select_columns([\"response\"])\n",
    "true_dataset = true_dataset.rename_column(\"response\", \"text\")\n",
    "\n",
    "# select random samples from true_dataset to match fake_dataset size\n",
    "true_dataset = true_dataset.shuffle(seed=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DatasetDict' object has no attribute 'select'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m true_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mtrue_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(fake_dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m])))\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# create label = 0 for true responses and label = 1 for fake responses\u001b[39;00m\n\u001b[1;32m      4\u001b[0m true_dataset \u001b[38;5;241m=\u001b[39m true_dataset\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m x: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m})\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DatasetDict' object has no attribute 'select'"
     ]
    }
   ],
   "source": [
    "true_dataset = true_dataset.select(range(len(fake_dataset[\"train\"])))\n",
    "\n",
    "# create label = 0 for true responses and label = 1 for fake responses\n",
    "true_dataset = true_dataset.map(lambda x: {\"label\": 0})\n",
    "fake_dataset = fake_dataset.map(lambda x: {\"label\": 1})\n",
    "\n",
    "# remove true_response from fake_dataset\n",
    "fake_dataset = fake_dataset.remove_columns([\"true_response\"])\n",
    "\n",
    "# rename generated_response to response\n",
    "fake_dataset = fake_dataset.rename_column(\"generated_response\", \"text\")\n",
    "fake_dataset = fake_dataset.select_columns([\"text\", \"label\"])\n",
    "\n",
    "# merge fake and true datasets\n",
    "merged = concatenate_datasets([true_dataset[\"train\"], fake_dataset[\"train\"]])\n",
    "merged_dataset = DatasetDict()\n",
    "merged_dataset[\"train\"] = merged\n",
    "\n",
    "# shuffle the dataset\n",
    "merged_dataset = merged_dataset.shuffle(seed=42)\n",
    "\n",
    "# save merged dataset\n",
    "merged_dataset.save_to_disk(\"merged_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load merged dataset\n",
    "human_fake_train_dataset = load_from_disk(\"merged_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 15011\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 5\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 15016\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the merged dataset with the detector tokenizer\n",
    "def tokenize_text(x, tokenizer):\n",
    "    return tokenizer(x[\"text\"], truncation=True, padding=\"max_length\")\n",
    "\n",
    "human_fake_train_dataset = human_fake_train_dataset.map(lambda x: tokenize_text(x, bert_tokenizer), batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    warmup_ratio=0.1,\n",
    "    weight_decay=0.01,\n",
    "    learning_rate=1e-3,\n",
    "    logging_steps=5,\n",
    "    logging_dir=\"./logs\",\n",
    "    report_to=\"wandb\",\n",
    "\n",
    "\n",
    "\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=detector_model,\n",
    "    args=training_args,\n",
    "    train_dataset=human_fake_train_dataset[\"train\"],\n",
    "    #eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhdasilva\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/marluxiaboss/llm_text_detector/new_repo/text_llm_detector/wandb/run-20240305_173642-hi3x9oxj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hdasilva/%3Cmy-amazing-project%3E/runs/hi3x9oxj' target=\"_blank\">bright-pine-6</a></strong> to <a href='https://wandb.ai/hdasilva/%3Cmy-amazing-project%3E' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hdasilva/%3Cmy-amazing-project%3E' target=\"_blank\">https://wandb.ai/hdasilva/%3Cmy-amazing-project%3E</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hdasilva/%3Cmy-amazing-project%3E/runs/hi3x9oxj' target=\"_blank\">https://wandb.ai/hdasilva/%3Cmy-amazing-project%3E/runs/hi3x9oxj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='99' max='45048' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   99/45048 00:33 < 4:21:30, 2.86 it/s, Epoch 0.01/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.803000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.367800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.834400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.779000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>2.246600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.046300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.158100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.919600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.213500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.060200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.161700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.441500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.004200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.000400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m wandb\u001b[38;5;241m.\u001b[39mfinish()\n",
      "File \u001b[0;32m~/anaconda3/envs/llm_detector/lib/python3.12/site-packages/transformers/trainer.py:1624\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1622\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1623\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1624\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1625\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1627\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1629\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/llm_detector/lib/python3.12/site-packages/transformers/trainer.py:1966\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1960\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m   1961\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1963\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1964\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1965\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m-> 1966\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1967\u001b[0m ):\n\u001b[1;32m   1968\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1969\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   1970\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DPO training BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./output\",\n",
    "    report_to=\"wandb\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_ratio=0.1,\n",
    "    weight_decay=0.01,\n",
    "    learning_rate=1e-3,\n",
    "    num_train_epochs=3,\n",
    ")\n",
    "\n",
    "dpo_trainer = DPOTrainer(\n",
    "    model,\n",
    "    model_ref=None,\n",
    "    args=training_args,\n",
    "    beta=0.1,\n",
    "    train_dataset=train_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "dpo_trainer.train()\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phi-2 with prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the pipeline\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-1_5\", torch_dtype=torch.float16, trust_remote_code=True).to(\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-1_5\", trust_remote_code=True)\n",
    "generator = pipeline('text-generation', model=model, tokenizer=tokenizer , device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a story about a cat and a dog:\n",
      "\n",
      "Story: The Cat and the Dog\n",
      "\n",
      "Once upon a time, there was a cat named Fluffy and a dog named Spot. Fluffy and Spot were best friends, and they liked to play together. They also liked to eat together, and they shared their food.\n",
      "\n",
      "One day, Fluffy and Spot went to the park with their owners. They saw a lot of other animals there, like birds, squirrels, rabbits, and horses. They also saw a lot of people there, like children, parents, teachers, and doctors.\n",
      "\n",
      "Fluffy and Spot were curious about the other animals and people. They wanted to learn more about them and their lives. They decided to explore the park and see what they could find.\n",
      "\n",
      "Fluffy and Spot saw a bird flying in the sky. They wondered what it was like to fly. They saw a squirrel climbing a tree. They wondered what it was like\n"
     ]
    }
   ],
   "source": [
    "text = '''Here is a story about a cat and a dog:\n",
    "\n",
    "Story:'''\n",
    "result = generator(text, max_length=200)\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a story about a cat and a dog written by Shakespeare:\n",
      "\n",
      "Story:\n",
      "\n",
      "Once upon a time, there was a cat named Whiskers and a dog named Spot. They lived in a cozy house with their owner, Mr. Brown. Whiskers and Spot were very different from each other. Whiskers was independent and aloof, while Spot was loyal and friendly.\n",
      "\n",
      "One day, Mr. Brown decided to take them on a trip to the park. He put them in a basket and drove them to the bus stop. Whiskers was nervous and hid under the basket, while Spot wagged his tail and barked happily.\n",
      "\n",
      "When they arrived at the park, Mr. Brown let them out of the basket. Whiskers cautiously explored the grass and trees, while Spot ran around and played with other dogs. Mr. Brown threw a ball for Spot to fetch, and Spot brought it back to him with a big smile.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = '''Here is a story about a cat and a dog written by Shakespeare:\n",
    "\n",
    "Story:'''\n",
    "result = generator(text, max_length=200)\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a story about a cat and a dog written by Victor Hugo:\n",
      "\n",
      "Story: The Cat and the Dog\n",
      "\n",
      "Once upon a time, there was a cat named Mimi and a dog named Max. Mimi and Max were best friends, but they had very different personalities. Mimi was shy and quiet, while Max was loud and energetic.\n",
      "\n",
      "One day, Mimi and Max decided to go on an adventure together. They wanted to explore the forest near their house, where they had never been before. They packed some food and water, and set off early in the morning.\n",
      "\n",
      "As they walked through the forest, Mimi and Max saw many interesting things. They saw birds, squirrels, rabbits, and butterflies. They also saw a river, a waterfall, and a cave. They were having a lot of fun, until they came across a fork in the road.\n",
      "\n",
      "One path led to the left, and the other led to the right.\n"
     ]
    }
   ],
   "source": [
    "text = '''Here is a story about a cat and a dog written by Victor Hugo:\n",
    "\n",
    "Story:'''\n",
    "result = generator(text, max_length=200)\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"As he awaits a crucial progress report on Iraq, President Bush is facing a difficult decision,\" the article stated. \"He must choose between the immediate threat of a terrorist attack and the long-term goal of a stable and prosperous Iraq.\"\n",
      "\n",
      "The article went on to explain that the President had already made a decision to withdraw U.S. troops from Iraq. However, he was now considering whether to extend the troops' mission or withdraw them completely.\n",
      "\n",
      "\"The President's decision will have far-reaching consequences,\" the article continued. \"It will determine the future of Iraq and the stability of the region.\"\n",
      "\n",
      "The article provided a detailed analysis of the President's options. It discussed the potential risks and benefits of each choice, as well as the potential impact on the lives of the Iraqi people.\n",
      "\n",
      "\"The decision is not an easy one,\" the article concluded. \"It requires careful consideration and a deep understanding of the complex issues at hand.\"\n",
      "\n",
      "As the article\n"
     ]
    }
   ],
   "source": [
    "text = '''\"As he awaits a crucial progress report on Iraq, President Bush'''\n",
    "result = generator(text, max_length=200)\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I saw this piece of news on the internet today:\n",
      "\n",
      "\"As he awaits a crucial progress report on Iraq, President Bush is being urged to take a more aggressive approach to the war. The Bush administration has been under increasing pressure to show that it is making progress in Iraq, but the situation remains complex and uncertain. The Bush administration has been criticized for its lack of transparency and for not providing enough details about the progress being made. Many people are concerned that the war is not going as planned and that the situation in Iraq is getting worse.\n",
      "\n",
      "I think it's important for the president to be honest with the public about the situation in Iraq. He needs to provide clear and detailed information about the progress being made and the challenges that still need to be overcome. The public needs to know that the war is not just about winning a conflict, but also about creating a stable and secure environment for the people of Iraq.\n",
      "\n",
      "I hope that the president will take these concerns seriously and work\n"
     ]
    }
   ],
   "source": [
    "text = '''I saw this piece of news on the internet today:\n",
    "\n",
    "\"As he awaits a crucial progress report on Iraq, President Bush'''\n",
    "result = generator(text, max_length=200)\n",
    "print(result[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the pipeline\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen1.5-0.5B-Chat\", torch_dtype=torch.bfloat16, trust_remote_code=True).to(\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen1.5-0.5B-Chat\", trust_remote_code=True)\n",
    "generator = pipeline('text-generation', model=model, tokenizer=tokenizer , device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Give me a short introduction to large language model.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "\n",
      "A Large Language Model is a type of artificial intelligence that uses large amounts of text data and algorithms to generate human-like responses to questions or prompts. These models are designed to be able to understand natural language, which means they can recognize and respond to variations in language styles, contexts, and phrasing.\n",
      "\n",
      "Large Language Models have become increasingly important in fields such as machine learning, deep learning, and natural language processing. They are used by many organizations to help automate tasks that involve handling large volumes of data and providing personalized assistance to users.\n",
      "\n",
      "Overall, Large Language Models have the potential to revolutionize the way we interact with technology and communicate with one another. As the field continues to evolve, we can expect to see even more sophisticated and advanced models being developed to help us better understand and navigate the world around us.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Give me a short introduction to large language model.\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "print(generator(text, max_length=200)[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful assistant. Your task is to continue the provided text.<|im_end|>\n",
      "<|im_start|>user\n",
      "As he awaits a crucial progress report on Iraq, President Bush<|im_end|>\n",
      "<|im_start|>assistant\n",
      "searches through internet for information about the conflict and the situation in Iraq. He visits various news sources and forums, including The New York Times and CNN, to get a sense of what is happening in the country.\n",
      "As he continues his search, he meets with top government officials and military personnel to discuss potential developments and challenges that may arise during the ongoing war. He also meets with local leaders and activists who are working to support their communities and fight against the conflict.\n",
      "Throughout the process, President Bush remains focused on the needs of his country and its people, and his determination to bring peace and stability back to Iraq is something he is deeply committed to. As he sits in his office, surrounded by his staff and advisors, he can hear the whispers of those around him, wondering how quickly the situation in\n"
     ]
    }
   ],
   "source": [
    "prompt = \"As he awaits a crucial progress report on Iraq, President Bush\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant. Your task is to continue the provided text.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "print(generator(text, max_length=200)[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful assistant. You write text in the style of Shakespeare. Your task is to continue the provided text with a Shakespearean style.<|im_end|>\n",
      "<|im_start|>user\n",
      "As he awaits a crucial progress report on Iraq, President Bush<|im_end|>\n",
      "<|im_start|>assistant\n",
      "attends his weekly meeting with fellow officials and advisors, intently scanning through the report's pages before nodding in approval as it becomes available. His eyes scan through each line, taking in all the details that need to be addressed for the report's success. As he moves on, he considers the potential implications of this report, considering the broader context in which it will impact the country.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"As he awaits a crucial progress report on Iraq, President Bush\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant. You write text in the style of Shakespeare. Your task is to continue the provided text with a Shakespearean style.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "print(generator(text, max_length=200)[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful assistant. You write text for the New York Times. Your task is to write a news article by continuing the provided text.<|im_end|>\n",
      "<|im_start|>user\n",
      "As he awaits a crucial progress report on Iraq, President Bush<|im_end|>\n",
      "<|im_start|>assistant\n",
      "confronts a group of colleagues who have been tasked with preparing his final report to the US Congress.\n",
      "Despite numerous obstacles and setbacks, President Bush remains determined to deliver a solid, unbiased report that addresses the challenges facing America in the region.\n",
      "In an interview with ABC News, President Bush said that he expects the report will be well-received by Congress and the American people.\n",
      "\"We have to do everything we can to ensure that this report is accurate,\" President Bush said. \"And if it doesn't meet our expectations, then we'll need to come up with new strategies.\"\n",
      "The release of the report is expected to take several months, but President Bush has promised that he will make sure that every detail is covered and that his team\n"
     ]
    }
   ],
   "source": [
    "prompt = \"As he awaits a crucial progress report on Iraq, President Bush\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant. You write text for the New York Times. Your task is to write a news article by continuing the provided text.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "print(generator(text, max_length=200)[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful assistant. You write text in the style of a high school student. Your task is to continue the provided text for an assignment.<|im_end|>\n",
      "<|im_start|>user\n",
      "As he awaits a crucial progress report on Iraq, President Bush<|im_end|>\n",
      "<|im_start|>assistant\n",
      "urged his team to focus and prioritize the task at hand. He knew that progress was key to maintaining stability and peace in the region, and that success in this area would help him build a strong legacy as the nation's leader.\n",
      "The progress report provided valuable insights into the situation in Iraq, including details about the current state of the country, the military operations going on, and any potential challenges or threats that may arise. It also highlighted areas where improvements could be made to ensure the safety and security of the people of Iraq.\n",
      "President Bush listened carefully to the progress report and used it as a basis for making decisions about how to proceed with the Iraq mission. He determined that it was essential to continue providing humanitarian assistance to the\n"
     ]
    }
   ],
   "source": [
    "prompt = \"As he awaits a crucial progress report on Iraq, President Bush\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant. You write text in the style of a high school student. Your task is to continue the provided text for an assignment.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "print(generator(text, max_length=200)[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try to detect if fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = RobertaConfig.from_pretrained('FacebookAI/roberta-base')\n",
    "model_path = \"best_model_phi2.pt\"\n",
    "state_dict = torch.load(model_path)\n",
    "model = RobertaForSequenceClassification(config)\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('FacebookAI/roberta-base')\n",
    "pipeline_classif_phi2 = pipeline('text-classification', model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_1', 'score': 0.9982835054397583}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text_fake = '''Story:\n",
    "\n",
    "Once upon a time, there was a cat named Whiskers and a dog named Spot. They lived in a cozy house with their owner, Mr. Brown. Whiskers and Spot were very different from each other. Whiskers was independent and aloof, while Spot was loyal and friendly.\n",
    "\n",
    "One day, Mr. Brown decided to take them on a trip to the park. He put them in a basket and drove them to the bus stop. Whiskers was nervous and hid under the basket, while Spot wagged his tail and barked happily.\n",
    "\n",
    "When they arrived at the park, Mr. Brown let them out of the basket. Whiskers cautiously explored the grass and trees, while Spot ran around and played with other dogs. Mr. Brown threw a ball for Spot to fetch, and Spot brought it back to him with a big smile.'''\n",
    "pipeline_classif_phi2(sample_text_fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_0', 'score': 0.6090961694717407}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text_fake = '''Story:\n",
    "\n",
    "Once upon a time, there was a cat named Whiskers and a dog named Spot. They lived in a cozy house with their owner, Mr. Brown. Whiskers and Spot were very different from each other. Whiskers was independent and aloof, while Spot was loyal and friendly.\n",
    "\n",
    "One day, Mr. Brown decided to take them on a trip to the park. He put them in a basket and drove them to the bus stop. Whiskers was nervous and hid under the basket, while Spot wagged his tail and barked happily.\n",
    "\n",
    "When they arrived at the park, Mr. Brown let them out of the basket. Whiskers cautiously explored the grass and trees, while Spot ran around and played with other dogs. Mr. Brown threw a ball for Spot to fetch, and Spot brought it back to him with a big smile.'''\n",
    "pipeline_classif_phi2(sample_text_fake)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_detector",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
