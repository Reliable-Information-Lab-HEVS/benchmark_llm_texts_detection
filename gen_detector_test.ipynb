{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import (AutoModelForCausalLM, AutoTokenizer, BertForSequenceClassification, BertTokenizer, BertModel,\n",
    " RobertaForSequenceClassification, RobertaTokenizer, RobertaModel, TrainingArguments, Trainer)\n",
    "\n",
    "\n",
    "from datasets import load_dataset, load_from_disk, Dataset, DatasetDict\n",
    "import evaluate\n",
    "import wandb\n",
    "import os\n",
    "\n",
    "os.environ[\"WANDB_PROJECT\"] = \"<my-amazing-project>\"  # name your W&B project\n",
    "os.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint\"  # log all model checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init Generator and Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GEN_PATH = \"microsoft/phi-2\"\n",
    "#GEN_PATH = \"openai-community/gpt2\"\n",
    "GEN_PATH = \"Qwen/Qwen1.5-0.5B-Chat\"\n",
    "#BERT_PATH = \"bert-base-uncased\"\n",
    "BERT_PATH = \"openai-community/roberta-base-openai-detector\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "class GPTGenerator(nn.Module):\n",
    "  def __init__(self, gpt_model, tokenizer):\n",
    "    super().__init__()\n",
    "\n",
    "    # gpt should already be trained\n",
    "    self.gpt = gpt_model\n",
    "    self.tokenizer = tokenizer\n",
    "\n",
    "  def forward(self, text, max_length=512, temperature=1, top_k=50, top_p=0.9, repetition_penalty=1):\n",
    "    # tokenize text using the tokenizer\n",
    "    input_ids = self.tokenizer.encode(text, return_tensors=\"pt\")\n",
    "\n",
    "    # generate text using the gpt model\n",
    "    output_ids = self.gpt.generate(input_ids, max_length=max_length, temperature=temperature, top_k=top_k, top_p=top_p)\n",
    "\n",
    "    # optional, remove input_ids from output_ids\n",
    "    #output_ids = [output_id[len(input_ids):] for input_id, output_id in zip(input_ids, output_ids)]\n",
    "\n",
    "    # decode the generated text\n",
    "    decoded_output = self.tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0]\n",
    "    return decoded_output\n",
    "\n",
    "class BertClassifier(nn.Module):\n",
    "  def __init__(self, bert_model, tokenizer, num_classes):\n",
    "    super().__init__()\n",
    "\n",
    "    self.tokenizer = tokenizer\n",
    "\n",
    "    # bert should already be trained\n",
    "    self.bert = bert_model\n",
    "\n",
    "    # set num_classes\n",
    "    self.num_classes = num_classes\n",
    "\n",
    "  def forward(self, text):\n",
    "\n",
    "    # tokenize text using the tokenizer\n",
    "    output = self.tokenizer(text, return_tensors=\"pt\")\n",
    "    input_ids = output[\"input_ids\"]\n",
    "    logits = self.bert(input_ids)[\"logits\"]\n",
    "\n",
    "    # apply sigmoid to get probabilities of each class\n",
    "    output = torch.sigmoid(logits)\n",
    "    return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def print_prime(n):\n",
      "   \"\"\"\n",
      "   Print all primes between 1 and n\n",
      "   \"\"\"   \n",
      "   if n == 0:\n",
      "       return \"0\"\n",
      "   \n",
      "   else:\n",
      "       for i in range(2, int(n ** 0.5) + 1):\n",
      "           if n % i == 0:\n",
      "               break \n",
      "       return \"Prime\"\n"
     ]
    }
   ],
   "source": [
    "torch.set_default_device(\"cuda\")\n",
    "\n",
    "gen_model = AutoModelForCausalLM.from_pretrained(GEN_PATH, torch_dtype=\"auto\").to(device)\n",
    "gen_tokenizer = AutoTokenizer.from_pretrained(GEN_PATH, trust_remote_code=True)\n",
    "generator = GPTGenerator(gen_model, gen_tokenizer)\n",
    "\n",
    "text_input = '''def print_prime(n):\n",
    "   \"\"\"\n",
    "   Print all primes between 1 and n\n",
    "   \"\"\"'''\n",
    "\n",
    "output = generator(text_input)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at openai-community/roberta-base-openai-detector were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "detector_model = RobertaForSequenceClassification.from_pretrained(BERT_PATH).to(device)\n",
    "bert_tokenizer = RobertaTokenizer.from_pretrained(BERT_PATH)\n",
    "detector = BertClassifier(detector_model, bert_tokenizer, 2)\n",
    "\n",
    "text = \"def print_prime(n):\\n   \\\"\\\"\\\"\\n   Print all primes between 1 and n\\n   \\\"\\\"\\\"\"\n",
    "\n",
    "logits = detector(text)\n",
    "fake = logits.argmax().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset of instructions and output with gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['instruction', 'context', 'response', 'category'],\n",
       "        num_rows: 15011\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_path = \"databricks/databricks-dolly-15k\"\n",
    "dataset = load_dataset(dataset_path)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'When did Virgin Australia start operating?',\n",
       " 'context': \"Virgin Australia, the trading name of Virgin Australia Airlines Pty Ltd, is an Australian-based airline. It is the largest airline by fleet size to use the Virgin brand. It commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route. It suddenly found itself as a major airline in Australia's domestic market after the collapse of Ansett Australia in September 2001. The airline has since grown to directly serve 32 cities in Australia, from hubs in Brisbane, Melbourne and Sydney.\",\n",
       " 'response': 'Virgin Australia commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route.',\n",
       " 'category': 'closed_qa'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.dataset_dict.DatasetDict"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.arrow_dataset.Dataset"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length of responses: 358.10419026047566\n"
     ]
    }
   ],
   "source": [
    "# compute mean length of the responses\n",
    "lengths_response = [len(x) for x in dataset[\"train\"][\"response\"]]\n",
    "print(\"Average length of responses:\", np.mean(lengths_response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length of instructions: 71.83938445140231\n"
     ]
    }
   ],
   "source": [
    "lengts_instruction = [len(x) for x in dataset[\"train\"][\"instruction\"]]\n",
    "print(\"Average length of instructions:\", np.mean(lengts_instruction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent of data discarded: 26.29%\n"
     ]
    }
   ],
   "source": [
    "# discard instructions that are more than max_nb_tokens_input tokens\n",
    "max_nb_tokens_input = 100\n",
    "\n",
    "# tokenize the instructions\n",
    "dataset = dataset.map(lambda x: {\"tokenized_instruction\": gen_tokenizer(x[\"instruction\"])})\n",
    "dataset = dataset.map(lambda x: {\"tokenized_context\": gen_tokenizer(x[\"context\"])})\n",
    "dataset_before_len = len(dataset[\"train\"])\n",
    "dataset = dataset.filter(lambda x: len(x[\"tokenized_instruction\"][\"input_ids\"]) + len(x[\"tokenized_context\"][\"input_ids\"]) <= max_nb_tokens_input)\n",
    "dataset_after_len = len(dataset[\"train\"])\n",
    "print(f\"Percent of data discarded: {100*(1 - dataset_after_len/dataset_before_len):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['instruction', 'context', 'response', 'category', 'tokenized_instruction', 'tokenized_context'],\n",
       "        num_rows: 11065\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'Which is a species of fish? Tope or Rope',\n",
       " 'context': '',\n",
       " 'response': 'Tope',\n",
       " 'category': 'classification',\n",
       " 'tokenized_instruction': {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "  'input_ids': [23085, 374, 264, 9419, 315, 7640, 30, 2014, 375, 476, 97896]},\n",
       " 'tokenized_context': {'attention_mask': [], 'input_ids': []}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test output with first instruction\n",
    "text = dataset[\"train\"][0]\n",
    "text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  Context:  \n",
      " Question: Which is a species of fish? Tope or Rope\n",
      "Generated answer:  Context:  \n",
      " Question: Which is a species of fish? Tope or Rope\n",
      "Real human answer:  Tope\n"
     ]
    }
   ],
   "source": [
    "text_instruction = f\"Context: {text[\"context\"]} \\n Question: {text[\"instruction\"]}\"\n",
    "output = generator(text_instruction)\n",
    "print(\"Question: \", text_instruction)\n",
    "#print()\n",
    "print(\"Generated answer: \", output)\n",
    "#print()\n",
    "print(\"Real human answer: \", text[\"response\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the capital of France?'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = generator(\"What is the capital of France?\")\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n",
    "]\n",
    "text = gen_tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system\n",
      "You are a helpful assistant.\n",
      "user\n",
      "What is the capital of France?\n",
      "assistant\n",
      "Paris\n"
     ]
    }
   ],
   "source": [
    "output = generator(text)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system\n",
      "You are a helpful assistant.\n",
      "user\n",
      "Context:  \n",
      " Question: Which is a species of fish? Tope or Rope\n",
      "assistant\n",
      "Tope is a species of fish, not rope. The correct answer is not rope, but tope. Tope is an Atlantic anglerfish species that can grow up to 2 feet (6 meters) in length and can weigh as much as 15 pounds (7 kilograms). They have thick, spiny scales on their bodies and are known for their large size and strong bite. Tope fish are commonly found in coastal waters such as the Atlantic Ocean and Indian Ocean.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": f\"{text_instruction}\"},\n",
    "]\n",
    "text = gen_tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "output = generator(text)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate fake dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see Open AI GPT-2 generated dataset: https://github.com/openai/gpt-2-output-dataset?tab=readme-ov-file\n",
    "and https://github.com/openai/gpt-2-output-dataset/tree/master/detector for their roberta detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fake_responses(generator, dataset):\n",
    "    \"\"\"\n",
    "    Traverse dataset and generate responses for each instruction\n",
    "    \"\"\"\n",
    "\n",
    "    fake_responses = []\n",
    "    for data in dataset:\n",
    "        # Create query in the format that the generator expects\n",
    "        text_instruction = f\"Context: {data['context']} \\n {data['instruction']}\"\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"{text_instruction}\"},\n",
    "        ]\n",
    "        text = gen_tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "\n",
    "        # Generate response\n",
    "        output = generator(text)\n",
    "        fake_responses.append(output)\n",
    "    return fake_responses\n",
    "\n",
    "def create_random_subset(dataset, n=10):\n",
    "    \"\"\"\n",
    "    Create a random subset of the dataset\n",
    "    \"\"\"\n",
    "    if n > len(dataset):\n",
    "        n = len(dataset)\n",
    "    indices = np.random.choice(len(dataset), n, replace=False)\n",
    "    subset = dataset.select(indices)\n",
    "    return subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a random subset of the dataset\n",
    "subset_size = 5\n",
    "train_subset = create_random_subset(dataset[\"train\"], n=subset_size)\n",
    "#test_subset = create_random_subset(dataset[\"test\"], n=10)\n",
    "#eval_subset = create_random_subset(dataset[\"validation\"], n=10)\n",
    "\n",
    "# generate fake responses for the subsets\n",
    "fake_responses_train = generate_fake_responses(generator, train_subset)\n",
    "#fake_responses_test = generate_fake_responses(generator, test_subset)\n",
    "#fake_responses_eval = generate_fake_responses(generator, eval_subset)\n",
    "\n",
    "#fake_dataset = Dataset.from_dict({\"train\": fake_responses_train, \"test\": fake_responses_test, \"validation\": fake_responses_eval})\n",
    "\n",
    "fake_responses_train = Dataset.from_dict({\"response\": fake_responses_train})\n",
    "fake_dataset = DatasetDict()\n",
    "fake_dataset[\"train\"] = fake_responses_train\n",
    "\n",
    "# save fake dataset\n",
    "fake_dataset.save_to_disk(\"fake_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['response'],\n",
       "        num_rows: 5\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load fake dataset\n",
    "fake_dataset = load_from_disk(\"fake_dataset\")\n",
    "fake_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"system\\nYou are a helpful assistant.\\nuser\\nContext:  \\n What is the month of Ramadan?\\nassistant\\nRamadan is one of the most important fasting months in Islam, which starts on June 28th and continues for almost two weeks (29 days). The month was named after the Prophet Muhammad's son, who was known as Ramadan. It is marked by religious rituals such as:: fasting during the day (a fasting day every night from dusk until morning) | giving up food to help build a strong body | performing physical labor to makeakah | washing hands frequently | going to market or market stalls | praying at dawn and evening; and lighting fire.\""
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_dataset[\"train\"][0][\"response\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 22\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m metric\u001b[38;5;241m.\u001b[39mcompute(predictions\u001b[38;5;241m=\u001b[39mpredictions, references\u001b[38;5;241m=\u001b[39mlabels)\n\u001b[1;32m      8\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m      9\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./results\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     10\u001b[0m     num_train_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m     report_to\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwandb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     19\u001b[0m )\n\u001b[1;32m     21\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m---> 22\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[43mmodel\u001b[49m,\n\u001b[1;32m     23\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m     24\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mhuman_fake_train_dataset,\n\u001b[1;32m     25\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39msmall_eval_dataset,\n\u001b[1;32m     26\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[1;32m     27\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_ratio=0.1,\n",
    "    weight_decay=0.01,\n",
    "    learning_rate=1e-3,\n",
    "    logging_steps=5,\n",
    "    logging_dir=\"./logs\",\n",
    "    report_to=\"wandb\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=human_fake_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DPO training BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./output\",\n",
    "    report_to=\"wandb\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_ratio=0.1,\n",
    "    weight_decay=0.01,\n",
    "    learning_rate=1e-3,\n",
    "    num_train_epochs=3,\n",
    ")\n",
    "\n",
    "dpo_trainer = DPOTrainer(\n",
    "    model,\n",
    "    model_ref=None,\n",
    "    args=training_args,\n",
    "    beta=0.1,\n",
    "    train_dataset=train_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "dpo_trainer.train()\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_detector",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
