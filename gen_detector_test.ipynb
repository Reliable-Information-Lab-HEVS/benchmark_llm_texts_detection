{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas\n",
    "from transformers import (AutoModelForCausalLM, AutoTokenizer, BertForSequenceClassification, BertTokenizer, BertModel,\n",
    " RobertaForSequenceClassification, RobertaTokenizer, RobertaModel, TrainingArguments, Trainer)\n",
    "\n",
    "\n",
    "from datasets import load_dataset, load_from_disk, Dataset, DatasetDict, concatenate_datasets\n",
    "import evaluate\n",
    "import wandb\n",
    "import os\n",
    "\n",
    "os.environ[\"WANDB_PROJECT\"] = \"<my-amazing-project>\"  # name your W&B project\n",
    "os.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint\"  # log all model checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init Generator and Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GEN_PATH = \"microsoft/phi-2\"\n",
    "#GEN_PATH = \"openai-community/gpt2\"\n",
    "GEN_PATH = \"Qwen/Qwen1.5-0.5B-Chat\"\n",
    "#BERT_PATH = \"bert-base-uncased\"\n",
    "BERT_PATH = \"openai-community/roberta-base-openai-detector\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "class GPTGenerator(nn.Module):\n",
    "  def __init__(self, gpt_model, tokenizer):\n",
    "    super().__init__()\n",
    "\n",
    "    # gpt should already be trained\n",
    "    self.gpt = gpt_model\n",
    "    self.tokenizer = tokenizer\n",
    "\n",
    "  def forward(self, text, max_length=512, temperature=1, top_k=50, top_p=0.9, repetition_penalty=1, skip_special_tokens=True):\n",
    "    # tokenize text using the tokenizer\n",
    "    input_ids = self.tokenizer.encode(text, return_tensors=\"pt\").to(device)\n",
    "    # generate text using the gpt model\n",
    "    output_ids = self.gpt.generate(input_ids, max_length=max_length, temperature=temperature, top_k=top_k, top_p=top_p, repetition_penalty=repetition_penalty)\n",
    "\n",
    "    # optional, remove input_ids from output_ids\n",
    "    #output_ids = [output_id[len(input_ids):] for input_id, output_id in zip(input_ids, output_ids)]\n",
    "\n",
    "    # decode the generated text\n",
    "    decoded_output = self.tokenizer.batch_decode(output_ids, skip_special_tokens=skip_special_tokens)[0]\n",
    "\n",
    "    #decoded_output = decoded_output.replace(text, \"\")\n",
    "    return decoded_output\n",
    "\n",
    "class BertClassifier(nn.Module):\n",
    "  def __init__(self, bert_model, tokenizer, num_classes):\n",
    "    super().__init__()\n",
    "\n",
    "    self.tokenizer = tokenizer\n",
    "\n",
    "    # bert should already be trained\n",
    "    self.bert = bert_model\n",
    "\n",
    "    # set num_classes\n",
    "    self.num_classes = num_classes\n",
    "\n",
    "  def forward(self, text):\n",
    "\n",
    "    # tokenize text using the tokenizer\n",
    "    output = self.tokenizer(text, return_tensors=\"pt\")\n",
    "    input_ids = output[\"input_ids\"].to(device)\n",
    "    logits = self.bert(input_ids)[\"logits\"]\n",
    "\n",
    "    # apply sigmoid to get probabilities of each class\n",
    "    output = torch.sigmoid(logits)\n",
    "    return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this causes an issue with Trainer\n",
    "#torch.set_default_device(\"cuda\")\n",
    "\n",
    "gen_model = AutoModelForCausalLM.from_pretrained(GEN_PATH, torch_dtype=\"auto\").to(device)\n",
    "gen_tokenizer = AutoTokenizer.from_pretrained(GEN_PATH, trust_remote_code=True)\n",
    "generator = GPTGenerator(gen_model, gen_tokenizer)\n",
    "\n",
    "text_input = '''def print_prime(n):\n",
    "   \"\"\"\n",
    "   Print all primes between 1 and n\n",
    "   \"\"\"'''\n",
    "\n",
    "output = generator(text_input)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at openai-community/roberta-base-openai-detector were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "detector_model = RobertaForSequenceClassification.from_pretrained(BERT_PATH).to(device)\n",
    "bert_tokenizer = RobertaTokenizer.from_pretrained(BERT_PATH)\n",
    "detector = BertClassifier(detector_model, bert_tokenizer, 2)\n",
    "\n",
    "text = \"def print_prime(n):\\n   \\\"\\\"\\\"\\n   Print all primes between 1 and n\\n   \\\"\\\"\\\"\"\n",
    "\n",
    "logits = detector(text)\n",
    "fake = logits.argmax().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset of instructions and output with gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['instruction', 'context', 'response', 'category'],\n",
       "        num_rows: 15011\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_path = \"databricks/databricks-dolly-15k\"\n",
    "dataset = load_dataset(dataset_path)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'When did Virgin Australia start operating?',\n",
       " 'context': \"Virgin Australia, the trading name of Virgin Australia Airlines Pty Ltd, is an Australian-based airline. It is the largest airline by fleet size to use the Virgin brand. It commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route. It suddenly found itself as a major airline in Australia's domestic market after the collapse of Ansett Australia in September 2001. The airline has since grown to directly serve 32 cities in Australia, from hubs in Brisbane, Melbourne and Sydney.\",\n",
       " 'response': 'Virgin Australia commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route.',\n",
       " 'category': 'closed_qa'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.dataset_dict.DatasetDict"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.arrow_dataset.Dataset"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length of responses: 358.10419026047566\n"
     ]
    }
   ],
   "source": [
    "# compute mean length of the responses\n",
    "lengths_response = [len(x) for x in dataset[\"train\"][\"response\"]]\n",
    "print(\"Average length of responses:\", np.mean(lengths_response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length of instructions: 71.83938445140231\n"
     ]
    }
   ],
   "source": [
    "lengts_instruction = [len(x) for x in dataset[\"train\"][\"instruction\"]]\n",
    "print(\"Average length of instructions:\", np.mean(lengts_instruction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent of data discarded: 26.29%\n"
     ]
    }
   ],
   "source": [
    "# discard instructions that are more than max_nb_tokens_input tokens\n",
    "max_nb_tokens_input = 100\n",
    "\n",
    "# tokenize the instructions\n",
    "dataset = dataset.map(lambda x: {\"tokenized_instruction\": gen_tokenizer(x[\"instruction\"])})\n",
    "dataset = dataset.map(lambda x: {\"tokenized_context\": gen_tokenizer(x[\"context\"])})\n",
    "dataset_before_len = len(dataset[\"train\"])\n",
    "dataset = dataset.filter(lambda x: len(x[\"tokenized_instruction\"][\"input_ids\"]) + len(x[\"tokenized_context\"][\"input_ids\"]) <= max_nb_tokens_input)\n",
    "dataset_after_len = len(dataset[\"train\"])\n",
    "print(f\"Percent of data discarded: {100*(1 - dataset_after_len/dataset_before_len):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['instruction', 'context', 'response', 'category', 'tokenized_instruction', 'tokenized_context'],\n",
       "        num_rows: 11065\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'Which is a species of fish? Tope or Rope',\n",
       " 'context': '',\n",
       " 'response': 'Tope',\n",
       " 'category': 'classification',\n",
       " 'tokenized_instruction': {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "  'input_ids': [23085, 374, 264, 9419, 315, 7640, 30, 2014, 375, 476, 97896]},\n",
       " 'tokenized_context': {'attention_mask': [], 'input_ids': []}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test output with first instruction\n",
    "text = dataset[\"train\"][0]\n",
    "text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  Context:  \n",
      " Question: Which is a species of fish? Tope or Rope\n",
      "Generated answer:  Context:  \n",
      " Question: Which is a species of fish? Tope or Rope\n",
      "Real human answer:  Tope\n"
     ]
    }
   ],
   "source": [
    "text_instruction = f\"Context: {text[\"context\"]} \\n Question: {text[\"instruction\"]}\"\n",
    "output = generator(text_instruction)\n",
    "print(\"Question: \", text_instruction)\n",
    "#print()\n",
    "print(\"Generated answer: \", output)\n",
    "#print()\n",
    "print(\"Real human answer: \", text[\"response\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the capital of France?'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = generator(\"What is the capital of France?\")\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n",
    "]\n",
    "text = gen_tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system\n",
      "You are a helpful assistant.\n",
      "user\n",
      "What is the capital of France?\n",
      "assistant\n",
      "Paris\n"
     ]
    }
   ],
   "source": [
    "output = generator(text)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system\n",
      "You are a helpful assistant.\n",
      "user\n",
      "Context:  \n",
      " Question: Which is a species of fish? Tope or Rope\n",
      "assistant\n",
      "Tope is a species of fish commonly referred to as the \"teal fish\" or \"teal fish.\" It is a popular aquarium fish species due to its smooth scales and sleek appearance, and it is also known for its strong immune system. rope is a type of fishing line used to catch fish in both freshwater and saltwater environments.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": f\"{text_instruction}\"},\n",
    "]\n",
    "text = gen_tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "output = generator(text)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate fake dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see Open AI GPT-2 generated dataset: https://github.com/openai/gpt-2-output-dataset?tab=readme-ov-file\n",
    "and https://github.com/openai/gpt-2-output-dataset/tree/master/detector for their roberta detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fake_responses(generator, dataset):\n",
    "    \"\"\"\n",
    "    Traverse dataset and generate responses for each instruction\n",
    "    \"\"\"\n",
    "\n",
    "    fake_responses = []\n",
    "    for data in dataset:\n",
    "        # Create query in the format that the generator expects\n",
    "        text_instruction = f\"Context: {data['context']} \\n {data['instruction']}\"\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"{text_instruction}\"},\n",
    "        ]\n",
    "        text = gen_tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "\n",
    "        # Generate response\n",
    "        output = generator(text, skip_special_tokens=False)\n",
    "        \n",
    "        fake_responses.append(output)\n",
    "    return fake_responses\n",
    "\n",
    "def create_random_subset(dataset, n=10):\n",
    "    \"\"\"\n",
    "    Create a random subset of the dataset\n",
    "    \"\"\"\n",
    "    if n > len(dataset):\n",
    "        n = len(dataset)\n",
    "    indices = np.random.choice(len(dataset), n, replace=False)\n",
    "    subset = dataset.select(indices)\n",
    "    return subset\n",
    "\n",
    "def filter_instruction(sample):\n",
    "    \"\"\"\n",
    "    Note: only works if special tokens are not removed\n",
    "    \"\"\"\n",
    "\n",
    "    text_instruction = f\"Context: {sample['context']} \\n {sample['instruction']}\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"{text_instruction}\"},\n",
    "    ]\n",
    "    text_template = gen_tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    generated_response = sample[\"generated_response\"]\n",
    "\n",
    "    response_without_instruction = generated_response.replace(text_template, \"\")\n",
    "    return {\"generated_response\": response_without_instruction}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4579de413734980b959e210f4cb6337",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# take a random subset of the dataset\n",
    "subset_size = 5\n",
    "train_subset = create_random_subset(dataset[\"train\"], n=subset_size)\n",
    "#test_subset = create_random_subset(dataset[\"test\"], n=10)\n",
    "#eval_subset = create_random_subset(dataset[\"validation\"], n=10)\n",
    "\n",
    "# generate fake responses for the subsets\n",
    "fake_responses_train = generate_fake_responses(generator, train_subset)\n",
    "#fake_responses_test = generate_fake_responses(generator, test_subset)\n",
    "#fake_responses_eval = generate_fake_responses(generator, eval_subset)\n",
    "\n",
    "#fake_dataset = Dataset.from_dict({\"train\": fake_responses_train, \"test\": fake_responses_test, \"validation\": fake_responses_eval})\n",
    "\n",
    "fake_responses_train = Dataset.from_dict({\"generated_response\": fake_responses_train, \"instruction\": train_subset[\"instruction\"],\n",
    "    \"context\": train_subset[\"context\"], \"true_response\": train_subset[\"response\"], \"category\": train_subset[\"category\"]})\n",
    "\n",
    "fake_dataset = DatasetDict()\n",
    "fake_dataset[\"train\"] = fake_responses_train\n",
    "\n",
    "# save fake dataset\n",
    "fake_dataset.save_to_disk(\"fake_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['generated_response', 'instruction', 'context', 'true_response', 'category'],\n",
       "        num_rows: 5\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_tokenizer = AutoTokenizer.from_pretrained(GEN_PATH, trust_remote_code=True)\n",
    "\n",
    "# load fake dataset\n",
    "fake_dataset = load_from_disk(\"fake_dataset\")\n",
    "fake_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why do humans like dogs?\n"
     ]
    }
   ],
   "source": [
    "print(fake_dataset[\"train\"][0][\"instruction\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Humans like dogs because they make us feel good. When a human pets a dog, oxytocin levels rise in both species, which make both feel emotionally closer. These feelings of closeness make us want to pet them more – and the dog wanting to be petted more – increasing feelings of closeness and connectedness while engaging in this biological and emotional feedback loop.\n"
     ]
    }
   ],
   "source": [
    "print(fake_dataset[\"train\"][0][\"true_response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Context:  \n",
      " Why do humans like dogs?<|im_end|>\n",
      "<|im_start|>assistant\n",
      " Humans enjoy dogs because they are loyal, affectionate, and typically intelligent pets. Dogs have been around for thousands of years and have a long history of being dogs in various forms, from small, shih-tzu to large, golden retrievers. Dogs can also be taught different behaviors and personalities through training.\n",
      "\n",
      "One of the reasons why humans like dogs is that dogs are social animals. Dogs require social interaction and are able to form strong bonds with their owners. They also make excellent companions, helping humans stay focused and relaxed while they spend time with their owners.\n",
      "\n",
      "Another reason why humans like dogs is that they are fun to play with. Dogs come in a variety of sizes, breeds, and colors, making them perfect for all types of dog owners. Dogs are also known for their affection and创造性 play.\n",
      "\n",
      "Finally, dogs are highly intelligent animals that can be trained to perform various tasks, such as hunting, searching, and serving as therapy dogs. They can also teach humans different things, such as empathy, patience, and problem-solving skills.\n",
      "\n",
      "Overall, dogs are beloved by humans for their loyal, affectionate, social, and intelligent qualities, making them a popular choice for many people.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "print(fake_dataset[\"train\"][0][\"generated_response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_dataset = fake_dataset.map(filter_instruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Humans enjoy dogs because they are loyal, affectionate, and typically intelligent pets. Dogs have been around for thousands of years and have a long history of being dogs in various forms, from small, shih-tzu to large, golden retrievers. Dogs can also be taught different behaviors and personalities through training.\n",
      "\n",
      "One of the reasons why humans like dogs is that dogs are social animals. Dogs require social interaction and are able to form strong bonds with their owners. They also make excellent companions, helping humans stay focused and relaxed while they spend time with their owners.\n",
      "\n",
      "Another reason why humans like dogs is that they are fun to play with. Dogs come in a variety of sizes, breeds, and colors, making them perfect for all types of dog owners. Dogs are also known for their affection and创造性 play.\n",
      "\n",
      "Finally, dogs are highly intelligent animals that can be trained to perform various tasks, such as hunting, searching, and serving as therapy dogs. They can also teach humans different things, such as empathy, patience, and problem-solving skills.\n",
      "\n",
      "Overall, dogs are beloved by humans for their loyal, affectionate, social, and intelligent qualities, making them a popular choice for many people.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "print(fake_dataset[\"train\"][0][\"generated_response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c33314500844d2c86b5db21728b1c24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/15016 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "true_dataset = dataset.select_columns([\"response\"])\n",
    "true_dataset = true_dataset.rename_column(\"response\", \"text\")\n",
    "\n",
    "# select random samples from true_dataset to match fake_dataset size\n",
    "true_dataset = true_dataset.shuffle(seed=42)\n",
    "true_dataset = true_dataset.select(range(len(fake_dataset[\"train\"])))\n",
    "\n",
    "# create label = 0 for true responses and label = 1 for fake responses\n",
    "true_dataset = true_dataset.map(lambda x: {\"label\": 0})\n",
    "fake_dataset = fake_dataset.map(lambda x: {\"label\": 1})\n",
    "\n",
    "# remove true_response from fake_dataset\n",
    "fake_dataset = fake_dataset.remove_columns([\"true_response\"])\n",
    "\n",
    "# rename generated_response to response\n",
    "fake_dataset = fake_dataset.rename_column(\"generated_response\", \"text\")\n",
    "fake_dataset = fake_dataset.select_columns([\"text\", \"label\"])\n",
    "\n",
    "# merge fake and true datasets\n",
    "merged = concatenate_datasets([true_dataset[\"train\"], fake_dataset[\"train\"]])\n",
    "merged_dataset = DatasetDict()\n",
    "merged_dataset[\"train\"] = merged\n",
    "\n",
    "# shuffle the dataset\n",
    "merged_dataset = merged_dataset.shuffle(seed=42)\n",
    "\n",
    "# save merged dataset\n",
    "merged_dataset.save_to_disk(\"merged_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load merged dataset\n",
    "human_fake_train_dataset = load_from_disk(\"merged_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 15011\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 5\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 15016\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the merged dataset with the detector tokenizer\n",
    "def tokenize_text(x, tokenizer):\n",
    "    return tokenizer(x[\"text\"], truncation=True, padding=\"max_length\")\n",
    "\n",
    "human_fake_train_dataset = human_fake_train_dataset.map(lambda x: tokenize_text(x, bert_tokenizer), batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    warmup_ratio=0.1,\n",
    "    weight_decay=0.01,\n",
    "    learning_rate=1e-3,\n",
    "    logging_steps=5,\n",
    "    logging_dir=\"./logs\",\n",
    "    report_to=\"wandb\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=detector_model,\n",
    "    args=training_args,\n",
    "    train_dataset=human_fake_train_dataset[\"train\"],\n",
    "    #eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhdasilva\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/marluxiaboss/llm_text_detector/new_repo/text_llm_detector/wandb/run-20240305_173642-hi3x9oxj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hdasilva/%3Cmy-amazing-project%3E/runs/hi3x9oxj' target=\"_blank\">bright-pine-6</a></strong> to <a href='https://wandb.ai/hdasilva/%3Cmy-amazing-project%3E' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hdasilva/%3Cmy-amazing-project%3E' target=\"_blank\">https://wandb.ai/hdasilva/%3Cmy-amazing-project%3E</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hdasilva/%3Cmy-amazing-project%3E/runs/hi3x9oxj' target=\"_blank\">https://wandb.ai/hdasilva/%3Cmy-amazing-project%3E/runs/hi3x9oxj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='99' max='45048' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   99/45048 00:33 < 4:21:30, 2.86 it/s, Epoch 0.01/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.803000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.367800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.834400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.779000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>2.246600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.046300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.158100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.919600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.213500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.060200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.161700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.441500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.004200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.000400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m wandb\u001b[38;5;241m.\u001b[39mfinish()\n",
      "File \u001b[0;32m~/anaconda3/envs/llm_detector/lib/python3.12/site-packages/transformers/trainer.py:1624\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1622\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1623\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1624\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1625\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1627\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1629\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/llm_detector/lib/python3.12/site-packages/transformers/trainer.py:1966\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1960\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m   1961\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1963\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1964\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1965\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m-> 1966\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1967\u001b[0m ):\n\u001b[1;32m   1968\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1969\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   1970\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DPO training BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./output\",\n",
    "    report_to=\"wandb\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_ratio=0.1,\n",
    "    weight_decay=0.01,\n",
    "    learning_rate=1e-3,\n",
    "    num_train_epochs=3,\n",
    ")\n",
    "\n",
    "dpo_trainer = DPOTrainer(\n",
    "    model,\n",
    "    model_ref=None,\n",
    "    args=training_args,\n",
    "    beta=0.1,\n",
    "    train_dataset=train_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "dpo_trainer.train()\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_detector",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
