{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import (AutoModelForCausalLM, AutoTokenizer, BertForSequenceClassification, BertTokenizer, BertModel,\n",
    " RobertaForSequenceClassification, RobertaTokenizer, RobertaModel, TrainingArguments, Trainer)\n",
    "\n",
    "\n",
    "from datasets import load_dataset, load_from_disk, Dataset, DatasetDict\n",
    "import evaluate\n",
    "import wandb\n",
    "import os\n",
    "\n",
    "os.environ[\"WANDB_PROJECT\"] = \"<my-amazing-project>\"  # name your W&B project\n",
    "os.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint\"  # log all model checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init Generator and Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GEN_PATH = \"microsoft/phi-2\"\n",
    "#GEN_PATH = \"openai-community/gpt2\"\n",
    "GEN_PATH = \"Qwen/Qwen1.5-0.5B-Chat\"\n",
    "#BERT_PATH = \"bert-base-uncased\"\n",
    "BERT_PATH = \"openai-community/roberta-base-openai-detector\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "class GPTGenerator(nn.Module):\n",
    "  def __init__(self, gpt_model, tokenizer):\n",
    "    super().__init__()\n",
    "\n",
    "    # gpt should already be trained\n",
    "    self.gpt = gpt_model\n",
    "    self.tokenizer = tokenizer\n",
    "\n",
    "  def forward(self, text, max_length=512, temperature=1, top_k=50, top_p=0.9, repetition_penalty=1, skip_special_tokens=True):\n",
    "    # tokenize text using the tokenizer\n",
    "    input_ids = self.tokenizer.encode(text, return_tensors=\"pt\")\n",
    "\n",
    "    # generate text using the gpt model\n",
    "    output_ids = self.gpt.generate(input_ids, max_length=max_length, temperature=temperature, top_k=top_k, top_p=top_p, repetition_penalty=repetition_penalty)\n",
    "\n",
    "    # optional, remove input_ids from output_ids\n",
    "    #output_ids = [output_id[len(input_ids):] for input_id, output_id in zip(input_ids, output_ids)]\n",
    "\n",
    "    # decode the generated text\n",
    "    decoded_output = self.tokenizer.batch_decode(output_ids, skip_special_tokens=skip_special_tokens)[0]\n",
    "\n",
    "    #decoded_output = decoded_output.replace(text, \"\")\n",
    "    return decoded_output\n",
    "\n",
    "class BertClassifier(nn.Module):\n",
    "  def __init__(self, bert_model, tokenizer, num_classes):\n",
    "    super().__init__()\n",
    "\n",
    "    self.tokenizer = tokenizer\n",
    "\n",
    "    # bert should already be trained\n",
    "    self.bert = bert_model\n",
    "\n",
    "    # set num_classes\n",
    "    self.num_classes = num_classes\n",
    "\n",
    "  def forward(self, text):\n",
    "\n",
    "    # tokenize text using the tokenizer\n",
    "    output = self.tokenizer(text, return_tensors=\"pt\")\n",
    "    input_ids = output[\"input_ids\"]\n",
    "    logits = self.bert(input_ids)[\"logits\"]\n",
    "\n",
    "    # apply sigmoid to get probabilities of each class\n",
    "    output = torch.sigmoid(logits)\n",
    "    return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def print_prime(n):\n",
      "   \"\"\"\n",
      "   Print all primes between 1 and n\n",
      "   \"\"\" \n",
      "   # Loop through all numbers from 1 to n and print them\n",
      "   for i in range(2, n + 1):\n",
      "       # Check if the number is prime\n",
      "       if i * i == n:\n",
      "           # If it is, print the number\n",
      "           print(i)\n",
      "   # Print a newline at the end\n",
      "   print(\"\\n\")\n"
     ]
    }
   ],
   "source": [
    "torch.set_default_device(\"cuda\")\n",
    "\n",
    "gen_model = AutoModelForCausalLM.from_pretrained(GEN_PATH, torch_dtype=\"auto\").to(device)\n",
    "gen_tokenizer = AutoTokenizer.from_pretrained(GEN_PATH, trust_remote_code=True)\n",
    "generator = GPTGenerator(gen_model, gen_tokenizer)\n",
    "\n",
    "text_input = '''def print_prime(n):\n",
    "   \"\"\"\n",
    "   Print all primes between 1 and n\n",
    "   \"\"\"'''\n",
    "\n",
    "output = generator(text_input)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at openai-community/roberta-base-openai-detector were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "detector_model = RobertaForSequenceClassification.from_pretrained(BERT_PATH).to(device)\n",
    "bert_tokenizer = RobertaTokenizer.from_pretrained(BERT_PATH)\n",
    "detector = BertClassifier(detector_model, bert_tokenizer, 2)\n",
    "\n",
    "text = \"def print_prime(n):\\n   \\\"\\\"\\\"\\n   Print all primes between 1 and n\\n   \\\"\\\"\\\"\"\n",
    "\n",
    "logits = detector(text)\n",
    "fake = logits.argmax().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset of instructions and output with gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['instruction', 'context', 'response', 'category'],\n",
       "        num_rows: 15011\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_path = \"databricks/databricks-dolly-15k\"\n",
    "dataset = load_dataset(dataset_path)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'When did Virgin Australia start operating?',\n",
       " 'context': \"Virgin Australia, the trading name of Virgin Australia Airlines Pty Ltd, is an Australian-based airline. It is the largest airline by fleet size to use the Virgin brand. It commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route. It suddenly found itself as a major airline in Australia's domestic market after the collapse of Ansett Australia in September 2001. The airline has since grown to directly serve 32 cities in Australia, from hubs in Brisbane, Melbourne and Sydney.\",\n",
       " 'response': 'Virgin Australia commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route.',\n",
       " 'category': 'closed_qa'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.dataset_dict.DatasetDict"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.arrow_dataset.Dataset"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length of responses: 358.10419026047566\n"
     ]
    }
   ],
   "source": [
    "# compute mean length of the responses\n",
    "lengths_response = [len(x) for x in dataset[\"train\"][\"response\"]]\n",
    "print(\"Average length of responses:\", np.mean(lengths_response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length of instructions: 71.83938445140231\n"
     ]
    }
   ],
   "source": [
    "lengts_instruction = [len(x) for x in dataset[\"train\"][\"instruction\"]]\n",
    "print(\"Average length of instructions:\", np.mean(lengts_instruction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent of data discarded: 26.29%\n"
     ]
    }
   ],
   "source": [
    "# discard instructions that are more than max_nb_tokens_input tokens\n",
    "max_nb_tokens_input = 100\n",
    "\n",
    "# tokenize the instructions\n",
    "dataset = dataset.map(lambda x: {\"tokenized_instruction\": gen_tokenizer(x[\"instruction\"])})\n",
    "dataset = dataset.map(lambda x: {\"tokenized_context\": gen_tokenizer(x[\"context\"])})\n",
    "dataset_before_len = len(dataset[\"train\"])\n",
    "dataset = dataset.filter(lambda x: len(x[\"tokenized_instruction\"][\"input_ids\"]) + len(x[\"tokenized_context\"][\"input_ids\"]) <= max_nb_tokens_input)\n",
    "dataset_after_len = len(dataset[\"train\"])\n",
    "print(f\"Percent of data discarded: {100*(1 - dataset_after_len/dataset_before_len):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['instruction', 'context', 'response', 'category', 'tokenized_instruction', 'tokenized_context'],\n",
       "        num_rows: 11065\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'Which is a species of fish? Tope or Rope',\n",
       " 'context': '',\n",
       " 'response': 'Tope',\n",
       " 'category': 'classification',\n",
       " 'tokenized_instruction': {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "  'input_ids': [23085, 374, 264, 9419, 315, 7640, 30, 2014, 375, 476, 97896]},\n",
       " 'tokenized_context': {'attention_mask': [], 'input_ids': []}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test output with first instruction\n",
    "text = dataset[\"train\"][0]\n",
    "text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  Context:  \n",
      " Question: Which is a species of fish? Tope or Rope\n",
      "Generated answer:  Context:  \n",
      " Question: Which is a species of fish? Tope or Rope\n",
      "Real human answer:  Tope\n"
     ]
    }
   ],
   "source": [
    "text_instruction = f\"Context: {text[\"context\"]} \\n Question: {text[\"instruction\"]}\"\n",
    "output = generator(text_instruction)\n",
    "print(\"Question: \", text_instruction)\n",
    "#print()\n",
    "print(\"Generated answer: \", output)\n",
    "#print()\n",
    "print(\"Real human answer: \", text[\"response\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the capital of France?'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = generator(\"What is the capital of France?\")\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n",
    "]\n",
    "text = gen_tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system\n",
      "You are a helpful assistant.\n",
      "user\n",
      "What is the capital of France?\n",
      "assistant\n",
      "Paris\n"
     ]
    }
   ],
   "source": [
    "output = generator(text)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system\n",
      "You are a helpful assistant.\n",
      "user\n",
      "Context:  \n",
      " Question: Which is a species of fish? Tope or Rope\n",
      "assistant\n",
      "Tope.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": f\"{text_instruction}\"},\n",
    "]\n",
    "text = gen_tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "output = generator(text)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate fake dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see Open AI GPT-2 generated dataset: https://github.com/openai/gpt-2-output-dataset?tab=readme-ov-file\n",
    "and https://github.com/openai/gpt-2-output-dataset/tree/master/detector for their roberta detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fake_responses(generator, dataset):\n",
    "    \"\"\"\n",
    "    Traverse dataset and generate responses for each instruction\n",
    "    \"\"\"\n",
    "\n",
    "    fake_responses = []\n",
    "    for data in dataset:\n",
    "        # Create query in the format that the generator expects\n",
    "        text_instruction = f\"Context: {data['context']} \\n {data['instruction']}\"\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"{text_instruction}\"},\n",
    "        ]\n",
    "        text = gen_tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "\n",
    "        # Generate response\n",
    "        output = generator(text, skip_special_tokens=False)\n",
    "        \n",
    "        fake_responses.append(output)\n",
    "    return fake_responses\n",
    "\n",
    "def create_random_subset(dataset, n=10):\n",
    "    \"\"\"\n",
    "    Create a random subset of the dataset\n",
    "    \"\"\"\n",
    "    if n > len(dataset):\n",
    "        n = len(dataset)\n",
    "    indices = np.random.choice(len(dataset), n, replace=False)\n",
    "    subset = dataset.select(indices)\n",
    "    return subset\n",
    "\n",
    "def filter_instruction(sample):\n",
    "    \"\"\"\n",
    "    Note: only works if special tokens are not removed\n",
    "    \"\"\"\n",
    "\n",
    "    text_instruction = f\"Context: {sample['context']} \\n {sample['instruction']}\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"{text_instruction}\"},\n",
    "    ]\n",
    "    text_template = gen_tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    generated_response = sample[\"generated_response\"]\n",
    "\n",
    "    response_without_instruction = generated_response.replace(text_template, \"\")\n",
    "    return {\"generated_response\": response_without_instruction}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35dd31483ee545c1a691918f5430fa18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# take a random subset of the dataset\n",
    "subset_size = 5\n",
    "train_subset = create_random_subset(dataset[\"train\"], n=subset_size)\n",
    "#test_subset = create_random_subset(dataset[\"test\"], n=10)\n",
    "#eval_subset = create_random_subset(dataset[\"validation\"], n=10)\n",
    "\n",
    "# generate fake responses for the subsets\n",
    "fake_responses_train = generate_fake_responses(generator, train_subset)\n",
    "#fake_responses_test = generate_fake_responses(generator, test_subset)\n",
    "#fake_responses_eval = generate_fake_responses(generator, eval_subset)\n",
    "\n",
    "#fake_dataset = Dataset.from_dict({\"train\": fake_responses_train, \"test\": fake_responses_test, \"validation\": fake_responses_eval})\n",
    "\n",
    "fake_responses_train = Dataset.from_dict({\"generated_response\": fake_responses_train, \"instruction\": train_subset[\"instruction\"],\n",
    "    \"context\": train_subset[\"context\"], \"true_response\": train_subset[\"response\"], \"category\": train_subset[\"category\"]})\n",
    "\n",
    "fake_dataset = DatasetDict()\n",
    "fake_dataset[\"train\"] = fake_responses_train\n",
    "\n",
    "# save fake dataset\n",
    "fake_dataset.save_to_disk(\"fake_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['generated_response', 'instruction', 'context', 'true_response', 'category'],\n",
       "        num_rows: 5\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load fake dataset\n",
    "fake_dataset = load_from_disk(\"fake_dataset\")\n",
    "fake_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Historically, what are the largest animals on earth? Please include dinosaurs\n"
     ]
    }
   ],
   "source": [
    "print(fake_dataset[\"train\"][0][\"instruction\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The largest animals to roam the earth were the dinosaurs. Of these, the Sauropods were the largest family of dinosaurs. Sauropods were herbivorous. The Diplodocus was the longest dinosaur found with a complete skeleton with a length of 26 metres or 85 feet.\n",
      "\n",
      "Larger dinosaurs did exist but only individual bones have been found.\n"
     ]
    }
   ],
   "source": [
    "print(fake_dataset[\"train\"][0][\"true_response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Context:  \n",
      " Historically, what are the largest animals on earth? Please include dinosaurs<|im_end|>\n",
      "<|im_start|>assistant\n",
      "The largest animals on Earth today are the human beings, which are approximately 5 feet (1 meter) tall and weigh around 2000 pounds (1150 kilograms). The rest of the animals on Earth are classified as small, medium, or small in size. Here are some of the largest animals on Earth by weight and classification:\n",
      "\n",
      "1. Elephas maximus: This is the largest animal on Earth, standing about 6 feet (1.8 meters) tall and weighing around 2,500 pounds (1,300 kilograms).\n",
      "\n",
      "2. Giraffe: This is one of the largest animals on Earth, standing about 7 feet (2 meters) tall and weighing around 3,000 pounds (1,700 kilograms).\n",
      "\n",
      "3. African elephant: This is the largest land animal on Earth, standing about 7 feet (2 meters) tall and weighing around 4,000 pounds (2,100 kilograms).\n",
      "\n",
      "4. Elephant (Loxodonta africana): This is the largest land animal on Earth, standing about 8 feet (2 meters) tall and weighing around 12,000 pounds (7,400 kilograms).\n",
      "\n",
      "5. Porcupine: This is a small, ground-dwelling mammal that is typically around the size of a small cat. It stands around 3 feet (1 meter) tall and weighs around 350 pounds (175 kilograms).\n",
      "\n",
      "6. Giraffe (Loxodonta africana): This is a small, ground-dwelling mammal that is typically around the size of a small cat. It stands around 3 feet (1 meter) tall and weighs around 300 pounds (175 kilograms).\n",
      "\n",
      "7. Giraffe (Loxodonta africana): This is a small, ground-dwelling mammal that is typically around the size of a small cat. It stands around 4 feet (1 meter) tall and weighs around 350 pounds (175 kilograms).\n",
      "\n",
      "In summary, while the human beings are the largest animals on Earth by weight and classification, there are still many other small animals on Earth that are also incredibly large and fascinating.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "print(fake_dataset[\"train\"][0][\"generated_response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "425fc87fe977440caf626da90076e66c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fake_dataset = fake_dataset.map(filter_instruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The largest animals on Earth today are the human beings, which are approximately 5 feet (1 meter) tall and weigh around 2000 pounds (1150 kilograms). The rest of the animals on Earth are classified as small, medium, or small in size. Here are some of the largest animals on Earth by weight and classification:\n",
      "\n",
      "1. Elephas maximus: This is the largest animal on Earth, standing about 6 feet (1.8 meters) tall and weighing around 2,500 pounds (1,300 kilograms).\n",
      "\n",
      "2. Giraffe: This is one of the largest animals on Earth, standing about 7 feet (2 meters) tall and weighing around 3,000 pounds (1,700 kilograms).\n",
      "\n",
      "3. African elephant: This is the largest land animal on Earth, standing about 7 feet (2 meters) tall and weighing around 4,000 pounds (2,100 kilograms).\n",
      "\n",
      "4. Elephant (Loxodonta africana): This is the largest land animal on Earth, standing about 8 feet (2 meters) tall and weighing around 12,000 pounds (7,400 kilograms).\n",
      "\n",
      "5. Porcupine: This is a small, ground-dwelling mammal that is typically around the size of a small cat. It stands around 3 feet (1 meter) tall and weighs around 350 pounds (175 kilograms).\n",
      "\n",
      "6. Giraffe (Loxodonta africana): This is a small, ground-dwelling mammal that is typically around the size of a small cat. It stands around 3 feet (1 meter) tall and weighs around 300 pounds (175 kilograms).\n",
      "\n",
      "7. Giraffe (Loxodonta africana): This is a small, ground-dwelling mammal that is typically around the size of a small cat. It stands around 4 feet (1 meter) tall and weighs around 350 pounds (175 kilograms).\n",
      "\n",
      "In summary, while the human beings are the largest animals on Earth by weight and classification, there are still many other small animals on Earth that are also incredibly large and fascinating.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "print(fake_dataset[\"train\"][0][\"generated_response\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 22\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m metric\u001b[38;5;241m.\u001b[39mcompute(predictions\u001b[38;5;241m=\u001b[39mpredictions, references\u001b[38;5;241m=\u001b[39mlabels)\n\u001b[1;32m      8\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m      9\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./results\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     10\u001b[0m     num_train_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m     report_to\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwandb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     19\u001b[0m )\n\u001b[1;32m     21\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m---> 22\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[43mmodel\u001b[49m,\n\u001b[1;32m     23\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m     24\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mhuman_fake_train_dataset,\n\u001b[1;32m     25\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39msmall_eval_dataset,\n\u001b[1;32m     26\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[1;32m     27\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_ratio=0.1,\n",
    "    weight_decay=0.01,\n",
    "    learning_rate=1e-3,\n",
    "    logging_steps=5,\n",
    "    logging_dir=\"./logs\",\n",
    "    report_to=\"wandb\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=human_fake_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DPO training BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./output\",\n",
    "    report_to=\"wandb\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_ratio=0.1,\n",
    "    weight_decay=0.01,\n",
    "    learning_rate=1e-3,\n",
    "    num_train_epochs=3,\n",
    ")\n",
    "\n",
    "dpo_trainer = DPOTrainer(\n",
    "    model,\n",
    "    model_ref=None,\n",
    "    args=training_args,\n",
    "    beta=0.1,\n",
    "    train_dataset=train_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "dpo_trainer.train()\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_detector",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
