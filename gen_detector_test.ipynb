{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BertForSequenceClassification, BertTokenizer, BertModel\n",
    "\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init Generator and Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GEN_PATH = \"microsoft/phi-2\"\n",
    "GEN_PATH = \"openai-community/gpt2\"\n",
    "BERT_PATH = \"bert-base-uncased\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "class GPTGenerator(nn.Module):\n",
    "  def __init__(self, gpt_model, tokenizer):\n",
    "    super().__init__()\n",
    "\n",
    "    # gpt should already be trained\n",
    "    self.gpt = gpt_model\n",
    "    self.tokenizer = tokenizer\n",
    "\n",
    "  def forward(self, text, max_length=200, temperature=1, top_k=50, top_p=0.9, repetition_penalty=1):\n",
    "    # tokenize text using the tokenizer\n",
    "    input_ids = self.tokenizer.encode(text, return_tensors=\"pt\")\n",
    "\n",
    "    # generate text using the gpt model\n",
    "    output = self.gpt.generate(input_ids, max_length=max_length, temperature=temperature, top_k=top_k, top_p=top_p)\n",
    "\n",
    "    # decode the generated text\n",
    "    decoded_output = self.tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return decoded_output\n",
    "\n",
    "class BertClassifier(nn.Module):\n",
    "  def __init__(self, bert_model, tokenizer, num_classes):\n",
    "    super().__init__()\n",
    "\n",
    "    self.tokenizer = tokenizer\n",
    "\n",
    "    # bert should already be trained\n",
    "    self.bert = bert_model\n",
    "\n",
    "    # set num_classes\n",
    "    self.num_classes = num_classes\n",
    "\n",
    "  def forward(self, text):\n",
    "\n",
    "    # tokenize text using the tokenizer\n",
    "    output = tokenizer(text, return_tensors=\"pt\")\n",
    "    input_ids = output[\"input_ids\"]\n",
    "    logits = self.bert(input_ids)[\"logits\"]\n",
    "\n",
    "    # apply sigmoid to get probabilities of each class\n",
    "    output = torch.sigmoid(logits)\n",
    "    return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def print_prime(n):\n",
      "   \"\"\"\n",
      "   Print all primes between 1 and n\n",
      "   \"\"\"    print_prime(n):    \"\"\"   print_prime(n):   \"\"\"   print_prime(n):   \"\"\"   print_prime(n):   \"\"\"   print_prime(n):   \"\"\"   print_prime(n):   \"\"\"   print_prime(n):   \"\"\"   print_prime(n):   \"\"\"   print_prime(n):   \"\"\"   print_prime(n):   \"\"\"   print_prime(n):   \"\"\"   print_prime(n):   \"\"\"   print_prime(n):   \"\"\"   print_prime(n):   \"\"\"   print_prime(n):   \"\"\"   print_prime(n\n"
     ]
    }
   ],
   "source": [
    "torch.set_default_device(\"cuda\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(GEN_PATH, torch_dtype=\"auto\", trust_remote_code=True).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(GEN_PATH, trust_remote_code=True)\n",
    "generator = GPTGenerator(model, tokenizer)\n",
    "\n",
    "text_input = '''def print_prime(n):\n",
    "   \"\"\"\n",
    "   Print all primes between 1 and n\n",
    "   \"\"\"'''\n",
    "\n",
    "output = generator(text_input)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\").to(device)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "detector = BertClassifier(model, tokenizer, 2)\n",
    "\n",
    "text = \"def print_prime(n):\\n   \\\"\\\"\\\"\\n   Print all primes between 1 and n\\n   \\\"\\\"\\\"\"\n",
    "\n",
    "logits = detector(text)\n",
    "fake = logits.argmax().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset of instructions and output with gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 8.20k/8.20k [00:00<00:00, 6.39MB/s]\n",
      "Downloading data: 100%|██████████| 13.1M/13.1M [00:01<00:00, 7.62MB/s]\n",
      "Generating train split: 15011 examples [00:00, 291808.94 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['instruction', 'context', 'response', 'category'],\n",
       "        num_rows: 15011\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_path = \"databricks/databricks-dolly-15k\"\n",
    "dataset = load_dataset(dataset_path)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'When did Virgin Australia start operating?',\n",
       " 'context': \"Virgin Australia, the trading name of Virgin Australia Airlines Pty Ltd, is an Australian-based airline. It is the largest airline by fleet size to use the Virgin brand. It commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route. It suddenly found itself as a major airline in Australia's domestic market after the collapse of Ansett Australia in September 2001. The airline has since grown to directly serve 32 cities in Australia, from hubs in Brisbane, Melbourne and Sydney.\",\n",
       " 'response': 'Virgin Australia commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route.',\n",
       " 'category': 'closed_qa'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test output with first instruction\n",
    "text = dataset[\"train\"][0]\n",
    "text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marluxiaboss/anaconda3/envs/llm_detector/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  Virgin Australia, the trading name of Virgin Australia Airlines Pty Ltd, is an Australian-based airline. It is the largest airline by fleet size to use the Virgin brand. It commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route. It suddenly found itself as a major airline in Australia's domestic market after the collapse of Ansett Australia in September 2001. The airline has since grown to directly serve 32 cities in Australia, from hubs in Brisbane, Melbourne and Sydney. \n",
      " When did Virgin Australia start operating?\n",
      "\n",
      "Generated answer:  Virgin Australia, the trading name of Virgin Australia Airlines Pty Ltd, is an Australian-based airline. It is the largest airline by fleet size to use the Virgin brand. It commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route. It suddenly found itself as a major airline in Australia's domestic market after the collapse of Ansett Australia in September 2001. The airline has since grown to directly serve 32 cities in Australia, from hubs in Brisbane, Melbourne and Sydney. \n",
      " When did Virgin Australia start operating? Virgin Australia was founded in 1887 by William and Mary, a family of Australian seamen. The family was the first to operate a commercial airline. The family's first flight was on the first of July 1887, when the family was on a mission to the Pacific Ocean. The family's first flight was on the first of August 1887, when the family was on a mission to the Pacific Ocean. The family's first flight was on the first\n",
      "\n",
      "Real human answer:  Virgin Australia commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route.\n"
     ]
    }
   ],
   "source": [
    "text_instruction = f\"{text[\"context\"]} \\n {text[\"instruction\"]}\"\n",
    "output = generator(text_instruction)\n",
    "print(\"Question: \", text_instruction)\n",
    "print()\n",
    "print(\"Generated answer: \", output)\n",
    "print()\n",
    "print(\"Real human answer: \", text[\"response\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_detector",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
