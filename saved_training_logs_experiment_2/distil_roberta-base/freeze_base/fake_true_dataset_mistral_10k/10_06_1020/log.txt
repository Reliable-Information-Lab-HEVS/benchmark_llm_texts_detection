log_loss_steps: 256
eval_steps: 512
check_degradation: 0
----------------Epoch 1/1----------------
Epoch 1/1, Loss after 192 samples: 0.6960
Epoch 1/1, Loss after 448 samples: 0.6859
Mean accuracy: 0.7353, std: 0.0101, lower bound: 0.7150, upper bound: 0.7556 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 448 samples: 0.7353 with eval loss: 0.6813
Best model with eval loss 0.6812700975325799 and eval accuracy 0.7352941176470589 with 448 samples seen is saved
Epoch 1/1, Loss after 704 samples: 0.6737
Epoch 1/1, Loss after 960 samples: 0.6572
Mean accuracy: 0.7321, std: 0.0098, lower bound: 0.7125, upper bound: 0.7510 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 960 samples: 0.7323 with eval loss: 0.6406
Best model with eval loss 0.6405548876331698 and eval accuracy 0.7322515212981744 with 960 samples seen is saved
Epoch 1/1, Loss after 1216 samples: 0.6351
Epoch 1/1, Loss after 1472 samples: 0.6134
Mean accuracy: 0.7644, std: 0.0096, lower bound: 0.7465, upper bound: 0.7835 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 1472 samples: 0.7647 with eval loss: 0.5904
Best model with eval loss 0.5903828951620287 and eval accuracy 0.7647058823529411 with 1472 samples seen is saved
Epoch 1/1, Loss after 1728 samples: 0.5829
Epoch 1/1, Loss after 1984 samples: 0.5509
Mean accuracy: 0.6893, std: 0.0107, lower bound: 0.6678, upper bound: 0.7099 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 1984 samples: 0.6891 with eval loss: 0.5607
Best model with eval loss 0.5607113953559629 and eval accuracy 0.6891480730223124 with 1984 samples seen is saved
Epoch 1/1, Loss after 2240 samples: 0.5383
Epoch 1/1, Loss after 2496 samples: 0.5019
Mean accuracy: 0.7388, std: 0.0097, lower bound: 0.7206, upper bound: 0.7586 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 2496 samples: 0.7394 with eval loss: 0.5172
Best model with eval loss 0.5172342754179432 and eval accuracy 0.7393509127789046 with 2496 samples seen is saved
Epoch 1/1, Loss after 2752 samples: 0.4815
Epoch 1/1, Loss after 3008 samples: 0.5109
Mean accuracy: 0.6965, std: 0.0102, lower bound: 0.6760, upper bound: 0.7165 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 3008 samples: 0.6968 with eval loss: 0.5245
Epoch 1/1, Loss after 3264 samples: 0.4789
Epoch 1/1, Loss after 3520 samples: 0.4830
Mean accuracy: 0.7520, std: 0.0097, lower bound: 0.7343, upper bound: 0.7713 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 3520 samples: 0.7520 with eval loss: 0.4755
Best model with eval loss 0.4754571232103532 and eval accuracy 0.7520283975659229 with 3520 samples seen is saved
Epoch 1/1, Loss after 3776 samples: 0.4757
Epoch 1/1, Loss after 4032 samples: 0.4742
Mean accuracy: 0.7990, std: 0.0091, lower bound: 0.7809, upper bound: 0.8164 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 4032 samples: 0.7992 with eval loss: 0.4425
Best model with eval loss 0.4425095223611401 and eval accuracy 0.7991886409736308 with 4032 samples seen is saved
Epoch 1/1, Loss after 4288 samples: 0.5260
Epoch 1/1, Loss after 4544 samples: 0.4565
Mean accuracy: 0.8035, std: 0.0091, lower bound: 0.7850, upper bound: 0.8220 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 4544 samples: 0.8032 with eval loss: 0.4355
Best model with eval loss 0.4354906332108282 and eval accuracy 0.8032454361054767 with 4544 samples seen is saved
Epoch 1/1, Loss after 4800 samples: 0.4509
Epoch 1/1, Loss after 5056 samples: 0.4697
Mean accuracy: 0.7926, std: 0.0093, lower bound: 0.7743, upper bound: 0.8114 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 5056 samples: 0.7926 with eval loss: 0.4398
Epoch 1/1, Loss after 5312 samples: 0.4247
Epoch 1/1, Loss after 5568 samples: 0.4643
Mean accuracy: 0.7890, std: 0.0095, lower bound: 0.7708, upper bound: 0.8073 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 5568 samples: 0.7890 with eval loss: 0.4402
Epoch 1/1, Loss after 5824 samples: 0.4764
Epoch 1/1, Loss after 6080 samples: 0.5079
Mean accuracy: 0.6695, std: 0.0107, lower bound: 0.6501, upper bound: 0.6907 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 6080 samples: 0.6699 with eval loss: 0.5631
Epoch 1/1, Loss after 6336 samples: 0.4820
Epoch 1/1, Loss after 6592 samples: 0.4689
Mean accuracy: 0.7151, std: 0.0101, lower bound: 0.6952, upper bound: 0.7343 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 6592 samples: 0.7145 with eval loss: 0.5065
Epoch 1/1, Loss after 6848 samples: 0.4719
Epoch 1/1, Loss after 7104 samples: 0.4540
Mean accuracy: 0.8054, std: 0.0090, lower bound: 0.7875, upper bound: 0.8235 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 7104 samples: 0.8053 with eval loss: 0.4268
Best model with eval loss 0.4267738663381146 and eval accuracy 0.8052738336713996 with 7104 samples seen is saved
Epoch 1/1, Loss after 7360 samples: 0.4526
Epoch 1/1, Loss after 7616 samples: 0.4354
Mean accuracy: 0.8161, std: 0.0085, lower bound: 0.7997, upper bound: 0.8327 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 7616 samples: 0.8159 with eval loss: 0.4154
Best model with eval loss 0.41541616282155436 and eval accuracy 0.815922920892495 with 7616 samples seen is saved
Epoch 1/1, Loss after 7872 samples: 0.4374
Epoch 1/1, Loss after 8128 samples: 0.3999
Mean accuracy: 0.7373, std: 0.0099, lower bound: 0.7170, upper bound: 0.7556 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 8128 samples: 0.7378 with eval loss: 0.4775
Epoch 1/1, Loss after 8384 samples: 0.4727
Epoch 1/1, Loss after 8640 samples: 0.4506
Mean accuracy: 0.7350, std: 0.0098, lower bound: 0.7160, upper bound: 0.7546 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 8640 samples: 0.7353 with eval loss: 0.4830
Epoch 1/1, Loss after 8896 samples: 0.3943
Epoch 1/1, Loss after 9152 samples: 0.4222
Mean accuracy: 0.8190, std: 0.0087, lower bound: 0.8027, upper bound: 0.8357 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 9152 samples: 0.8190 with eval loss: 0.4119
Best model with eval loss 0.4118850827217102 and eval accuracy 0.8189655172413793 with 9152 samples seen is saved
Epoch 1/1, Loss after 9408 samples: 0.4434
Epoch 1/1, Loss after 9664 samples: 0.4381
Mean accuracy: 0.7734, std: 0.0095, lower bound: 0.7546, upper bound: 0.7921 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 9664 samples: 0.7733 with eval loss: 0.4343
Epoch 1/1, Loss after 9920 samples: 0.3950
Epoch 1/1, Loss after 10176 samples: 0.4888
Mean accuracy: 0.7697, std: 0.0095, lower bound: 0.7505, upper bound: 0.7886 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 10176 samples: 0.7703 with eval loss: 0.4373
Epoch 1/1, Loss after 10432 samples: 0.4415
Epoch 1/1, Loss after 10688 samples: 0.4046
Mean accuracy: 0.7698, std: 0.0098, lower bound: 0.7505, upper bound: 0.7896 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 10688 samples: 0.7703 with eval loss: 0.4366
Epoch 1/1, Loss after 10944 samples: 0.4607
Epoch 1/1, Loss after 11200 samples: 0.4279
Mean accuracy: 0.7849, std: 0.0091, lower bound: 0.7672, upper bound: 0.8027 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 11200 samples: 0.7845 with eval loss: 0.4258
Epoch 1/1, Loss after 11456 samples: 0.4448
Epoch 1/1, Loss after 11712 samples: 0.4681
Mean accuracy: 0.7713, std: 0.0097, lower bound: 0.7515, upper bound: 0.7906 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 11712 samples: 0.7718 with eval loss: 0.4384
Epoch 1/1, Loss after 11968 samples: 0.3731
Epoch 1/1, Loss after 12224 samples: 0.4594
Mean accuracy: 0.7891, std: 0.0093, lower bound: 0.7703, upper bound: 0.8073 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 12224 samples: 0.7890 with eval loss: 0.4210
Epoch 1/1, Loss after 12480 samples: 0.4049
Epoch 1/1, Loss after 12736 samples: 0.4297
Mean accuracy: 0.7744, std: 0.0099, lower bound: 0.7551, upper bound: 0.7936 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 12736 samples: 0.7754 with eval loss: 0.4349
Epoch 1/1, Loss after 12992 samples: 0.4430
Epoch 1/1, Loss after 13248 samples: 0.4344
Mean accuracy: 0.7814, std: 0.0094, lower bound: 0.7637, upper bound: 0.7997 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 13248 samples: 0.7814 with eval loss: 0.4262
Epoch 1/1, Loss after 13504 samples: 0.3863
Epoch 1/1, Loss after 13760 samples: 0.4330
Mean accuracy: 0.7790, std: 0.0093, lower bound: 0.7622, upper bound: 0.7972 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 13760 samples: 0.7789 with eval loss: 0.4302
Epoch 1/1, Loss after 14016 samples: 0.4141
Epoch 1/1, Loss after 14272 samples: 0.3806
Mean accuracy: 0.7817, std: 0.0092, lower bound: 0.7637, upper bound: 0.8002 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 14272 samples: 0.7814 with eval loss: 0.4264
Epoch 1/1, Loss after 14528 samples: 0.3641
Epoch 1/1, Loss after 14784 samples: 0.4165
Mean accuracy: 0.8034, std: 0.0093, lower bound: 0.7855, upper bound: 0.8225 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 14784 samples: 0.8038 with eval loss: 0.4114
Best model with eval loss 0.4114433632742974 and eval accuracy 0.8037525354969574 with 14784 samples seen is saved
Epoch 1/1, Loss after 15040 samples: 0.4607
Epoch 1/1, Loss after 15296 samples: 0.4964
Mean accuracy: 0.7880, std: 0.0090, lower bound: 0.7708, upper bound: 0.8068 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 15296 samples: 0.7880 with eval loss: 0.4202
Epoch 1/1, Loss after 15552 samples: 0.3994
Training signal is False, stopping training
Training finished
Best model: {'eval_acc': 0.8037525354969574, 'nb_samples': 14784, 'eval_loss': 0.4114433632742974}
Training loss logs: [{'samples': 192, 'loss': 0.6959552764892578}, {'samples': 448, 'loss': 0.6859130859375}, {'samples': 704, 'loss': 0.6737394332885742}, {'samples': 960, 'loss': 0.6572027206420898}, {'samples': 1216, 'loss': 0.6350560188293457}, {'samples': 1472, 'loss': 0.6133742332458496}, {'samples': 1728, 'loss': 0.5829439163208008}, {'samples': 1984, 'loss': 0.5508935451507568}, {'samples': 2240, 'loss': 0.5382674932479858}, {'samples': 2496, 'loss': 0.5019016861915588}, {'samples': 2752, 'loss': 0.4815259650349617}, {'samples': 3008, 'loss': 0.5108738243579865}, {'samples': 3264, 'loss': 0.47887881100177765}, {'samples': 3520, 'loss': 0.48300735652446747}, {'samples': 3776, 'loss': 0.4756856858730316}, {'samples': 4032, 'loss': 0.47416870296001434}, {'samples': 4288, 'loss': 0.526001513004303}, {'samples': 4544, 'loss': 0.4565451592206955}, {'samples': 4800, 'loss': 0.4508834183216095}, {'samples': 5056, 'loss': 0.46968407928943634}, {'samples': 5312, 'loss': 0.4247272238135338}, {'samples': 5568, 'loss': 0.4642913118004799}, {'samples': 5824, 'loss': 0.47640252113342285}, {'samples': 6080, 'loss': 0.5078548714518547}, {'samples': 6336, 'loss': 0.48202404379844666}, {'samples': 6592, 'loss': 0.4689439460635185}, {'samples': 6848, 'loss': 0.47193925827741623}, {'samples': 7104, 'loss': 0.4539680480957031}, {'samples': 7360, 'loss': 0.45260313898324966}, {'samples': 7616, 'loss': 0.4353986009955406}, {'samples': 7872, 'loss': 0.43742242455482483}, {'samples': 8128, 'loss': 0.3998665362596512}, {'samples': 8384, 'loss': 0.4727458730340004}, {'samples': 8640, 'loss': 0.45063353329896927}, {'samples': 8896, 'loss': 0.3942742198705673}, {'samples': 9152, 'loss': 0.42219630628824234}, {'samples': 9408, 'loss': 0.4433617815375328}, {'samples': 9664, 'loss': 0.4380873814225197}, {'samples': 9920, 'loss': 0.3950084447860718}, {'samples': 10176, 'loss': 0.48878245055675507}, {'samples': 10432, 'loss': 0.441529743373394}, {'samples': 10688, 'loss': 0.4046129137277603}, {'samples': 10944, 'loss': 0.4606660529971123}, {'samples': 11200, 'loss': 0.4279133155941963}, {'samples': 11456, 'loss': 0.4448279142379761}, {'samples': 11712, 'loss': 0.4680795967578888}, {'samples': 11968, 'loss': 0.3730771765112877}, {'samples': 12224, 'loss': 0.45942336320877075}, {'samples': 12480, 'loss': 0.4048898294568062}, {'samples': 12736, 'loss': 0.4296678304672241}, {'samples': 12992, 'loss': 0.4429994896054268}, {'samples': 13248, 'loss': 0.4343680664896965}, {'samples': 13504, 'loss': 0.38634809851646423}, {'samples': 13760, 'loss': 0.4329843074083328}, {'samples': 14016, 'loss': 0.4141280800104141}, {'samples': 14272, 'loss': 0.3806278482079506}, {'samples': 14528, 'loss': 0.36410041898489}, {'samples': 14784, 'loss': 0.416492335498333}, {'samples': 15040, 'loss': 0.46071503311395645}, {'samples': 15296, 'loss': 0.4963589161634445}, {'samples': 15552, 'loss': 0.3994305059313774}]
Evaluation accuracy logs: [{'samples': 448, 'accuracy': 0.7352641987829615, 'std': 0.01010906826487777, 'lower_bound': 0.7150101419878296, 'upper_bound': 0.755578093306288}, {'samples': 960, 'accuracy': 0.7320780933062881, 'std': 0.009829504467215339, 'lower_bound': 0.712474645030426, 'upper_bound': 0.7510268762677486}, {'samples': 1472, 'accuracy': 0.7643706896551724, 'std': 0.009617686135289577, 'lower_bound': 0.7464503042596349, 'upper_bound': 0.7834812373225153}, {'samples': 1984, 'accuracy': 0.6893286004056794, 'std': 0.010709445291029261, 'lower_bound': 0.6678498985801217, 'upper_bound': 0.7099391480730223}, {'samples': 2496, 'accuracy': 0.7388316430020284, 'std': 0.00966717426111941, 'lower_bound': 0.7205882352941176, 'upper_bound': 0.7586206896551724}, {'samples': 3008, 'accuracy': 0.6964609533468561, 'std': 0.010213109638921957, 'lower_bound': 0.6759508113590263, 'upper_bound': 0.7165314401622718}, {'samples': 3520, 'accuracy': 0.7520451318458418, 'std': 0.009739000734366551, 'lower_bound': 0.7342799188640974, 'upper_bound': 0.7712981744421906}, {'samples': 4032, 'accuracy': 0.7990055780933063, 'std': 0.009142802041173046, 'lower_bound': 0.7809203853955374, 'upper_bound': 0.8164300202839757}, {'samples': 4544, 'accuracy': 0.8035334685598378, 'std': 0.00913695978341057, 'lower_bound': 0.7849898580121704, 'upper_bound': 0.8220207910750508}, {'samples': 5056, 'accuracy': 0.7926120689655172, 'std': 0.009290083535739421, 'lower_bound': 0.7743407707910751, 'upper_bound': 0.8113590263691683}, {'samples': 5568, 'accuracy': 0.7889888438133873, 'std': 0.009471267402640869, 'lower_bound': 0.77079107505071, 'upper_bound': 0.8073022312373225}, {'samples': 6080, 'accuracy': 0.6694918864097362, 'std': 0.010693923721767437, 'lower_bound': 0.6500887423935091, 'upper_bound': 0.6906693711967545}, {'samples': 6592, 'accuracy': 0.7150694726166329, 'std': 0.010115600955470797, 'lower_bound': 0.6952332657200812, 'upper_bound': 0.7342799188640974}, {'samples': 7104, 'accuracy': 0.805357505070994, 'std': 0.008980210152012209, 'lower_bound': 0.787525354969574, 'upper_bound': 0.8235294117647058}, {'samples': 7616, 'accuracy': 0.8161485801217039, 'std': 0.008512701741666222, 'lower_bound': 0.7996957403651116, 'upper_bound': 0.832669878296146}, {'samples': 8128, 'accuracy': 0.7373372210953346, 'std': 0.009947451707411481, 'lower_bound': 0.7170385395537525, 'upper_bound': 0.755578093306288}, {'samples': 8640, 'accuracy': 0.7350456389452332, 'std': 0.009829960495413187, 'lower_bound': 0.716024340770791, 'upper_bound': 0.7545638945233266}, {'samples': 9152, 'accuracy': 0.8189939148073023, 'std': 0.008662244197078913, 'lower_bound': 0.8027256592292089, 'upper_bound': 0.8356997971602435}, {'samples': 9664, 'accuracy': 0.7734066937119676, 'std': 0.009492882134046964, 'lower_bound': 0.7545512170385396, 'upper_bound': 0.7920892494929006}, {'samples': 10176, 'accuracy': 0.7697251521298175, 'std': 0.009514644085917193, 'lower_bound': 0.7505070993914807, 'upper_bound': 0.7885522312373225}, {'samples': 10688, 'accuracy': 0.7697773833671401, 'std': 0.009759483160570272, 'lower_bound': 0.7504944219066937, 'upper_bound': 0.789553752535497}, {'samples': 11200, 'accuracy': 0.7849056795131846, 'std': 0.009096413904140066, 'lower_bound': 0.7672413793103449, 'upper_bound': 0.802738336713996}, {'samples': 11712, 'accuracy': 0.7713002028397566, 'std': 0.009663909320233447, 'lower_bound': 0.7515086206896552, 'upper_bound': 0.7905679513184585}, {'samples': 12224, 'accuracy': 0.7891095334685598, 'std': 0.009271055965653705, 'lower_bound': 0.7702839756592292, 'upper_bound': 0.8073022312373225}, {'samples': 12736, 'accuracy': 0.7743514198782961, 'std': 0.009850561449816624, 'lower_bound': 0.7550583164300202, 'upper_bound': 0.7936105476673428}, {'samples': 13248, 'accuracy': 0.7813504056795132, 'std': 0.009447898183593763, 'lower_bound': 0.7636916835699797, 'upper_bound': 0.7997084178498987}, {'samples': 13760, 'accuracy': 0.7789685598377282, 'std': 0.009265444491608554, 'lower_bound': 0.7621703853955375, 'upper_bound': 0.7971602434077079}, {'samples': 14272, 'accuracy': 0.7817013184584178, 'std': 0.009227283069801659, 'lower_bound': 0.7636916835699797, 'upper_bound': 0.8002281947261664}, {'samples': 14784, 'accuracy': 0.8034239350912779, 'std': 0.009293709700882578, 'lower_bound': 0.7854969574036511, 'upper_bound': 0.8225152129817445}, {'samples': 15296, 'accuracy': 0.7880324543610547, 'std': 0.00900517716490383, 'lower_bound': 0.7707783975659229, 'upper_bound': 0.8067951318458418}]
Evaluating the best model on the test set of dataset ./fake_true_datasets/fake_true_dataset_mistral_10k...
Test metrics:
accuracy: 0.7915324543610548
precision: 0.7379343049603753
recall: 0.9031918570036096
f1_score: 0.8121745925136221
fp_rate: 0.3198102914416323
tp_rate: 0.9031918570036096
std_accuracy: 0.008739433889075485
std_precision: 0.012299943177275474
std_recall: 0.009565570405986582
std_f1_score: 0.008885186193895812
std_fp_rate: 0.014269625333233789
std_tp_rate: 0.009565570405986582
TP: 889.251
TN: 671.651
FP: 315.79
FN: 95.308
roc_auc: 0.9138933918674835
fpr: [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.0010142  0.0010142
 0.0010142  0.0010142  0.0010142  0.0010142  0.0010142  0.0010142
 0.0010142  0.0010142  0.0010142  0.0010142  0.0010142  0.0010142
 0.0020284  0.0020284  0.0020284  0.0020284  0.0040568  0.0040568
 0.0040568  0.0040568  0.0040568  0.0040568  0.0040568  0.0040568
 0.0040568  0.0040568  0.0040568  0.0040568  0.0040568  0.0040568
 0.0040568  0.0040568  0.0040568  0.0040568  0.0040568  0.0040568
 0.0040568  0.0040568  0.0040568  0.0040568  0.0040568  0.0040568
 0.0040568  0.0040568  0.0040568  0.0040568  0.0040568  0.0040568
 0.00507099 0.00507099 0.00507099 0.00507099 0.00507099 0.00507099
 0.00608519 0.00608519 0.00608519 0.00608519 0.00608519 0.00608519
 0.00709939 0.00709939 0.00709939 0.00811359 0.00811359 0.00912779
 0.00912779 0.00912779 0.00912779 0.00912779 0.00912779 0.00912779
 0.00912779 0.01014199 0.01014199 0.01014199 0.01014199 0.01115619
 0.01115619 0.01115619 0.01115619 0.01217039 0.01217039 0.01217039
 0.01217039 0.01217039 0.01217039 0.01217039 0.01217039 0.01217039
 0.01318458 0.01318458 0.01419878 0.01419878 0.01419878 0.01419878
 0.01419878 0.01419878 0.01419878 0.01521298 0.01521298 0.01521298
 0.01521298 0.01521298 0.01622718 0.01622718 0.01724138 0.01724138
 0.01724138 0.01724138 0.01724138 0.01724138 0.01825558 0.01825558
 0.01825558 0.01825558 0.01926978 0.01926978 0.02028398 0.02028398
 0.02028398 0.02028398 0.02129817 0.02129817 0.02129817 0.02129817
 0.02231237 0.02231237 0.02332657 0.02535497 0.02535497 0.02535497
 0.02535497 0.02535497 0.02535497 0.02636917 0.02636917 0.02636917
 0.02636917 0.02636917 0.02738337 0.02738337 0.02839757 0.02839757
 0.02839757 0.02839757 0.02941176 0.02941176 0.03042596 0.03042596
 0.03042596 0.03042596 0.03144016 0.03144016 0.03245436 0.03245436
 0.03346856 0.03346856 0.03448276 0.03448276 0.03549696 0.03549696
 0.03651116 0.03651116 0.03752535 0.03752535 0.03853955 0.03853955
 0.03955375 0.03955375 0.03955375 0.03955375 0.04056795 0.04056795
 0.04259635 0.04259635 0.04462475 0.04462475 0.04563895 0.04563895
 0.04665314 0.04665314 0.04665314 0.04665314 0.04766734 0.04766734
 0.04969574 0.04969574 0.05172414 0.05172414 0.05273834 0.05375254
 0.05780933 0.05780933 0.05882353 0.05882353 0.06085193 0.06085193
 0.06186613 0.06186613 0.06592292 0.06592292 0.06693712 0.06693712
 0.06896552 0.06896552 0.06997972 0.07099391 0.07099391 0.07403651
 0.07403651 0.07606491 0.07606491 0.07707911 0.07707911 0.07809331
 0.07809331 0.0801217  0.0801217  0.0811359  0.0811359  0.0821501
 0.0821501  0.0831643  0.0831643  0.0841785  0.0841785  0.0851927
 0.0851927  0.0862069  0.0862069  0.08823529 0.08823529 0.08924949
 0.08924949 0.09127789 0.09127789 0.09229209 0.09229209 0.09229209
 0.09330629 0.09330629 0.09634888 0.09634888 0.09736308 0.09736308
 0.09736308 0.09837728 0.09939148 0.09939148 0.10040568 0.10040568
 0.10649087 0.10649087 0.10750507 0.10750507 0.10851927 0.10851927
 0.10953347 0.10953347 0.10953347 0.10953347 0.10953347 0.10953347
 0.11054767 0.11054767 0.11156187 0.11156187 0.11156187 0.11257606
 0.11359026 0.11359026 0.11460446 0.11460446 0.11764706 0.11764706
 0.11866126 0.11866126 0.11866126 0.11866126 0.11967546 0.11967546
 0.12068966 0.12068966 0.12170385 0.12170385 0.12474645 0.12474645
 0.12576065 0.12576065 0.12677485 0.12778905 0.12880325 0.12880325
 0.12981744 0.12981744 0.13083164 0.13184584 0.13184584 0.13286004
 0.13488844 0.13488844 0.13590264 0.13793103 0.13793103 0.14097363
 0.14097363 0.14198783 0.14198783 0.14401623 0.14604462 0.14604462
 0.14705882 0.14705882 0.14807302 0.14807302 0.15010142 0.15010142
 0.15212982 0.15212982 0.15314402 0.15314402 0.15922921 0.15922921
 0.16125761 0.16125761 0.1643002  0.1643002  0.17241379 0.17241379
 0.17342799 0.17545639 0.17647059 0.17849899 0.17849899 0.17951318
 0.17951318 0.18356998 0.18356998 0.18864097 0.18864097 0.18965517
 0.18965517 0.19269777 0.19269777 0.19472617 0.19472617 0.19776876
 0.19776876 0.20081136 0.20081136 0.20283976 0.20283976 0.20993915
 0.20993915 0.21196755 0.21196755 0.21298174 0.21298174 0.21501014
 0.21501014 0.21805274 0.21805274 0.21906694 0.21906694 0.22312373
 0.22312373 0.22413793 0.22413793 0.22718053 0.22718053 0.22920892
 0.22920892 0.23427992 0.23427992 0.24036511 0.24036511 0.24137931
 0.24137931 0.24239351 0.24239351 0.2494929  0.2494929  0.2535497
 0.2535497  0.26572008 0.26572008 0.26774848 0.26977688 0.26977688
 0.27484787 0.27484787 0.27789047 0.27789047 0.28701826 0.28701826
 0.28904665 0.28904665 0.29411765 0.29411765 0.29614604 0.29614604
 0.30223124 0.30223124 0.30324544 0.30425963 0.30831643 0.30831643
 0.31440162 0.31440162 0.31744422 0.31744422 0.32048682 0.32048682
 0.32150101 0.32150101 0.3296146  0.3296146  0.34077079 0.34077079
 0.34685598 0.34685598 0.34787018 0.34787018 0.34888438 0.34888438
 0.35091278 0.35091278 0.35192698 0.35192698 0.35294118 0.35395538
 0.35496957 0.35496957 0.35598377 0.35598377 0.36105477 0.36105477
 0.36612576 0.36612576 0.36713996 0.36713996 0.37221095 0.37221095
 0.37626775 0.37626775 0.37728195 0.37931034 0.38133874 0.38133874
 0.38235294 0.38235294 0.38843813 0.39046653 0.39046653 0.39249493
 0.39249493 0.39858012 0.39858012 0.40669371 0.40669371 0.40872211
 0.40973631 0.41176471 0.41176471 0.4148073  0.4158215  0.42089249
 0.42089249 0.43509128 0.43711968 0.44320487 0.44320487 0.44624746
 0.44827586 0.45030426 0.45131846 0.45131846 0.45537525 0.45740365
 0.46551724 0.46551724 0.46653144 0.47058824 0.47363083 0.47363083
 0.47870183 0.48073022 0.48174442 0.48377282 0.48478702 0.48681542
 0.48985801 0.48985801 0.49087221 0.49087221 0.49188641 0.49188641
 0.5010142  0.5010142  0.50507099 0.50507099 0.50709939 0.50912779
 0.51115619 0.51419878 0.51521298 0.52028398 0.52129817 0.52129817
 0.52434077 0.52434077 0.53042596 0.53042596 0.53144016 0.53144016
 0.53752535 0.53955375 0.53955375 0.54361055 0.54361055 0.55273834
 0.55273834 0.55882353 0.55882353 0.55983773 0.55983773 0.57099391
 0.57302231 0.57505071 0.57809331 0.57910751 0.5821501  0.5821501
 0.5851927  0.5851927  0.58823529 0.59026369 0.59026369 0.59736308
 0.59736308 0.59939148 0.59939148 0.61967546 0.61967546 0.62170385
 0.62373225 0.62576065 0.62778905 0.62880325 0.62880325 0.62981744
 0.63184584 0.63590264 0.63590264 0.63793103 0.63894523 0.65010142
 0.65010142 0.65415822 0.65517241 0.65720081 0.66024341 0.66227181
 0.663286   0.6673428  0.6693712  0.67241379 0.67241379 0.67545639
 0.67545639 0.68559838 0.68559838 0.68762677 0.68965517 0.69066937
 0.69269777 0.70993915 0.71196755 0.71805274 0.71805274 0.72616633
 0.72616633 0.72819473 0.73022312 0.73123732 0.73123732 0.73935091
 0.74036511 0.7474645  0.7494929  0.75456389 0.75862069 0.76470588
 0.76673428 0.76673428 0.76977688 0.76977688 0.77180527 0.77180527
 0.77281947 0.77281947 0.77586207 0.77586207 0.78803245 0.79208925
 0.80020284 0.80020284 0.80223124 0.80324544 0.80527383 0.80730223
 0.81034483 0.81034483 0.81135903 0.81237323 0.81338742 0.81541582
 0.82048682 0.82251521 0.8296146  0.831643   0.836714   0.83874239
 0.84787018 0.84989858 0.86815416 0.87018256 0.87119675 0.87423935
 0.87525355 0.87931034 0.88843813 0.89046653 0.89452333 0.89655172
 0.90973631 0.9158215  0.93610548 0.93813387 0.94320487 0.94320487
 0.94523327 0.94726166 0.95638945 0.95638945 0.97464503 0.97667343
 1.        ]
tpr: [0.         0.0010142  0.01926978 0.02129817 0.03042596 0.03245436
 0.03853955 0.04056795 0.04158215 0.04361055 0.04969574 0.05172414
 0.05679513 0.05882353 0.06288032 0.06693712 0.07200811 0.07403651
 0.0811359  0.0831643  0.0841785  0.08823529 0.09026369 0.09229209
 0.09432049 0.09634888 0.09736308 0.10141988 0.10446247 0.10649087
 0.10750507 0.10953347 0.11359026 0.11663286 0.11866126 0.12068966
 0.12271805 0.12373225 0.12576065 0.13286004 0.13488844 0.14908722
 0.15111562 0.15618661 0.15821501 0.16125761 0.16125761 0.17444219
 0.17647059 0.17748479 0.17951318 0.18458418 0.18661258 0.18864097
 0.19066937 0.19168357 0.19371197 0.19472617 0.19675456 0.20486815
 0.20486815 0.20791075 0.20993915 0.21196755 0.21196755 0.23326572
 0.23529412 0.23833671 0.24036511 0.24543611 0.2474645  0.2494929
 0.2525355  0.2535497  0.25659229 0.25963489 0.26166329 0.26572008
 0.26774848 0.27383367 0.27586207 0.27890467 0.28093306 0.28600406
 0.28803245 0.28904665 0.29107505 0.29614604 0.29817444 0.30324544
 0.30527383 0.30628803 0.30831643 0.31338742 0.31541582 0.31744422
 0.31845842 0.32048682 0.34279919 0.34482759 0.34787018 0.35091278
 0.35091278 0.35294118 0.35496957 0.36409736 0.36713996 0.36815416
 0.36916836 0.37829615 0.38032454 0.38235294 0.38742394 0.38945233
 0.39452333 0.39655172 0.39756592 0.40060852 0.40263692 0.40466531
 0.40567951 0.40567951 0.4137931  0.4168357  0.4178499  0.4178499
 0.4198783  0.42292089 0.42494929 0.42494929 0.43509128 0.43813387
 0.44016227 0.44320487 0.44523327 0.44827586 0.45233266 0.46044625
 0.46146045 0.46551724 0.46653144 0.46855984 0.47870183 0.48174442
 0.48681542 0.49087221 0.51926978 0.51926978 0.52129817 0.52332657
 0.52636917 0.52738337 0.52738337 0.54056795 0.54056795 0.54462475
 0.54665314 0.55273834 0.55476673 0.55679513 0.55679513 0.55780933
 0.55983773 0.56186613 0.56186613 0.56389452 0.56389452 0.56997972
 0.57200811 0.57910751 0.57910751 0.5831643  0.5851927  0.59127789
 0.59127789 0.59432049 0.59432049 0.59533469 0.59939148 0.60141988
 0.60344828 0.60547667 0.61054767 0.61054767 0.61359026 0.61561866
 0.62170385 0.62373225 0.62373225 0.62981744 0.62981744 0.63083164
 0.63286004 0.63793103 0.63793103 0.63894523 0.63894523 0.64300203
 0.64503043 0.65314402 0.65314402 0.65415822 0.65415822 0.65618661
 0.65618661 0.65922921 0.65922921 0.66024341 0.66024341 0.663286
 0.663286   0.6643002  0.6643002  0.6663286  0.6673428  0.6703854
 0.6703854  0.67139959 0.67444219 0.67545639 0.67545639 0.67647059
 0.67647059 0.67849899 0.67849899 0.67951318 0.67951318 0.68154158
 0.68154158 0.68965517 0.69168357 0.69269777 0.69371197 0.69472617
 0.69472617 0.69979716 0.69979716 0.70385396 0.70385396 0.70486815
 0.70486815 0.70689655 0.70791075 0.71298174 0.71298174 0.71501014
 0.71501014 0.71906694 0.71906694 0.72210953 0.72210953 0.72413793
 0.72413793 0.72616633 0.72718053 0.72718053 0.72819473 0.72819473
 0.73123732 0.73123732 0.73225152 0.73225152 0.73630832 0.73630832
 0.73732252 0.73732252 0.74137931 0.74137931 0.74239351 0.74239351
 0.74442191 0.74442191 0.7474645  0.7484787  0.7515213  0.7515213
 0.7525355  0.7525355  0.75557809 0.75557809 0.75659229 0.75760649
 0.75963489 0.75963489 0.76166329 0.76166329 0.76369168 0.76572008
 0.76572008 0.76977688 0.76977688 0.77079108 0.77079108 0.77281947
 0.77383367 0.77484787 0.77484787 0.77586207 0.77586207 0.77687627
 0.77687627 0.77890467 0.77890467 0.78296146 0.78296146 0.78397566
 0.78397566 0.78498986 0.78701826 0.78803245 0.79107505 0.79208925
 0.79208925 0.79310345 0.79310345 0.79513185 0.79614604 0.79716024
 0.79716024 0.80020284 0.80020284 0.80121704 0.80121704 0.80223124
 0.80223124 0.80324544 0.80527383 0.80730223 0.80730223 0.80933063
 0.80933063 0.81034483 0.81034483 0.81338742 0.81338742 0.81440162
 0.81440162 0.81541582 0.81541582 0.81643002 0.81643002 0.81947262
 0.81947262 0.82150101 0.82251521 0.82251521 0.82454361 0.82454361
 0.82454361 0.82555781 0.82555781 0.82555781 0.82657201 0.82657201
 0.8296146  0.8296146  0.8306288  0.8306288  0.8306288  0.8326572
 0.8326572  0.8336714  0.8336714  0.8356998  0.8356998  0.836714
 0.836714   0.83772819 0.83772819 0.84178499 0.84178499 0.84482759
 0.84482759 0.84584178 0.84584178 0.84685598 0.84685598 0.84787018
 0.84787018 0.84787018 0.84787018 0.84787018 0.84989858 0.84989858
 0.85192698 0.85192698 0.85294118 0.85294118 0.85395538 0.85395538
 0.85699797 0.85699797 0.85801217 0.85801217 0.85902637 0.85902637
 0.86105477 0.86105477 0.86206897 0.86206897 0.86308316 0.86308316
 0.86511156 0.86511156 0.86612576 0.86612576 0.86916836 0.86916836
 0.87221095 0.87221095 0.87322515 0.87322515 0.87423935 0.87423935
 0.87525355 0.87525355 0.87931034 0.87931034 0.88133874 0.88133874
 0.88336714 0.88336714 0.88539554 0.88539554 0.88640974 0.88640974
 0.88742394 0.88742394 0.88945233 0.88945233 0.89046653 0.89046653
 0.89148073 0.89148073 0.89249493 0.89249493 0.89249493 0.89350913
 0.89350913 0.89452333 0.89452333 0.89553753 0.89553753 0.89655172
 0.89655172 0.89756592 0.89756592 0.89858012 0.89858012 0.89959432
 0.89959432 0.90060852 0.90060852 0.90162272 0.90162272 0.90263692
 0.90263692 0.90365112 0.90365112 0.90466531 0.90466531 0.90567951
 0.90567951 0.90669371 0.90669371 0.90770791 0.90770791 0.90872211
 0.90872211 0.90973631 0.90973631 0.91075051 0.91075051 0.91176471
 0.91176471 0.9137931  0.9137931  0.9168357  0.9168357  0.9178499
 0.9178499  0.9198783  0.9198783  0.92190669 0.92190669 0.92292089
 0.92292089 0.92393509 0.92393509 0.92494929 0.92494929 0.92596349
 0.92596349 0.92697769 0.92697769 0.92697769 0.92697769 0.92799189
 0.92799189 0.92900609 0.92900609 0.92900609 0.93103448 0.93103448
 0.93204868 0.93204868 0.93306288 0.93306288 0.93407708 0.93407708
 0.93509128 0.93509128 0.93711968 0.93711968 0.93813387 0.93813387
 0.93914807 0.93914807 0.93914807 0.93914807 0.94016227 0.94016227
 0.94016227 0.94016227 0.94117647 0.94320487 0.94320487 0.94320487
 0.94320487 0.94421907 0.94421907 0.94421907 0.94421907 0.94523327
 0.94523327 0.94523327 0.94523327 0.94523327 0.94523327 0.94523327
 0.94523327 0.94624746 0.94624746 0.94726166 0.94726166 0.95030426
 0.95030426 0.95131846 0.95131846 0.95233266 0.95233266 0.95233266
 0.95233266 0.95233266 0.95334686 0.95334686 0.95436105 0.95537525
 0.95537525 0.95638945 0.95638945 0.95740365 0.95740365 0.95841785
 0.95841785 0.95943205 0.96044625 0.96044625 0.96146045 0.96146045
 0.96247465 0.96247465 0.96348884 0.96348884 0.96450304 0.96450304
 0.96551724 0.96551724 0.96551724 0.96653144 0.96653144 0.96754564
 0.96754564 0.96855984 0.96855984 0.96855984 0.96957404 0.96957404
 0.97058824 0.97058824 0.97160243 0.97160243 0.97261663 0.97261663
 0.97261663 0.97261663 0.97261663 0.97261663 0.97363083 0.97363083
 0.97464503 0.97464503 0.97565923 0.97565923 0.97667343 0.97667343
 0.97870183 0.97870183 0.98073022 0.98073022 0.98073022 0.98073022
 0.98073022 0.98073022 0.98073022 0.98073022 0.98174442 0.98174442
 0.98275862 0.98275862 0.98377282 0.98377282 0.98377282 0.98377282
 0.98377282 0.98377282 0.98377282 0.98377282 0.98478702 0.98478702
 0.98580122 0.98580122 0.98580122 0.98580122 0.98681542 0.98681542
 0.98782961 0.98782961 0.98782961 0.98782961 0.98782961 0.98782961
 0.98782961 0.98884381 0.98884381 0.99087221 0.99087221 0.99188641
 0.99188641 0.99290061 0.99290061 0.99391481 0.99391481 0.99391481
 0.99391481 0.99492901 0.99492901 0.9959432  0.9959432  0.9959432
 0.9959432  0.9969574  0.9969574  0.9979716  0.9979716  0.9979716
 0.9979716  0.9979716  0.9979716  0.9979716  0.9979716  0.9979716
 0.9979716  0.9979716  0.9979716  0.9979716  0.9979716  0.9979716
 0.9979716  0.9979716  0.9979716  0.9979716  0.9979716  0.9979716
 0.9979716  0.9979716  0.9979716  0.9979716  0.9979716  0.9989858
 0.9989858  0.9989858  0.9989858  1.         1.         1.
 1.        ]
thresholds: [            inf  4.09765625e+00  3.15039062e+00  3.13085938e+00
  2.90234375e+00  2.86718750e+00  2.80078125e+00  2.78710938e+00
  2.77539062e+00  2.77343750e+00  2.62304688e+00  2.62109375e+00
  2.57031250e+00  2.54882812e+00  2.49218750e+00  2.48437500e+00
  2.44921875e+00  2.44140625e+00  2.38281250e+00  2.37890625e+00
  2.37109375e+00  2.33789062e+00  2.31445312e+00  2.31250000e+00
  2.29296875e+00  2.28125000e+00  2.26953125e+00  2.25781250e+00
  2.23242188e+00  2.22851562e+00  2.22460938e+00  2.20703125e+00
  2.15820312e+00  2.15625000e+00  2.12304688e+00  2.11523438e+00
  2.10156250e+00  2.09765625e+00  2.09375000e+00  2.02148438e+00
  2.00585938e+00  1.92871094e+00  1.92285156e+00  1.88085938e+00
  1.87792969e+00  1.87109375e+00  1.85546875e+00  1.77343750e+00
  1.77050781e+00  1.76074219e+00  1.75976562e+00  1.73925781e+00
  1.73828125e+00  1.73242188e+00  1.72656250e+00  1.72167969e+00
  1.72070312e+00  1.71289062e+00  1.71093750e+00  1.67968750e+00
  1.67773438e+00  1.65820312e+00  1.65332031e+00  1.64062500e+00
  1.63671875e+00  1.53515625e+00  1.53417969e+00  1.53027344e+00
  1.52929688e+00  1.51074219e+00  1.50390625e+00  1.49804688e+00
  1.49707031e+00  1.48925781e+00  1.48828125e+00  1.47949219e+00
  1.47070312e+00  1.46093750e+00  1.45605469e+00  1.44042969e+00
  1.43847656e+00  1.43066406e+00  1.42578125e+00  1.41308594e+00
  1.40429688e+00  1.40234375e+00  1.40136719e+00  1.39160156e+00
  1.38964844e+00  1.37597656e+00  1.37304688e+00  1.36523438e+00
  1.36328125e+00  1.34082031e+00  1.33789062e+00  1.33496094e+00
  1.33398438e+00  1.33105469e+00  1.25195312e+00  1.24902344e+00
  1.24707031e+00  1.23828125e+00  1.23242188e+00  1.22949219e+00
  1.22851562e+00  1.20312500e+00  1.19726562e+00  1.19628906e+00
  1.19531250e+00  1.17773438e+00  1.17285156e+00  1.16992188e+00
  1.16015625e+00  1.15625000e+00  1.14160156e+00  1.13964844e+00
  1.13867188e+00  1.13574219e+00  1.12792969e+00  1.12695312e+00
  1.12500000e+00  1.12011719e+00  1.10156250e+00  1.09570312e+00
  1.09179688e+00  1.08984375e+00  1.08691406e+00  1.08203125e+00
  1.08007812e+00  1.07910156e+00  1.05468750e+00  1.05175781e+00
  1.04980469e+00  1.04492188e+00  1.04003906e+00  1.03515625e+00
  1.03320312e+00  1.01171875e+00  1.00976562e+00  9.97558594e-01
  9.87792969e-01  9.86328125e-01  9.62402344e-01  9.60937500e-01
  9.43847656e-01  9.35058594e-01  8.73046875e-01  8.71582031e-01
  8.66210938e-01  8.64257812e-01  8.63769531e-01  8.61816406e-01
  8.61328125e-01  8.43261719e-01  8.42773438e-01  8.32031250e-01
  8.31542969e-01  8.21289062e-01  8.17382812e-01  8.12011719e-01
  8.11523438e-01  8.10546875e-01  8.10058594e-01  8.06152344e-01
  8.02246094e-01  7.96386719e-01  7.90527344e-01  7.73925781e-01
  7.70507812e-01  7.56347656e-01  7.53417969e-01  7.39257812e-01
  7.37304688e-01  7.24121094e-01  7.21191406e-01  7.17285156e-01
  7.16796875e-01  7.15820312e-01  7.06542969e-01  7.05078125e-01
  7.02636719e-01  7.00683594e-01  6.93847656e-01  6.92871094e-01
  6.87988281e-01  6.87011719e-01  6.81640625e-01  6.78710938e-01
  6.68945312e-01  6.61621094e-01  6.61132812e-01  6.60644531e-01
  6.59667969e-01  6.49414062e-01  6.41113281e-01  6.38183594e-01
  6.36230469e-01  6.27929688e-01  6.26464844e-01  6.06933594e-01
  6.05957031e-01  6.04492188e-01  6.04003906e-01  6.01562500e-01
  6.00097656e-01  5.93261719e-01  5.92285156e-01  5.90332031e-01
  5.89843750e-01  5.87890625e-01  5.83007812e-01  5.82031250e-01
  5.81542969e-01  5.77636719e-01  5.76171875e-01  5.74707031e-01
  5.73730469e-01  5.72753906e-01  5.71289062e-01  5.70312500e-01
  5.69335938e-01  5.67871094e-01  5.62500000e-01  5.60546875e-01
  5.55664062e-01  5.55175781e-01  5.53222656e-01  5.48828125e-01
  5.47851562e-01  5.31738281e-01  5.27832031e-01  5.27343750e-01
  5.24414062e-01  5.20996094e-01  5.18554688e-01  5.10253906e-01
  5.05859375e-01  4.97314453e-01  4.96826172e-01  4.91943359e-01
  4.84375000e-01  4.79736328e-01  4.74853516e-01  4.63378906e-01
  4.58740234e-01  4.58007812e-01  4.54833984e-01  4.44091797e-01
  4.35058594e-01  4.31152344e-01  4.30175781e-01  4.29199219e-01
  4.23339844e-01  4.22607422e-01  4.22363281e-01  4.21630859e-01
  4.21142578e-01  4.18945312e-01  4.11865234e-01  4.05273438e-01
  4.04785156e-01  4.04052734e-01  3.97460938e-01  3.96484375e-01
  3.94775391e-01  3.94287109e-01  3.86718750e-01  3.86474609e-01
  3.85498047e-01  3.84521484e-01  3.81103516e-01  3.77929688e-01
  3.69873047e-01  3.67187500e-01  3.64013672e-01  3.62304688e-01
  3.61816406e-01  3.60839844e-01  3.54003906e-01  3.47167969e-01
  3.44970703e-01  3.43750000e-01  3.42529297e-01  3.41796875e-01
  3.39355469e-01  3.37646484e-01  3.36914062e-01  3.35937500e-01
  3.33251953e-01  3.27392578e-01  3.25195312e-01  3.24707031e-01
  3.23974609e-01  3.20556641e-01  3.20068359e-01  3.18359375e-01
  3.16894531e-01  3.16650391e-01  3.15673828e-01  3.15429688e-01
  3.09570312e-01  3.05908203e-01  3.05664062e-01  2.91748047e-01
  2.91259766e-01  2.88085938e-01  2.85644531e-01  2.84912109e-01
  2.82470703e-01  2.80273438e-01  2.79296875e-01  2.75634766e-01
  2.75146484e-01  2.74902344e-01  2.73925781e-01  2.71728516e-01
  2.70507812e-01  2.69775391e-01  2.69042969e-01  2.67822266e-01
  2.66357422e-01  2.64404297e-01  2.60498047e-01  2.59765625e-01
  2.58789062e-01  2.57324219e-01  2.57080078e-01  2.55371094e-01
  2.52441406e-01  2.48657227e-01  2.47802734e-01  2.47436523e-01
  2.44995117e-01  2.37915039e-01  2.36938477e-01  2.36206055e-01
  2.35229492e-01  2.33520508e-01  2.31079102e-01  2.29370117e-01
  2.28393555e-01  2.20947266e-01  2.20336914e-01  2.13745117e-01
  2.12768555e-01  2.12158203e-01  2.07763672e-01  2.04589844e-01
  2.04467773e-01  2.03857422e-01  2.00683594e-01  1.98852539e-01
  1.97021484e-01  1.94946289e-01  1.91284180e-01  1.91040039e-01
  1.89331055e-01  1.88354492e-01  1.86157227e-01  1.82861328e-01
  1.80786133e-01  1.80297852e-01  1.79931641e-01  1.78588867e-01
  1.76147461e-01  1.74072266e-01  1.71020508e-01  1.70898438e-01
  1.69433594e-01  1.64062500e-01  1.59301758e-01  1.54174805e-01
  1.53320312e-01  1.52465820e-01  1.51123047e-01  1.49291992e-01
  1.37939453e-01  1.36962891e-01  1.35375977e-01  1.34399414e-01
  1.34155273e-01  1.33422852e-01  1.32324219e-01  1.31713867e-01
  1.30737305e-01  1.23413086e-01  1.20971680e-01  1.11938477e-01
  1.10900879e-01  1.10534668e-01  1.08520508e-01  1.04125977e-01
  1.03271484e-01  1.02111816e-01  1.01623535e-01  1.00891113e-01
  9.96093750e-02  9.63745117e-02  9.52758789e-02  9.32617188e-02
  9.17358398e-02  8.54492188e-02  8.42895508e-02  8.35571289e-02
  8.25195312e-02  8.13598633e-02  7.86743164e-02  7.70263672e-02
  7.55004883e-02  7.15332031e-02  7.12280273e-02  7.07397461e-02
  6.96411133e-02  6.61621094e-02  6.59179688e-02  6.54296875e-02
  5.97534180e-02  5.43823242e-02  5.35583496e-02  5.13916016e-02
  4.61120605e-02  4.21447754e-02  4.06494141e-02  3.82995605e-02
  3.29589844e-02  3.14331055e-02  2.95257568e-02  2.94189453e-02
  2.83355713e-02  1.20010376e-02  9.20104980e-03  1.60408020e-03
 -9.05990601e-04 -9.29260254e-03 -9.63592529e-03 -1.11083984e-02
 -1.22909546e-02 -1.25274658e-02 -1.56860352e-02 -1.70440674e-02
 -2.06909180e-02 -2.08587646e-02 -3.05633545e-02 -3.09906006e-02
 -3.30200195e-02 -3.39050293e-02 -3.80859375e-02 -3.90014648e-02
 -3.95202637e-02 -4.17480469e-02 -5.05676270e-02 -5.27954102e-02
 -5.55419922e-02 -5.74035645e-02 -6.21948242e-02 -6.28662109e-02
 -7.20214844e-02 -7.31201172e-02 -7.80029297e-02 -8.07495117e-02
 -8.25195312e-02 -8.34350586e-02 -8.38623047e-02 -8.39233398e-02
 -8.87451172e-02 -8.94775391e-02 -1.02905273e-01 -1.03759766e-01
 -1.08947754e-01 -1.09130859e-01 -1.14379883e-01 -1.14501953e-01
 -1.15478516e-01 -1.15600586e-01 -1.19445801e-01 -1.27319336e-01
 -1.28662109e-01 -1.31469727e-01 -1.33056641e-01 -1.33178711e-01
 -1.34643555e-01 -1.38061523e-01 -1.39648438e-01 -1.41235352e-01
 -1.46728516e-01 -1.47705078e-01 -1.50512695e-01 -1.51123047e-01
 -1.51367188e-01 -1.51855469e-01 -1.57348633e-01 -1.57592773e-01
 -1.62475586e-01 -1.62963867e-01 -1.64672852e-01 -1.66381836e-01
 -1.68334961e-01 -1.68579102e-01 -1.68823242e-01 -1.69799805e-01
 -1.75415039e-01 -1.75537109e-01 -1.77001953e-01 -1.77368164e-01
 -1.78100586e-01 -1.88232422e-01 -1.91406250e-01 -1.99707031e-01
 -1.99829102e-01 -2.00927734e-01 -2.01049805e-01 -2.01293945e-01
 -2.03491211e-01 -2.09350586e-01 -2.11425781e-01 -2.18627930e-01
 -2.19726562e-01 -2.33154297e-01 -2.33276367e-01 -2.37792969e-01
 -2.38647461e-01 -2.42919922e-01 -2.43652344e-01 -2.45971680e-01
 -2.46459961e-01 -2.49023438e-01 -2.52441406e-01 -2.54150391e-01
 -2.63427734e-01 -2.65625000e-01 -2.70751953e-01 -2.74902344e-01
 -2.79785156e-01 -2.81982422e-01 -2.85644531e-01 -2.86621094e-01
 -2.90039062e-01 -2.90283203e-01 -2.90771484e-01 -2.91992188e-01
 -2.95410156e-01 -2.95898438e-01 -2.99072266e-01 -2.99560547e-01
 -3.00537109e-01 -3.03955078e-01 -3.18603516e-01 -3.22265625e-01
 -3.23730469e-01 -3.24462891e-01 -3.26904297e-01 -3.30566406e-01
 -3.31787109e-01 -3.34472656e-01 -3.35937500e-01 -3.44238281e-01
 -3.45947266e-01 -3.49853516e-01 -3.51806641e-01 -3.52783203e-01
 -3.59863281e-01 -3.61816406e-01 -3.62304688e-01 -3.64257812e-01
 -3.72314453e-01 -3.77685547e-01 -3.78662109e-01 -3.83300781e-01
 -3.85498047e-01 -3.97705078e-01 -3.98681641e-01 -4.03076172e-01
 -4.04052734e-01 -4.05029297e-01 -4.07226562e-01 -4.30175781e-01
 -4.30419922e-01 -4.32373047e-01 -4.34570312e-01 -4.35791016e-01
 -4.42626953e-01 -4.43847656e-01 -4.46533203e-01 -4.48242188e-01
 -4.49462891e-01 -4.49951172e-01 -4.50439453e-01 -4.57763672e-01
 -4.59960938e-01 -4.60693359e-01 -4.62890625e-01 -4.93652344e-01
 -4.94628906e-01 -4.98291016e-01 -5.00000000e-01 -5.02929688e-01
 -5.07812500e-01 -5.08789062e-01 -5.11230469e-01 -5.14648438e-01
 -5.15625000e-01 -5.22460938e-01 -5.26367188e-01 -5.27343750e-01
 -5.29785156e-01 -5.52246094e-01 -5.54687500e-01 -5.57617188e-01
 -5.58105469e-01 -5.60058594e-01 -5.62988281e-01 -5.64453125e-01
 -5.67871094e-01 -5.68847656e-01 -5.72265625e-01 -5.76171875e-01
 -5.77636719e-01 -5.83984375e-01 -5.85937500e-01 -6.00585938e-01
 -6.06445312e-01 -6.09863281e-01 -6.13281250e-01 -6.14746094e-01
 -6.19628906e-01 -6.61132812e-01 -6.63574219e-01 -6.80175781e-01
 -6.85546875e-01 -6.94335938e-01 -6.95800781e-01 -6.96777344e-01
 -6.97265625e-01 -6.98242188e-01 -7.02636719e-01 -7.16796875e-01
 -7.18261719e-01 -7.25585938e-01 -7.26562500e-01 -7.35351562e-01
 -7.46093750e-01 -7.60742188e-01 -7.61718750e-01 -7.62695312e-01
 -7.73925781e-01 -7.75390625e-01 -7.75878906e-01 -7.77832031e-01
 -7.80761719e-01 -7.83203125e-01 -7.92968750e-01 -7.94433594e-01
 -8.23730469e-01 -8.25683594e-01 -8.44238281e-01 -8.47656250e-01
 -8.55957031e-01 -8.57910156e-01 -8.67675781e-01 -8.68164062e-01
 -8.79882812e-01 -8.80859375e-01 -8.81347656e-01 -8.83789062e-01
 -8.84277344e-01 -8.84765625e-01 -8.96972656e-01 -8.97460938e-01
 -9.09667969e-01 -9.11132812e-01 -9.18945312e-01 -9.20410156e-01
 -9.58496094e-01 -9.58984375e-01 -1.00878906e+00 -1.00976562e+00
 -1.01562500e+00 -1.01757812e+00 -1.01953125e+00 -1.03125000e+00
 -1.05859375e+00 -1.05957031e+00 -1.06835938e+00 -1.07519531e+00
 -1.12792969e+00 -1.14843750e+00 -1.27246094e+00 -1.27343750e+00
 -1.29687500e+00 -1.29785156e+00 -1.31835938e+00 -1.32421875e+00
 -1.37500000e+00 -1.37890625e+00 -1.58398438e+00 -1.58593750e+00
 -2.21875000e+00]
