log_loss_steps: 256
eval_steps: 512
check_degradation: 0
----------------Epoch 1/1----------------
Epoch 1/1, Loss after 192 samples: 0.7011
Epoch 1/1, Loss after 448 samples: 0.6990
Mean accuracy: 0.5017, std: 0.0116, lower bound: 0.4782, upper bound: 0.5243 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 448 samples: 0.5010 with eval loss: 0.6870
Best model with eval loss 0.6870431361659881 and eval accuracy 0.5010111223458038 with 448 samples seen is saved
Epoch 1/1, Loss after 704 samples: 0.6901
Epoch 1/1, Loss after 960 samples: 0.6745
Mean accuracy: 0.5502, std: 0.0113, lower bound: 0.5288, upper bound: 0.5728 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 960 samples: 0.5501 with eval loss: 0.6613
Best model with eval loss 0.6612783901153072 and eval accuracy 0.5500505561172901 with 960 samples seen is saved
Epoch 1/1, Loss after 1216 samples: 0.6495
Epoch 1/1, Loss after 1472 samples: 0.6466
Mean accuracy: 0.5787, std: 0.0109, lower bound: 0.5571, upper bound: 0.5996 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 1472 samples: 0.5789 with eval loss: 0.6343
Best model with eval loss 0.6343494038428029 and eval accuracy 0.5788675429726997 with 1472 samples seen is saved
Epoch 1/1, Loss after 1728 samples: 0.6115
Epoch 1/1, Loss after 1984 samples: 0.5860
Mean accuracy: 0.6840, std: 0.0102, lower bound: 0.6648, upper bound: 0.7032 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 1984 samples: 0.6840 with eval loss: 0.5939
Best model with eval loss 0.5938805899312419 and eval accuracy 0.6840242669362993 with 1984 samples seen is saved
Epoch 1/1, Loss after 2240 samples: 0.5755
Epoch 1/1, Loss after 2496 samples: 0.5739
Mean accuracy: 0.6870, std: 0.0104, lower bound: 0.6673, upper bound: 0.7068 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 2496 samples: 0.6866 with eval loss: 0.5647
Best model with eval loss 0.5646666961331521 and eval accuracy 0.6865520728008089 with 2496 samples seen is saved
Epoch 1/1, Loss after 2752 samples: 0.5361
Epoch 1/1, Loss after 3008 samples: 0.5113
Mean accuracy: 0.7533, std: 0.0097, lower bound: 0.7336, upper bound: 0.7720 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 3008 samples: 0.7533 with eval loss: 0.5187
Best model with eval loss 0.5187314883355172 and eval accuracy 0.7532861476238625 with 3008 samples seen is saved
Epoch 1/1, Loss after 3264 samples: 0.5401
Epoch 1/1, Loss after 3520 samples: 0.5075
Mean accuracy: 0.7660, std: 0.0095, lower bound: 0.7467, upper bound: 0.7841 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 3520 samples: 0.7659 with eval loss: 0.4925
Best model with eval loss 0.49249385153093644 and eval accuracy 0.7659251769464105 with 3520 samples seen is saved
Epoch 1/1, Loss after 3776 samples: 0.4829
Epoch 1/1, Loss after 4032 samples: 0.4692
Mean accuracy: 0.7633, std: 0.0097, lower bound: 0.7442, upper bound: 0.7821 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 4032 samples: 0.7634 with eval loss: 0.4774
Best model with eval loss 0.4774135091612416 and eval accuracy 0.7633973710819009 with 4032 samples seen is saved
Epoch 1/1, Loss after 4288 samples: 0.4767
Epoch 1/1, Loss after 4544 samples: 0.4743
Mean accuracy: 0.7324, std: 0.0099, lower bound: 0.7128, upper bound: 0.7503 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 4544 samples: 0.7326 with eval loss: 0.4944
Epoch 1/1, Loss after 4800 samples: 0.4963
Epoch 1/1, Loss after 5056 samples: 0.4496
Mean accuracy: 0.7488, std: 0.0097, lower bound: 0.7305, upper bound: 0.7679 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 5056 samples: 0.7487 with eval loss: 0.4692
Best model with eval loss 0.46915594127870375 and eval accuracy 0.7487360970677452 with 5056 samples seen is saved
Epoch 1/1, Loss after 5312 samples: 0.4636
Epoch 1/1, Loss after 5568 samples: 0.4523
Mean accuracy: 0.7363, std: 0.0098, lower bound: 0.7169, upper bound: 0.7543 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 5568 samples: 0.7366 with eval loss: 0.4760
Epoch 1/1, Loss after 5824 samples: 0.4240
Epoch 1/1, Loss after 6080 samples: 0.4737
Mean accuracy: 0.7511, std: 0.0095, lower bound: 0.7326, upper bound: 0.7710 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 6080 samples: 0.7513 with eval loss: 0.4589
Best model with eval loss 0.458903371326385 and eval accuracy 0.7512639029322548 with 6080 samples seen is saved
Epoch 1/1, Loss after 6336 samples: 0.4819
Epoch 1/1, Loss after 6592 samples: 0.4672
Mean accuracy: 0.7152, std: 0.0103, lower bound: 0.6956, upper bound: 0.7361 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 6592 samples: 0.7154 with eval loss: 0.5008
Epoch 1/1, Loss after 6848 samples: 0.4281
Epoch 1/1, Loss after 7104 samples: 0.4014
Mean accuracy: 0.7852, std: 0.0097, lower bound: 0.7669, upper bound: 0.8038 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 7104 samples: 0.7851 with eval loss: 0.4365
Best model with eval loss 0.43645238684069726 and eval accuracy 0.7851365015166836 with 7104 samples seen is saved
Epoch 1/1, Loss after 7360 samples: 0.4670
Epoch 1/1, Loss after 7616 samples: 0.4210
Mean accuracy: 0.8221, std: 0.0083, lower bound: 0.8054, upper bound: 0.8387 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 7616 samples: 0.8220 with eval loss: 0.4056
Best model with eval loss 0.40563296598772847 and eval accuracy 0.8220424671385238 with 7616 samples seen is saved
Epoch 1/1, Loss after 7872 samples: 0.4474
Epoch 1/1, Loss after 8128 samples: 0.4207
Mean accuracy: 0.8268, std: 0.0081, lower bound: 0.8104, upper bound: 0.8418 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 8128 samples: 0.8266 with eval loss: 0.4010
Best model with eval loss 0.40102298125143976 and eval accuracy 0.826592517694641 with 8128 samples seen is saved
Epoch 1/1, Loss after 8384 samples: 0.4590
Epoch 1/1, Loss after 8640 samples: 0.4421
Mean accuracy: 0.7572, std: 0.0095, lower bound: 0.7391, upper bound: 0.7760 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 8640 samples: 0.7568 with eval loss: 0.4567
Epoch 1/1, Loss after 8896 samples: 0.3530
Epoch 1/1, Loss after 9152 samples: 0.4605
Mean accuracy: 0.7683, std: 0.0094, lower bound: 0.7503, upper bound: 0.7867 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 9152 samples: 0.7679 with eval loss: 0.4474
Epoch 1/1, Loss after 9408 samples: 0.4078
Epoch 1/1, Loss after 9664 samples: 0.4178
Mean accuracy: 0.8218, std: 0.0084, lower bound: 0.8043, upper bound: 0.8372 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 9664 samples: 0.8215 with eval loss: 0.3967
Best model with eval loss 0.39672459229346246 and eval accuracy 0.8215369059656218 with 9664 samples seen is saved
Epoch 1/1, Loss after 9920 samples: 0.4153
Epoch 1/1, Loss after 10176 samples: 0.3836
Mean accuracy: 0.7711, std: 0.0093, lower bound: 0.7538, upper bound: 0.7907 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 10176 samples: 0.7710 with eval loss: 0.4386
Epoch 1/1, Loss after 10432 samples: 0.4337
Epoch 1/1, Loss after 10688 samples: 0.4022
Mean accuracy: 0.7903, std: 0.0093, lower bound: 0.7725, upper bound: 0.8084 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 10688 samples: 0.7907 with eval loss: 0.4175
Epoch 1/1, Loss after 10944 samples: 0.3808
Epoch 1/1, Loss after 11200 samples: 0.3933
Mean accuracy: 0.7728, std: 0.0093, lower bound: 0.7548, upper bound: 0.7912 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 11200 samples: 0.7725 with eval loss: 0.4314
Epoch 1/1, Loss after 11456 samples: 0.4324
Epoch 1/1, Loss after 11712 samples: 0.4140
Mean accuracy: 0.8231, std: 0.0087, lower bound: 0.8064, upper bound: 0.8402 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 11712 samples: 0.8231 with eval loss: 0.3917
Best model with eval loss 0.3916642108271199 and eval accuracy 0.8230535894843276 with 11712 samples seen is saved
Epoch 1/1, Loss after 11968 samples: 0.4652
Epoch 1/1, Loss after 12224 samples: 0.4331
Mean accuracy: 0.7452, std: 0.0097, lower bound: 0.7260, upper bound: 0.7639 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 12224 samples: 0.7452 with eval loss: 0.4727
Epoch 1/1, Loss after 12480 samples: 0.3429
Epoch 1/1, Loss after 12736 samples: 0.4038
Mean accuracy: 0.8180, std: 0.0087, lower bound: 0.8013, upper bound: 0.8352 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 12736 samples: 0.8180 with eval loss: 0.3949
Epoch 1/1, Loss after 12992 samples: 0.4321
Epoch 1/1, Loss after 13248 samples: 0.4116
Mean accuracy: 0.7603, std: 0.0098, lower bound: 0.7411, upper bound: 0.7796 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 13248 samples: 0.7604 with eval loss: 0.4483
Epoch 1/1, Loss after 13504 samples: 0.3674
Epoch 1/1, Loss after 13760 samples: 0.4373
Mean accuracy: 0.7964, std: 0.0092, lower bound: 0.7786, upper bound: 0.8140 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 13760 samples: 0.7958 with eval loss: 0.4061
Epoch 1/1, Loss after 14016 samples: 0.3476
Epoch 1/1, Loss after 14272 samples: 0.4321
Mean accuracy: 0.7733, std: 0.0098, lower bound: 0.7533, upper bound: 0.7922 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 14272 samples: 0.7735 with eval loss: 0.4354
Epoch 1/1, Loss after 14528 samples: 0.3983
Epoch 1/1, Loss after 14784 samples: 0.4304
Mean accuracy: 0.7732, std: 0.0095, lower bound: 0.7548, upper bound: 0.7927 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 14784 samples: 0.7730 with eval loss: 0.4351
Epoch 1/1, Loss after 15040 samples: 0.4449
Epoch 1/1, Loss after 15296 samples: 0.4197
Mean accuracy: 0.8048, std: 0.0092, lower bound: 0.7867, upper bound: 0.8231 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 15296 samples: 0.8049 with eval loss: 0.3991
Epoch 1/1, Loss after 15552 samples: 0.4526
Epoch 1/1, Loss after 15808 samples: 0.4491
Mean accuracy: 0.7990, std: 0.0087, lower bound: 0.7821, upper bound: 0.8160 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 15808 samples: 0.7993 with eval loss: 0.4051
Training signal is False, stopping training
Training finished
Best model: {'eval_acc': 0.8230535894843276, 'nb_samples': 11712, 'eval_loss': 0.3916642108271199}
Training loss logs: [{'samples': 192, 'loss': 0.7011327743530273}, {'samples': 448, 'loss': 0.6989555358886719}, {'samples': 704, 'loss': 0.6900663375854492}, {'samples': 960, 'loss': 0.6744964122772217}, {'samples': 1216, 'loss': 0.6495323181152344}, {'samples': 1472, 'loss': 0.6466009616851807}, {'samples': 1728, 'loss': 0.6114916801452637}, {'samples': 1984, 'loss': 0.5860118865966797}, {'samples': 2240, 'loss': 0.5755255222320557}, {'samples': 2496, 'loss': 0.5739424228668213}, {'samples': 2752, 'loss': 0.5360540747642517}, {'samples': 3008, 'loss': 0.5112822651863098}, {'samples': 3264, 'loss': 0.5401285588741302}, {'samples': 3520, 'loss': 0.5075282976031303}, {'samples': 3776, 'loss': 0.48287472128868103}, {'samples': 4032, 'loss': 0.46919266879558563}, {'samples': 4288, 'loss': 0.47671429067850113}, {'samples': 4544, 'loss': 0.47425805032253265}, {'samples': 4800, 'loss': 0.496323861181736}, {'samples': 5056, 'loss': 0.44958969205617905}, {'samples': 5312, 'loss': 0.4636223167181015}, {'samples': 5568, 'loss': 0.4523055776953697}, {'samples': 5824, 'loss': 0.423992358148098}, {'samples': 6080, 'loss': 0.47372429072856903}, {'samples': 6336, 'loss': 0.4819461703300476}, {'samples': 6592, 'loss': 0.46717119961977005}, {'samples': 6848, 'loss': 0.4281293898820877}, {'samples': 7104, 'loss': 0.40136297047138214}, {'samples': 7360, 'loss': 0.466967836022377}, {'samples': 7616, 'loss': 0.42097342759370804}, {'samples': 7872, 'loss': 0.4474004805088043}, {'samples': 8128, 'loss': 0.4206748306751251}, {'samples': 8384, 'loss': 0.45902711898088455}, {'samples': 8640, 'loss': 0.4421437978744507}, {'samples': 8896, 'loss': 0.35303419828414917}, {'samples': 9152, 'loss': 0.4605061858892441}, {'samples': 9408, 'loss': 0.4077783226966858}, {'samples': 9664, 'loss': 0.4178403243422508}, {'samples': 9920, 'loss': 0.41527123004198074}, {'samples': 10176, 'loss': 0.3836197331547737}, {'samples': 10432, 'loss': 0.4336809068918228}, {'samples': 10688, 'loss': 0.40215932577848434}, {'samples': 10944, 'loss': 0.3807847276329994}, {'samples': 11200, 'loss': 0.39331692457199097}, {'samples': 11456, 'loss': 0.432422012090683}, {'samples': 11712, 'loss': 0.41397570073604584}, {'samples': 11968, 'loss': 0.4652393236756325}, {'samples': 12224, 'loss': 0.43308641761541367}, {'samples': 12480, 'loss': 0.34291496127843857}, {'samples': 12736, 'loss': 0.40384749323129654}, {'samples': 12992, 'loss': 0.43211761862039566}, {'samples': 13248, 'loss': 0.4116441309452057}, {'samples': 13504, 'loss': 0.3674028292298317}, {'samples': 13760, 'loss': 0.43733200430870056}, {'samples': 14016, 'loss': 0.34759051352739334}, {'samples': 14272, 'loss': 0.4321235194802284}, {'samples': 14528, 'loss': 0.3982914388179779}, {'samples': 14784, 'loss': 0.43038301914930344}, {'samples': 15040, 'loss': 0.4449251666665077}, {'samples': 15296, 'loss': 0.4196610525250435}, {'samples': 15552, 'loss': 0.4526103287935257}, {'samples': 15808, 'loss': 0.4491402879357338}]
Evaluation accuracy logs: [{'samples': 448, 'accuracy': 0.5016774519716886, 'std': 0.011643205261484626, 'lower_bound': 0.4782482305358949, 'upper_bound': 0.5242669362992922}, {'samples': 960, 'accuracy': 0.550216885743175, 'std': 0.011278374729713618, 'lower_bound': 0.5288169868554095, 'upper_bound': 0.5728008088978767}, {'samples': 1472, 'accuracy': 0.5786668351870576, 'std': 0.010859371510420272, 'lower_bound': 0.5571284125379171, 'upper_bound': 0.5995955510616785}, {'samples': 1984, 'accuracy': 0.6840075834175935, 'std': 0.010176205634721998, 'lower_bound': 0.6648003033367037, 'upper_bound': 0.7032355915065723}, {'samples': 2496, 'accuracy': 0.6870176946410516, 'std': 0.010369955121435048, 'lower_bound': 0.6673407482305359, 'upper_bound': 0.7067745197168858}, {'samples': 3008, 'accuracy': 0.7532891809908998, 'std': 0.00971622238849973, 'lower_bound': 0.7335692618806876, 'upper_bound': 0.7720045500505561}, {'samples': 3520, 'accuracy': 0.7660480283114257, 'std': 0.00954614030109059, 'lower_bound': 0.7467012133468149, 'upper_bound': 0.7841253791708797}, {'samples': 4032, 'accuracy': 0.763303842264914, 'std': 0.009659986546342194, 'lower_bound': 0.7441860465116279, 'upper_bound': 0.782103134479272}, {'samples': 4544, 'accuracy': 0.7323887765419616, 'std': 0.009862522489479468, 'lower_bound': 0.7128412537917088, 'upper_bound': 0.7502654196157735}, {'samples': 5056, 'accuracy': 0.7488468149646108, 'std': 0.009690739185406316, 'lower_bound': 0.7305358948432761, 'upper_bound': 0.7679474216380182}, {'samples': 5568, 'accuracy': 0.7362644084934277, 'std': 0.009789881618312787, 'lower_bound': 0.7168731041456016, 'upper_bound': 0.7543099089989889}, {'samples': 6080, 'accuracy': 0.7510702730030333, 'std': 0.009512403320865035, 'lower_bound': 0.7325581395348837, 'upper_bound': 0.7709807886754297}, {'samples': 6592, 'accuracy': 0.715211324570273, 'std': 0.010256921163903544, 'lower_bound': 0.6956395348837209, 'upper_bound': 0.7360970677451971}, {'samples': 7104, 'accuracy': 0.7852254802831142, 'std': 0.009699017164857621, 'lower_bound': 0.7669236602628918, 'upper_bound': 0.8038422649140546}, {'samples': 7616, 'accuracy': 0.8221395348837209, 'std': 0.008330125416724496, 'lower_bound': 0.8053589484327603, 'upper_bound': 0.8387259858442871}, {'samples': 8128, 'accuracy': 0.8268458038422649, 'std': 0.00807405092169995, 'lower_bound': 0.8104145601617796, 'upper_bound': 0.8417719919110213}, {'samples': 8640, 'accuracy': 0.7571627906976743, 'std': 0.009475486618642646, 'lower_bound': 0.7391304347826086, 'upper_bound': 0.776036400404449}, {'samples': 9152, 'accuracy': 0.7683104145601618, 'std': 0.009445209795836379, 'lower_bound': 0.750252780586451, 'upper_bound': 0.7866531850353893}, {'samples': 9664, 'accuracy': 0.8217664307381194, 'std': 0.008431327879630643, 'lower_bound': 0.804335187057634, 'upper_bound': 0.837221941354904}, {'samples': 10176, 'accuracy': 0.771061678463094, 'std': 0.00931444017055892, 'lower_bound': 0.7537790697674418, 'upper_bound': 0.7906976744186046}, {'samples': 10688, 'accuracy': 0.7902987866531851, 'std': 0.009300793371307032, 'lower_bound': 0.7724974721941354, 'upper_bound': 0.8083923154701719}, {'samples': 11200, 'accuracy': 0.772829625884732, 'std': 0.009281466356136732, 'lower_bound': 0.7548028311425683, 'upper_bound': 0.7912032355915066}, {'samples': 11712, 'accuracy': 0.8230697674418604, 'std': 0.008704391873916187, 'lower_bound': 0.8063700707785642, 'upper_bound': 0.840242669362993}, {'samples': 12224, 'accuracy': 0.7452431749241658, 'std': 0.009724199673348108, 'lower_bound': 0.7259858442871587, 'upper_bound': 0.7639155712841255}, {'samples': 12736, 'accuracy': 0.8180414560161781, 'std': 0.008704151500338465, 'lower_bound': 0.801314459049545, 'upper_bound': 0.8351996966632963}, {'samples': 13248, 'accuracy': 0.7602760364004044, 'std': 0.00975082852557067, 'lower_bound': 0.7411400404448938, 'upper_bound': 0.7795753286147624}, {'samples': 13760, 'accuracy': 0.7963725985844288, 'std': 0.00917846796724736, 'lower_bound': 0.7785642062689585, 'upper_bound': 0.813953488372093}, {'samples': 14272, 'accuracy': 0.7733417593528817, 'std': 0.009825059497205652, 'lower_bound': 0.7532861476238625, 'upper_bound': 0.7922143579373104}, {'samples': 14784, 'accuracy': 0.7731996966632962, 'std': 0.009503227956473887, 'lower_bound': 0.7548028311425683, 'upper_bound': 0.7927199191102123}, {'samples': 15296, 'accuracy': 0.8047780586450961, 'std': 0.009156532129149702, 'lower_bound': 0.7866531850353893, 'upper_bound': 0.8230535894843276}, {'samples': 15808, 'accuracy': 0.7990192113245703, 'std': 0.008715798375771173, 'lower_bound': 0.782103134479272, 'upper_bound': 0.8159757330637007}]
Evaluating the best model on the test set of dataset ./fake_true_datasets/fake_true_dataset_gemma_10k...
Test metrics:
accuracy: 0.8164878665318503
precision: 0.7653767197371035
recall: 0.9128484726571078
f1_score: 0.8325672983507051
fp_rate: 0.27990854528478953
tp_rate: 0.9128484726571078
std_accuracy: 0.008745723625793487
std_precision: 0.01255788651721331
std_recall: 0.00894776381741061
std_f1_score: 0.008778883285243306
std_fp_rate: 0.014566091477721248
std_tp_rate: 0.00894776381741061
TP: 902.941
TN: 712.072
FP: 276.792
FN: 86.195
roc_auc: 0.9248487661546987
fpr: [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00101112 0.00101112
 0.00101112 0.00101112 0.00101112 0.00101112 0.00101112 0.00101112
 0.00101112 0.00202224 0.00202224 0.00303337 0.00303337 0.00404449
 0.00404449 0.00505561 0.00505561 0.00505561 0.00505561 0.00505561
 0.00505561 0.00505561 0.00505561 0.00505561 0.00505561 0.00505561
 0.00505561 0.00505561 0.00505561 0.00505561 0.00505561 0.00505561
 0.00505561 0.00505561 0.00505561 0.00606673 0.00606673 0.00707786
 0.00707786 0.00808898 0.00808898 0.00808898 0.00808898 0.00808898
 0.00808898 0.00808898 0.00808898 0.00808898 0.00808898 0.00808898
 0.00808898 0.00808898 0.0091001  0.0091001  0.01011122 0.01011122
 0.01011122 0.01011122 0.01011122 0.01011122 0.01011122 0.01011122
 0.01011122 0.01011122 0.01011122 0.01011122 0.01011122 0.01011122
 0.01011122 0.01011122 0.01011122 0.01112235 0.01112235 0.01112235
 0.01112235 0.01213347 0.01213347 0.01213347 0.01213347 0.01314459
 0.01314459 0.01415571 0.01415571 0.01516684 0.01516684 0.01617796
 0.01617796 0.01617796 0.01617796 0.01617796 0.01617796 0.01617796
 0.01617796 0.01617796 0.01617796 0.01617796 0.01718908 0.01718908
 0.01718908 0.0182002  0.0182002  0.0182002  0.0182002  0.0182002
 0.0182002  0.0182002  0.0182002  0.01921132 0.01921132 0.02022245
 0.02022245 0.02022245 0.02123357 0.02123357 0.02123357 0.02123357
 0.02123357 0.02123357 0.02224469 0.02224469 0.02224469 0.02224469
 0.02224469 0.02224469 0.02325581 0.02325581 0.02426694 0.02426694
 0.02527806 0.02527806 0.02527806 0.02628918 0.02628918 0.0273003
 0.0273003  0.02831143 0.02831143 0.02932255 0.02932255 0.03033367
 0.03134479 0.03134479 0.03235592 0.03235592 0.03235592 0.03235592
 0.03336704 0.03336704 0.03336704 0.03437816 0.03437816 0.03437816
 0.03437816 0.03538928 0.03538928 0.03741153 0.03741153 0.03842265
 0.03842265 0.03943377 0.04044489 0.04246714 0.04246714 0.04246714
 0.04246714 0.04347826 0.04448938 0.04550051 0.04550051 0.04651163
 0.04651163 0.04752275 0.04752275 0.04853387 0.04853387 0.04954499
 0.04954499 0.05156724 0.05156724 0.05257836 0.05257836 0.05358948
 0.05358948 0.05358948 0.05358948 0.05460061 0.05460061 0.05460061
 0.05460061 0.05561173 0.05561173 0.05662285 0.05662285 0.05662285
 0.05662285 0.05763397 0.05763397 0.05763397 0.05763397 0.0586451
 0.0586451  0.06066734 0.06066734 0.06167846 0.06167846 0.06268959
 0.06268959 0.06370071 0.06370071 0.06572295 0.06572295 0.06673407
 0.06673407 0.06673407 0.0677452  0.06976744 0.06976744 0.07077856
 0.07077856 0.07178969 0.07178969 0.07280081 0.07280081 0.07583418
 0.0768453  0.0768453  0.07785642 0.07886754 0.07886754 0.07987867
 0.08190091 0.08392315 0.08392315 0.08493428 0.08493428 0.0859454
 0.0859454  0.08695652 0.08695652 0.08796764 0.08897877 0.08897877
 0.09201213 0.09201213 0.09302326 0.09403438 0.09403438 0.0950455
 0.0950455  0.09605662 0.09605662 0.09706775 0.09706775 0.09908999
 0.09908999 0.10111223 0.10111223 0.10313448 0.10313448 0.1041456
 0.1041456  0.10717897 0.10717897 0.10819009 0.10819009 0.10819009
 0.11122346 0.11223458 0.11425683 0.11425683 0.11526795 0.11627907
 0.11627907 0.11830131 0.11830131 0.1223458  0.1223458  0.12335693
 0.12335693 0.12436805 0.12436805 0.12740142 0.12740142 0.12942366
 0.12942366 0.13043478 0.13043478 0.1314459  0.1314459  0.13245703
 0.13245703 0.13346815 0.13346815 0.13852376 0.13852376 0.14054601
 0.14054601 0.14256825 0.14256825 0.14357937 0.14357937 0.14560162
 0.14560162 0.14863498 0.14863498 0.14964611 0.15065723 0.15065723
 0.15166835 0.15166835 0.15267947 0.15267947 0.15470172 0.15470172
 0.15571284 0.15672396 0.15773509 0.15874621 0.15874621 0.16177958
 0.16177958 0.1627907  0.16380182 0.16380182 0.16481294 0.16481294
 0.16683519 0.16683519 0.16784631 0.16784631 0.16885743 0.16885743
 0.17087968 0.17087968 0.17593529 0.17593529 0.17795753 0.17795753
 0.1809909  0.1809909  0.18200202 0.18200202 0.18503539 0.18503539
 0.18705763 0.18705763 0.18907988 0.18907988 0.19211325 0.19211325
 0.19716886 0.19716886 0.20020222 0.20020222 0.20121335 0.20121335
 0.20222447 0.20222447 0.20424671 0.20424671 0.20525784 0.20626896
 0.20728008 0.20728008 0.20930233 0.21233569 0.21233569 0.21334681
 0.21334681 0.21435794 0.21435794 0.21638018 0.21638018 0.21840243
 0.21941355 0.21941355 0.22042467 0.22042467 0.22244692 0.22244692
 0.22446916 0.22446916 0.22649141 0.22851365 0.23053589 0.23053589
 0.23458038 0.23458038 0.23761375 0.23761375 0.24266936 0.24368049
 0.24368049 0.25176946 0.25176946 0.25278059 0.25278059 0.25581395
 0.25581395 0.26289181 0.26289181 0.27199191 0.27199191 0.27300303
 0.27502528 0.27704752 0.27704752 0.27805865 0.27805865 0.27906977
 0.28109201 0.28210313 0.28210313 0.28311426 0.28311426 0.28412538
 0.28412538 0.28614762 0.28614762 0.29019211 0.29221436 0.29221436
 0.29929221 0.29929221 0.30131446 0.30131446 0.30232558 0.30232558
 0.30839232 0.30839232 0.30940344 0.30940344 0.31041456 0.31041456
 0.31547017 0.31951466 0.32962588 0.32962588 0.33063701 0.33063701
 0.33771486 0.33771486 0.34782609 0.34782609 0.3528817  0.3528817
 0.35389282 0.35389282 0.35490394 0.35490394 0.35692619 0.35692619
 0.35793731 0.35793731 0.36905966 0.36905966 0.37714863 0.37714863
 0.37815976 0.380182   0.38119312 0.38119312 0.38523761 0.38523761
 0.39029323 0.39231547 0.39635996 0.39635996 0.39737108 0.39737108
 0.3983822  0.3983822  0.40141557 0.40141557 0.40950455 0.40950455
 0.41658241 0.41658241 0.42163802 0.42163802 0.43983822 0.43983822
 0.44287159 0.44287159 0.45197169 0.45197169 0.45500506 0.45500506
 0.45601618 0.45601618 0.45904954 0.46107179 0.46208291 0.46208291
 0.46814965 0.47017189 0.47017189 0.47826087 0.47826087 0.47927199
 0.47927199 0.48533873 0.48634985 0.48736097 0.48736097 0.48837209
 0.48837209 0.4934277  0.4934277  0.50455005 0.50455005 0.5065723
 0.5065723  0.50960566 0.51061678 0.51061678 0.51263903 0.51263903
 0.51466127 0.51466127 0.52376138 0.52376138 0.52881699 0.52881699
 0.53589484 0.53589484 0.53892821 0.53892821 0.54095046 0.5429727
 0.56218402 0.56420627 0.56420627 0.57532861 0.57735086 0.57735086
 0.58240647 0.58645096 0.58948433 0.58948433 0.59656218 0.59656218
 0.60262892 0.60262892 0.6107179  0.6107179  0.61274014 0.61476239
 0.64004044 0.64004044 0.64610718 0.65015167 0.65318504 0.65722952
 0.66936299 0.66936299 0.67644085 0.67644085 0.69565217 0.69565217
 0.70171891 0.70374115 0.70879676 0.70879676 0.71587462 0.71991911
 0.72194135 0.7239636  0.72699697 0.72699697 0.74721941 0.74924166
 0.75530839 0.75733064 0.760364   0.76238625 0.76643074 0.76845298
 0.77957533 0.77957533 0.78665319 0.78665319 0.84226491 0.84226491
 0.84529828 0.84732053 0.84833165 0.85035389 0.86956522 0.87158746
 0.87259858 0.87462083 0.8776542  0.87967644 0.89484328 0.89686552
 0.90192113 0.90192113 0.90394338 0.90596562 0.91506572 0.91708797
 0.92618807 0.92821031 0.94034378 0.94236603 0.95045501 0.95247725
 0.96663296 0.96865521 0.99696663 0.99898888 1.        ]
tpr: [0.         0.00101112 0.00505561 0.00707786 0.02932255 0.03134479
 0.04550051 0.04954499 0.07583418 0.07785642 0.10212336 0.1041456
 0.10717897 0.10920121 0.11223458 0.11830131 0.12032356 0.12133468
 0.12335693 0.12841254 0.13043478 0.1314459  0.13346815 0.1445905
 0.14661274 0.15672396 0.15874621 0.16683519 0.16885743 0.17290192
 0.17492417 0.17694641 0.17896866 0.17997978 0.18200202 0.19514661
 0.1991911  0.22446916 0.22649141 0.23458038 0.23559151 0.23761375
 0.23862487 0.24064712 0.24266936 0.24469161 0.25075834 0.25278059
 0.2578362  0.25884732 0.26086957 0.26188069 0.26289181 0.26289181
 0.26390293 0.26390293 0.2669363  0.26895854 0.28008089 0.28210313
 0.28614762 0.28816987 0.2942366  0.29625885 0.30839232 0.31041456
 0.32355915 0.32760364 0.32861476 0.33063701 0.33265925 0.3346815
 0.33670374 0.33973711 0.3437816  0.34479272 0.35187058 0.35187058
 0.35591507 0.35591507 0.36299292 0.36501517 0.37310415 0.37714863
 0.38523761 0.38827098 0.39029323 0.40141557 0.40343782 0.40647118
 0.40849343 0.41456016 0.41456016 0.41860465 0.41860465 0.4206269
 0.42264914 0.42871587 0.43073812 0.4388271  0.44084934 0.44388271
 0.44590495 0.44691608 0.44893832 0.44994944 0.45197169 0.45601618
 0.45803842 0.46208291 0.4661274  0.4661274  0.46713852 0.47118301
 0.47219414 0.47320526 0.4752275  0.48129424 0.48634985 0.48634985
 0.48837209 0.48837209 0.49544995 0.49544995 0.49848332 0.49949444
 0.50758342 0.50960566 0.51466127 0.51769464 0.52376138 0.52578362
 0.52679474 0.52982811 0.53589484 0.53791709 0.53892821 0.54095046
 0.54196158 0.54196158 0.54701719 0.54903943 0.55308392 0.55510617
 0.55915066 0.5611729  0.56218402 0.56218402 0.56319515 0.56319515
 0.56521739 0.56723964 0.56723964 0.56926188 0.57532861 0.57735086
 0.5793731  0.58139535 0.58240647 0.58341759 0.58543984 0.58645096
 0.58847321 0.59049545 0.59049545 0.59251769 0.59352882 0.59656218
 0.59656218 0.59858443 0.60060667 0.6016178  0.60667341 0.60667341
 0.60869565 0.60869565 0.60970677 0.60970677 0.6107179  0.6107179
 0.61274014 0.61577351 0.61577351 0.61779575 0.619818   0.62689585
 0.62689585 0.6289181  0.63599596 0.63599596 0.64004044 0.64307381
 0.6471183  0.64812942 0.64914055 0.64914055 0.65217391 0.65217391
 0.65419616 0.65419616 0.65520728 0.65520728 0.66026289 0.66228514
 0.66430738 0.66430738 0.6653185  0.6653185  0.66632963 0.66734075
 0.67542973 0.67542973 0.67644085 0.67644085 0.68048534 0.68048534
 0.68351871 0.68351871 0.68452983 0.68452983 0.68655207 0.68655207
 0.6875632  0.68958544 0.69160768 0.69160768 0.6966633  0.69868554
 0.69969666 0.69969666 0.70070779 0.70171891 0.70374115 0.7057634
 0.70778564 0.70778564 0.71081901 0.71284125 0.7148635  0.7148635
 0.71688574 0.71688574 0.71890799 0.71890799 0.71991911 0.71991911
 0.72093023 0.72194135 0.72800809 0.72800809 0.72901921 0.73003033
 0.73205258 0.73913043 0.74014156 0.74014156 0.74115268 0.74115268
 0.74823054 0.74823054 0.75025278 0.75025278 0.7512639  0.7512639
 0.75227503 0.75328615 0.75429727 0.75429727 0.75530839 0.75631951
 0.75733064 0.75733064 0.75834176 0.75834176 0.760364   0.760364
 0.76238625 0.76238625 0.76643074 0.76744186 0.76744186 0.76946411
 0.76946411 0.77047523 0.77148635 0.77148635 0.77350859 0.77553084
 0.77654196 0.77654196 0.78058645 0.78159757 0.78361982 0.78463094
 0.78564206 0.78564206 0.78665319 0.78665319 0.78766431 0.78968655
 0.79777553 0.79777553 0.79979778 0.79979778 0.80182002 0.80283114
 0.80283114 0.80384226 0.80384226 0.80485339 0.80485339 0.80586451
 0.80687563 0.80687563 0.80889788 0.80889788 0.81193124 0.81294237
 0.81496461 0.81496461 0.81597573 0.81597573 0.81799798 0.81799798
 0.82103134 0.82103134 0.82204247 0.82204247 0.82406471 0.82507583
 0.82608696 0.82608696 0.82709808 0.82709808 0.82912032 0.82912032
 0.83215369 0.83215369 0.83518706 0.83518706 0.83619818 0.83619818
 0.8372093  0.8372093  0.83923155 0.83923155 0.84024267 0.84125379
 0.84125379 0.84226491 0.84226491 0.84529828 0.84529828 0.8463094
 0.84732053 0.84732053 0.84833165 0.84833165 0.84934277 0.84934277
 0.85136502 0.85136502 0.85237614 0.85338726 0.85338726 0.8554095
 0.8554095  0.85642063 0.85642063 0.85844287 0.85844287 0.85945399
 0.85945399 0.86248736 0.86248736 0.86552073 0.86552073 0.86754297
 0.86754297 0.8685541  0.8685541  0.86956522 0.86956522 0.87057634
 0.87057634 0.87462083 0.87462083 0.87563195 0.87563195 0.87664307
 0.87664307 0.8776542  0.8776542  0.87866532 0.87866532 0.87967644
 0.87967644 0.88270981 0.88270981 0.88473205 0.88473205 0.88574317
 0.88574317 0.8867543  0.88877654 0.88877654 0.88978766 0.88978766
 0.89180991 0.89180991 0.89282103 0.89282103 0.89383215 0.89383215
 0.89383215 0.89484328 0.89484328 0.8958544  0.8958544  0.89686552
 0.89686552 0.89787664 0.89787664 0.89787664 0.89787664 0.89888777
 0.89888777 0.90091001 0.90091001 0.90293225 0.90293225 0.90394338
 0.9049545  0.9049545  0.90596562 0.90596562 0.90697674 0.90697674
 0.90798787 0.90798787 0.90899899 0.90899899 0.91001011 0.91001011
 0.91001011 0.91001011 0.91102123 0.91102123 0.91304348 0.91304348
 0.91304348 0.91304348 0.91506572 0.91506572 0.91607685 0.91607685
 0.91708797 0.91708797 0.91809909 0.91809909 0.91809909 0.92012133
 0.92012133 0.92113246 0.92113246 0.92214358 0.92214358 0.9231547
 0.9231547  0.92416582 0.92416582 0.92517695 0.92517695 0.92719919
 0.92719919 0.92719919 0.92719919 0.92821031 0.92821031 0.92922144
 0.92922144 0.93023256 0.93023256 0.9322548  0.9322548  0.93326593
 0.93326593 0.93528817 0.93528817 0.93731041 0.93731041 0.93832154
 0.93832154 0.93933266 0.93933266 0.94034378 0.94034378 0.9413549
 0.9413549  0.9413549  0.9413549  0.94236603 0.94236603 0.94438827
 0.94438827 0.94438827 0.94438827 0.94539939 0.94539939 0.94641052
 0.94641052 0.94843276 0.94843276 0.95146613 0.95146613 0.95247725
 0.95247725 0.95348837 0.95348837 0.95449949 0.95449949 0.95551062
 0.95551062 0.95753286 0.95753286 0.95955511 0.95955511 0.96157735
 0.96157735 0.9635996  0.9635996  0.9635996  0.9635996  0.96461072
 0.96461072 0.96461072 0.96562184 0.96562184 0.96663296 0.96663296
 0.96764408 0.96764408 0.96865521 0.96865521 0.97067745 0.97067745
 0.97168857 0.97168857 0.9726997  0.9726997  0.97371082 0.97371082
 0.97472194 0.97472194 0.97573306 0.97674419 0.97674419 0.97775531
 0.97775531 0.97876643 0.97876643 0.97977755 0.97977755 0.98078868
 0.98078868 0.9817998  0.9817998  0.98281092 0.98281092 0.98281092
 0.98281092 0.98281092 0.98382204 0.98382204 0.98382204 0.98483316
 0.98483316 0.98483316 0.98483316 0.98584429 0.98584429 0.98786653
 0.98786653 0.98887765 0.98887765 0.98988878 0.98988878 0.98988878
 0.98988878 0.9908999  0.9908999  0.9908999  0.9908999  0.9908999
 0.9908999  0.99191102 0.99191102 0.99292214 0.99292214 0.99393327
 0.99393327 0.99393327 0.99393327 0.99494439 0.99494439 0.99494439
 0.99494439 0.99494439 0.99494439 0.99595551 0.99595551 0.99595551
 0.99595551 0.99595551 0.99595551 0.99595551 0.99595551 0.99595551
 0.99595551 0.99696663 0.99696663 0.99797776 0.99797776 0.99898888
 0.99898888 0.99898888 0.99898888 0.99898888 0.99898888 0.99898888
 0.99898888 0.99898888 0.99898888 0.99898888 0.99898888 0.99898888
 0.99898888 1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.        ]
thresholds: [            inf  4.49609375e+00  3.77734375e+00  3.68554688e+00
  3.17773438e+00  3.16796875e+00  2.98632812e+00  2.97265625e+00
  2.61132812e+00  2.59375000e+00  2.46484375e+00  2.46289062e+00
  2.45703125e+00  2.44335938e+00  2.44140625e+00  2.37500000e+00
  2.37304688e+00  2.36523438e+00  2.36132812e+00  2.29687500e+00
  2.29492188e+00  2.29296875e+00  2.28515625e+00  2.20898438e+00
  2.20703125e+00  2.16406250e+00  2.15820312e+00  2.08984375e+00
  2.08593750e+00  2.04687500e+00  2.04492188e+00  2.02734375e+00
  2.02343750e+00  2.02148438e+00  2.01562500e+00  1.95507812e+00
  1.94726562e+00  1.84179688e+00  1.83789062e+00  1.80761719e+00
  1.80664062e+00  1.80175781e+00  1.78613281e+00  1.78515625e+00
  1.77539062e+00  1.77050781e+00  1.74902344e+00  1.74609375e+00
  1.73437500e+00  1.73339844e+00  1.72851562e+00  1.72167969e+00
  1.71386719e+00  1.71093750e+00  1.70996094e+00  1.70410156e+00
  1.69726562e+00  1.69531250e+00  1.63964844e+00  1.63085938e+00
  1.62500000e+00  1.62304688e+00  1.60937500e+00  1.60351562e+00
  1.57617188e+00  1.57421875e+00  1.53613281e+00  1.52636719e+00
  1.52343750e+00  1.52246094e+00  1.51757812e+00  1.51562500e+00
  1.51269531e+00  1.50585938e+00  1.49414062e+00  1.48046875e+00
  1.45507812e+00  1.45410156e+00  1.44628906e+00  1.44335938e+00
  1.42089844e+00  1.41796875e+00  1.40332031e+00  1.40136719e+00
  1.38867188e+00  1.38378906e+00  1.37890625e+00  1.34667969e+00
  1.34375000e+00  1.33789062e+00  1.33691406e+00  1.32812500e+00
  1.32128906e+00  1.30664062e+00  1.30273438e+00  1.29296875e+00
  1.29101562e+00  1.27148438e+00  1.26953125e+00  1.25488281e+00
  1.25195312e+00  1.24218750e+00  1.24121094e+00  1.23730469e+00
  1.23632812e+00  1.23339844e+00  1.23242188e+00  1.21875000e+00
  1.21484375e+00  1.20703125e+00  1.20019531e+00  1.19628906e+00
  1.19335938e+00  1.18945312e+00  1.18750000e+00  1.18359375e+00
  1.17773438e+00  1.17480469e+00  1.14843750e+00  1.14746094e+00
  1.14355469e+00  1.14257812e+00  1.12890625e+00  1.12500000e+00
  1.12109375e+00  1.11621094e+00  1.09667969e+00  1.08984375e+00
  1.06933594e+00  1.06445312e+00  1.04492188e+00  1.04394531e+00
  1.04101562e+00  1.04003906e+00  1.02246094e+00  1.02148438e+00
  1.01855469e+00  1.01660156e+00  1.01269531e+00  1.00781250e+00
  9.94140625e-01  9.93164062e-01  9.65820312e-01  9.65332031e-01
  9.56542969e-01  9.51171875e-01  9.49218750e-01  9.37011719e-01
  9.36523438e-01  9.36035156e-01  9.33593750e-01  9.30664062e-01
  9.29199219e-01  9.28710938e-01  9.18457031e-01  9.15527344e-01
  9.13085938e-01  9.12597656e-01  9.09667969e-01  9.05761719e-01
  9.04785156e-01  9.03320312e-01  8.97460938e-01  8.95507812e-01
  8.93554688e-01  8.91113281e-01  8.89160156e-01  8.85253906e-01
  8.83300781e-01  8.82324219e-01  8.80371094e-01  8.78906250e-01
  8.71582031e-01  8.70605469e-01  8.69140625e-01  8.66699219e-01
  8.64257812e-01  8.62304688e-01  8.61816406e-01  8.61328125e-01
  8.60839844e-01  8.57421875e-01  8.54492188e-01  8.52050781e-01
  8.51562500e-01  8.41308594e-01  8.39843750e-01  8.39355469e-01
  8.23242188e-01  8.22753906e-01  8.21289062e-01  8.16406250e-01
  8.11523438e-01  8.08593750e-01  8.08105469e-01  8.02734375e-01
  7.99316406e-01  7.98339844e-01  7.97363281e-01  7.94433594e-01
  7.93457031e-01  7.88574219e-01  7.82226562e-01  7.80761719e-01
  7.74414062e-01  7.73925781e-01  7.69531250e-01  7.68066406e-01
  7.67578125e-01  7.63183594e-01  7.54882812e-01  7.52441406e-01
  7.50976562e-01  7.50000000e-01  7.45117188e-01  7.42675781e-01
  7.39257812e-01  7.36328125e-01  7.34375000e-01  7.32910156e-01
  7.26074219e-01  7.25097656e-01  7.24121094e-01  7.23632812e-01
  7.22167969e-01  7.20703125e-01  7.13867188e-01  7.12402344e-01
  7.09472656e-01  7.08984375e-01  7.08496094e-01  7.07519531e-01
  7.02636719e-01  7.02148438e-01  7.00195312e-01  6.99218750e-01
  6.92871094e-01  6.90917969e-01  6.87011719e-01  6.86523438e-01
  6.83593750e-01  6.79687500e-01  6.77734375e-01  6.77246094e-01
  6.74804688e-01  6.73339844e-01  6.71875000e-01  6.69921875e-01
  6.56738281e-01  6.49902344e-01  6.48925781e-01  6.48437500e-01
  6.46484375e-01  6.33789062e-01  6.31835938e-01  6.27929688e-01
  6.26953125e-01  6.25488281e-01  6.05957031e-01  6.03515625e-01
  5.97167969e-01  5.93750000e-01  5.93261719e-01  5.89355469e-01
  5.87402344e-01  5.85449219e-01  5.82519531e-01  5.81542969e-01
  5.80566406e-01  5.79101562e-01  5.78125000e-01  5.71289062e-01
  5.70800781e-01  5.68847656e-01  5.67382812e-01  5.66894531e-01
  5.64453125e-01  5.60546875e-01  5.50781250e-01  5.48339844e-01
  5.45410156e-01  5.44433594e-01  5.39550781e-01  5.39062500e-01
  5.38574219e-01  5.38085938e-01  5.36132812e-01  5.35156250e-01
  5.34179688e-01  5.32714844e-01  5.27343750e-01  5.23925781e-01
  5.21972656e-01  5.19531250e-01  5.18554688e-01  5.15625000e-01
  5.12695312e-01  5.10742188e-01  5.09277344e-01  5.08789062e-01
  5.01953125e-01  4.94384766e-01  4.92431641e-01  4.90722656e-01
  4.90478516e-01  4.89013672e-01  4.86083984e-01  4.81201172e-01
  4.78759766e-01  4.77539062e-01  4.76562500e-01  4.76074219e-01
  4.72167969e-01  4.67041016e-01  4.62402344e-01  4.56787109e-01
  4.53613281e-01  4.53125000e-01  4.52148438e-01  4.51660156e-01
  4.50927734e-01  4.48974609e-01  4.47021484e-01  4.44091797e-01
  4.41650391e-01  4.41162109e-01  4.40917969e-01  4.39941406e-01
  4.39453125e-01  4.39208984e-01  4.33349609e-01  4.33105469e-01
  4.32861328e-01  4.18212891e-01  4.16992188e-01  4.15527344e-01
  4.12109375e-01  4.07958984e-01  4.03564453e-01  4.01123047e-01
  3.99902344e-01  3.99658203e-01  3.97460938e-01  3.95751953e-01
  3.95019531e-01  3.94287109e-01  3.93554688e-01  3.90380859e-01
  3.89892578e-01  3.88183594e-01  3.87451172e-01  3.84765625e-01
  3.83544922e-01  3.79882812e-01  3.78662109e-01  3.77929688e-01
  3.77441406e-01  3.76708984e-01  3.76220703e-01  3.67431641e-01
  3.66455078e-01  3.65478516e-01  3.64746094e-01  3.63525391e-01
  3.59863281e-01  3.58398438e-01  3.57421875e-01  3.55224609e-01
  3.52783203e-01  3.51318359e-01  3.49365234e-01  3.49121094e-01
  3.45214844e-01  3.43261719e-01  3.37158203e-01  3.31054688e-01
  3.23974609e-01  3.21533203e-01  3.18847656e-01  3.18603516e-01
  3.17871094e-01  3.17626953e-01  3.12011719e-01  3.10302734e-01
  3.06640625e-01  3.01025391e-01  3.00537109e-01  2.99560547e-01
  2.94921875e-01  2.94189453e-01  2.87109375e-01  2.86865234e-01
  2.82714844e-01  2.81738281e-01  2.81494141e-01  2.80761719e-01
  2.80517578e-01  2.74902344e-01  2.70507812e-01  2.67333984e-01
  2.66845703e-01  2.66357422e-01  2.65380859e-01  2.64160156e-01
  2.60986328e-01  2.57080078e-01  2.56591797e-01  2.52929688e-01
  2.49145508e-01  2.48779297e-01  2.48291016e-01  2.46215820e-01
  2.45605469e-01  2.45361328e-01  2.44506836e-01  2.44262695e-01
  2.41821289e-01  2.40722656e-01  2.40112305e-01  2.39624023e-01
  2.37792969e-01  2.35961914e-01  2.33154297e-01  2.33032227e-01
  2.25708008e-01  2.25097656e-01  2.21313477e-01  2.18139648e-01
  2.07153320e-01  2.05810547e-01  1.95922852e-01  1.95190430e-01
  1.94213867e-01  1.83471680e-01  1.82006836e-01  1.80664062e-01
  1.77612305e-01  1.72363281e-01  1.71875000e-01  1.66748047e-01
  1.64672852e-01  1.58691406e-01  1.56738281e-01  1.55029297e-01
  1.53686523e-01  1.52221680e-01  1.50390625e-01  1.49047852e-01
  1.48437500e-01  1.48315430e-01  1.48071289e-01  1.45874023e-01
  1.39526367e-01  1.39282227e-01  1.38671875e-01  1.38305664e-01
  1.36474609e-01  1.33666992e-01  1.33178711e-01  1.30249023e-01
  1.27197266e-01  1.26342773e-01  1.19567871e-01  1.18957520e-01
  1.16699219e-01  1.14929199e-01  1.12609863e-01  1.06811523e-01
  1.00646973e-01  1.00524902e-01  1.00158691e-01  9.93041992e-02
  9.90600586e-02  9.72290039e-02  9.19799805e-02  9.05151367e-02
  7.81860352e-02  7.64770508e-02  7.50122070e-02  7.37915039e-02
  6.59179688e-02  6.43920898e-02  5.47485352e-02  4.82788086e-02
  4.15039062e-02  4.06494141e-02  3.88488770e-02  3.81774902e-02
  3.73840332e-02  3.55224609e-02  3.46374512e-02  3.46069336e-02
  3.16467285e-02  3.14636230e-02  2.38037109e-02  2.31628418e-02
  5.94329834e-03  4.50897217e-03  4.41741943e-03  3.89862061e-03
  2.09808350e-03  1.82342529e-03 -4.67681885e-03 -7.72857666e-03
 -1.60827637e-02 -1.86157227e-02 -2.38952637e-02 -2.41088867e-02
 -2.53295898e-02 -2.56042480e-02 -2.78320312e-02 -2.85644531e-02
 -3.14331055e-02 -3.74145508e-02 -5.13305664e-02 -5.22460938e-02
 -5.62133789e-02 -5.71899414e-02 -6.21643066e-02 -6.29272461e-02
 -7.59277344e-02 -7.64770508e-02 -8.32519531e-02 -8.41674805e-02
 -9.62524414e-02 -9.90600586e-02 -1.02539062e-01 -1.05529785e-01
 -1.05773926e-01 -1.09436035e-01 -1.16088867e-01 -1.18652344e-01
 -1.18957520e-01 -1.21337891e-01 -1.29150391e-01 -1.29394531e-01
 -1.29516602e-01 -1.38793945e-01 -1.39282227e-01 -1.39526367e-01
 -1.39770508e-01 -1.44287109e-01 -1.45141602e-01 -1.46728516e-01
 -1.48559570e-01 -1.48925781e-01 -1.49414062e-01 -1.54785156e-01
 -1.56250000e-01 -1.73461914e-01 -1.74194336e-01 -1.75781250e-01
 -1.81762695e-01 -1.83837891e-01 -1.86401367e-01 -1.86523438e-01
 -1.88354492e-01 -1.89331055e-01 -1.94091797e-01 -1.95068359e-01
 -2.05688477e-01 -2.05932617e-01 -2.12524414e-01 -2.14355469e-01
 -2.28759766e-01 -2.29248047e-01 -2.33032227e-01 -2.34863281e-01
 -2.36328125e-01 -2.37182617e-01 -2.50000000e-01 -2.51708984e-01
 -2.51953125e-01 -2.64648438e-01 -2.65136719e-01 -2.66357422e-01
 -2.74169922e-01 -2.75634766e-01 -2.80273438e-01 -2.80761719e-01
 -2.97851562e-01 -2.99804688e-01 -3.08349609e-01 -3.09326172e-01
 -3.26171875e-01 -3.26660156e-01 -3.27148438e-01 -3.29101562e-01
 -3.68408203e-01 -3.69628906e-01 -3.85986328e-01 -3.89160156e-01
 -3.96484375e-01 -4.00634766e-01 -4.14794922e-01 -4.17724609e-01
 -4.27734375e-01 -4.28955078e-01 -4.61669922e-01 -4.61914062e-01
 -4.69726562e-01 -4.70214844e-01 -4.79003906e-01 -4.79248047e-01
 -4.95361328e-01 -5.03417969e-01 -5.04882812e-01 -5.07812500e-01
 -5.18066406e-01 -5.18554688e-01 -5.66894531e-01 -5.67871094e-01
 -5.94238281e-01 -5.95703125e-01 -6.03027344e-01 -6.04980469e-01
 -6.19140625e-01 -6.22070312e-01 -6.60644531e-01 -6.61621094e-01
 -6.76269531e-01 -6.76757812e-01 -8.21289062e-01 -8.23730469e-01
 -8.31054688e-01 -8.36914062e-01 -8.39355469e-01 -8.44238281e-01
 -9.09667969e-01 -9.13085938e-01 -9.15527344e-01 -9.18457031e-01
 -9.29687500e-01 -9.31640625e-01 -9.77539062e-01 -9.79980469e-01
 -9.95605469e-01 -9.96093750e-01 -1.00292969e+00 -1.00390625e+00
 -1.04296875e+00 -1.04687500e+00 -1.07812500e+00 -1.08007812e+00
 -1.13085938e+00 -1.14746094e+00 -1.21289062e+00 -1.21777344e+00
 -1.37207031e+00 -1.38671875e+00 -1.97753906e+00 -1.97949219e+00
 -2.06640625e+00]
