log_loss_steps: 256
eval_steps: 512
check_degradation: 0
----------------Epoch 1/1----------------
Epoch 1/1, Loss after 192 samples: 0.7005
Epoch 1/1, Loss after 448 samples: 0.7016
Mean accuracy: 0.4999, std: 0.0119, lower bound: 0.4764, upper bound: 0.5226 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 448 samples: 0.5000 with eval loss: 0.6874
Best model with eval loss 0.6874144077301025 and eval accuracy 0.5 with 448 samples seen is saved
Epoch 1/1, Loss after 704 samples: 0.6834
Epoch 1/1, Loss after 960 samples: 0.6559
Mean accuracy: 0.5637, std: 0.0114, lower bound: 0.5412, upper bound: 0.5863 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 960 samples: 0.5638 with eval loss: 0.6473
Best model with eval loss 0.6473280191421509 and eval accuracy 0.5637550200803213 with 960 samples seen is saved
Epoch 1/1, Loss after 1216 samples: 0.6398
Epoch 1/1, Loss after 1472 samples: 0.5988
Mean accuracy: 0.7966, std: 0.0089, lower bound: 0.7786, upper bound: 0.8138 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 1472 samples: 0.7967 with eval loss: 0.5859
Best model with eval loss 0.5859037637710571 and eval accuracy 0.7966867469879518 with 1472 samples seen is saved
Epoch 1/1, Loss after 1728 samples: 0.5658
Epoch 1/1, Loss after 1984 samples: 0.5570
Mean accuracy: 0.8030, std: 0.0094, lower bound: 0.7851, upper bound: 0.8218 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 1984 samples: 0.8027 with eval loss: 0.5302
Best model with eval loss 0.5301982462406158 and eval accuracy 0.802710843373494 with 1984 samples seen is saved
Epoch 1/1, Loss after 2240 samples: 0.5309
Epoch 1/1, Loss after 2496 samples: 0.4941
Mean accuracy: 0.8157, std: 0.0087, lower bound: 0.7977, upper bound: 0.8333 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 2496 samples: 0.8153 with eval loss: 0.4774
Best model with eval loss 0.4774114042520523 and eval accuracy 0.8152610441767069 with 2496 samples seen is saved
Epoch 1/1, Loss after 2752 samples: 0.4925
Epoch 1/1, Loss after 3008 samples: 0.4694
Mean accuracy: 0.8276, std: 0.0086, lower bound: 0.8097, upper bound: 0.8449 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 3008 samples: 0.8273 with eval loss: 0.4353
Best model with eval loss 0.43528201431035995 and eval accuracy 0.8273092369477911 with 3008 samples seen is saved
Epoch 1/1, Loss after 3264 samples: 0.4324
Epoch 1/1, Loss after 3520 samples: 0.4433
Mean accuracy: 0.8335, std: 0.0085, lower bound: 0.8163, upper bound: 0.8489 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 3520 samples: 0.8338 with eval loss: 0.4101
Best model with eval loss 0.41014701314270496 and eval accuracy 0.8338353413654619 with 3520 samples seen is saved
Epoch 1/1, Loss after 3776 samples: 0.4035
Epoch 1/1, Loss after 4032 samples: 0.4411
Mean accuracy: 0.8253, std: 0.0084, lower bound: 0.8097, upper bound: 0.8424 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 4032 samples: 0.8253 with eval loss: 0.4009
Best model with eval loss 0.40086466167122126 and eval accuracy 0.8253012048192772 with 4032 samples seen is saved
Epoch 1/1, Loss after 4288 samples: 0.4496
Epoch 1/1, Loss after 4544 samples: 0.4405
Mean accuracy: 0.8485, std: 0.0080, lower bound: 0.8328, upper bound: 0.8640 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 4544 samples: 0.8489 with eval loss: 0.3725
Best model with eval loss 0.37252218555659056 and eval accuracy 0.8488955823293173 with 4544 samples seen is saved
Epoch 1/1, Loss after 4800 samples: 0.4499
Epoch 1/1, Loss after 5056 samples: 0.4041
Mean accuracy: 0.8369, std: 0.0084, lower bound: 0.8208, upper bound: 0.8529 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 5056 samples: 0.8368 with eval loss: 0.3813
Epoch 1/1, Loss after 5312 samples: 0.4443
Epoch 1/1, Loss after 5568 samples: 0.3941
Mean accuracy: 0.8278, std: 0.0087, lower bound: 0.8107, upper bound: 0.8449 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 5568 samples: 0.8278 with eval loss: 0.3884
Epoch 1/1, Loss after 5824 samples: 0.4398
Epoch 1/1, Loss after 6080 samples: 0.4069
Mean accuracy: 0.7821, std: 0.0092, lower bound: 0.7631, upper bound: 0.8007 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 6080 samples: 0.7821 with eval loss: 0.4274
Epoch 1/1, Loss after 6336 samples: 0.4471
Epoch 1/1, Loss after 6592 samples: 0.3888
Mean accuracy: 0.7614, std: 0.0094, lower bound: 0.7425, upper bound: 0.7796 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 6592 samples: 0.7610 with eval loss: 0.4559
Epoch 1/1, Loss after 6848 samples: 0.4184
Epoch 1/1, Loss after 7104 samples: 0.4155
Mean accuracy: 0.8150, std: 0.0087, lower bound: 0.7977, upper bound: 0.8318 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 7104 samples: 0.8153 with eval loss: 0.3938
Epoch 1/1, Loss after 7360 samples: 0.3805
Epoch 1/1, Loss after 7616 samples: 0.4601
Mean accuracy: 0.8663, std: 0.0076, lower bound: 0.8514, upper bound: 0.8805 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 7616 samples: 0.8665 with eval loss: 0.3476
Best model with eval loss 0.3475686516612768 and eval accuracy 0.8664658634538153 with 7616 samples seen is saved
Epoch 1/1, Loss after 7872 samples: 0.3861
Epoch 1/1, Loss after 8128 samples: 0.4212
Mean accuracy: 0.8668, std: 0.0075, lower bound: 0.8524, upper bound: 0.8815 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 8128 samples: 0.8665 with eval loss: 0.3424
Best model with eval loss 0.34242998203262687 and eval accuracy 0.8664658634538153 with 8128 samples seen is saved
Epoch 1/1, Loss after 8384 samples: 0.3978
Epoch 1/1, Loss after 8640 samples: 0.3954
Mean accuracy: 0.7048, std: 0.0102, lower bound: 0.6847, upper bound: 0.7244 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 8640 samples: 0.7048 with eval loss: 0.5234
Epoch 1/1, Loss after 8896 samples: 0.4422
Epoch 1/1, Loss after 9152 samples: 0.4183
Mean accuracy: 0.8592, std: 0.0076, lower bound: 0.8449, upper bound: 0.8740 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 9152 samples: 0.8589 with eval loss: 0.3509
Epoch 1/1, Loss after 9408 samples: 0.3934
Epoch 1/1, Loss after 9664 samples: 0.3723
Mean accuracy: 0.8478, std: 0.0080, lower bound: 0.8323, upper bound: 0.8635 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 9664 samples: 0.8474 with eval loss: 0.3572
Epoch 1/1, Loss after 9920 samples: 0.3926
Epoch 1/1, Loss after 10176 samples: 0.3467
Mean accuracy: 0.7847, std: 0.0089, lower bound: 0.7671, upper bound: 0.8017 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 10176 samples: 0.7846 with eval loss: 0.4207
Epoch 1/1, Loss after 10432 samples: 0.3352
Epoch 1/1, Loss after 10688 samples: 0.3728
Mean accuracy: 0.8677, std: 0.0075, lower bound: 0.8534, upper bound: 0.8820 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 10688 samples: 0.8675 with eval loss: 0.3284
Best model with eval loss 0.3284325357526541 and eval accuracy 0.8674698795180723 with 10688 samples seen is saved
Epoch 1/1, Loss after 10944 samples: 0.3361
Epoch 1/1, Loss after 11200 samples: 0.3752
Mean accuracy: 0.7797, std: 0.0096, lower bound: 0.7605, upper bound: 0.7992 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 11200 samples: 0.7796 with eval loss: 0.4288
Epoch 1/1, Loss after 11456 samples: 0.3557
Epoch 1/1, Loss after 11712 samples: 0.4244
Mean accuracy: 0.8669, std: 0.0076, lower bound: 0.8529, upper bound: 0.8810 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 11712 samples: 0.8670 with eval loss: 0.3292
Epoch 1/1, Loss after 11968 samples: 0.3592
Epoch 1/1, Loss after 12224 samples: 0.4707
Mean accuracy: 0.7628, std: 0.0095, lower bound: 0.7440, upper bound: 0.7811 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 12224 samples: 0.7631 with eval loss: 0.4444
Epoch 1/1, Loss after 12480 samples: 0.3639
Epoch 1/1, Loss after 12736 samples: 0.3667
Mean accuracy: 0.8590, std: 0.0079, lower bound: 0.8424, upper bound: 0.8745 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 12736 samples: 0.8584 with eval loss: 0.3416
Epoch 1/1, Loss after 12992 samples: 0.3739
Epoch 1/1, Loss after 13248 samples: 0.3542
Mean accuracy: 0.8427, std: 0.0080, lower bound: 0.8268, upper bound: 0.8579 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 13248 samples: 0.8429 with eval loss: 0.3557
Epoch 1/1, Loss after 13504 samples: 0.3292
Epoch 1/1, Loss after 13760 samples: 0.3740
Mean accuracy: 0.8076, std: 0.0088, lower bound: 0.7902, upper bound: 0.8253 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 13760 samples: 0.8072 with eval loss: 0.3926
Epoch 1/1, Loss after 14016 samples: 0.3586
Epoch 1/1, Loss after 14272 samples: 0.3600
Mean accuracy: 0.8450, std: 0.0080, lower bound: 0.8293, upper bound: 0.8599 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 14272 samples: 0.8449 with eval loss: 0.3517
Epoch 1/1, Loss after 14528 samples: 0.3557
Epoch 1/1, Loss after 14784 samples: 0.3916
Mean accuracy: 0.8491, std: 0.0082, lower bound: 0.8323, upper bound: 0.8650 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 14784 samples: 0.8489 with eval loss: 0.3459
Epoch 1/1, Loss after 15040 samples: 0.3737
Epoch 1/1, Loss after 15296 samples: 0.4023
Mean accuracy: 0.8378, std: 0.0079, lower bound: 0.8218, upper bound: 0.8534 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 15296 samples: 0.8373 with eval loss: 0.3570
Epoch 1/1, Loss after 15552 samples: 0.3738
Epoch 1/1, Loss after 15808 samples: 0.3731
Mean accuracy: 0.8311, std: 0.0082, lower bound: 0.8152, upper bound: 0.8469 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 15808 samples: 0.8313 with eval loss: 0.3632
Training signal is False, stopping training
Training finished
Best model: {'eval_acc': 0.8674698795180723, 'nb_samples': 10688, 'eval_loss': 0.3284325357526541}
Training loss logs: [{'samples': 192, 'loss': 0.7005243301391602}, {'samples': 448, 'loss': 0.7016229629516602}, {'samples': 704, 'loss': 0.6834383010864258}, {'samples': 960, 'loss': 0.6558914184570312}, {'samples': 1216, 'loss': 0.639808177947998}, {'samples': 1472, 'loss': 0.5987911224365234}, {'samples': 1728, 'loss': 0.5658402442932129}, {'samples': 1984, 'loss': 0.5570178031921387}, {'samples': 2240, 'loss': 0.5309219360351562}, {'samples': 2496, 'loss': 0.4941272735595703}, {'samples': 2752, 'loss': 0.4924826994538307}, {'samples': 3008, 'loss': 0.46938586235046387}, {'samples': 3264, 'loss': 0.4324447065591812}, {'samples': 3520, 'loss': 0.4433201402425766}, {'samples': 3776, 'loss': 0.40350528061389923}, {'samples': 4032, 'loss': 0.44110121577978134}, {'samples': 4288, 'loss': 0.449551559984684}, {'samples': 4544, 'loss': 0.4404953271150589}, {'samples': 4800, 'loss': 0.4498591423034668}, {'samples': 5056, 'loss': 0.4040907621383667}, {'samples': 5312, 'loss': 0.4443110153079033}, {'samples': 5568, 'loss': 0.3940510079264641}, {'samples': 5824, 'loss': 0.4397961050271988}, {'samples': 6080, 'loss': 0.4068874567747116}, {'samples': 6336, 'loss': 0.44713738560676575}, {'samples': 6592, 'loss': 0.3888235166668892}, {'samples': 6848, 'loss': 0.41844186931848526}, {'samples': 7104, 'loss': 0.4154549241065979}, {'samples': 7360, 'loss': 0.3804975003004074}, {'samples': 7616, 'loss': 0.4600590467453003}, {'samples': 7872, 'loss': 0.38608022034168243}, {'samples': 8128, 'loss': 0.4211704432964325}, {'samples': 8384, 'loss': 0.3977658897638321}, {'samples': 8640, 'loss': 0.3953549638390541}, {'samples': 8896, 'loss': 0.44216233491897583}, {'samples': 9152, 'loss': 0.4182902052998543}, {'samples': 9408, 'loss': 0.39337773621082306}, {'samples': 9664, 'loss': 0.37228626012802124}, {'samples': 9920, 'loss': 0.39262018352746964}, {'samples': 10176, 'loss': 0.34674714505672455}, {'samples': 10432, 'loss': 0.3351510837674141}, {'samples': 10688, 'loss': 0.3727869838476181}, {'samples': 10944, 'loss': 0.336090512573719}, {'samples': 11200, 'loss': 0.3751586154103279}, {'samples': 11456, 'loss': 0.3557044416666031}, {'samples': 11712, 'loss': 0.42439154535532}, {'samples': 11968, 'loss': 0.3592025265097618}, {'samples': 12224, 'loss': 0.4706621840596199}, {'samples': 12480, 'loss': 0.36391083896160126}, {'samples': 12736, 'loss': 0.366705559194088}, {'samples': 12992, 'loss': 0.37385230511426926}, {'samples': 13248, 'loss': 0.3542192131280899}, {'samples': 13504, 'loss': 0.3292320668697357}, {'samples': 13760, 'loss': 0.3740050420165062}, {'samples': 14016, 'loss': 0.3585839197039604}, {'samples': 14272, 'loss': 0.3599882498383522}, {'samples': 14528, 'loss': 0.355664424598217}, {'samples': 14784, 'loss': 0.39156613126397133}, {'samples': 15040, 'loss': 0.3737221658229828}, {'samples': 15296, 'loss': 0.4022975414991379}, {'samples': 15552, 'loss': 0.37380486726760864}, {'samples': 15808, 'loss': 0.37307410687208176}]
Evaluation accuracy logs: [{'samples': 448, 'accuracy': 0.4998504016064257, 'std': 0.011911765190653262, 'lower_bound': 0.4764056224899598, 'upper_bound': 0.5225903614457831}, {'samples': 960, 'accuracy': 0.5636541164658635, 'std': 0.011430124291392124, 'lower_bound': 0.5411646586345381, 'upper_bound': 0.5863453815261044}, {'samples': 1472, 'accuracy': 0.7966079317269075, 'std': 0.008929936278908816, 'lower_bound': 0.7786144578313253, 'upper_bound': 0.8137675702811245}, {'samples': 1984, 'accuracy': 0.8029809236947791, 'std': 0.009358950647291177, 'lower_bound': 0.785140562248996, 'upper_bound': 0.8217871485943775}, {'samples': 2496, 'accuracy': 0.8156706827309237, 'std': 0.008660360891822027, 'lower_bound': 0.7976782128514056, 'upper_bound': 0.8333333333333334}, {'samples': 3008, 'accuracy': 0.8275567269076305, 'std': 0.008604598541045576, 'lower_bound': 0.8097389558232931, 'upper_bound': 0.8448795180722891}, {'samples': 3520, 'accuracy': 0.833507530120482, 'std': 0.00847134041338567, 'lower_bound': 0.8162525100401606, 'upper_bound': 0.8489081325301204}, {'samples': 4032, 'accuracy': 0.8252946787148594, 'std': 0.008398353287167766, 'lower_bound': 0.8097264056224899, 'upper_bound': 0.8423820281124498}, {'samples': 4544, 'accuracy': 0.8485366465863453, 'std': 0.007991043502046071, 'lower_bound': 0.8328313253012049, 'upper_bound': 0.8639683734939758}, {'samples': 5056, 'accuracy': 0.8369171686746988, 'std': 0.00838557316845487, 'lower_bound': 0.8207831325301205, 'upper_bound': 0.8529241967871485}, {'samples': 5568, 'accuracy': 0.8277821285140562, 'std': 0.008714498481873985, 'lower_bound': 0.8107429718875502, 'upper_bound': 0.8448920682730923}, {'samples': 6080, 'accuracy': 0.7820788152610442, 'std': 0.009241846513451406, 'lower_bound': 0.7630522088353414, 'upper_bound': 0.800715361445783}, {'samples': 6592, 'accuracy': 0.761445281124498, 'std': 0.009435034950010052, 'lower_bound': 0.742457329317269, 'upper_bound': 0.7796184738955824}, {'samples': 7104, 'accuracy': 0.8149839357429719, 'std': 0.008711760317313912, 'lower_bound': 0.7976907630522089, 'upper_bound': 0.8318273092369478}, {'samples': 7616, 'accuracy': 0.8662961847389559, 'std': 0.007616204002139333, 'lower_bound': 0.8514056224899599, 'upper_bound': 0.8805220883534136}, {'samples': 8128, 'accuracy': 0.8667976907630521, 'std': 0.00750232619783059, 'lower_bound': 0.8523845381526105, 'upper_bound': 0.8815386546184738}, {'samples': 8640, 'accuracy': 0.7047876506024097, 'std': 0.010154259197257234, 'lower_bound': 0.6847264056224899, 'upper_bound': 0.7244226907630522}, {'samples': 9152, 'accuracy': 0.859234437751004, 'std': 0.007550457393716287, 'lower_bound': 0.8448669678714859, 'upper_bound': 0.873995983935743}, {'samples': 9664, 'accuracy': 0.8478052208835342, 'std': 0.007955717366492037, 'lower_bound': 0.8323167670682731, 'upper_bound': 0.8634538152610441}, {'samples': 10176, 'accuracy': 0.7846817269076305, 'std': 0.008918757320920973, 'lower_bound': 0.7670682730923695, 'upper_bound': 0.8017193775100401}, {'samples': 10688, 'accuracy': 0.8676501004016063, 'std': 0.007473146562165376, 'lower_bound': 0.8534011044176706, 'upper_bound': 0.8820281124497992}, {'samples': 11200, 'accuracy': 0.779664156626506, 'std': 0.009556006316851934, 'lower_bound': 0.7605170682730924, 'upper_bound': 0.7992093373493975}, {'samples': 11712, 'accuracy': 0.8669342369477911, 'std': 0.007584716919509969, 'lower_bound': 0.8529116465863453, 'upper_bound': 0.8810366465863453}, {'samples': 12224, 'accuracy': 0.7627911646586345, 'std': 0.009451434155035647, 'lower_bound': 0.7439633534136546, 'upper_bound': 0.7811370481927711}, {'samples': 12736, 'accuracy': 0.859043172690763, 'std': 0.007918947243666713, 'lower_bound': 0.8423694779116466, 'upper_bound': 0.8744979919678715}, {'samples': 13248, 'accuracy': 0.8426937751004016, 'std': 0.008046183172921132, 'lower_bound': 0.8268072289156626, 'upper_bound': 0.8579442771084337}, {'samples': 13760, 'accuracy': 0.8075918674698797, 'std': 0.00879830154454763, 'lower_bound': 0.7901606425702812, 'upper_bound': 0.8253012048192772}, {'samples': 14272, 'accuracy': 0.8450406626506025, 'std': 0.007976241928720504, 'lower_bound': 0.8293172690763052, 'upper_bound': 0.8599397590361446}, {'samples': 14784, 'accuracy': 0.8491390562248996, 'std': 0.00815338590528725, 'lower_bound': 0.8323167670682731, 'upper_bound': 0.8649598393574297}, {'samples': 15296, 'accuracy': 0.8378177710843374, 'std': 0.007948429857969506, 'lower_bound': 0.8217745983935743, 'upper_bound': 0.8534136546184738}, {'samples': 15808, 'accuracy': 0.831129016064257, 'std': 0.008212859368398846, 'lower_bound': 0.8152484939759036, 'upper_bound': 0.8469001004016063}]
Evaluating the best model on the test set of dataset ./fake_true_datasets/fake_true_dataset_phi_10k...
Test metrics:
accuracy: 0.8243077309236949
precision: 0.7618345183258777
recall: 0.9440040382074099
f1_score: 0.8431300987473821
fp_rate: 0.2955623314514892
tp_rate: 0.9440040382074099
std_accuracy: 0.0086308680295449
std_precision: 0.012406823086891677
std_recall: 0.007111028164694764
std_f1_score: 0.00838122622409082
std_fp_rate: 0.014731404194380156
std_tp_rate: 0.007111028164694764
TP: 940.943
TN: 701.078
FP: 294.164
FN: 55.815
roc_auc: 0.9376940492895276
fpr: [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00100402 0.00100402 0.00100402
 0.00100402 0.00100402 0.00100402 0.00100402 0.00100402 0.00100402
 0.00100402 0.00100402 0.00100402 0.00100402 0.00100402 0.00100402
 0.00100402 0.00100402 0.00100402 0.00100402 0.00100402 0.00100402
 0.00100402 0.00100402 0.00100402 0.00100402 0.00100402 0.00100402
 0.00100402 0.00100402 0.00100402 0.00100402 0.00100402 0.00100402
 0.00100402 0.00100402 0.00100402 0.00100402 0.00100402 0.00100402
 0.00100402 0.00200803 0.00200803 0.00200803 0.00200803 0.00200803
 0.00200803 0.00200803 0.00200803 0.00200803 0.00301205 0.00301205
 0.00301205 0.00301205 0.00301205 0.00301205 0.00401606 0.00401606
 0.00401606 0.00401606 0.00502008 0.00502008 0.00502008 0.00502008
 0.00502008 0.00502008 0.0060241  0.0060241  0.0060241  0.0060241
 0.00702811 0.00702811 0.00702811 0.00702811 0.00702811 0.00702811
 0.00702811 0.00702811 0.00702811 0.00702811 0.00702811 0.00702811
 0.00702811 0.00702811 0.00702811 0.00702811 0.00702811 0.00702811
 0.00702811 0.00702811 0.00702811 0.00702811 0.00702811 0.00702811
 0.00702811 0.00702811 0.00803213 0.00803213 0.00803213 0.00803213
 0.00803213 0.00803213 0.00803213 0.00803213 0.00803213 0.00803213
 0.00803213 0.00803213 0.00903614 0.00903614 0.01004016 0.01004016
 0.01104418 0.01104418 0.01204819 0.01204819 0.01204819 0.01305221
 0.01305221 0.01405622 0.01405622 0.01405622 0.01506024 0.01506024
 0.01506024 0.01506024 0.01506024 0.01506024 0.01506024 0.01506024
 0.01506024 0.01506024 0.01506024 0.01606426 0.01606426 0.01606426
 0.01706827 0.01706827 0.01706827 0.01706827 0.01706827 0.01706827
 0.01706827 0.01706827 0.01807229 0.01807229 0.01807229 0.01807229
 0.01907631 0.01907631 0.02008032 0.02008032 0.02008032 0.02008032
 0.02008032 0.02008032 0.02108434 0.02108434 0.02108434 0.02108434
 0.02309237 0.02309237 0.02409639 0.02409639 0.02409639 0.02610442
 0.02610442 0.02610442 0.02610442 0.02710843 0.02710843 0.02710843
 0.02710843 0.02710843 0.02710843 0.02710843 0.02710843 0.02811245
 0.02811245 0.02911647 0.02911647 0.03012048 0.03212851 0.03212851
 0.03212851 0.03212851 0.03212851 0.03212851 0.03313253 0.03313253
 0.03413655 0.03413655 0.03413655 0.03413655 0.03413655 0.03514056
 0.03514056 0.03514056 0.03614458 0.03614458 0.03614458 0.03714859
 0.03714859 0.03714859 0.03714859 0.03815261 0.03815261 0.03815261
 0.03815261 0.03815261 0.03815261 0.03915663 0.03915663 0.03915663
 0.03915663 0.04016064 0.04016064 0.04016064 0.04216867 0.04216867
 0.04216867 0.04317269 0.04317269 0.04317269 0.04317269 0.04417671
 0.04417671 0.04518072 0.04518072 0.04718876 0.04718876 0.04718876
 0.0502008  0.0502008  0.05120482 0.05120482 0.05220884 0.05220884
 0.05421687 0.05421687 0.0562249  0.0562249  0.0562249  0.0562249
 0.05722892 0.05722892 0.05923695 0.05923695 0.06024096 0.06024096
 0.062249   0.062249   0.06425703 0.06425703 0.06626506 0.06626506
 0.06726908 0.06726908 0.06726908 0.06726908 0.06827309 0.06927711
 0.06927711 0.07028112 0.07028112 0.07128514 0.07128514 0.07228916
 0.07228916 0.07329317 0.07329317 0.07429719 0.07931727 0.07931727
 0.07931727 0.07931727 0.08232932 0.08232932 0.08634538 0.08634538
 0.0873494  0.0873494  0.08935743 0.08935743 0.09036145 0.09036145
 0.09136546 0.09136546 0.09236948 0.09236948 0.09337349 0.09337349
 0.09437751 0.09437751 0.09538153 0.09538153 0.09638554 0.09638554
 0.09738956 0.09738956 0.09839357 0.09839357 0.10040161 0.10040161
 0.10140562 0.10140562 0.10240964 0.10240964 0.10943775 0.10943775
 0.11044177 0.11144578 0.11144578 0.1124498  0.1124498  0.11546185
 0.11546185 0.11646586 0.11646586 0.11746988 0.11746988 0.11947791
 0.11947791 0.12048193 0.12048193 0.12248996 0.12248996 0.12449799
 0.12449799 0.12550201 0.12550201 0.12851406 0.12851406 0.13353414
 0.13353414 0.13554217 0.13554217 0.13654618 0.13654618 0.13855422
 0.14457831 0.14457831 0.14658635 0.14759036 0.14959839 0.14959839
 0.15160643 0.15461847 0.15461847 0.15662651 0.15662651 0.16365462
 0.16365462 0.16566265 0.16566265 0.1746988  0.1746988  0.17570281
 0.17570281 0.17670683 0.17771084 0.17971888 0.17971888 0.17971888
 0.17971888 0.187751   0.187751   0.19277108 0.19477912 0.19678715
 0.19779116 0.20180723 0.20180723 0.20481928 0.20682731 0.20983936
 0.20983936 0.21385542 0.21385542 0.21485944 0.21485944 0.21586345
 0.21686747 0.2188755  0.2188755  0.21987952 0.21987952 0.22088353
 0.22088353 0.22791165 0.22791165 0.22891566 0.22891566 0.23493976
 0.23493976 0.23594378 0.23594378 0.23694779 0.23694779 0.24096386
 0.24297189 0.24698795 0.24698795 0.24899598 0.24899598 0.25100402
 0.25200803 0.25502008 0.25502008 0.25702811 0.25702811 0.26606426
 0.26606426 0.26907631 0.26907631 0.27208835 0.27208835 0.27309237
 0.27309237 0.27409639 0.27409639 0.28313253 0.28313253 0.28514056
 0.28514056 0.28815261 0.28815261 0.29016064 0.29016064 0.29317269
 0.29317269 0.29518072 0.29518072 0.30321285 0.30321285 0.30522088
 0.30522088 0.30823293 0.30823293 0.31526104 0.31726908 0.31927711
 0.31927711 0.32429719 0.32429719 0.3313253  0.3313253  0.33433735
 0.33433735 0.33634538 0.34036145 0.34036145 0.34337349 0.34337349
 0.34638554 0.34638554 0.34839357 0.34839357 0.35040161 0.35441767
 0.3564257  0.35943775 0.35943775 0.36044177 0.3624498  0.36947791
 0.36947791 0.37048193 0.37048193 0.37751004 0.37751004 0.37951807
 0.3815261  0.39056225 0.39056225 0.39156627 0.39156627 0.39457831
 0.39457831 0.39759036 0.39759036 0.40963855 0.40963855 0.41164659
 0.41164659 0.41767068 0.41967871 0.41967871 0.42168675 0.4246988
 0.42771084 0.42971888 0.44076305 0.44076305 0.44176707 0.4437751
 0.45481928 0.45481928 0.46686747 0.46686747 0.48192771 0.48192771
 0.49698795 0.49698795 0.49799197 0.49799197 0.49899598 0.50100402
 0.50903614 0.51004016 0.52409639 0.52409639 0.52610442 0.52811245
 0.53212851 0.53413655 0.53514056 0.54216867 0.54618474 0.54618474
 0.54819277 0.56024096 0.562249   0.56425703 0.56526104 0.57429719
 0.57630522 0.58032129 0.58032129 0.58232932 0.58232932 0.58333333
 0.58333333 0.58433735 0.58634538 0.58835341 0.59036145 0.59738956
 0.60040161 0.60240964 0.60341365 0.60341365 0.60542169 0.60742972
 0.60943775 0.6124498  0.61445783 0.61947791 0.62148594 0.62449799
 0.62650602 0.62951807 0.6315261  0.6375502  0.6375502  0.64658635
 0.64859438 0.65060241 0.65261044 0.65461847 0.65461847 0.65662651
 0.65863454 0.6626506  0.66365462 0.67871486 0.68273092 0.68473896
 0.68875502 0.69176707 0.6937751  0.70281124 0.70481928 0.71084337
 0.71084337 0.71285141 0.71385542 0.71586345 0.71787149 0.7188755
 0.73293173 0.73493976 0.74698795 0.74698795 0.74899598 0.74899598
 0.75502008 0.75702811 0.77008032 0.77208835 0.77911647 0.7811245
 0.79016064 0.79216867 0.80722892 0.80923695 0.82028112 0.82028112
 0.82128514 0.82128514 0.82931727 0.82931727 0.83232932 0.83433735
 0.84337349 0.84638554 0.84738956 0.84939759 0.85040161 0.85240964
 0.87048193 0.87449799 0.87650602 0.88353414 0.88554217 0.8875502
 0.88955823 0.90261044 0.90562249 0.91064257 0.91465863 0.91767068
 0.9186747  0.94176707 0.94578313 1.        ]
tpr: [0.         0.00100402 0.01807229 0.02008032 0.0502008  0.05220884
 0.05722892 0.05923695 0.06726908 0.06927711 0.07630522 0.07931727
 0.08433735 0.08634538 0.08935743 0.08935743 0.09136546 0.09337349
 0.09638554 0.09839357 0.10441767 0.10843373 0.11144578 0.11345382
 0.1184739  0.12148594 0.12550201 0.12751004 0.12851406 0.13052209
 0.13453815 0.13654618 0.14156627 0.1435743  0.14859438 0.15060241
 0.15261044 0.15562249 0.15763052 0.15963855 0.16064257 0.17068273
 0.17369478 0.17570281 0.17771084 0.17971888 0.18273092 0.18574297
 0.19176707 0.19779116 0.1997992  0.20281124 0.21084337 0.21184739
 0.21385542 0.21485944 0.21686747 0.2188755  0.22088353 0.23293173
 0.23493976 0.23694779 0.24096386 0.24799197 0.24799197 0.24899598
 0.25100402 0.25301205 0.25502008 0.26706827 0.26706827 0.2811245
 0.28313253 0.29016064 0.29016064 0.29216867 0.29417671 0.3002008
 0.30321285 0.31024096 0.31124498 0.31325301 0.31526104 0.31726908
 0.31726908 0.32028112 0.32329317 0.34638554 0.34839357 0.34939759
 0.35240964 0.35441767 0.3624498  0.36445783 0.36746988 0.36947791
 0.38453815 0.38654618 0.39257028 0.39457831 0.39759036 0.40060241
 0.40261044 0.40662651 0.40863454 0.41164659 0.41566265 0.42369478
 0.42570281 0.43072289 0.43072289 0.43273092 0.43373494 0.43574297
 0.437751   0.43975904 0.44277108 0.44678715 0.45180723 0.45983936
 0.46184739 0.46987952 0.47088353 0.47389558 0.47389558 0.4748996
 0.4748996  0.47791165 0.47891566 0.48493976 0.48694779 0.48795181
 0.49899598 0.49899598 0.50502008 0.50702811 0.50803213 0.50903614
 0.51104418 0.51706827 0.51907631 0.52008032 0.52409639 0.53212851
 0.53413655 0.53614458 0.53815261 0.53815261 0.54016064 0.54417671
 0.54417671 0.54819277 0.55220884 0.55321285 0.55522088 0.56325301
 0.56626506 0.56827309 0.56827309 0.57128514 0.57329317 0.57429719
 0.57429719 0.57730924 0.57831325 0.5813253  0.58333333 0.58935743
 0.59136546 0.59437751 0.59437751 0.59538153 0.59738956 0.59939759
 0.59939759 0.60040161 0.60040161 0.60240964 0.60441767 0.60441767
 0.60742972 0.60943775 0.6124498  0.6124498  0.61445783 0.61646586
 0.62048193 0.62248996 0.62550201 0.62751004 0.63052209 0.63052209
 0.63353414 0.63353414 0.64859438 0.64959839 0.64959839 0.65160643
 0.65361446 0.65662651 0.65863454 0.66566265 0.66566265 0.6686747
 0.6686747  0.67068273 0.67168675 0.67369478 0.6746988  0.6746988
 0.67670683 0.67771084 0.67871486 0.68072289 0.68273092 0.68273092
 0.68574297 0.68975904 0.69277108 0.6937751  0.69578313 0.69779116
 0.69879518 0.70180723 0.70281124 0.70281124 0.70381526 0.70582329
 0.70783133 0.70783133 0.71285141 0.71485944 0.71485944 0.71787149
 0.71987952 0.71987952 0.72088353 0.72289157 0.72389558 0.72389558
 0.72590361 0.72590361 0.72791165 0.72791165 0.72891566 0.73293173
 0.73293173 0.73493976 0.73493976 0.73795181 0.73795181 0.73895582
 0.73895582 0.74096386 0.74096386 0.74196787 0.7439759  0.74598394
 0.74598394 0.74899598 0.74899598 0.75200803 0.75200803 0.75502008
 0.75502008 0.7560241  0.7560241  0.75803213 0.75803213 0.75903614
 0.75903614 0.76104418 0.76305221 0.76506024 0.76506024 0.76606426
 0.76706827 0.76807229 0.77309237 0.77309237 0.7751004  0.7751004
 0.77911647 0.7811245  0.78614458 0.78714859 0.78714859 0.79116466
 0.79518072 0.79618474 0.79618474 0.79919679 0.79919679 0.80120482
 0.80120482 0.80321285 0.80321285 0.8062249  0.8062249  0.80923695
 0.80923695 0.81024096 0.81024096 0.81124498 0.81124498 0.81425703
 0.81425703 0.81526104 0.81526104 0.81726908 0.81726908 0.82028112
 0.82028112 0.82429719 0.82429719 0.83534137 0.83534137 0.8373494
 0.8373494  0.84136546 0.84136546 0.84236948 0.84236948 0.84939759
 0.85040161 0.85040161 0.85140562 0.85140562 0.85240964 0.85240964
 0.85341365 0.85341365 0.85542169 0.85542169 0.8564257  0.8564257
 0.85843373 0.85843373 0.85943775 0.85943775 0.86044177 0.86044177
 0.86144578 0.86144578 0.86546185 0.86746988 0.86947791 0.86947791
 0.87048193 0.87048193 0.87148594 0.87148594 0.87449799 0.87449799
 0.87449799 0.87751004 0.87751004 0.87851406 0.87851406 0.88052209
 0.88052209 0.88052209 0.88554217 0.88554217 0.88654618 0.88654618
 0.8875502  0.8875502  0.89156627 0.89156627 0.89457831 0.89457831
 0.89558233 0.89558233 0.89658635 0.89658635 0.89759036 0.89959839
 0.90060241 0.90060241 0.90261044 0.90261044 0.90261044 0.90261044
 0.90361446 0.90361446 0.90461847 0.90461847 0.90562249 0.90562249
 0.90662651 0.90662651 0.90763052 0.90763052 0.90863454 0.90863454
 0.90963855 0.90963855 0.91064257 0.91064257 0.9126506  0.9126506
 0.91365462 0.91365462 0.91666667 0.91666667 0.92168675 0.92168675
 0.92269076 0.92269076 0.92369478 0.92369478 0.9246988  0.9246988
 0.9246988  0.9246988  0.92570281 0.92570281 0.92670683 0.92670683
 0.92771084 0.92771084 0.92871486 0.92871486 0.92971888 0.92971888
 0.93172691 0.93172691 0.93273092 0.93273092 0.93373494 0.93373494
 0.93574297 0.93574297 0.93674699 0.93674699 0.937751   0.937751
 0.93875502 0.93875502 0.93975904 0.93975904 0.9437751  0.9437751
 0.94578313 0.94578313 0.94678715 0.94678715 0.94779116 0.94779116
 0.94879518 0.94879518 0.95080321 0.95080321 0.95080321 0.95080321
 0.95180723 0.95180723 0.95281124 0.95281124 0.95381526 0.95381526
 0.95481928 0.95481928 0.95481928 0.95582329 0.95582329 0.95682731
 0.95682731 0.95983936 0.95983936 0.96084337 0.96084337 0.96084337
 0.96084337 0.96084337 0.96184739 0.96184739 0.96184739 0.96184739
 0.96285141 0.96285141 0.96385542 0.96385542 0.96485944 0.96485944
 0.96485944 0.96485944 0.96586345 0.96586345 0.96686747 0.96686747
 0.96787149 0.96787149 0.9688755  0.9688755  0.96987952 0.96987952
 0.97088353 0.97088353 0.97088353 0.97188755 0.97188755 0.97188755
 0.97188755 0.97188755 0.97188755 0.97289157 0.97289157 0.97289157
 0.97289157 0.97389558 0.97389558 0.97590361 0.97590361 0.97690763
 0.97690763 0.97891566 0.97891566 0.97991968 0.97991968 0.97991968
 0.97991968 0.98092369 0.98092369 0.98192771 0.98192771 0.98192771
 0.98192771 0.98192771 0.98293173 0.98293173 0.98293173 0.98393574
 0.98393574 0.98393574 0.98393574 0.98393574 0.98493976 0.98493976
 0.98493976 0.98493976 0.98594378 0.98594378 0.98694779 0.98694779
 0.98795181 0.98795181 0.98795181 0.98795181 0.98795181 0.98795181
 0.98795181 0.98795181 0.98795181 0.98895582 0.98895582 0.98895582
 0.98895582 0.98895582 0.98895582 0.98895582 0.98895582 0.98895582
 0.98895582 0.98895582 0.98895582 0.98895582 0.98995984 0.98995984
 0.98995984 0.98995984 0.98995984 0.98995984 0.99096386 0.99096386
 0.99096386 0.99096386 0.99196787 0.99196787 0.99196787 0.99196787
 0.99196787 0.99196787 0.99196787 0.99196787 0.99196787 0.99196787
 0.99297189 0.99297189 0.99297189 0.99297189 0.99297189 0.9939759
 0.9939759  0.9939759  0.9939759  0.99497992 0.99497992 0.99598394
 0.99598394 0.99598394 0.99598394 0.99598394 0.99598394 0.99598394
 0.99598394 0.99598394 0.99598394 0.99598394 0.99598394 0.99698795
 0.99698795 0.99799197 0.99799197 0.99899598 0.99899598 0.99899598
 0.99899598 0.99899598 0.99899598 0.99899598 0.99899598 0.99899598
 0.99899598 0.99899598 0.99899598 0.99899598 0.99899598 0.99899598
 0.99899598 0.99899598 0.99899598 0.99899598 0.99899598 0.99899598
 1.         1.         1.         1.        ]
thresholds: [            inf  4.54687500e+00  3.18750000e+00  3.09570312e+00
  2.80468750e+00  2.77929688e+00  2.73828125e+00  2.72656250e+00
  2.60351562e+00  2.59375000e+00  2.55664062e+00  2.55468750e+00
  2.50000000e+00  2.49804688e+00  2.48828125e+00  2.48632812e+00
  2.47656250e+00  2.47265625e+00  2.44531250e+00  2.43945312e+00
  2.39257812e+00  2.39062500e+00  2.38085938e+00  2.36718750e+00
  2.34570312e+00  2.34375000e+00  2.33398438e+00  2.33203125e+00
  2.32617188e+00  2.31250000e+00  2.29296875e+00  2.27343750e+00
  2.25390625e+00  2.25000000e+00  2.18750000e+00  2.18359375e+00
  2.17968750e+00  2.17578125e+00  2.16601562e+00  2.16406250e+00
  2.15039062e+00  2.13085938e+00  2.12109375e+00  2.11523438e+00
  2.10546875e+00  2.10351562e+00  2.08789062e+00  2.08203125e+00
  2.07617188e+00  2.03906250e+00  2.02539062e+00  2.01171875e+00
  1.98437500e+00  1.97656250e+00  1.97363281e+00  1.96972656e+00
  1.96679688e+00  1.95605469e+00  1.95312500e+00  1.90234375e+00
  1.90039062e+00  1.89160156e+00  1.88378906e+00  1.86621094e+00
  1.86035156e+00  1.85156250e+00  1.85058594e+00  1.84765625e+00
  1.84667969e+00  1.80273438e+00  1.80175781e+00  1.74121094e+00
  1.73828125e+00  1.72558594e+00  1.72460938e+00  1.71777344e+00
  1.70703125e+00  1.68066406e+00  1.67968750e+00  1.65625000e+00
  1.65527344e+00  1.64550781e+00  1.64453125e+00  1.63574219e+00
  1.62890625e+00  1.61425781e+00  1.61328125e+00  1.55761719e+00
  1.55273438e+00  1.55175781e+00  1.54394531e+00  1.54101562e+00
  1.52441406e+00  1.52246094e+00  1.51464844e+00  1.50781250e+00
  1.46679688e+00  1.46582031e+00  1.45214844e+00  1.44726562e+00
  1.43945312e+00  1.43457031e+00  1.43164062e+00  1.40527344e+00
  1.40429688e+00  1.39257812e+00  1.39062500e+00  1.37304688e+00
  1.37207031e+00  1.36132812e+00  1.35937500e+00  1.35351562e+00
  1.34863281e+00  1.34765625e+00  1.34082031e+00  1.33984375e+00
  1.33398438e+00  1.33203125e+00  1.32812500e+00  1.30371094e+00
  1.29589844e+00  1.27636719e+00  1.27539062e+00  1.27050781e+00
  1.26855469e+00  1.26660156e+00  1.26171875e+00  1.25878906e+00
  1.25781250e+00  1.24707031e+00  1.24609375e+00  1.24023438e+00
  1.19726562e+00  1.19433594e+00  1.18652344e+00  1.18554688e+00
  1.18359375e+00  1.17968750e+00  1.17382812e+00  1.15234375e+00
  1.15039062e+00  1.14941406e+00  1.14648438e+00  1.13281250e+00
  1.12792969e+00  1.12402344e+00  1.12207031e+00  1.12011719e+00
  1.11718750e+00  1.11523438e+00  1.11425781e+00  1.10839844e+00
  1.10351562e+00  1.09765625e+00  1.09277344e+00  1.07128906e+00
  1.06933594e+00  1.06542969e+00  1.06347656e+00  1.05957031e+00
  1.05664062e+00  1.05468750e+00  1.05175781e+00  1.04394531e+00
  1.04199219e+00  1.03417969e+00  1.03125000e+00  1.01757812e+00
  1.01367188e+00  1.00976562e+00  1.00878906e+00  1.00585938e+00
  1.00390625e+00  9.95605469e-01  9.91699219e-01  9.89257812e-01
  9.87304688e-01  9.85351562e-01  9.84863281e-01  9.79492188e-01
  9.69726562e-01  9.64355469e-01  9.61425781e-01  9.55566406e-01
  9.53613281e-01  9.52148438e-01  9.44335938e-01  9.42871094e-01
  9.39453125e-01  9.37988281e-01  9.34082031e-01  9.29687500e-01
  9.24804688e-01  9.22363281e-01  8.80371094e-01  8.79394531e-01
  8.76953125e-01  8.71093750e-01  8.68652344e-01  8.59375000e-01
  8.56933594e-01  8.46191406e-01  8.44238281e-01  8.37402344e-01
  8.34472656e-01  8.31542969e-01  8.30078125e-01  8.29101562e-01
  8.26171875e-01  8.25195312e-01  8.24707031e-01  8.22265625e-01
  8.19335938e-01  8.14453125e-01  8.12988281e-01  8.07617188e-01
  7.99804688e-01  7.91015625e-01  7.83203125e-01  7.80761719e-01
  7.73437500e-01  7.72949219e-01  7.69531250e-01  7.69042969e-01
  7.68554688e-01  7.68066406e-01  7.67089844e-01  7.66601562e-01
  7.60253906e-01  7.59765625e-01  7.53417969e-01  7.52441406e-01
  7.44628906e-01  7.40722656e-01  7.37792969e-01  7.36328125e-01
  7.33886719e-01  7.31933594e-01  7.27539062e-01  7.26074219e-01
  7.24609375e-01  7.22167969e-01  7.13867188e-01  7.05566406e-01
  7.05078125e-01  7.02148438e-01  6.98242188e-01  6.97753906e-01
  6.97265625e-01  6.87988281e-01  6.87500000e-01  6.86035156e-01
  6.83105469e-01  6.78222656e-01  6.73339844e-01  6.71875000e-01
  6.70898438e-01  6.66503906e-01  6.62597656e-01  6.53320312e-01
  6.49902344e-01  6.47460938e-01  6.45019531e-01  6.39648438e-01
  6.32812500e-01  6.31347656e-01  6.29882812e-01  6.25976562e-01
  6.18652344e-01  6.17675781e-01  6.17187500e-01  6.11328125e-01
  6.09863281e-01  6.04980469e-01  6.02050781e-01  6.01562500e-01
  5.99609375e-01  5.93750000e-01  5.85937500e-01  5.84472656e-01
  5.79589844e-01  5.78125000e-01  5.73730469e-01  5.64941406e-01
  5.52734375e-01  5.51269531e-01  5.39062500e-01  5.34667969e-01
  5.33203125e-01  5.29785156e-01  5.26855469e-01  5.20019531e-01
  5.12695312e-01  5.08789062e-01  5.07812500e-01  5.03906250e-01
  5.00488281e-01  4.96582031e-01  4.94628906e-01  4.84863281e-01
  4.84619141e-01  4.84375000e-01  4.82910156e-01  4.80957031e-01
  4.74853516e-01  4.67529297e-01  4.62402344e-01  4.58984375e-01
  4.55566406e-01  4.53369141e-01  4.52636719e-01  4.49462891e-01
  4.47753906e-01  4.45312500e-01  4.37988281e-01  4.24804688e-01
  4.22851562e-01  4.15771484e-01  4.14306641e-01  4.11132812e-01
  4.09912109e-01  4.07470703e-01  3.85986328e-01  3.72558594e-01
  3.70361328e-01  3.69384766e-01  3.68896484e-01  3.62548828e-01
  3.61328125e-01  3.54980469e-01  3.52050781e-01  3.50097656e-01
  3.49853516e-01  3.47412109e-01  3.43505859e-01  3.41552734e-01
  3.37402344e-01  3.30566406e-01  3.29101562e-01  3.24951172e-01
  3.23730469e-01  3.19335938e-01  3.15185547e-01  3.13720703e-01
  3.08837891e-01  3.08349609e-01  3.06640625e-01  2.97851562e-01
  2.96875000e-01  2.94921875e-01  2.92236328e-01  2.88818359e-01
  2.81005859e-01  2.79296875e-01  2.67822266e-01  2.59765625e-01
  2.58300781e-01  2.53173828e-01  2.49755859e-01  2.43164062e-01
  2.41821289e-01  2.35839844e-01  2.29980469e-01  2.28393555e-01
  2.28149414e-01  2.17285156e-01  2.12768555e-01  2.11669922e-01
  2.02026367e-01  1.93725586e-01  1.88842773e-01  1.85791016e-01
  1.84448242e-01  1.83227539e-01  1.81884766e-01  1.78710938e-01
  1.74804688e-01  1.74682617e-01  1.73095703e-01  1.59790039e-01
  1.56250000e-01  1.49902344e-01  1.47705078e-01  1.42944336e-01
  1.41967773e-01  1.40502930e-01  1.36840820e-01  1.33544922e-01
  1.32812500e-01  1.24023438e-01  1.23352051e-01  1.09680176e-01
  1.09313965e-01  1.08886719e-01  1.08581543e-01  1.07971191e-01
  1.06750488e-01  1.04797363e-01  1.01928711e-01  1.01501465e-01
  1.00524902e-01  9.45434570e-02  9.37500000e-02  8.30688477e-02
  8.17260742e-02  8.16040039e-02  7.37915039e-02  6.78710938e-02
  6.64672852e-02  6.57958984e-02  6.48803711e-02  6.36596680e-02
  6.28662109e-02  6.04248047e-02  6.00585938e-02  5.79833984e-02
  5.74340820e-02  5.55419922e-02  5.46875000e-02  4.74243164e-02
  4.54711914e-02  3.90014648e-02  3.75976562e-02  3.64379883e-02
  3.63769531e-02  2.22015381e-02  2.10113525e-02  1.76239014e-02
  1.67541504e-02  1.23901367e-02  1.04064941e-02  9.75036621e-03
  6.35528564e-03  5.80596924e-03  1.14440918e-05 -1.94854736e-02
 -2.14385986e-02 -2.46276855e-02 -2.72064209e-02 -2.93731689e-02
 -3.01055908e-02 -3.33251953e-02 -4.47998047e-02 -4.97436523e-02
 -5.53588867e-02 -5.75561523e-02 -5.91430664e-02 -7.61718750e-02
 -7.75756836e-02 -8.34960938e-02 -8.53271484e-02 -8.97216797e-02
 -9.88159180e-02 -1.13525391e-01 -1.19140625e-01 -1.22924805e-01
 -1.23168945e-01 -1.31958008e-01 -1.32446289e-01 -1.48803711e-01
 -1.49536133e-01 -1.52221680e-01 -1.55029297e-01 -1.59057617e-01
 -1.68457031e-01 -1.69067383e-01 -1.74438477e-01 -1.75292969e-01
 -1.78344727e-01 -1.80297852e-01 -1.83105469e-01 -1.84326172e-01
 -1.89453125e-01 -1.95312500e-01 -1.96411133e-01 -1.98852539e-01
 -2.00439453e-01 -2.03247070e-01 -2.05322266e-01 -2.13745117e-01
 -2.13867188e-01 -2.15087891e-01 -2.15454102e-01 -2.26928711e-01
 -2.27416992e-01 -2.36450195e-01 -2.37060547e-01 -2.45849609e-01
 -2.48413086e-01 -2.49755859e-01 -2.50488281e-01 -2.61230469e-01
 -2.61474609e-01 -2.64648438e-01 -2.65136719e-01 -2.89550781e-01
 -2.92236328e-01 -2.98828125e-01 -2.99560547e-01 -3.13476562e-01
 -3.14941406e-01 -3.16162109e-01 -3.20068359e-01 -3.21289062e-01
 -3.24951172e-01 -3.25683594e-01 -3.37890625e-01 -3.39111328e-01
 -3.40087891e-01 -3.41064453e-01 -3.61816406e-01 -3.64257812e-01
 -3.82568359e-01 -3.86474609e-01 -4.03076172e-01 -4.04296875e-01
 -4.44335938e-01 -4.45556641e-01 -4.47509766e-01 -4.49462891e-01
 -4.49707031e-01 -4.50683594e-01 -4.58984375e-01 -4.60449219e-01
 -4.84130859e-01 -4.85351562e-01 -4.88037109e-01 -4.89990234e-01
 -4.91210938e-01 -4.92675781e-01 -4.94140625e-01 -5.08789062e-01
 -5.13671875e-01 -5.16601562e-01 -5.18554688e-01 -5.30761719e-01
 -5.31738281e-01 -5.33691406e-01 -5.34179688e-01 -5.53222656e-01
 -5.54199219e-01 -5.63476562e-01 -5.63964844e-01 -5.64453125e-01
 -5.64941406e-01 -5.67382812e-01 -5.69335938e-01 -5.70312500e-01
 -5.71777344e-01 -5.74218750e-01 -5.76660156e-01 -5.87402344e-01
 -5.88378906e-01 -5.88867188e-01 -5.91308594e-01 -5.91796875e-01
 -5.94726562e-01 -5.99121094e-01 -6.03027344e-01 -6.09863281e-01
 -6.12304688e-01 -6.22070312e-01 -6.26464844e-01 -6.32812500e-01
 -6.33300781e-01 -6.38183594e-01 -6.40625000e-01 -6.50878906e-01
 -6.51367188e-01 -6.65527344e-01 -6.66503906e-01 -6.68945312e-01
 -6.72363281e-01 -6.74316406e-01 -6.75781250e-01 -6.77246094e-01
 -6.83105469e-01 -6.96777344e-01 -7.01660156e-01 -7.26562500e-01
 -7.29003906e-01 -7.31445312e-01 -7.40234375e-01 -7.40722656e-01
 -7.41699219e-01 -7.63183594e-01 -7.69042969e-01 -7.83203125e-01
 -7.87597656e-01 -7.88085938e-01 -7.89550781e-01 -7.90527344e-01
 -7.96875000e-01 -7.97363281e-01 -8.32031250e-01 -8.33984375e-01
 -8.53027344e-01 -8.55468750e-01 -8.60839844e-01 -8.65722656e-01
 -8.82812500e-01 -8.83789062e-01 -9.23339844e-01 -9.27246094e-01
 -9.40917969e-01 -9.42382812e-01 -9.64355469e-01 -9.67773438e-01
 -1.03710938e+00 -1.03808594e+00 -1.07617188e+00 -1.07812500e+00
 -1.08105469e+00 -1.08300781e+00 -1.10253906e+00 -1.10351562e+00
 -1.11328125e+00 -1.11523438e+00 -1.13769531e+00 -1.14648438e+00
 -1.15039062e+00 -1.15136719e+00 -1.15429688e+00 -1.16210938e+00
 -1.22949219e+00 -1.23046875e+00 -1.23828125e+00 -1.25195312e+00
 -1.25390625e+00 -1.25781250e+00 -1.25976562e+00 -1.33300781e+00
 -1.33398438e+00 -1.35644531e+00 -1.36914062e+00 -1.37500000e+00
 -1.37792969e+00 -1.56933594e+00 -1.58398438e+00 -2.43554688e+00]
