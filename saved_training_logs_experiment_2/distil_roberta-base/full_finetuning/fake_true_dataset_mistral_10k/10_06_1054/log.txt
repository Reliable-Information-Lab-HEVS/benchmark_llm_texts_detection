log_loss_steps: 208
eval_steps: 208
check_degradation: 0
----------------Epoch 1/1----------------
Epoch 1/1, Loss after 192 samples: 0.6923
Mean accuracy: 0.5379, std: 0.0113, lower bound: 0.5152, upper bound: 0.5609 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 192 samples: 0.5380 with eval loss: 0.6914
Best model with eval loss 0.6913626886183216 and eval accuracy 0.5380324543610547 with 192 samples seen is saved
Epoch 1/1, Loss after 400 samples: 0.6900
Mean accuracy: 0.5352, std: 0.0112, lower bound: 0.5137, upper bound: 0.5573 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 400 samples: 0.5360 with eval loss: 0.6869
Best model with eval loss 0.6869056455550655 and eval accuracy 0.5360040567951319 with 400 samples seen is saved
Epoch 1/1, Loss after 608 samples: 0.6825
Mean accuracy: 0.7449, std: 0.0095, lower bound: 0.7262, upper bound: 0.7622 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 608 samples: 0.7449 with eval loss: 0.6689
Best model with eval loss 0.6689096266223539 and eval accuracy 0.7449290060851927 with 608 samples seen is saved
Epoch 1/1, Loss after 816 samples: 0.6056
Mean accuracy: 0.8287, std: 0.0084, lower bound: 0.8124, upper bound: 0.8453 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 816 samples: 0.8286 with eval loss: 0.4683
Best model with eval loss 0.4683259533297631 and eval accuracy 0.8286004056795132 with 816 samples seen is saved
Epoch 1/1, Loss after 1024 samples: 0.4706
Mean accuracy: 0.8659, std: 0.0075, lower bound: 0.8509, upper bound: 0.8798 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 1024 samples: 0.8656 with eval loss: 0.3689
Best model with eval loss 0.3688941078801309 and eval accuracy 0.8656186612576064 with 1024 samples seen is saved
Epoch 1/1, Loss after 1232 samples: 0.3690
Mean accuracy: 0.7790, std: 0.0093, lower bound: 0.7612, upper bound: 0.7967 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 1232 samples: 0.7789 with eval loss: 0.4394
Epoch 1/1, Loss after 1440 samples: 0.4094
Mean accuracy: 0.8805, std: 0.0073, lower bound: 0.8651, upper bound: 0.8945 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 1440 samples: 0.8803 with eval loss: 0.3120
Best model with eval loss 0.31202268312054293 and eval accuracy 0.8803245436105477 with 1440 samples seen is saved
Epoch 1/1, Loss after 1648 samples: 0.3699
Mean accuracy: 0.8971, std: 0.0068, lower bound: 0.8844, upper bound: 0.9108 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 1648 samples: 0.8971 with eval loss: 0.2678
Best model with eval loss 0.26777183576937647 and eval accuracy 0.8970588235294118 with 1648 samples seen is saved
Epoch 1/1, Loss after 1856 samples: 0.3079
Mean accuracy: 0.8958, std: 0.0069, lower bound: 0.8829, upper bound: 0.9097 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 1856 samples: 0.8955 with eval loss: 0.2449
Best model with eval loss 0.24489439110602101 and eval accuracy 0.8955375253549696 with 1856 samples seen is saved
Epoch 1/1, Loss after 2064 samples: 0.2343
Mean accuracy: 0.8617, std: 0.0078, lower bound: 0.8458, upper bound: 0.8758 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 2064 samples: 0.8616 with eval loss: 0.3330
Epoch 1/1, Loss after 2272 samples: 0.2587
Mean accuracy: 0.8343, std: 0.0085, lower bound: 0.8159, upper bound: 0.8509 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 2272 samples: 0.8347 with eval loss: 0.3642
Epoch 1/1, Loss after 2480 samples: 0.2305
Mean accuracy: 0.8970, std: 0.0069, lower bound: 0.8839, upper bound: 0.9097 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 2480 samples: 0.8971 with eval loss: 0.2310
Best model with eval loss 0.23097491168206738 and eval accuracy 0.8970588235294118 with 2480 samples seen is saved
Epoch 1/1, Loss after 2688 samples: 0.2197
Mean accuracy: 0.8793, std: 0.0075, lower bound: 0.8641, upper bound: 0.8935 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 2688 samples: 0.8793 with eval loss: 0.2768
Epoch 1/1, Loss after 2896 samples: 0.2505
Mean accuracy: 0.9139, std: 0.0065, lower bound: 0.9016, upper bound: 0.9265 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 2896 samples: 0.9138 with eval loss: 0.2058
Best model with eval loss 0.20578101782068128 and eval accuracy 0.9137931034482759 with 2896 samples seen is saved
Epoch 1/1, Loss after 3104 samples: 0.2426
Mean accuracy: 0.8866, std: 0.0070, lower bound: 0.8722, upper bound: 0.8996 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 3104 samples: 0.8864 with eval loss: 0.2529
Epoch 1/1, Loss after 3312 samples: 0.2412
Mean accuracy: 0.9206, std: 0.0060, lower bound: 0.9082, upper bound: 0.9316 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 3312 samples: 0.9204 with eval loss: 0.1856
Best model with eval loss 0.18556643397577346 and eval accuracy 0.9203853955375254 with 3312 samples seen is saved
Epoch 1/1, Loss after 3520 samples: 0.3132
Mean accuracy: 0.9333, std: 0.0059, lower bound: 0.9209, upper bound: 0.9447 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 3520 samples: 0.9331 with eval loss: 0.1697
Best model with eval loss 0.16973831672822276 and eval accuracy 0.9330628803245437 with 3520 samples seen is saved
Epoch 1/1, Loss after 3728 samples: 0.2275
Mean accuracy: 0.9226, std: 0.0060, lower bound: 0.9107, upper bound: 0.9341 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 3728 samples: 0.9229 with eval loss: 0.1801
Epoch 1/1, Loss after 3936 samples: 0.2876
Mean accuracy: 0.9228, std: 0.0058, lower bound: 0.9113, upper bound: 0.9336 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 3936 samples: 0.9229 with eval loss: 0.1772
Epoch 1/1, Loss after 4144 samples: 0.3078
Mean accuracy: 0.9183, std: 0.0062, lower bound: 0.9057, upper bound: 0.9305 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 4144 samples: 0.9184 with eval loss: 0.2005
Epoch 1/1, Loss after 4352 samples: 0.2499
Mean accuracy: 0.9055, std: 0.0067, lower bound: 0.8925, upper bound: 0.9184 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 4352 samples: 0.9057 with eval loss: 0.2124
Epoch 1/1, Loss after 4560 samples: 0.2094
Mean accuracy: 0.8933, std: 0.0070, lower bound: 0.8793, upper bound: 0.9072 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 4560 samples: 0.8935 with eval loss: 0.2324
Epoch 1/1, Loss after 4768 samples: 0.1745
Mean accuracy: 0.8896, std: 0.0070, lower bound: 0.8763, upper bound: 0.9026 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 4768 samples: 0.8895 with eval loss: 0.2538
Epoch 1/1, Loss after 4976 samples: 0.1933
Mean accuracy: 0.9140, std: 0.0064, lower bound: 0.9021, upper bound: 0.9260 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 4976 samples: 0.9138 with eval loss: 0.2087
Epoch 1/1, Loss after 5184 samples: 0.1821
Mean accuracy: 0.9268, std: 0.0060, lower bound: 0.9153, upper bound: 0.9381 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 5184 samples: 0.9265 with eval loss: 0.1750
Epoch 1/1, Loss after 5392 samples: 0.2101
Mean accuracy: 0.9159, std: 0.0063, lower bound: 0.9031, upper bound: 0.9285 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 5392 samples: 0.9163 with eval loss: 0.1930
Epoch 1/1, Loss after 5600 samples: 0.2096
Mean accuracy: 0.8744, std: 0.0075, lower bound: 0.8595, upper bound: 0.8890 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 5600 samples: 0.8742 with eval loss: 0.2683
Epoch 1/1, Loss after 5808 samples: 0.2015
Mean accuracy: 0.8902, std: 0.0074, lower bound: 0.8763, upper bound: 0.9052 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 5808 samples: 0.8905 with eval loss: 0.2463
Epoch 1/1, Loss after 6016 samples: 0.2038
Mean accuracy: 0.9108, std: 0.0062, lower bound: 0.8981, upper bound: 0.9224 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 6016 samples: 0.9108 with eval loss: 0.2120
Epoch 1/1, Loss after 6224 samples: 0.2199
Mean accuracy: 0.8994, std: 0.0067, lower bound: 0.8864, upper bound: 0.9123 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 6224 samples: 0.8996 with eval loss: 0.2302
Epoch 1/1, Loss after 6432 samples: 0.1985
Mean accuracy: 0.9128, std: 0.0061, lower bound: 0.9006, upper bound: 0.9239 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 6432 samples: 0.9128 with eval loss: 0.2014
Epoch 1/1, Loss after 6640 samples: 0.1443
Mean accuracy: 0.8309, std: 0.0081, lower bound: 0.8144, upper bound: 0.8459 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 6640 samples: 0.8311 with eval loss: 0.4000
Epoch 1/1, Loss after 6848 samples: 0.1433
Mean accuracy: 0.9221, std: 0.0060, lower bound: 0.9102, upper bound: 0.9336 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 6848 samples: 0.9224 with eval loss: 0.1819
Epoch 1/1, Loss after 7056 samples: 0.1644
Mean accuracy: 0.9250, std: 0.0060, lower bound: 0.9133, upper bound: 0.9366 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 7056 samples: 0.9249 with eval loss: 0.1863
Epoch 1/1, Loss after 7264 samples: 0.2741
Mean accuracy: 0.8529, std: 0.0081, lower bound: 0.8372, upper bound: 0.8692 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 7264 samples: 0.8529 with eval loss: 0.3472
Epoch 1/1, Loss after 7472 samples: 0.1297
Mean accuracy: 0.9105, std: 0.0066, lower bound: 0.8976, upper bound: 0.9234 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 7472 samples: 0.9102 with eval loss: 0.1980
Epoch 1/1, Loss after 7680 samples: 0.1896
Mean accuracy: 0.9272, std: 0.0058, lower bound: 0.9158, upper bound: 0.9381 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 7680 samples: 0.9270 with eval loss: 0.1654
Best model with eval loss 0.16539242755501501 and eval accuracy 0.9269776876267748 with 7680 samples seen is saved
Epoch 1/1, Loss after 7888 samples: 0.1544
Mean accuracy: 0.9405, std: 0.0052, lower bound: 0.9305, upper bound: 0.9508 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 7888 samples: 0.9407 with eval loss: 0.1464
Best model with eval loss 0.1463518170339446 and eval accuracy 0.9406693711967545 with 7888 samples seen is saved
Epoch 1/1, Loss after 8096 samples: 0.1814
Mean accuracy: 0.8929, std: 0.0066, lower bound: 0.8803, upper bound: 0.9062 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 8096 samples: 0.8925 with eval loss: 0.2354
Epoch 1/1, Loss after 8304 samples: 0.1663
Mean accuracy: 0.8947, std: 0.0071, lower bound: 0.8803, upper bound: 0.9082 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 8304 samples: 0.8945 with eval loss: 0.2511
Epoch 1/1, Loss after 8512 samples: 0.1026
Mean accuracy: 0.9198, std: 0.0059, lower bound: 0.9082, upper bound: 0.9310 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 8512 samples: 0.9199 with eval loss: 0.1860
Epoch 1/1, Loss after 8720 samples: 0.1517
Mean accuracy: 0.9358, std: 0.0055, lower bound: 0.9244, upper bound: 0.9462 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 8720 samples: 0.9361 with eval loss: 0.1526
Epoch 1/1, Loss after 8928 samples: 0.1805
Mean accuracy: 0.8228, std: 0.0087, lower bound: 0.8058, upper bound: 0.8398 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 8928 samples: 0.8230 with eval loss: 0.5120
Epoch 1/1, Loss after 9136 samples: 0.1326
Mean accuracy: 0.8995, std: 0.0069, lower bound: 0.8869, upper bound: 0.9128 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 9136 samples: 0.8996 with eval loss: 0.2366
Epoch 1/1, Loss after 9344 samples: 0.0816
Mean accuracy: 0.9065, std: 0.0064, lower bound: 0.8940, upper bound: 0.9189 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 9344 samples: 0.9067 with eval loss: 0.2169
Epoch 1/1, Loss after 9552 samples: 0.1822
Mean accuracy: 0.8697, std: 0.0075, lower bound: 0.8550, upper bound: 0.8829 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 9552 samples: 0.8697 with eval loss: 0.3419
Epoch 1/1, Loss after 9760 samples: 0.1899
Mean accuracy: 0.9351, std: 0.0056, lower bound: 0.9239, upper bound: 0.9452 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 9760 samples: 0.9351 with eval loss: 0.1420
Best model with eval loss 0.14200956098014308 and eval accuracy 0.9350912778904665 with 9760 samples seen is saved
Epoch 1/1, Loss after 9968 samples: 0.2079
Mean accuracy: 0.8800, std: 0.0074, lower bound: 0.8661, upper bound: 0.8940 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 9968 samples: 0.8798 with eval loss: 0.3003
Epoch 1/1, Loss after 10176 samples: 0.1644
Mean accuracy: 0.8873, std: 0.0073, lower bound: 0.8722, upper bound: 0.9011 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 10176 samples: 0.8874 with eval loss: 0.2704
Epoch 1/1, Loss after 10384 samples: 0.1829
Mean accuracy: 0.9107, std: 0.0064, lower bound: 0.8976, upper bound: 0.9229 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 10384 samples: 0.9108 with eval loss: 0.2048
Epoch 1/1, Loss after 10592 samples: 0.1292
Mean accuracy: 0.9059, std: 0.0066, lower bound: 0.8920, upper bound: 0.9189 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 10592 samples: 0.9057 with eval loss: 0.2193
Epoch 1/1, Loss after 10800 samples: 0.1120
Mean accuracy: 0.9257, std: 0.0057, lower bound: 0.9148, upper bound: 0.9366 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 10800 samples: 0.9255 with eval loss: 0.1764
Epoch 1/1, Loss after 11008 samples: 0.1734
Mean accuracy: 0.8640, std: 0.0078, lower bound: 0.8479, upper bound: 0.8788 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 11008 samples: 0.8641 with eval loss: 0.3733
Epoch 1/1, Loss after 11216 samples: 0.1168
Mean accuracy: 0.9355, std: 0.0057, lower bound: 0.9234, upper bound: 0.9468 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 11216 samples: 0.9356 with eval loss: 0.1423
Epoch 1/1, Loss after 11424 samples: 0.1793
Mean accuracy: 0.8691, std: 0.0074, lower bound: 0.8540, upper bound: 0.8844 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 11424 samples: 0.8692 with eval loss: 0.3395
Epoch 1/1, Loss after 11632 samples: 0.1314
Mean accuracy: 0.9354, std: 0.0055, lower bound: 0.9239, upper bound: 0.9457 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 11632 samples: 0.9356 with eval loss: 0.1477
Epoch 1/1, Loss after 11840 samples: 0.1094
Mean accuracy: 0.9405, std: 0.0052, lower bound: 0.9310, upper bound: 0.9503 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 11840 samples: 0.9407 with eval loss: 0.1346
Best model with eval loss 0.1346493862689503 and eval accuracy 0.9406693711967545 with 11840 samples seen is saved
Epoch 1/1, Loss after 12048 samples: 0.1427
Mean accuracy: 0.9203, std: 0.0062, lower bound: 0.9077, upper bound: 0.9321 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 12048 samples: 0.9204 with eval loss: 0.1928
Epoch 1/1, Loss after 12256 samples: 0.1929
Mean accuracy: 0.8634, std: 0.0078, lower bound: 0.8479, upper bound: 0.8783 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 12256 samples: 0.8636 with eval loss: 0.3702
Epoch 1/1, Loss after 12464 samples: 0.1332
Mean accuracy: 0.9184, std: 0.0063, lower bound: 0.9052, upper bound: 0.9300 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 12464 samples: 0.9184 with eval loss: 0.1881
Epoch 1/1, Loss after 12672 samples: 0.1391
Mean accuracy: 0.9124, std: 0.0066, lower bound: 0.8991, upper bound: 0.9249 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 12672 samples: 0.9123 with eval loss: 0.2015
Epoch 1/1, Loss after 12880 samples: 0.1423
Mean accuracy: 0.9047, std: 0.0067, lower bound: 0.8915, upper bound: 0.9173 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 12880 samples: 0.9047 with eval loss: 0.2083
Epoch 1/1, Loss after 13088 samples: 0.1135
Mean accuracy: 0.8815, std: 0.0070, lower bound: 0.8681, upper bound: 0.8945 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 13088 samples: 0.8818 with eval loss: 0.2939
Epoch 1/1, Loss after 13296 samples: 0.1343
Mean accuracy: 0.9380, std: 0.0053, lower bound: 0.9280, upper bound: 0.9483 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 13296 samples: 0.9381 with eval loss: 0.1390
Epoch 1/1, Loss after 13504 samples: 0.0892
Mean accuracy: 0.8702, std: 0.0075, lower bound: 0.8555, upper bound: 0.8854 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 13504 samples: 0.8702 with eval loss: 0.3573
Epoch 1/1, Loss after 13712 samples: 0.1646
Mean accuracy: 0.9071, std: 0.0066, lower bound: 0.8940, upper bound: 0.9204 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 13712 samples: 0.9072 with eval loss: 0.2280
Epoch 1/1, Loss after 13920 samples: 0.0687
Mean accuracy: 0.9007, std: 0.0069, lower bound: 0.8869, upper bound: 0.9143 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 13920 samples: 0.9006 with eval loss: 0.2500
Epoch 1/1, Loss after 14128 samples: 0.1700
Mean accuracy: 0.9255, std: 0.0058, lower bound: 0.9138, upper bound: 0.9366 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 14128 samples: 0.9255 with eval loss: 0.1657
Epoch 1/1, Loss after 14336 samples: 0.1579
Mean accuracy: 0.8999, std: 0.0069, lower bound: 0.8854, upper bound: 0.9128 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 14336 samples: 0.9001 with eval loss: 0.2418
Epoch 1/1, Loss after 14544 samples: 0.1005
Mean accuracy: 0.9278, std: 0.0059, lower bound: 0.9168, upper bound: 0.9397 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 14544 samples: 0.9280 with eval loss: 0.1636
Epoch 1/1, Loss after 14752 samples: 0.1553
Mean accuracy: 0.9113, std: 0.0063, lower bound: 0.8991, upper bound: 0.9234 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 14752 samples: 0.9118 with eval loss: 0.2058
Epoch 1/1, Loss after 14960 samples: 0.0904
Mean accuracy: 0.8893, std: 0.0068, lower bound: 0.8753, upper bound: 0.9021 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 14960 samples: 0.8895 with eval loss: 0.2740
Epoch 1/1, Loss after 15168 samples: 0.1239
Mean accuracy: 0.9036, std: 0.0066, lower bound: 0.8905, upper bound: 0.9168 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 15168 samples: 0.9037 with eval loss: 0.2425
Epoch 1/1, Loss after 15376 samples: 0.1903
Mean accuracy: 0.8985, std: 0.0068, lower bound: 0.8839, upper bound: 0.9113 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 15376 samples: 0.8986 with eval loss: 0.2564
Epoch 1/1, Loss after 15584 samples: 0.1577
Mean accuracy: 0.8970, std: 0.0069, lower bound: 0.8844, upper bound: 0.9097 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 15584 samples: 0.8971 with eval loss: 0.2575
Training signal is False, stopping training
Training finished
Best model: {'eval_acc': 0.9406693711967545, 'nb_samples': 11840, 'eval_loss': 0.1346493862689503}
Training loss logs: [{'samples': 192, 'loss': 0.6922771747295673}, {'samples': 400, 'loss': 0.6899578387920673}, {'samples': 608, 'loss': 0.6824599045973557}, {'samples': 816, 'loss': 0.60559815626878}, {'samples': 1024, 'loss': 0.47063262646014875}, {'samples': 1232, 'loss': 0.36904401045579177}, {'samples': 1440, 'loss': 0.40937796922830433}, {'samples': 1648, 'loss': 0.369892771427448}, {'samples': 1856, 'loss': 0.30786720605996937}, {'samples': 2064, 'loss': 0.23425532304323637}, {'samples': 2272, 'loss': 0.2587402371259836}, {'samples': 2480, 'loss': 0.2304502519277426}, {'samples': 2688, 'loss': 0.2196699197475727}, {'samples': 2896, 'loss': 0.2505368406956012}, {'samples': 3104, 'loss': 0.2425617277622223}, {'samples': 3312, 'loss': 0.24123363311474139}, {'samples': 3520, 'loss': 0.31315589868105376}, {'samples': 3728, 'loss': 0.22748772914593035}, {'samples': 3936, 'loss': 0.2875878856732295}, {'samples': 4144, 'loss': 0.30779492396574754}, {'samples': 4352, 'loss': 0.24991936867053693}, {'samples': 4560, 'loss': 0.20938257070688102}, {'samples': 4768, 'loss': 0.17445118152178252}, {'samples': 4976, 'loss': 0.19332300241176897}, {'samples': 5184, 'loss': 0.18205294471520644}, {'samples': 5392, 'loss': 0.21005657086005577}, {'samples': 5600, 'loss': 0.2096263628739577}, {'samples': 5808, 'loss': 0.2014777408196376}, {'samples': 6016, 'loss': 0.20382501528813288}, {'samples': 6224, 'loss': 0.21985347454364484}, {'samples': 6432, 'loss': 0.19846763977637658}, {'samples': 6640, 'loss': 0.14433469336766463}, {'samples': 6848, 'loss': 0.14332345013435072}, {'samples': 7056, 'loss': 0.16440637753559992}, {'samples': 7264, 'loss': 0.27411892207769245}, {'samples': 7472, 'loss': 0.12966174116501442}, {'samples': 7680, 'loss': 0.1896341832784506}, {'samples': 7888, 'loss': 0.1544282745856505}, {'samples': 8096, 'loss': 0.18144021928310394}, {'samples': 8304, 'loss': 0.166330031477488}, {'samples': 8512, 'loss': 0.10260624037339137}, {'samples': 8720, 'loss': 0.15165175612156206}, {'samples': 8928, 'loss': 0.18053745879576757}, {'samples': 9136, 'loss': 0.13261843873904303}, {'samples': 9344, 'loss': 0.08156647991675597}, {'samples': 9552, 'loss': 0.182182897741978}, {'samples': 9760, 'loss': 0.18993296646154845}, {'samples': 9968, 'loss': 0.20788892415853646}, {'samples': 10176, 'loss': 0.1643772847377337}, {'samples': 10384, 'loss': 0.1829472929239273}, {'samples': 10592, 'loss': 0.1292199337711701}, {'samples': 10800, 'loss': 0.11196801410271572}, {'samples': 11008, 'loss': 0.1734126221675139}, {'samples': 11216, 'loss': 0.1168299804513271}, {'samples': 11424, 'loss': 0.17933022460112205}, {'samples': 11632, 'loss': 0.13137724995613098}, {'samples': 11840, 'loss': 0.1094373819919733}, {'samples': 12048, 'loss': 0.14271611261826295}, {'samples': 12256, 'loss': 0.19290534808085516}, {'samples': 12464, 'loss': 0.13317797848811516}, {'samples': 12672, 'loss': 0.13906127902177665}, {'samples': 12880, 'loss': 0.14226701855659485}, {'samples': 13088, 'loss': 0.11349639067283043}, {'samples': 13296, 'loss': 0.13425246282265738}, {'samples': 13504, 'loss': 0.08915345428081659}, {'samples': 13712, 'loss': 0.16458135155531076}, {'samples': 13920, 'loss': 0.06866341771987769}, {'samples': 14128, 'loss': 0.1699667630287317}, {'samples': 14336, 'loss': 0.15792223983086073}, {'samples': 14544, 'loss': 0.10053927623308621}, {'samples': 14752, 'loss': 0.15531738560933334}, {'samples': 14960, 'loss': 0.09040444802779418}, {'samples': 15168, 'loss': 0.1239132800927529}, {'samples': 15376, 'loss': 0.19028834196237418}, {'samples': 15584, 'loss': 0.15774426551965567}]
Evaluation accuracy logs: [{'samples': 192, 'accuracy': 0.5379087221095336, 'std': 0.011335939665292762, 'lower_bound': 0.5152129817444219, 'upper_bound': 0.5608519269776876}, {'samples': 400, 'accuracy': 0.5352261663286004, 'std': 0.011194072265582182, 'lower_bound': 0.5136916835699797, 'upper_bound': 0.5573149087221095}, {'samples': 608, 'accuracy': 0.7448610547667344, 'std': 0.009539482318428608, 'lower_bound': 0.7261663286004056, 'upper_bound': 0.7621703853955375}, {'samples': 816, 'accuracy': 0.8286653144016227, 'std': 0.008362768788746793, 'lower_bound': 0.8123732251521298, 'upper_bound': 0.8453346855983773}, {'samples': 1024, 'accuracy': 0.8658676470588236, 'std': 0.007549242290379397, 'lower_bound': 0.8509127789046653, 'upper_bound': 0.879830121703854}, {'samples': 1232, 'accuracy': 0.7790233265720081, 'std': 0.009270648502525028, 'lower_bound': 0.7611561866125761, 'upper_bound': 0.7966531440162272}, {'samples': 1440, 'accuracy': 0.8805248478701826, 'std': 0.0073087251205762194, 'lower_bound': 0.8651115618661258, 'upper_bound': 0.8945233265720081}, {'samples': 1648, 'accuracy': 0.8971125760649087, 'std': 0.006842173985522304, 'lower_bound': 0.8843813387423936, 'upper_bound': 0.9107505070993914}, {'samples': 1856, 'accuracy': 0.8958093306288032, 'std': 0.006891715880027671, 'lower_bound': 0.8828600405679513, 'upper_bound': 0.90973630831643}, {'samples': 2064, 'accuracy': 0.8616602434077079, 'std': 0.00781690650943169, 'lower_bound': 0.845841784989858, 'upper_bound': 0.8757733265720081}, {'samples': 2272, 'accuracy': 0.8343463488843813, 'std': 0.008488235499516711, 'lower_bound': 0.815922920892495, 'upper_bound': 0.8509127789046653}, {'samples': 2480, 'accuracy': 0.8969705882352942, 'std': 0.006863955526637238, 'lower_bound': 0.8838742393509128, 'upper_bound': 0.90973630831643}, {'samples': 2688, 'accuracy': 0.8793138945233265, 'std': 0.007506320667160367, 'lower_bound': 0.8640973630831643, 'upper_bound': 0.8935218052738337}, {'samples': 2896, 'accuracy': 0.9139391480730223, 'std': 0.006470219743174742, 'lower_bound': 0.9016227180527383, 'upper_bound': 0.9264705882352942}, {'samples': 3104, 'accuracy': 0.8865796146044624, 'std': 0.0070036424791502586, 'lower_bound': 0.8722109533468559, 'upper_bound': 0.8996069979716025}, {'samples': 3312, 'accuracy': 0.9206201825557809, 'std': 0.00603134199463679, 'lower_bound': 0.9082150101419878, 'upper_bound': 0.9315542596348885}, {'samples': 3520, 'accuracy': 0.9332875253549695, 'std': 0.005861216814295333, 'lower_bound': 0.920892494929006, 'upper_bound': 0.9447261663286004}, {'samples': 3728, 'accuracy': 0.9225720081135902, 'std': 0.005991173956251009, 'lower_bound': 0.9107378296146044, 'upper_bound': 0.934077079107505}, {'samples': 3936, 'accuracy': 0.9228265720081137, 'std': 0.005849461357597874, 'lower_bound': 0.9112576064908722, 'upper_bound': 0.9335826572008113}, {'samples': 4144, 'accuracy': 0.9183301217038539, 'std': 0.006228910350849052, 'lower_bound': 0.9056795131845842, 'upper_bound': 0.9305273833671399}, {'samples': 4352, 'accuracy': 0.9054802231237322, 'std': 0.006678030200818353, 'lower_bound': 0.8924949290060852, 'upper_bound': 0.9183569979716024}, {'samples': 4560, 'accuracy': 0.8933362068965516, 'std': 0.007006981863660451, 'lower_bound': 0.8793103448275862, 'upper_bound': 0.9072008113590264}, {'samples': 4768, 'accuracy': 0.8895583164300204, 'std': 0.0069850860105282, 'lower_bound': 0.8762550709939148, 'upper_bound': 0.9026369168356998}, {'samples': 4976, 'accuracy': 0.9139624746450304, 'std': 0.006360822476690698, 'lower_bound': 0.902117139959432, 'upper_bound': 0.9259634888438134}, {'samples': 5184, 'accuracy': 0.9267545638945233, 'std': 0.005978880318438299, 'lower_bound': 0.915314401622718, 'upper_bound': 0.938146551724138}, {'samples': 5392, 'accuracy': 0.9159056795131846, 'std': 0.0063102899771564904, 'lower_bound': 0.9031440162271805, 'upper_bound': 0.928498985801217}, {'samples': 5600, 'accuracy': 0.8743640973630832, 'std': 0.007468551229979624, 'lower_bound': 0.8595334685598377, 'upper_bound': 0.8889579107505071}, {'samples': 5808, 'accuracy': 0.8902216024340771, 'std': 0.007363525134255172, 'lower_bound': 0.8762677484787018, 'upper_bound': 0.9051724137931034}, {'samples': 6016, 'accuracy': 0.9108291075050711, 'std': 0.006242164362731522, 'lower_bound': 0.8980730223123732, 'upper_bound': 0.9224137931034483}, {'samples': 6224, 'accuracy': 0.8993904665314401, 'std': 0.006681299497074422, 'lower_bound': 0.8864097363083164, 'upper_bound': 0.9122718052738337}, {'samples': 6432, 'accuracy': 0.9127672413793104, 'std': 0.006088202243447075, 'lower_bound': 0.9005958417849899, 'upper_bound': 0.9239477687626775}, {'samples': 6640, 'accuracy': 0.8309061866125761, 'std': 0.008096898890359071, 'lower_bound': 0.8144016227180527, 'upper_bound': 0.845867139959432}, {'samples': 6848, 'accuracy': 0.9221196754563894, 'std': 0.005992052579473781, 'lower_bound': 0.9102434077079108, 'upper_bound': 0.9335699797160243}, {'samples': 7056, 'accuracy': 0.9250177484787019, 'std': 0.005980565523194499, 'lower_bound': 0.9132860040567952, 'upper_bound': 0.9366125760649088}, {'samples': 7264, 'accuracy': 0.8529163286004058, 'std': 0.008073630665382992, 'lower_bound': 0.8372210953346856, 'upper_bound': 0.8691683569979716}, {'samples': 7472, 'accuracy': 0.9104528397565923, 'std': 0.006633015715928116, 'lower_bound': 0.8975659229208925, 'upper_bound': 0.9234279918864098}, {'samples': 7680, 'accuracy': 0.927168864097363, 'std': 0.005774587969216521, 'lower_bound': 0.9158215010141988, 'upper_bound': 0.9381338742393509}, {'samples': 7888, 'accuracy': 0.9404822515212982, 'std': 0.005218613006269258, 'lower_bound': 0.9305273833671399, 'upper_bound': 0.9508113590263692}, {'samples': 8096, 'accuracy': 0.8929107505070994, 'std': 0.006628074211178548, 'lower_bound': 0.8803245436105477, 'upper_bound': 0.9061866125760649}, {'samples': 8304, 'accuracy': 0.8946764705882354, 'std': 0.007134670313583094, 'lower_bound': 0.8803118661257606, 'upper_bound': 0.9082276876267749}, {'samples': 8512, 'accuracy': 0.9197545638945233, 'std': 0.005889047739548427, 'lower_bound': 0.9082023326572007, 'upper_bound': 0.9310344827586207}, {'samples': 8720, 'accuracy': 0.9358316430020284, 'std': 0.005454852856772021, 'lower_bound': 0.9244421906693712, 'upper_bound': 0.9462474645030426}, {'samples': 8928, 'accuracy': 0.8227829614604463, 'std': 0.00865782503913587, 'lower_bound': 0.8057682555780933, 'upper_bound': 0.8397565922920892}, {'samples': 9136, 'accuracy': 0.8995218052738337, 'std': 0.006924730475592945, 'lower_bound': 0.8869041582150101, 'upper_bound': 0.9127789046653144}, {'samples': 9344, 'accuracy': 0.9065015212981743, 'std': 0.0064121782448538, 'lower_bound': 0.8940162271805274, 'upper_bound': 0.9188640973630832}, {'samples': 9552, 'accuracy': 0.8697165314401623, 'std': 0.0074751754877951165, 'lower_bound': 0.8549695740365112, 'upper_bound': 0.8828600405679513}, {'samples': 9760, 'accuracy': 0.9350745436105476, 'std': 0.005606967289643394, 'lower_bound': 0.9239350912778904, 'upper_bound': 0.9452459432048682}, {'samples': 9968, 'accuracy': 0.8799553752535496, 'std': 0.007355773672801077, 'lower_bound': 0.8661130831643001, 'upper_bound': 0.8940289046653144}, {'samples': 10176, 'accuracy': 0.887331135902637, 'std': 0.00730260998496582, 'lower_bound': 0.8722109533468559, 'upper_bound': 0.9011282961460446}, {'samples': 10384, 'accuracy': 0.9106744421906694, 'std': 0.00644629844550207, 'lower_bound': 0.8975659229208925, 'upper_bound': 0.922920892494929}, {'samples': 10592, 'accuracy': 0.9058930020283976, 'std': 0.006577916901979762, 'lower_bound': 0.8919751521298174, 'upper_bound': 0.9188640973630832}, {'samples': 10800, 'accuracy': 0.9257195740365112, 'std': 0.005687377718307326, 'lower_bound': 0.9147946247464502, 'upper_bound': 0.9366125760649088}, {'samples': 11008, 'accuracy': 0.8640314401622718, 'std': 0.00782701134611717, 'lower_bound': 0.847870182555781, 'upper_bound': 0.8788159229208925}, {'samples': 11216, 'accuracy': 0.9354822515212982, 'std': 0.005680612374617373, 'lower_bound': 0.9234279918864098, 'upper_bound': 0.9467545638945233}, {'samples': 11424, 'accuracy': 0.8691303245436106, 'std': 0.007437531453242818, 'lower_bound': 0.8539553752535497, 'upper_bound': 0.8843813387423936}, {'samples': 11632, 'accuracy': 0.9354442190669371, 'std': 0.005518061253364773, 'lower_bound': 0.9239350912778904, 'upper_bound': 0.9457403651115619}, {'samples': 11840, 'accuracy': 0.9405015212981743, 'std': 0.005202143782851563, 'lower_bound': 0.9310218052738336, 'upper_bound': 0.9503042596348884}, {'samples': 12048, 'accuracy': 0.9203098377281947, 'std': 0.006247801990798847, 'lower_bound': 0.90769523326572, 'upper_bound': 0.9320613590263692}, {'samples': 12256, 'accuracy': 0.8633995943204869, 'std': 0.0077964222495865735, 'lower_bound': 0.847870182555781, 'upper_bound': 0.8783088235294118}, {'samples': 12464, 'accuracy': 0.9183549695740364, 'std': 0.006296328362513689, 'lower_bound': 0.9051724137931034, 'upper_bound': 0.9300202839756593}, {'samples': 12672, 'accuracy': 0.9124066937119675, 'std': 0.006572627168274563, 'lower_bound': 0.8990872210953347, 'upper_bound': 0.9249492900608519}, {'samples': 12880, 'accuracy': 0.9046582150101419, 'std': 0.006711892989814182, 'lower_bound': 0.8914807302231237, 'upper_bound': 0.9173427991886409}, {'samples': 13088, 'accuracy': 0.881460953346856, 'std': 0.007031311484532367, 'lower_bound': 0.8681414807302231, 'upper_bound': 0.8945360040567951}, {'samples': 13296, 'accuracy': 0.938038032454361, 'std': 0.005262784859959957, 'lower_bound': 0.9279792089249492, 'upper_bound': 0.9482758620689655}, {'samples': 13504, 'accuracy': 0.8701911764705883, 'std': 0.007515149867793331, 'lower_bound': 0.8554766734279919, 'upper_bound': 0.8853955375253549}, {'samples': 13712, 'accuracy': 0.9070689655172414, 'std': 0.0065764437364059, 'lower_bound': 0.8940162271805274, 'upper_bound': 0.9203853955375254}, {'samples': 13920, 'accuracy': 0.9007221095334684, 'std': 0.006945666597067238, 'lower_bound': 0.8869168356997972, 'upper_bound': 0.9143128803245436}, {'samples': 14128, 'accuracy': 0.9255466531440162, 'std': 0.005824457960186117, 'lower_bound': 0.9137931034482759, 'upper_bound': 0.9366125760649088}, {'samples': 14336, 'accuracy': 0.8999437119675456, 'std': 0.006878112635976239, 'lower_bound': 0.8853955375253549, 'upper_bound': 0.9127789046653144}, {'samples': 14544, 'accuracy': 0.9277839756592292, 'std': 0.005887332210972615, 'lower_bound': 0.9168230223123732, 'upper_bound': 0.9396551724137931}, {'samples': 14752, 'accuracy': 0.9113326572008114, 'std': 0.006271039985952946, 'lower_bound': 0.8990872210953347, 'upper_bound': 0.9234279918864098}, {'samples': 14960, 'accuracy': 0.8893260649087221, 'std': 0.006836124855871563, 'lower_bound': 0.8752535496957403, 'upper_bound': 0.902129817444219}, {'samples': 15168, 'accuracy': 0.903552738336714, 'std': 0.006635495115829472, 'lower_bound': 0.8904538539553752, 'upper_bound': 0.9168356997971603}, {'samples': 15376, 'accuracy': 0.8984984787018255, 'std': 0.006841616136305642, 'lower_bound': 0.8838742393509128, 'upper_bound': 0.9112576064908722}, {'samples': 15584, 'accuracy': 0.8970076064908722, 'std': 0.006856778863812844, 'lower_bound': 0.8843813387423936, 'upper_bound': 0.9097489858012171}]
Evaluating the best model on the test set of dataset ./fake_true_datasets/fake_true_dataset_mistral_10k...
Test metrics:
accuracy: 0.9012890466531439
precision: 0.8383817816382488
recall: 0.9938777973734438
f1_score: 0.90949496004805
fp_rate: 0.19103120346045332
tp_rate: 0.9938777973734438
std_accuracy: 0.006515527385253673
std_precision: 0.010512668757731053
std_recall: 0.002518139602623821
std_f1_score: 0.006296053253392812
std_fp_rate: 0.012142020406187459
std_tp_rate: 0.002518139602623821
TP: 978.531
TN: 798.811
FP: 188.63
FN: 6.028
roc_auc: 0.991584515879514
fpr: [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.0010142  0.0010142
 0.0010142  0.0010142  0.0010142  0.0010142  0.0010142  0.0010142
 0.0010142  0.0010142  0.0010142  0.0010142  0.0010142  0.0010142
 0.0010142  0.0010142  0.0010142  0.0010142  0.0010142  0.0010142
 0.0010142  0.0010142  0.0010142  0.0010142  0.0010142  0.0010142
 0.0010142  0.0010142  0.0020284  0.0020284  0.0020284  0.0020284
 0.0020284  0.0030426  0.0030426  0.0030426  0.0030426  0.0030426
 0.0030426  0.0030426  0.0030426  0.0030426  0.0030426  0.0030426
 0.0030426  0.0030426  0.0030426  0.0030426  0.0030426  0.0030426
 0.0030426  0.0030426  0.0030426  0.0030426  0.0030426  0.0030426
 0.0030426  0.0030426  0.0030426  0.0030426  0.0030426  0.0030426
 0.0030426  0.0030426  0.0030426  0.0030426  0.0030426  0.0030426
 0.00507099 0.00507099 0.00608519 0.00608519 0.00608519 0.00608519
 0.00608519 0.00608519 0.00608519 0.00608519 0.00709939 0.00709939
 0.00811359 0.00811359 0.00912779 0.01014199 0.01014199 0.01115619
 0.01115619 0.01217039 0.01318458 0.01318458 0.01318458 0.01318458
 0.01318458 0.01318458 0.01318458 0.01318458 0.01419878 0.01419878
 0.01419878 0.01419878 0.01521298 0.01521298 0.01521298 0.01724138
 0.01724138 0.01724138 0.01724138 0.01825558 0.01825558 0.01926978
 0.01926978 0.02028398 0.02028398 0.02028398 0.02028398 0.02129817
 0.02129817 0.02231237 0.02231237 0.02231237 0.02231237 0.02332657
 0.02332657 0.02332657 0.02332657 0.02434077 0.02434077 0.02535497
 0.02535497 0.02738337 0.02738337 0.02839757 0.02839757 0.02839757
 0.02941176 0.02941176 0.03042596 0.03042596 0.03144016 0.03144016
 0.03245436 0.03245436 0.03245436 0.03448276 0.03448276 0.03651116
 0.03651116 0.03853955 0.03853955 0.04361055 0.04361055 0.04665314
 0.04665314 0.04766734 0.04766734 0.05273834 0.05273834 0.05578093
 0.05780933 0.05882353 0.05882353 0.06186613 0.06186613 0.06389452
 0.06389452 0.06795132 0.06795132 0.07099391 0.07099391 0.07505071
 0.07505071 0.07809331 0.07809331 0.0872211  0.0872211  0.08924949
 0.08924949 0.09229209 0.09229209 0.09736308 0.09736308 0.09939148
 0.09939148 0.10953347 0.10953347 0.11359026 0.11359026 0.12170385
 0.12373225 0.12778905 0.12778905 0.12880325 0.12880325 0.13286004
 0.13286004 0.13995943 0.14198783 0.14300203 0.14300203 0.15922921
 0.15922921 0.17849899 0.17849899 0.20081136 0.20081136 0.22109533
 0.22109533 0.2464503  0.2484787  0.26572008 0.26572008 0.26977688
 0.27180527 0.27484787 0.27687627 0.31744422 0.32150101 0.32758621
 0.3296146  0.33772819 0.33772819 0.34888438 0.35091278 0.36206897
 0.36409736 0.38235294 0.38438134 0.39452333 0.39756592 0.40973631
 0.41176471 0.4137931  0.4158215  0.42900609 0.43103448 0.45436105
 0.45841785 0.46450304 0.46653144 0.46957404 0.47160243 0.47261663
 0.47464503 0.47971602 0.48174442 0.48580122 0.48782961 0.50912779
 0.51115619 0.52738337 0.52941176 0.53346856 0.53549696 0.54361055
 0.54361055 0.55780933 0.55983773 0.56389452 0.56592292 0.56693712
 0.56896552 0.56997972 0.57302231 0.57606491 0.57809331 0.5811359
 0.5831643  0.5841785  0.5862069  0.5872211  0.58924949 0.59330629
 0.59533469 0.59634888 0.60040568 0.60344828 0.60547667 0.61054767
 0.61257606 0.61561866 0.61967546 0.62373225 0.62576065 0.63184584
 0.63387424 0.63488844 0.63691684 0.63995943 0.64807302 0.65111562
 0.65314402 0.65720081 0.65922921 0.66024341 0.66227181 0.6693712
 0.67139959 0.67444219 0.67545639 0.67748479 0.67849899 0.68154158
 0.68356998 0.68458418 0.68762677 0.68965517 0.69168357 0.69776876
 0.70081136 0.70385396 0.70588235 0.70689655 0.70892495 0.70993915
 0.71196755 0.71298174 0.71501014 0.72109533 0.72210953 0.72413793
 0.72718053 0.73326572 0.73732252 0.73935091 0.74137931 0.74340771
 0.7464503  0.7484787  0.7515213  0.75456389 0.75760649 0.76166329
 0.76267748 0.76673428 0.76977688 0.77586207 0.77789047 0.78904665
 0.79107505 0.79208925 0.79411765 0.79614604 0.79817444 0.80223124
 0.80425963 0.80628803 0.80933063 0.81744422 0.81947262 0.82454361
 0.82657201 0.8306288  0.8326572  0.8356998  0.83874239 0.84077079
 0.84381339 0.84584178 0.84787018 0.84888438 0.85091278 0.85192698
 0.85395538 0.86511156 0.86815416 0.87018256 0.87221095 0.87525355
 0.87931034 0.88133874 0.88336714 0.88945233 0.89249493 0.89452333
 0.90060852 0.90263692 0.90466531 0.90669371 0.90872211 0.90973631
 0.91176471 0.9168357  0.92089249 0.92190669 0.92596349 0.92697769
 0.92900609 0.93002028 0.93204868 0.93509128 0.93711968 0.93914807
 0.94117647 0.94320487 0.94726166 0.95233266 0.95841785 0.95943205
 0.96247465 0.96653144 0.96957404 0.97160243 0.97261663 0.97464503
 0.99290061 0.99492901 1.        ]
tpr: [0.         0.0030426  0.00709939 0.00811359 0.01318458 0.01419878
 0.01724138 0.02129817 0.02535497 0.03042596 0.03448276 0.04056795
 0.06490872 0.07200811 0.0811359  0.08823529 0.09330629 0.09736308
 0.10750507 0.11156187 0.11764706 0.12271805 0.12981744 0.14503043
 0.16125761 0.168357   0.17241379 0.18052738 0.19269777 0.20081136
 0.20283976 0.20791075 0.21399594 0.21906694 0.22515213 0.22819473
 0.22920892 0.23326572 0.23935091 0.24137931 0.24442191 0.2525355
 0.25456389 0.25557809 0.25760649 0.26166329 0.26267748 0.26673428
 0.26876268 0.27789047 0.27991886 0.28093306 0.28701826 0.28803245
 0.29006085 0.29107505 0.29310345 0.29716024 0.30121704 0.30223124
 0.30425963 0.30730223 0.30933063 0.31135903 0.31440162 0.31643002
 0.31845842 0.31947262 0.32150101 0.32454361 0.32657201 0.32758621
 0.3326572  0.34279919 0.34381339 0.34584178 0.34787018 0.34989858
 0.35294118 0.35496957 0.36308316 0.36511156 0.36713996 0.36916836
 0.37931034 0.38133874 0.38843813 0.39046653 0.39350913 0.39553753
 0.39858012 0.40060852 0.40365112 0.40567951 0.40973631 0.41176471
 0.4168357  0.42292089 0.42900609 0.43103448 0.43204868 0.43509128
 0.43610548 0.44016227 0.44117647 0.44320487 0.44523327 0.44827586
 0.45030426 0.45233266 0.45841785 0.46044625 0.46247465 0.46450304
 0.46551724 0.47160243 0.47363083 0.47565923 0.47667343 0.47971602
 0.48174442 0.48275862 0.48478702 0.48681542 0.48884381 0.49188641
 0.49391481 0.4959432  0.4979716  0.4989858  0.5010142  0.50608519
 0.50811359 0.51419878 0.51724138 0.51926978 0.52129817 0.52535497
 0.52636917 0.52839757 0.53144016 0.53346856 0.53448276 0.54056795
 0.54259635 0.54361055 0.54665314 0.55273834 0.55679513 0.55882353
 0.56186613 0.56490872 0.56693712 0.56896552 0.57200811 0.57505071
 0.57910751 0.5821501  0.5841785  0.58823529 0.59229209 0.59432049
 0.59634888 0.59736308 0.60040568 0.60344828 0.60344828 0.60547667
 0.60649087 0.61257606 0.61460446 0.61764706 0.62170385 0.62576065
 0.63488844 0.63691684 0.63793103 0.63995943 0.64198783 0.65010142
 0.65111562 0.65922921 0.66227181 0.6643002  0.6653144  0.6673428
 0.6703854  0.67139959 0.67444219 0.67951318 0.68255578 0.68458418
 0.68559838 0.68762677 0.68864097 0.69066937 0.69269777 0.69371197
 0.69675456 0.69675456 0.69878296 0.69979716 0.70182556 0.70486815
 0.70892495 0.70993915 0.71805274 0.71906694 0.72109533 0.72210953
 0.72616633 0.72920892 0.73123732 0.73225152 0.73427992 0.73529412
 0.73732252 0.7474645  0.7515213  0.7535497  0.75963489 0.76572008
 0.76774848 0.77180527 0.77383367 0.77991886 0.78194726 0.78397566
 0.78803245 0.79107505 0.79411765 0.79614604 0.79817444 0.80121704
 0.80121704 0.80324544 0.80324544 0.80831643 0.81034483 0.81237323
 0.81541582 0.81845842 0.82048682 0.82555781 0.82555781 0.82657201
 0.82657201 0.82860041 0.82860041 0.8306288  0.8326572  0.8326572
 0.836714   0.83772819 0.83772819 0.83975659 0.84279919 0.84482759
 0.84989858 0.85192698 0.85801217 0.86004057 0.86004057 0.86206897
 0.86409736 0.86511156 0.86511156 0.87119675 0.88032454 0.88032454
 0.88438134 0.88640974 0.88843813 0.88843813 0.89046653 0.89046653
 0.89249493 0.89249493 0.89655172 0.89858012 0.90060852 0.90060852
 0.90263692 0.90365112 0.91176471 0.9137931  0.9148073  0.9158215
 0.9168357  0.9188641  0.92089249 0.92089249 0.92292089 0.92393509
 0.92494929 0.92494929 0.93103448 0.93103448 0.93306288 0.93509128
 0.93509128 0.93711968 0.93711968 0.93914807 0.93914807 0.94016227
 0.94016227 0.94219067 0.94523327 0.94523327 0.94726166 0.94726166
 0.95233266 0.95233266 0.95638945 0.95638945 0.96044625 0.96044625
 0.96146045 0.96146045 0.96247465 0.96247465 0.96348884 0.96348884
 0.96348884 0.96348884 0.96450304 0.96450304 0.96653144 0.96653144
 0.97160243 0.97160243 0.97261663 0.97261663 0.97464503 0.97464503
 0.97565923 0.97565923 0.97667343 0.97667343 0.97768763 0.97768763
 0.97971602 0.97971602 0.98174442 0.98174442 0.98377282 0.98377282
 0.98478702 0.98478702 0.98580122 0.98580122 0.98681542 0.98681542
 0.98681542 0.98681542 0.98782961 0.98782961 0.98884381 0.98884381
 0.98985801 0.98985801 0.98985801 0.98985801 0.99087221 0.99087221
 0.99188641 0.99188641 0.99391481 0.99391481 0.99492901 0.99492901
 0.9959432  0.9959432  0.9959432  0.9959432  0.9969574  0.9969574
 0.9969574  0.9969574  0.9969574  0.9969574  0.9969574  0.9969574
 0.9969574  0.9969574  0.9989858  0.9989858  0.9989858  0.9989858
 0.9989858  0.9989858  0.9989858  0.9989858  0.9989858  0.9989858
 0.9989858  0.9989858  0.9989858  0.9989858  0.9989858  0.9989858
 0.9989858  0.9989858  0.9989858  0.9989858  0.9989858  0.9989858
 0.9989858  0.9989858  0.9989858  0.9989858  0.9989858  0.9989858
 0.9989858  0.9989858  0.9989858  0.9989858  0.9989858  0.9989858
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.        ]
thresholds: [        inf  4.4570312   4.4453125   4.4414062   4.4375      4.4335938
  4.4296875   4.4257812   4.4179688   4.4140625   4.4101562   4.40625
  4.3945312   4.390625    4.3867188   4.3828125   4.3789062   4.375
  4.3710938   4.3671875   4.3632812   4.359375    4.3554688   4.34375
  4.3359375   4.3320312   4.328125    4.3242188   4.3125      4.3085938
  4.3046875   4.3007812   4.2929688   4.2890625   4.2851562   4.28125
  4.2773438   4.2734375   4.265625    4.2578125   4.2539062   4.2460938
  4.2421875   4.2382812   4.234375    4.2304688   4.2226562   4.21875
  4.2148438   4.1992188   4.1953125   4.1914062   4.1796875   4.171875
  4.1679688   4.1640625   4.1601562   4.1328125   4.1210938   4.1171875
  4.109375    4.0898438   4.0859375   4.078125    4.0742188   4.0585938
  4.0546875   4.0507812   4.046875    4.0273438   4.0195312   4.0117188
  4.0078125   3.9765625   3.9726562   3.9648438   3.9609375   3.9492188
  3.9414062   3.9394531   3.8964844   3.8730469   3.8632812   3.859375
  3.7929688   3.7851562   3.7558594   3.7441406   3.734375    3.7265625
  3.7011719   3.6933594   3.6816406   3.6796875   3.6660156   3.6640625
  3.640625    3.6308594   3.6171875   3.6132812   3.6113281   3.609375
  3.6054688   3.5996094   3.5976562   3.5957031   3.5917969   3.5859375
  3.5800781   3.578125    3.5429688   3.5351562   3.5292969   3.5273438
  3.5195312   3.5117188   3.5058594   3.5039062   3.5019531   3.4941406
  3.4902344   3.4804688   3.4785156   3.4746094   3.4726562   3.4648438
  3.4628906   3.4589844   3.4570312   3.4550781   3.4511719   3.4316406
  3.4296875   3.4179688   3.4023438   3.4003906   3.3964844   3.390625
  3.3886719   3.3847656   3.3730469   3.3691406   3.3652344   3.3554688
  3.3535156   3.3515625   3.3457031   3.3359375   3.3222656   3.3144531
  3.3085938   3.3066406   3.296875    3.2949219   3.2832031   3.28125
  3.2734375   3.265625    3.2617188   3.2382812   3.2363281   3.2304688
  3.2285156   3.2246094   3.2226562   3.2128906   3.2070312   3.2050781
  3.203125    3.1972656   3.1914062   3.1875      3.1757812   3.1640625
  3.1328125   3.1308594   3.1289062   3.1269531   3.1171875   3.1132812
  3.1074219   3.0800781   3.0742188   3.0722656   3.0703125   3.0683594
  3.0625      3.0566406   3.046875    3.0253906   3.0175781   3.0136719
  3.0117188   3.0078125   3.0058594   3.          2.9980469   2.9960938
  2.9921875   2.9902344   2.9863281   2.9824219   2.9804688   2.9785156
  2.96875     2.9667969   2.9453125   2.9394531   2.9375      2.9335938
  2.9277344   2.9199219   2.9179688   2.9140625   2.9121094   2.9042969
  2.9003906   2.8496094   2.84375     2.8398438   2.8339844   2.7988281
  2.7929688   2.78125     2.7773438   2.7402344   2.7363281   2.7324219
  2.7304688   2.7207031   2.71875     2.7109375   2.7050781   2.6992188
  2.6933594   2.6875      2.6816406   2.6601562   2.6542969   2.6503906
  2.6445312   2.6328125   2.6289062   2.6191406   2.6152344   2.609375
  2.6074219   2.59375     2.5898438   2.5859375   2.5703125   2.5683594
  2.5371094   2.5332031   2.5234375   2.5117188   2.5         2.4980469
  2.4785156   2.4726562   2.4511719   2.4375      2.4335938   2.4160156
  2.4121094   2.4101562   2.40625     2.3886719   2.3105469   2.3007812
  2.2851562   2.2773438   2.2539062   2.2421875   2.2246094   2.2226562
  2.21875     2.2109375   2.1855469   2.1777344   2.1738281   2.1621094
  2.1582031   2.1464844   2.0957031   2.0839844   2.0800781   2.0742188
  2.0625      2.0605469   2.0546875   2.015625    2.0117188   2.0058594
  1.9921875   1.9853516   1.9404297   1.9306641   1.9257812   1.9169922
  1.9111328   1.8818359   1.8730469   1.8359375   1.8251953   1.7919922
  1.7910156   1.7792969   1.7294922   1.7216797   1.6972656   1.6865234
  1.6464844   1.6152344   1.5830078   1.5537109   1.4716797   1.3945312
  1.390625    1.3896484   1.3867188   1.3613281   1.3603516   1.3095703
  1.3017578   1.2929688   1.2900391   1.2626953   1.2460938   1.2080078
  1.1699219   1.1523438   1.1308594   1.1025391   1.0859375   1.0146484
  1.0126953   0.97509766  0.9707031   0.8496094   0.84814453  0.8198242
  0.8178711   0.80029297  0.7915039   0.7548828   0.7504883   0.73339844
  0.7290039   0.6894531   0.66064453  0.62353516  0.6201172   0.5336914
  0.52441406  0.4892578   0.48388672  0.4794922   0.46826172  0.43310547
  0.43286133  0.3779297   0.37231445  0.37036133  0.36914062  0.28442383
  0.28125     0.11102295  0.10272217 -0.03662109 -0.03826904 -0.22167969
 -0.22192383 -0.42260742 -0.4260254  -0.55615234 -0.5654297  -0.5756836
 -0.5878906  -0.60595703 -0.62353516 -0.875      -0.87939453 -0.9321289
 -0.9326172  -0.9550781  -0.96435547 -1.0761719  -1.0800781  -1.1533203
 -1.1542969  -1.2705078  -1.2773438  -1.3349609  -1.3359375  -1.4277344
 -1.4296875  -1.4365234  -1.4423828  -1.5029297  -1.5048828  -1.6298828
 -1.6347656  -1.6503906  -1.6689453  -1.6728516  -1.6835938  -1.6855469
 -1.6865234  -1.7041016  -1.7060547  -1.7197266  -1.734375   -1.8369141
 -1.84375    -1.890625   -1.8925781  -1.9101562  -1.9121094  -1.9365234
 -1.9443359  -2.0058594  -2.0078125  -2.0253906  -2.0292969  -2.0332031
 -2.0488281  -2.0546875  -2.0566406  -2.0644531  -2.0683594  -2.0742188
 -2.0800781  -2.0917969  -2.0957031  -2.0996094  -2.1015625  -2.1328125
 -2.1367188  -2.1386719  -2.1621094  -2.1816406  -2.1875     -2.1992188
 -2.203125   -2.2128906  -2.2324219  -2.2578125  -2.2636719  -2.2890625
 -2.2929688  -2.2949219  -2.296875   -2.3300781  -2.3417969  -2.34375
 -2.3457031  -2.3574219  -2.3613281  -2.3632812  -2.3671875  -2.3808594
 -2.3867188  -2.3886719  -2.3925781  -2.3945312  -2.3964844  -2.4003906
 -2.4023438  -2.4042969  -2.4082031  -2.4179688  -2.421875   -2.4472656
 -2.4511719  -2.4570312  -2.4628906  -2.4648438  -2.46875    -2.4707031
 -2.4765625  -2.4785156  -2.4804688  -2.4863281  -2.4902344  -2.4921875
 -2.5        -2.5019531  -2.5195312  -2.5214844  -2.5253906  -2.53125
 -2.5410156  -2.5429688  -2.5488281  -2.5546875  -2.5742188  -2.5820312
 -2.5839844  -2.5898438  -2.5917969  -2.6230469  -2.625      -2.6679688
 -2.6699219  -2.671875   -2.6738281  -2.6835938  -2.6875     -2.703125
 -2.7089844  -2.7148438  -2.71875    -2.7402344  -2.75       -2.7734375
 -2.7890625  -2.8046875  -2.8085938  -2.8105469  -2.8242188  -2.8300781
 -2.8339844  -2.8476562  -2.8496094  -2.8515625  -2.859375   -2.8613281
 -2.8632812  -2.9003906  -2.9023438  -2.9082031  -2.9121094  -2.9296875
 -2.9335938  -2.9433594  -2.9492188  -2.9667969  -2.9707031  -2.9785156
 -2.9882812  -2.9921875  -2.9960938  -3.0039062  -3.0058594  -3.0078125
 -3.0097656  -3.0253906  -3.03125    -3.0371094  -3.0410156  -3.0429688
 -3.0488281  -3.0507812  -3.0527344  -3.0625     -3.0703125  -3.0761719
 -3.0820312  -3.0878906  -3.0957031  -3.1113281  -3.1191406  -3.1210938
 -3.1289062  -3.140625   -3.1503906  -3.1542969  -3.1582031  -3.1621094
 -3.2890625  -3.3007812  -3.4394531 ]
