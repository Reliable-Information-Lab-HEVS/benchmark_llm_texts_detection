log_loss_steps: 208
eval_steps: 208
check_degradation: 0
----------------Epoch 1/1----------------
Epoch 1/1, Loss after 192 samples: 0.6929
Mean accuracy: 0.5002, std: 0.0111, lower bound: 0.4788, upper bound: 0.5228 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 192 samples: 0.5000 with eval loss: 0.6918
Best model with eval loss 0.6918257681592819 and eval accuracy 0.5 with 192 samples seen is saved
Epoch 1/1, Loss after 400 samples: 0.6938
Mean accuracy: 0.5338, std: 0.0111, lower bound: 0.5116, upper bound: 0.5561 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 400 samples: 0.5344 with eval loss: 0.6870
Best model with eval loss 0.6869597896452873 and eval accuracy 0.5343781597573306 with 400 samples seen is saved
Epoch 1/1, Loss after 608 samples: 0.6858
Mean accuracy: 0.6904, std: 0.0100, lower bound: 0.6704, upper bound: 0.7098 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 608 samples: 0.6901 with eval loss: 0.6722
Best model with eval loss 0.6721925797962374 and eval accuracy 0.6900910010111223 with 608 samples seen is saved
Epoch 1/1, Loss after 816 samples: 0.6324
Mean accuracy: 0.8345, std: 0.0088, lower bound: 0.8175, upper bound: 0.8524 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 816 samples: 0.8347 with eval loss: 0.4876
Best model with eval loss 0.48758249585666963 and eval accuracy 0.8346814964610718 with 816 samples seen is saved
Epoch 1/1, Loss after 1024 samples: 0.4105
Mean accuracy: 0.8596, std: 0.0080, lower bound: 0.8438, upper bound: 0.8761 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 1024 samples: 0.8600 with eval loss: 0.3326
Best model with eval loss 0.3325690362722643 and eval accuracy 0.8599595551061678 with 1024 samples seen is saved
Epoch 1/1, Loss after 1232 samples: 0.3506
Mean accuracy: 0.8166, std: 0.0086, lower bound: 0.8008, upper bound: 0.8337 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 1232 samples: 0.8165 with eval loss: 0.4035
Epoch 1/1, Loss after 1440 samples: 0.3018
Mean accuracy: 0.8909, std: 0.0069, lower bound: 0.8787, upper bound: 0.9050 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 1440 samples: 0.8908 with eval loss: 0.2528
Best model with eval loss 0.25279026478528976 and eval accuracy 0.890798786653185 with 1440 samples seen is saved
Epoch 1/1, Loss after 1648 samples: 0.3031
Mean accuracy: 0.9149, std: 0.0066, lower bound: 0.9024, upper bound: 0.9272 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 1648 samples: 0.9151 with eval loss: 0.2105
Best model with eval loss 0.21050376108577173 and eval accuracy 0.9150657229524772 with 1648 samples seen is saved
Epoch 1/1, Loss after 1856 samples: 0.2959
Mean accuracy: 0.8172, std: 0.0088, lower bound: 0.8013, upper bound: 0.8342 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 1856 samples: 0.8175 with eval loss: 0.3613
Epoch 1/1, Loss after 2064 samples: 0.2218
Mean accuracy: 0.8449, std: 0.0082, lower bound: 0.8291, upper bound: 0.8600 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 2064 samples: 0.8448 with eval loss: 0.3663
Epoch 1/1, Loss after 2272 samples: 0.2722
Mean accuracy: 0.8264, std: 0.0086, lower bound: 0.8094, upper bound: 0.8428 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 2272 samples: 0.8266 with eval loss: 0.4065
Epoch 1/1, Loss after 2480 samples: 0.2539
Mean accuracy: 0.9123, std: 0.0063, lower bound: 0.8989, upper bound: 0.9237 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 2480 samples: 0.9120 with eval loss: 0.2022
Best model with eval loss 0.20224632202617585 and eval accuracy 0.9120323559150657 with 2480 samples seen is saved
Epoch 1/1, Loss after 2688 samples: 0.2235
Mean accuracy: 0.9135, std: 0.0061, lower bound: 0.9019, upper bound: 0.9257 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 2688 samples: 0.9135 with eval loss: 0.2044
Epoch 1/1, Loss after 2896 samples: 0.1440
Mean accuracy: 0.9283, std: 0.0059, lower bound: 0.9171, upper bound: 0.9393 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 2896 samples: 0.9282 with eval loss: 0.1709
Best model with eval loss 0.1709235097251592 and eval accuracy 0.9282103134479271 with 2896 samples seen is saved
Epoch 1/1, Loss after 3104 samples: 0.1745
Mean accuracy: 0.8946, std: 0.0069, lower bound: 0.8807, upper bound: 0.9080 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 3104 samples: 0.8943 with eval loss: 0.2655
Epoch 1/1, Loss after 3312 samples: 0.2170
Mean accuracy: 0.9306, std: 0.0059, lower bound: 0.9191, upper bound: 0.9419 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 3312 samples: 0.9307 with eval loss: 0.1637
Best model with eval loss 0.1636744210376374 and eval accuracy 0.9307381193124368 with 3312 samples seen is saved
Epoch 1/1, Loss after 3520 samples: 0.2213
Mean accuracy: 0.9080, std: 0.0062, lower bound: 0.8959, upper bound: 0.9201 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 3520 samples: 0.9080 with eval loss: 0.2134
Epoch 1/1, Loss after 3728 samples: 0.1477
Mean accuracy: 0.9240, std: 0.0061, lower bound: 0.9115, upper bound: 0.9353 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 3728 samples: 0.9242 with eval loss: 0.1791
Epoch 1/1, Loss after 3936 samples: 0.1624
Mean accuracy: 0.9361, std: 0.0054, lower bound: 0.9257, upper bound: 0.9469 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 3936 samples: 0.9358 with eval loss: 0.1534
Best model with eval loss 0.15337533975440648 and eval accuracy 0.935793731041456 with 3936 samples seen is saved
Epoch 1/1, Loss after 4144 samples: 0.2614
Mean accuracy: 0.9201, std: 0.0062, lower bound: 0.9075, upper bound: 0.9317 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 4144 samples: 0.9201 with eval loss: 0.1876
Epoch 1/1, Loss after 4352 samples: 0.2452
Mean accuracy: 0.9434, std: 0.0054, lower bound: 0.9328, upper bound: 0.9540 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 4352 samples: 0.9434 with eval loss: 0.1474
Best model with eval loss 0.1474499067952556 and eval accuracy 0.9433771486349848 with 4352 samples seen is saved
Epoch 1/1, Loss after 4560 samples: 0.1222
Mean accuracy: 0.9328, std: 0.0056, lower bound: 0.9216, upper bound: 0.9429 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 4560 samples: 0.9328 with eval loss: 0.1631
Epoch 1/1, Loss after 4768 samples: 0.1009
Mean accuracy: 0.9145, std: 0.0060, lower bound: 0.9019, upper bound: 0.9257 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 4768 samples: 0.9146 with eval loss: 0.2637
Epoch 1/1, Loss after 4976 samples: 0.3146
Mean accuracy: 0.9370, std: 0.0055, lower bound: 0.9262, upper bound: 0.9484 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 4976 samples: 0.9368 with eval loss: 0.1661
Epoch 1/1, Loss after 5184 samples: 0.2713
Mean accuracy: 0.9033, std: 0.0068, lower bound: 0.8898, upper bound: 0.9166 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 5184 samples: 0.9029 with eval loss: 0.2338
Epoch 1/1, Loss after 5392 samples: 0.1701
Mean accuracy: 0.9267, std: 0.0061, lower bound: 0.9151, upper bound: 0.9388 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 5392 samples: 0.9267 with eval loss: 0.1702
Epoch 1/1, Loss after 5600 samples: 0.1469
Mean accuracy: 0.9393, std: 0.0055, lower bound: 0.9282, upper bound: 0.9499 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 5600 samples: 0.9393 with eval loss: 0.1460
Best model with eval loss 0.1460387882806601 and eval accuracy 0.9393326592517695 with 5600 samples seen is saved
Epoch 1/1, Loss after 5808 samples: 0.1333
Mean accuracy: 0.9294, std: 0.0058, lower bound: 0.9176, upper bound: 0.9408 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 5808 samples: 0.9292 with eval loss: 0.1721
Epoch 1/1, Loss after 6016 samples: 0.1259
Mean accuracy: 0.9060, std: 0.0067, lower bound: 0.8928, upper bound: 0.9191 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 6016 samples: 0.9060 with eval loss: 0.2498
Epoch 1/1, Loss after 6224 samples: 0.2304
Mean accuracy: 0.8964, std: 0.0067, lower bound: 0.8832, upper bound: 0.9095 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 6224 samples: 0.8964 with eval loss: 0.2338
Epoch 1/1, Loss after 6432 samples: 0.1694
Mean accuracy: 0.9309, std: 0.0056, lower bound: 0.9186, upper bound: 0.9414 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 6432 samples: 0.9307 with eval loss: 0.1606
Epoch 1/1, Loss after 6640 samples: 0.1366
Mean accuracy: 0.9107, std: 0.0065, lower bound: 0.8969, upper bound: 0.9232 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 6640 samples: 0.9105 with eval loss: 0.2110
Epoch 1/1, Loss after 6848 samples: 0.1156
Mean accuracy: 0.9376, std: 0.0054, lower bound: 0.9272, upper bound: 0.9479 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 6848 samples: 0.9378 with eval loss: 0.1389
Best model with eval loss 0.13888629703151603 and eval accuracy 0.9378159757330637 with 6848 samples seen is saved
Epoch 1/1, Loss after 7056 samples: 0.1470
Mean accuracy: 0.8547, std: 0.0079, lower bound: 0.8387, upper bound: 0.8701 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 7056 samples: 0.8544 with eval loss: 0.3597
Epoch 1/1, Loss after 7264 samples: 0.2266
Mean accuracy: 0.9056, std: 0.0065, lower bound: 0.8938, upper bound: 0.9181 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 7264 samples: 0.9055 with eval loss: 0.2008
Epoch 1/1, Loss after 7472 samples: 0.1540
Mean accuracy: 0.9292, std: 0.0058, lower bound: 0.9176, upper bound: 0.9403 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 7472 samples: 0.9292 with eval loss: 0.1599
Epoch 1/1, Loss after 7680 samples: 0.1286
Mean accuracy: 0.9334, std: 0.0053, lower bound: 0.9226, upper bound: 0.9429 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 7680 samples: 0.9333 with eval loss: 0.1428
Epoch 1/1, Loss after 7888 samples: 0.1653
Mean accuracy: 0.9368, std: 0.0057, lower bound: 0.9252, upper bound: 0.9484 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 7888 samples: 0.9368 with eval loss: 0.1402
Epoch 1/1, Loss after 8096 samples: 0.1296
Mean accuracy: 0.8895, std: 0.0070, lower bound: 0.8751, upper bound: 0.9024 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 8096 samples: 0.8893 with eval loss: 0.2587
Epoch 1/1, Loss after 8304 samples: 0.1091
Mean accuracy: 0.9247, std: 0.0061, lower bound: 0.9125, upper bound: 0.9363 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 8304 samples: 0.9247 with eval loss: 0.1810
Epoch 1/1, Loss after 8512 samples: 0.0354
Mean accuracy: 0.9407, std: 0.0052, lower bound: 0.9307, upper bound: 0.9515 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 8512 samples: 0.9408 with eval loss: 0.1473
Epoch 1/1, Loss after 8720 samples: 0.1094
Mean accuracy: 0.9034, std: 0.0065, lower bound: 0.8903, upper bound: 0.9156 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 8720 samples: 0.9034 with eval loss: 0.2702
Epoch 1/1, Loss after 8928 samples: 0.0644
Mean accuracy: 0.9416, std: 0.0052, lower bound: 0.9307, upper bound: 0.9520 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 8928 samples: 0.9419 with eval loss: 0.1482
Epoch 1/1, Loss after 9136 samples: 0.0899
Mean accuracy: 0.8583, std: 0.0075, lower bound: 0.8433, upper bound: 0.8731 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 9136 samples: 0.8584 with eval loss: 0.3961
Epoch 1/1, Loss after 9344 samples: 0.1483
Mean accuracy: 0.8847, std: 0.0075, lower bound: 0.8701, upper bound: 0.8989 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 9344 samples: 0.8847 with eval loss: 0.2966
Epoch 1/1, Loss after 9552 samples: 0.1754
Mean accuracy: 0.8818, std: 0.0074, lower bound: 0.8675, upper bound: 0.8959 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 9552 samples: 0.8817 with eval loss: 0.2831
Epoch 1/1, Loss after 9760 samples: 0.1558
Mean accuracy: 0.9456, std: 0.0050, lower bound: 0.9363, upper bound: 0.9550 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 9760 samples: 0.9459 with eval loss: 0.1167
Best model with eval loss 0.11672043675676949 and eval accuracy 0.9459049544994944 with 9760 samples seen is saved
Epoch 1/1, Loss after 9968 samples: 0.1254
Mean accuracy: 0.8927, std: 0.0070, lower bound: 0.8792, upper bound: 0.9055 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 9968 samples: 0.8923 with eval loss: 0.2509
Epoch 1/1, Loss after 10176 samples: 0.1232
Mean accuracy: 0.9295, std: 0.0059, lower bound: 0.9181, upper bound: 0.9409 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 10176 samples: 0.9292 with eval loss: 0.1725
Epoch 1/1, Loss after 10384 samples: 0.1178
Mean accuracy: 0.8586, std: 0.0081, lower bound: 0.8428, upper bound: 0.8751 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 10384 samples: 0.8584 with eval loss: 0.3604
Epoch 1/1, Loss after 10592 samples: 0.1242
Mean accuracy: 0.9440, std: 0.0055, lower bound: 0.9328, upper bound: 0.9540 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 10592 samples: 0.9439 with eval loss: 0.1325
Epoch 1/1, Loss after 10800 samples: 0.1026
Mean accuracy: 0.9212, std: 0.0060, lower bound: 0.9100, upper bound: 0.9328 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 10800 samples: 0.9211 with eval loss: 0.2043
Epoch 1/1, Loss after 11008 samples: 0.0820
Mean accuracy: 0.9304, std: 0.0057, lower bound: 0.9181, upper bound: 0.9414 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 11008 samples: 0.9302 with eval loss: 0.1752
Epoch 1/1, Loss after 11216 samples: 0.1171
Mean accuracy: 0.9382, std: 0.0053, lower bound: 0.9277, upper bound: 0.9479 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 11216 samples: 0.9383 with eval loss: 0.1589
Epoch 1/1, Loss after 11424 samples: 0.1148
Mean accuracy: 0.9071, std: 0.0066, lower bound: 0.8933, upper bound: 0.9191 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 11424 samples: 0.9070 with eval loss: 0.2521
Epoch 1/1, Loss after 11632 samples: 0.1051
Mean accuracy: 0.9346, std: 0.0057, lower bound: 0.9226, upper bound: 0.9459 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 11632 samples: 0.9348 with eval loss: 0.1520
Epoch 1/1, Loss after 11840 samples: 0.1195
Mean accuracy: 0.9350, std: 0.0054, lower bound: 0.9242, upper bound: 0.9449 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 11840 samples: 0.9348 with eval loss: 0.1592
Epoch 1/1, Loss after 12048 samples: 0.1271
Mean accuracy: 0.9292, std: 0.0055, lower bound: 0.9181, upper bound: 0.9398 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 12048 samples: 0.9292 with eval loss: 0.1684
Epoch 1/1, Loss after 12256 samples: 0.0918
Mean accuracy: 0.8973, std: 0.0068, lower bound: 0.8842, upper bound: 0.9100 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 12256 samples: 0.8974 with eval loss: 0.2697
Epoch 1/1, Loss after 12464 samples: 0.0961
Mean accuracy: 0.9297, std: 0.0058, lower bound: 0.9181, upper bound: 0.9408 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 12464 samples: 0.9297 with eval loss: 0.1657
Epoch 1/1, Loss after 12672 samples: 0.1163
Mean accuracy: 0.9159, std: 0.0062, lower bound: 0.9029, upper bound: 0.9287 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 12672 samples: 0.9156 with eval loss: 0.2118
Epoch 1/1, Loss after 12880 samples: 0.1483
Mean accuracy: 0.9502, std: 0.0049, lower bound: 0.9403, upper bound: 0.9590 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 12880 samples: 0.9505 with eval loss: 0.1233
Epoch 1/1, Loss after 13088 samples: 0.1208
Mean accuracy: 0.8672, std: 0.0075, lower bound: 0.8524, upper bound: 0.8822 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 13088 samples: 0.8675 with eval loss: 0.3303
Epoch 1/1, Loss after 13296 samples: 0.1608
Mean accuracy: 0.9332, std: 0.0053, lower bound: 0.9226, upper bound: 0.9439 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 13296 samples: 0.9333 with eval loss: 0.1605
Epoch 1/1, Loss after 13504 samples: 0.0961
Mean accuracy: 0.9296, std: 0.0057, lower bound: 0.9176, upper bound: 0.9403 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 13504 samples: 0.9297 with eval loss: 0.1648
Epoch 1/1, Loss after 13712 samples: 0.1317
Mean accuracy: 0.8855, std: 0.0069, lower bound: 0.8721, upper bound: 0.8994 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 13712 samples: 0.8857 with eval loss: 0.2881
Epoch 1/1, Loss after 13920 samples: 0.0724
Mean accuracy: 0.9227, std: 0.0058, lower bound: 0.9110, upper bound: 0.9333 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 13920 samples: 0.9226 with eval loss: 0.1982
Epoch 1/1, Loss after 14128 samples: 0.1049
Mean accuracy: 0.9309, std: 0.0058, lower bound: 0.9191, upper bound: 0.9419 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 14128 samples: 0.9307 with eval loss: 0.1695
Epoch 1/1, Loss after 14336 samples: 0.1098
Mean accuracy: 0.9161, std: 0.0063, lower bound: 0.9039, upper bound: 0.9282 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 14336 samples: 0.9161 with eval loss: 0.2128
Epoch 1/1, Loss after 14544 samples: 0.0910
Mean accuracy: 0.9237, std: 0.0058, lower bound: 0.9125, upper bound: 0.9358 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 14544 samples: 0.9237 with eval loss: 0.1970
Epoch 1/1, Loss after 14752 samples: 0.1152
Mean accuracy: 0.9330, std: 0.0056, lower bound: 0.9216, upper bound: 0.9434 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 14752 samples: 0.9333 with eval loss: 0.1716
Epoch 1/1, Loss after 14960 samples: 0.1118
Mean accuracy: 0.9217, std: 0.0059, lower bound: 0.9100, upper bound: 0.9328 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 14960 samples: 0.9216 with eval loss: 0.2068
Epoch 1/1, Loss after 15168 samples: 0.0792
Mean accuracy: 0.9231, std: 0.0058, lower bound: 0.9115, upper bound: 0.9343 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 15168 samples: 0.9232 with eval loss: 0.2045
Epoch 1/1, Loss after 15376 samples: 0.0853
Mean accuracy: 0.9283, std: 0.0058, lower bound: 0.9161, upper bound: 0.9393 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 15376 samples: 0.9282 with eval loss: 0.1912
Epoch 1/1, Loss after 15584 samples: 0.0869
Mean accuracy: 0.9236, std: 0.0062, lower bound: 0.9110, upper bound: 0.9353 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 15584 samples: 0.9237 with eval loss: 0.2041
Epoch 1/1, Loss after 15792 samples: 0.1372
Mean accuracy: 0.9222, std: 0.0059, lower bound: 0.9110, upper bound: 0.9338 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 15792 samples: 0.9221 with eval loss: 0.2095
Training signal is False, stopping training
Training finished
Best model: {'eval_acc': 0.9459049544994944, 'nb_samples': 9760, 'eval_loss': 0.11672043675676949}
Training loss logs: [{'samples': 192, 'loss': 0.6928945688100961}, {'samples': 400, 'loss': 0.6938100961538461}, {'samples': 608, 'loss': 0.6858191856971154}, {'samples': 816, 'loss': 0.6324087289663461}, {'samples': 1024, 'loss': 0.41052499184241664}, {'samples': 1232, 'loss': 0.35060734015244704}, {'samples': 1440, 'loss': 0.3018334095294659}, {'samples': 1648, 'loss': 0.3030880322823158}, {'samples': 1856, 'loss': 0.29588698423825777}, {'samples': 2064, 'loss': 0.22182105137751654}, {'samples': 2272, 'loss': 0.2721900939941406}, {'samples': 2480, 'loss': 0.2539082307081956}, {'samples': 2688, 'loss': 0.22347046778752253}, {'samples': 2896, 'loss': 0.14402440878061148}, {'samples': 3104, 'loss': 0.1745217121564425}, {'samples': 3312, 'loss': 0.2170474322942587}, {'samples': 3520, 'loss': 0.22129860061865586}, {'samples': 3728, 'loss': 0.14774792927962083}, {'samples': 3936, 'loss': 0.16243329414954552}, {'samples': 4144, 'loss': 0.2614014630134289}, {'samples': 4352, 'loss': 0.24517379586513227}, {'samples': 4560, 'loss': 0.12221523431631234}, {'samples': 4768, 'loss': 0.10088998308548561}, {'samples': 4976, 'loss': 0.314628745500858}, {'samples': 5184, 'loss': 0.2713254460921654}, {'samples': 5392, 'loss': 0.17011519578786996}, {'samples': 5600, 'loss': 0.14685817865224984}, {'samples': 5808, 'loss': 0.13326980058963483}, {'samples': 6016, 'loss': 0.12586678908421442}, {'samples': 6224, 'loss': 0.23040523895850548}, {'samples': 6432, 'loss': 0.16942177598293012}, {'samples': 6640, 'loss': 0.13662531742682824}, {'samples': 6848, 'loss': 0.11556563239831191}, {'samples': 7056, 'loss': 0.14698650515996492}, {'samples': 7264, 'loss': 0.22657603942430937}, {'samples': 7472, 'loss': 0.15399711636396554}, {'samples': 7680, 'loss': 0.1286002626785865}, {'samples': 7888, 'loss': 0.16525848782979524}, {'samples': 8096, 'loss': 0.12958348714388335}, {'samples': 8304, 'loss': 0.10912847518920898}, {'samples': 8512, 'loss': 0.03537851342788109}, {'samples': 8720, 'loss': 0.10937903477595402}, {'samples': 8928, 'loss': 0.06439856038643764}, {'samples': 9136, 'loss': 0.08985638733093555}, {'samples': 9344, 'loss': 0.14830882618060479}, {'samples': 9552, 'loss': 0.17541092404952416}, {'samples': 9760, 'loss': 0.15584682042782122}, {'samples': 9968, 'loss': 0.12535248811428362}, {'samples': 10176, 'loss': 0.12322336206069359}, {'samples': 10384, 'loss': 0.11781533864828256}, {'samples': 10592, 'loss': 0.12424036058095786}, {'samples': 10800, 'loss': 0.10262078619920291}, {'samples': 11008, 'loss': 0.08201208710670471}, {'samples': 11216, 'loss': 0.11705740369283237}, {'samples': 11424, 'loss': 0.11481945445904365}, {'samples': 11632, 'loss': 0.10510753840208054}, {'samples': 11840, 'loss': 0.11952934700709122}, {'samples': 12048, 'loss': 0.12711884425236628}, {'samples': 12256, 'loss': 0.09184490831998679}, {'samples': 12464, 'loss': 0.09605134221223685}, {'samples': 12672, 'loss': 0.11628402311068314}, {'samples': 12880, 'loss': 0.14827493750132048}, {'samples': 13088, 'loss': 0.12075847960435428}, {'samples': 13296, 'loss': 0.160796283529355}, {'samples': 13504, 'loss': 0.09606406092643738}, {'samples': 13712, 'loss': 0.13171427181133857}, {'samples': 13920, 'loss': 0.07239813758776738}, {'samples': 14128, 'loss': 0.1049350706430582}, {'samples': 14336, 'loss': 0.10980488933049716}, {'samples': 14544, 'loss': 0.09095105299582848}, {'samples': 14752, 'loss': 0.11524385786973514}, {'samples': 14960, 'loss': 0.11177056683943822}, {'samples': 15168, 'loss': 0.0791625311741462}, {'samples': 15376, 'loss': 0.08533424482895778}, {'samples': 15584, 'loss': 0.08685919986321376}, {'samples': 15792, 'loss': 0.13721400040846604}]
Evaluation accuracy logs: [{'samples': 192, 'accuracy': 0.5001683518705763, 'std': 0.011093769141986797, 'lower_bound': 0.4787537917087968, 'upper_bound': 0.522762891809909}, {'samples': 400, 'accuracy': 0.5338205257836198, 'std': 0.011067615030335858, 'lower_bound': 0.5116279069767442, 'upper_bound': 0.5561425682507583}, {'samples': 608, 'accuracy': 0.6904089989888776, 'std': 0.009965396333094392, 'lower_bound': 0.6703741152679474, 'upper_bound': 0.7098078867542973}, {'samples': 816, 'accuracy': 0.8344848331648129, 'std': 0.00882150906224656, 'lower_bound': 0.8174924165824065, 'upper_bound': 0.8523761375126391}, {'samples': 1024, 'accuracy': 0.8596157735085944, 'std': 0.007982930985994747, 'lower_bound': 0.8437815975733064, 'upper_bound': 0.8761375126390293}, {'samples': 1232, 'accuracy': 0.8166274014155713, 'std': 0.008624664374187479, 'lower_bound': 0.8008088978766431, 'upper_bound': 0.8336703741152679}, {'samples': 1440, 'accuracy': 0.8909443882709809, 'std': 0.006852826331028364, 'lower_bound': 0.8786653185035389, 'upper_bound': 0.9049544994944388}, {'samples': 1648, 'accuracy': 0.9148943377148636, 'std': 0.006556644642422616, 'lower_bound': 0.9024266936299292, 'upper_bound': 0.9271991911021233}, {'samples': 1856, 'accuracy': 0.8171971688574318, 'std': 0.008755334957736961, 'lower_bound': 0.8013018200202224, 'upper_bound': 0.8341759352881699}, {'samples': 2064, 'accuracy': 0.8448604651162791, 'std': 0.00824012051143887, 'lower_bound': 0.8291203235591507, 'upper_bound': 0.8599721941354904}, {'samples': 2272, 'accuracy': 0.8264054600606673, 'std': 0.008612944963212736, 'lower_bound': 0.8093907987866531, 'upper_bound': 0.8427704752275025}, {'samples': 2480, 'accuracy': 0.9123382204246714, 'std': 0.006270160665426242, 'lower_bound': 0.8988877654196158, 'upper_bound': 0.9236602628918099}, {'samples': 2688, 'accuracy': 0.9135111223458039, 'std': 0.00613817422636469, 'lower_bound': 0.9019211324570273, 'upper_bound': 0.9256951466127402}, {'samples': 2896, 'accuracy': 0.9283235591506572, 'std': 0.005855084298598257, 'lower_bound': 0.917087967644085, 'upper_bound': 0.9393326592517695}, {'samples': 3104, 'accuracy': 0.894566734074823, 'std': 0.006904044351666453, 'lower_bound': 0.8806875631951466, 'upper_bound': 0.9079878665318504}, {'samples': 3312, 'accuracy': 0.930649140546006, 'std': 0.005890927214799379, 'lower_bound': 0.91909757330637, 'upper_bound': 0.9418604651162791}, {'samples': 3520, 'accuracy': 0.9080283114256825, 'std': 0.0061754581815614855, 'lower_bound': 0.8958543983822043, 'upper_bound': 0.9201213346814965}, {'samples': 3728, 'accuracy': 0.9240106167846309, 'std': 0.006098307438773806, 'lower_bound': 0.9115267947421638, 'upper_bound': 0.9352881698685541}, {'samples': 3936, 'accuracy': 0.9360854398382203, 'std': 0.005374095554354711, 'lower_bound': 0.9256825075834176, 'upper_bound': 0.9469160768452983}, {'samples': 4144, 'accuracy': 0.9200803842264913, 'std': 0.0061727019086172535, 'lower_bound': 0.9074823053589485, 'upper_bound': 0.9317492416582407}, {'samples': 4352, 'accuracy': 0.9434342770475228, 'std': 0.0053929002276058655, 'lower_bound': 0.9327603640040445, 'upper_bound': 0.9539939332659252}, {'samples': 4560, 'accuracy': 0.9327724974721942, 'std': 0.005572316805817122, 'lower_bound': 0.9216253791708796, 'upper_bound': 0.9428715874620829}, {'samples': 4768, 'accuracy': 0.9145419615773508, 'std': 0.005990136260862361, 'lower_bound': 0.9019211324570273, 'upper_bound': 0.9256825075834176}, {'samples': 4976, 'accuracy': 0.937020728008089, 'std': 0.005474101684575094, 'lower_bound': 0.9261880687563195, 'upper_bound': 0.948432760364004}, {'samples': 5184, 'accuracy': 0.9033003033367037, 'std': 0.006833459555183048, 'lower_bound': 0.8897876643073812, 'upper_bound': 0.916582406471183}, {'samples': 5392, 'accuracy': 0.9267239635995955, 'std': 0.0060905833885059055, 'lower_bound': 0.9150657229524772, 'upper_bound': 0.9388270980788676}, {'samples': 5600, 'accuracy': 0.9392876643073812, 'std': 0.005515834094788382, 'lower_bound': 0.9282103134479271, 'upper_bound': 0.9499494438827099}, {'samples': 5808, 'accuracy': 0.9293528816986855, 'std': 0.005817741764887452, 'lower_bound': 0.9175935288169869, 'upper_bound': 0.9408493427704753}, {'samples': 6016, 'accuracy': 0.9060197168857431, 'std': 0.006734058711118553, 'lower_bound': 0.8928210313447927, 'upper_bound': 0.9191102123356926}, {'samples': 6224, 'accuracy': 0.8964342770475228, 'std': 0.006699847105350397, 'lower_bound': 0.8832153690596563, 'upper_bound': 0.9095171890798787}, {'samples': 6432, 'accuracy': 0.930917087967644, 'std': 0.005614977425439049, 'lower_bound': 0.9186046511627907, 'upper_bound': 0.9413549039433772}, {'samples': 6640, 'accuracy': 0.9107143579373105, 'std': 0.006530366194976458, 'lower_bound': 0.8968655207280081, 'upper_bound': 0.923154701718908}, {'samples': 6848, 'accuracy': 0.937608190091001, 'std': 0.005366194973630999, 'lower_bound': 0.9271991911021233, 'upper_bound': 0.9479398382204247}, {'samples': 7056, 'accuracy': 0.8547310414560162, 'std': 0.007913785247602013, 'lower_bound': 0.8387259858442871, 'upper_bound': 0.8700707785642062}, {'samples': 7264, 'accuracy': 0.9055783619817999, 'std': 0.006535035630010612, 'lower_bound': 0.893819514661274, 'upper_bound': 0.9180990899898888}, {'samples': 7472, 'accuracy': 0.929190091001011, 'std': 0.005804541464983767, 'lower_bound': 0.9175935288169869, 'upper_bound': 0.9403437815975733}, {'samples': 7680, 'accuracy': 0.9334024266936299, 'std': 0.005327211573431003, 'lower_bound': 0.9226365015166835, 'upper_bound': 0.9428715874620829}, {'samples': 7888, 'accuracy': 0.9368139534883722, 'std': 0.005676028658836105, 'lower_bound': 0.9251769464105156, 'upper_bound': 0.948432760364004}, {'samples': 8096, 'accuracy': 0.8894747219413548, 'std': 0.007040161049963745, 'lower_bound': 0.8751263902932255, 'upper_bound': 0.9024393326592518}, {'samples': 8304, 'accuracy': 0.924654196157735, 'std': 0.006080512748728862, 'lower_bound': 0.9125379170879676, 'upper_bound': 0.9362992922143579}, {'samples': 8512, 'accuracy': 0.9407087967644084, 'std': 0.00518215531598956, 'lower_bound': 0.9307381193124368, 'upper_bound': 0.9514661274014156}, {'samples': 8720, 'accuracy': 0.9033902932254803, 'std': 0.006521056469550341, 'lower_bound': 0.8902805864509605, 'upper_bound': 0.9155839231547017}, {'samples': 8928, 'accuracy': 0.9416172901921133, 'std': 0.005196609144517287, 'lower_bound': 0.9307381193124368, 'upper_bound': 0.9519716885743175}, {'samples': 9136, 'accuracy': 0.8583392315470172, 'std': 0.007522250365959727, 'lower_bound': 0.8432633973710819, 'upper_bound': 0.8731041456016178}, {'samples': 9344, 'accuracy': 0.884655207280081, 'std': 0.00754995813677556, 'lower_bound': 0.8700707785642062, 'upper_bound': 0.8989004044489384}, {'samples': 9552, 'accuracy': 0.881845803842265, 'std': 0.0073888127596318295, 'lower_bound': 0.8675429726996967, 'upper_bound': 0.8958670374115268}, {'samples': 9760, 'accuracy': 0.9455935288169869, 'std': 0.004961266916078774, 'lower_bound': 0.9362992922143579, 'upper_bound': 0.955005055611729}, {'samples': 9968, 'accuracy': 0.8926683518705764, 'std': 0.007034258254188132, 'lower_bound': 0.8791708796764408, 'upper_bound': 0.9054600606673407}, {'samples': 10176, 'accuracy': 0.9294959555106168, 'std': 0.005902048344204551, 'lower_bound': 0.9180990899898888, 'upper_bound': 0.9408619817997979}, {'samples': 10384, 'accuracy': 0.8586263902932254, 'std': 0.00813273728740708, 'lower_bound': 0.8427704752275025, 'upper_bound': 0.8751263902932255}, {'samples': 10592, 'accuracy': 0.9440267947421638, 'std': 0.005476974864182817, 'lower_bound': 0.9327603640040445, 'upper_bound': 0.9539939332659252}, {'samples': 10800, 'accuracy': 0.9211693629929221, 'std': 0.005959141803726163, 'lower_bound': 0.910010111223458, 'upper_bound': 0.9327603640040445}, {'samples': 11008, 'accuracy': 0.9303998988877654, 'std': 0.005697776225244522, 'lower_bound': 0.9180990899898888, 'upper_bound': 0.9413549039433772}, {'samples': 11216, 'accuracy': 0.9381587462082912, 'std': 0.005256628448827048, 'lower_bound': 0.9277047522750252, 'upper_bound': 0.9479271991911021}, {'samples': 11424, 'accuracy': 0.9070642062689586, 'std': 0.006595995825033013, 'lower_bound': 0.8933139534883721, 'upper_bound': 0.9191228513650151}, {'samples': 11632, 'accuracy': 0.9345985844287159, 'std': 0.005710145126474244, 'lower_bound': 0.9226491405460061, 'upper_bound': 0.9459049544994944}, {'samples': 11840, 'accuracy': 0.9349797775530838, 'std': 0.005432698826896347, 'lower_bound': 0.9241658240647118, 'upper_bound': 0.9449064711830132}, {'samples': 12048, 'accuracy': 0.9292396359959555, 'std': 0.0055123489638502325, 'lower_bound': 0.9180990899898888, 'upper_bound': 0.9398382204246714}, {'samples': 12256, 'accuracy': 0.8973144590495449, 'std': 0.006784048050739448, 'lower_bound': 0.8842138523761375, 'upper_bound': 0.9100227502527806}, {'samples': 12464, 'accuracy': 0.9296855409504549, 'std': 0.005758545400586219, 'lower_bound': 0.9180990899898888, 'upper_bound': 0.9408493427704753}, {'samples': 12672, 'accuracy': 0.9159246713852376, 'std': 0.00617828847625623, 'lower_bound': 0.9029322548028311, 'upper_bound': 0.9287158746208292}, {'samples': 12880, 'accuracy': 0.9501936299292215, 'std': 0.004921728535408332, 'lower_bound': 0.9403437815975733, 'upper_bound': 0.9590495449949444}, {'samples': 13088, 'accuracy': 0.8672340748230536, 'std': 0.007485885771262419, 'lower_bound': 0.8523761375126391, 'upper_bound': 0.8822042467138523}, {'samples': 13296, 'accuracy': 0.933221435793731, 'std': 0.00534357487310271, 'lower_bound': 0.9226365015166835, 'upper_bound': 0.9438827098078868}, {'samples': 13504, 'accuracy': 0.9295535894843276, 'std': 0.005732077258861786, 'lower_bound': 0.9175935288169869, 'upper_bound': 0.9403437815975733}, {'samples': 13712, 'accuracy': 0.8854873609706774, 'std': 0.006877043734794175, 'lower_bound': 0.872093023255814, 'upper_bound': 0.8993933265925177}, {'samples': 13920, 'accuracy': 0.9227310414560161, 'std': 0.005750161215234934, 'lower_bound': 0.9110085945399393, 'upper_bound': 0.9332659251769464}, {'samples': 14128, 'accuracy': 0.9308832153690596, 'std': 0.005809217550588625, 'lower_bound': 0.9191102123356926, 'upper_bound': 0.9418731041456017}, {'samples': 14336, 'accuracy': 0.9160652173913044, 'std': 0.006261584246022011, 'lower_bound': 0.9039307381193123, 'upper_bound': 0.9282103134479271}, {'samples': 14544, 'accuracy': 0.9237184024266936, 'std': 0.005780308306035629, 'lower_bound': 0.9125379170879676, 'upper_bound': 0.935793731041456}, {'samples': 14752, 'accuracy': 0.9329656218402427, 'std': 0.005642686995912851, 'lower_bound': 0.9216253791708796, 'upper_bound': 0.9433897876643074}, {'samples': 14960, 'accuracy': 0.9216880687563195, 'std': 0.005932264346184624, 'lower_bound': 0.910010111223458, 'upper_bound': 0.9327603640040445}, {'samples': 15168, 'accuracy': 0.9230652173913044, 'std': 0.0057652080378209946, 'lower_bound': 0.9115141557128412, 'upper_bound': 0.9342770475227502}, {'samples': 15376, 'accuracy': 0.9282694641051568, 'std': 0.005753484081636073, 'lower_bound': 0.916076845298281, 'upper_bound': 0.9393326592517695}, {'samples': 15584, 'accuracy': 0.9236066734074823, 'std': 0.006164421714991687, 'lower_bound': 0.9110085945399393, 'upper_bound': 0.9353008088978767}, {'samples': 15792, 'accuracy': 0.9222007077856421, 'std': 0.005914854658199713, 'lower_bound': 0.9110085945399393, 'upper_bound': 0.9337714863498483}]
Evaluating the best model on the test set of dataset ./fake_true_datasets/fake_true_dataset_gemma_10k...
Test metrics:
accuracy: 0.9150171890798787
precision: 0.8572183860219625
recall: 0.9959420822383012
f1_score: 0.9213552706988776
fp_rate: 0.1659367506878622
tp_rate: 0.9959420822383012
std_accuracy: 0.006236231479581333
std_precision: 0.01020607244397429
std_recall: 0.001986319504206073
std_f1_score: 0.006010695382087732
std_fp_rate: 0.011760198526025136
std_tp_rate: 0.001986319504206073
TP: 985.123
TN: 824.781
FP: 164.083
FN: 4.013
roc_auc: 0.9947143553813894
fpr: [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00101112 0.00101112 0.00101112 0.00101112
 0.00101112 0.00101112 0.00101112 0.00101112 0.00101112 0.00101112
 0.00101112 0.00101112 0.00101112 0.00101112 0.00101112 0.00101112
 0.00101112 0.00101112 0.00101112 0.00101112 0.00101112 0.00101112
 0.00101112 0.00101112 0.00202224 0.00202224 0.00202224 0.00202224
 0.00202224 0.00202224 0.00202224 0.00202224 0.00202224 0.00202224
 0.00202224 0.00202224 0.00202224 0.00202224 0.00202224 0.00303337
 0.00303337 0.00303337 0.00303337 0.00404449 0.00404449 0.00505561
 0.00505561 0.00505561 0.00505561 0.00606673 0.00606673 0.00606673
 0.00606673 0.00606673 0.00606673 0.00606673 0.00606673 0.00707786
 0.00808898 0.00808898 0.00808898 0.00808898 0.0091001  0.0091001
 0.01011122 0.01011122 0.01011122 0.01011122 0.01011122 0.01011122
 0.01112235 0.01112235 0.01213347 0.01213347 0.01314459 0.01314459
 0.01314459 0.01415571 0.01415571 0.01516684 0.01516684 0.01617796
 0.01617796 0.01718908 0.01718908 0.01921132 0.01921132 0.02022245
 0.02022245 0.02325581 0.02325581 0.02426694 0.02426694 0.02527806
 0.02527806 0.02628918 0.02628918 0.0273003  0.0273003  0.02831143
 0.02831143 0.02831143 0.02831143 0.03033367 0.03033367 0.03235592
 0.03235592 0.03437816 0.03437816 0.03538928 0.03538928 0.0364004
 0.0364004  0.03842265 0.03842265 0.03943377 0.03943377 0.04145602
 0.04145602 0.04246714 0.04347826 0.04651163 0.04651163 0.04954499
 0.04954499 0.05156724 0.05156724 0.05257836 0.05257836 0.05358948
 0.05358948 0.05662285 0.05662285 0.05763397 0.05763397 0.05965622
 0.05965622 0.06167846 0.06167846 0.06268959 0.06268959 0.06673407
 0.06673407 0.06875632 0.06875632 0.06976744 0.07178969 0.07178969
 0.07482305 0.07482305 0.07886754 0.07886754 0.08291203 0.08493428
 0.0859454  0.0859454  0.09302326 0.0950455  0.09706775 0.09706775
 0.10212336 0.10212336 0.1041456  0.1041456  0.11021234 0.11223458
 0.19110212 0.19110212 0.20323559 0.20323559 0.21031345 0.21233569
 0.22042467 0.22244692 0.24165824 0.24165824 0.25278059 0.25682508
 0.32457027 0.32659252 0.32659252 0.32861476 0.33063701 0.35187058
 0.35389282 0.35894843 0.36097068 0.36299292 0.36602629 0.37815976
 0.380182   0.40444894 0.40647118 0.43073812 0.43276036 0.44186047
 0.44388271 0.44893832 0.45096057 0.45197169 0.45500506 0.45803842
 0.46006067 0.47219414 0.47421638 0.4752275  0.47724975 0.49140546
 0.4934277  0.49646107 0.49848332 0.50050556 0.50252781 0.50556117
 0.50758342 0.51466127 0.51668352 0.51769464 0.51971689 0.53993933
 0.54398382 0.54701719 0.54903943 0.55510617 0.55915066 0.56016178
 0.56420627 0.570273   0.57431749 0.57836198 0.58543984 0.58746208
 0.59453994 0.59656218 0.60060667 0.60262892 0.60364004 0.60667341
 0.60869565 0.6107179  0.619818   0.62184024 0.62285137 0.62487361
 0.63296259 0.63599596 0.64105157 0.64307381 0.65015167 0.65217391
 0.65318504 0.65520728 0.65824065 0.66026289 0.6653185  0.66734075
 0.67846309 0.68048534 0.69160768 0.69464105 0.6966633  0.69868554
 0.70070779 0.70475228 0.7057634  0.70980789 0.71081901 0.71284125
 0.71385238 0.71587462 0.71991911 0.72093023 0.72295248 0.72901921
 0.7330637  0.73508595 0.73811931 0.74014156 0.74418605 0.74721941
 0.74924166 0.75025278 0.75227503 0.75429727 0.75631951 0.76137513
 0.76339737 0.76541962 0.76744186 0.77047523 0.77249747 0.77451972
 0.77755308 0.78159757 0.79069767 0.79271992 0.79373104 0.79575329
 0.79676441 0.79878665 0.79979778 0.80182002 0.80586451 0.80889788
 0.81395349 0.81597573 0.82002022 0.82406471 0.82709808 0.8281092
 0.83013145 0.83215369 0.83518706 0.83619818 0.83822042 0.83923155
 0.84125379 0.84732053 0.84934277 0.85035389 0.85237614 0.85338726
 0.85642063 0.85844287 0.86248736 0.86450961 0.8685541  0.87057634
 0.87462083 0.87866532 0.87967644 0.88270981 0.88372093 0.88574317
 0.8867543  0.88978766 0.89180991 0.89484328 0.89686552 0.89989889
 0.90596562 0.90899899 0.91203236 0.9140546  0.91809909 0.91911021
 0.92214358 0.92719919 0.92922144 0.93326593 0.93933266 0.9413549
 0.94539939 0.94944388 0.95247725 0.95955511 0.96157735 0.96562184
 0.96865521 0.96966633 0.97371082 0.97472194 0.98078868 0.98281092
 0.98584429 0.98887765 0.9908999  1.        ]
tpr: [0.         0.00101112 0.00202224 0.00404449 0.00808898 0.01112235
 0.01314459 0.01617796 0.0182002  0.01921132 0.02224469 0.02628918
 0.0273003  0.03033367 0.0364004  0.03943377 0.04347826 0.04550051
 0.04853387 0.05055612 0.05156724 0.05460061 0.05561173 0.06167846
 0.06370071 0.0677452  0.07381193 0.07886754 0.08493428 0.08695652
 0.09100101 0.09302326 0.09706775 0.09908999 0.10616785 0.11021234
 0.11122346 0.11627907 0.12335693 0.12537917 0.13043478 0.13650152
 0.1445905  0.15065723 0.16481294 0.17087968 0.17593529 0.17997978
 0.18503539 0.18705763 0.18806876 0.19110212 0.19312437 0.19413549
 0.20121335 0.20323559 0.20930233 0.21233569 0.22042467 0.22244692
 0.22750253 0.22952477 0.239636   0.24064712 0.24368049 0.24772497
 0.25278059 0.25884732 0.27401416 0.2760364  0.28008089 0.28210313
 0.28614762 0.29120324 0.2942366  0.29625885 0.30434783 0.30535895
 0.30738119 0.3124368  0.31344793 0.31951466 0.32457027 0.3255814
 0.32962588 0.33164813 0.33265925 0.33872599 0.34175935 0.34580384
 0.34782609 0.35085945 0.35389282 0.35591507 0.35894843 0.35995956
 0.36400404 0.36501517 0.36905966 0.37209302 0.37411527 0.37917088
 0.38523761 0.38725986 0.39130435 0.39737108 0.40343782 0.40546006
 0.40849343 0.41354904 0.41759353 0.41860465 0.42163802 0.42568251
 0.42770475 0.43073812 0.43276036 0.43579373 0.4388271  0.44287159
 0.44388271 0.44590495 0.44994944 0.45197169 0.45601618 0.45803842
 0.46107179 0.46309403 0.4661274  0.46713852 0.47017189 0.47421638
 0.4752275  0.47724975 0.48028311 0.48129424 0.48331648 0.48533873
 0.49747219 0.49949444 0.50151668 0.50556117 0.50859454 0.51263903
 0.51466127 0.51870576 0.52072801 0.52275025 0.52780586 0.52881699
 0.53286148 0.5338726  0.53589484 0.53892821 0.5429727  0.54398382
 0.54701719 0.54903943 0.55106168 0.55308392 0.55712841 0.55813953
 0.5611729  0.56622851 0.56825076 0.57431749 0.57532861 0.58139535
 0.58442872 0.58847321 0.58948433 0.59555106 0.60060667 0.60364004
 0.60768453 0.61274014 0.61880688 0.62588473 0.62790698 0.6289181
 0.63296259 0.63397371 0.63599596 0.6380182  0.64105157 0.64509606
 0.6471183  0.64812942 0.65015167 0.65116279 0.65318504 0.65419616
 0.65925177 0.66127401 0.66329626 0.6653185  0.67037412 0.6744186
 0.67644085 0.67947422 0.68149646 0.68351871 0.68554095 0.68655207
 0.69059656 0.69261881 0.69464105 0.6966633  0.69767442 0.69969666
 0.7057634  0.70778564 0.71688574 0.71991911 0.72194135 0.7239636
 0.72598584 0.72901921 0.73104146 0.73508595 0.73913043 0.74115268
 0.74519717 0.74721941 0.75631951 0.75834176 0.75935288 0.76238625
 0.76339737 0.76541962 0.76643074 0.76744186 0.76946411 0.77047523
 0.77350859 0.77553084 0.77856421 0.78665319 0.78867543 0.79069767
 0.79373104 0.79777553 0.79878665 0.8008089  0.80283114 0.80283114
 0.80384226 0.80586451 0.809909   0.809909   0.81597573 0.81597573
 0.82002022 0.82103134 0.82305359 0.82406471 0.83316481 0.83518706
 0.83923155 0.84125379 0.84226491 0.84428716 0.85743175 0.85743175
 0.85844287 0.86147624 0.86450961 0.86956522 0.86956522 0.87462083
 0.87563195 0.88068756 0.88270981 0.88776542 0.88978766 0.89180991
 0.89180991 0.89787664 0.89787664 0.90091001 0.90091001 0.9049545
 0.90899899 0.91001011 0.91304348 0.9140546  0.91809909 0.91809909
 0.92113246 0.92113246 0.92618807 0.92618807 0.92821031 0.92821031
 0.93023256 0.93023256 0.9322548  0.9322548  0.93427705 0.93427705
 0.93629929 0.93629929 0.94034378 0.94034378 0.9413549  0.9413549
 0.94337715 0.94539939 0.94742164 0.94742164 0.94843276 0.94843276
 0.95247725 0.95247725 0.95449949 0.95449949 0.95854398 0.95854398
 0.95955511 0.95955511 0.96056623 0.96056623 0.9635996  0.9635996
 0.96663296 0.96663296 0.96764408 0.96764408 0.9726997  0.9726997
 0.97371082 0.97371082 0.97472194 0.97472194 0.97573306 0.97573306
 0.97876643 0.97876643 0.97977755 0.98078868 0.9817998  0.9817998
 0.98382204 0.98382204 0.98483316 0.98483316 0.98584429 0.98584429
 0.98685541 0.98685541 0.98786653 0.98786653 0.98786653 0.98887765
 0.98887765 0.9908999  0.9908999  0.99191102 0.99191102 0.99191102
 0.99191102 0.99292214 0.99292214 0.99292214 0.99292214 0.99393327
 0.99393327 0.99494439 0.99494439 0.99595551 0.99595551 0.99595551
 0.99595551 0.99696663 0.99696663 0.99797776 0.99797776 0.99797776
 0.99797776 0.99797776 0.99797776 0.99898888 0.99898888 0.99898888
 0.99898888 0.99898888 1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.        ]
thresholds: [        inf  3.8222656   3.8203125   3.8183594   3.8105469   3.7988281
  3.796875    3.7890625   3.7871094   3.7851562   3.7832031   3.7773438
  3.7753906   3.7734375   3.7714844   3.765625    3.7636719   3.7597656
  3.7578125   3.7558594   3.7539062   3.7519531   3.75        3.7480469
  3.7460938   3.7441406   3.7421875   3.7402344   3.7382812   3.7363281
  3.7324219   3.7304688   3.7285156   3.7265625   3.7246094   3.7226562
  3.7207031   3.71875     3.7167969   3.7148438   3.7128906   3.7109375
  3.7070312   3.7050781   3.7011719   3.6992188   3.6972656   3.6953125
  3.6933594   3.6914062   3.6894531   3.6875      3.6855469   3.6835938
  3.6816406   3.6796875   3.6777344   3.6757812   3.6738281   3.671875
  3.6699219   3.6679688   3.6640625   3.6621094   3.6601562   3.6582031
  3.65625     3.6542969   3.6484375   3.6464844   3.6445312   3.6425781
  3.640625    3.6386719   3.6367188   3.6347656   3.6308594   3.6289062
  3.6269531   3.625       3.6230469   3.6191406   3.6171875   3.6152344
  3.6132812   3.6113281   3.6074219   3.59375     3.5917969   3.5898438
  3.5878906   3.5859375   3.5761719   3.5742188   3.5722656   3.5683594
  3.5664062   3.5625      3.5585938   3.5566406   3.5527344   3.5390625
  3.5332031   3.5253906   3.5195312   3.5058594   3.5019531   3.5
  3.4980469   3.4882812   3.4863281   3.484375    3.4824219   3.4804688
  3.4785156   3.4726562   3.4707031   3.4648438   3.4628906   3.4609375
  3.4589844   3.4570312   3.4492188   3.4472656   3.4453125   3.4414062
  3.4394531   3.4355469   3.4335938   3.4316406   3.4296875   3.4277344
  3.4257812   3.4238281   3.421875    3.4199219   3.4179688   3.4101562
  3.4003906   3.3984375   3.3945312   3.390625    3.3886719   3.3867188
  3.3847656   3.375       3.3730469   3.3691406   3.3671875   3.3652344
  3.3613281   3.359375    3.3574219   3.3554688   3.3515625   3.3496094
  3.3476562   3.3417969   3.3398438   3.3359375   3.3320312   3.3300781
  3.3242188   3.3203125   3.3125      3.3085938   3.3066406   3.3007812
  3.2929688   3.2871094   3.2832031   3.2792969   3.2597656   3.2558594
  3.2460938   3.2363281   3.2304688   3.2167969   3.2148438   3.2128906
  3.2089844   3.2070312   3.2050781   3.1992188   3.1933594   3.1699219
  3.1679688   3.1660156   3.1640625   3.1621094   3.1601562   3.1582031
  3.1542969   3.1523438   3.1484375   3.1425781   3.1269531   3.125
  3.1230469   3.1074219   3.1035156   3.0976562   3.0957031   3.09375
  3.0917969   3.0878906   3.0839844   3.0800781   3.078125    3.0761719
  3.0566406   3.0546875   3.0136719   3.0117188   3.0039062   3.0019531
  2.9960938   2.9941406   2.9882812   2.9726562   2.9648438   2.9589844
  2.9511719   2.9433594   2.9101562   2.9082031   2.9042969   2.9023438
  2.9003906   2.890625    2.8867188   2.8847656   2.8808594   2.8789062
  2.875       2.8691406   2.8652344   2.8300781   2.828125    2.8144531
  2.8125      2.7871094   2.7851562   2.7832031   2.7773438   2.7753906
  2.7734375   2.765625    2.7460938   2.7441406   2.7265625   2.7207031
  2.7050781   2.7011719   2.6933594   2.6894531   2.6308594   2.6289062
  2.6054688   2.6015625   2.5996094   2.5917969   2.5         2.4921875
  2.4902344   2.4765625   2.46875     2.4355469   2.4335938   2.4101562
  2.4082031   2.3789062   2.3671875   2.3476562   2.3300781   2.3222656
  2.3183594   2.2792969   2.2773438   2.2597656   2.2460938   2.2363281
  2.2050781   2.1738281   2.1328125   2.125       2.0957031   2.0917969
  2.0429688   2.0410156   1.9931641   1.9716797   1.953125    1.9501953
  1.9335938   1.8925781   1.8818359   1.8769531   1.8691406   1.8671875
  1.8447266   1.8388672   1.7822266   1.7792969   1.7763672   1.765625
  1.7451172   1.7304688   1.7275391   1.7255859   1.7021484   1.6972656
  1.6503906   1.6464844   1.6328125   1.6220703   1.6015625   1.5771484
  1.5742188   1.5498047   1.5351562   1.5302734   1.4921875   1.4628906
  1.4179688   1.4130859   1.3896484   1.3662109   1.3183594   1.2832031
  1.2783203   1.2744141   1.2568359   1.2539062   1.2285156   1.2236328
  1.2001953   1.1679688   1.1533203   1.1455078   1.1445312   1.0986328
  1.0566406   1.0146484   1.0068359   1.0048828   0.9970703   0.96435547
  0.96191406  0.9345703   0.9321289   0.9243164   0.9238281   0.91796875
  0.9013672   0.8964844   0.86083984  0.8598633   0.8251953   0.8232422
  0.80566406  0.79345703  0.71728516  0.70703125  0.69091797  0.67578125
  0.65527344  0.6269531   0.62158203  0.61621094  0.5571289   0.5498047
 -0.12158203 -0.13195801 -0.27783203 -0.296875   -0.37158203 -0.3798828
 -0.41357422 -0.41967773 -0.55615234 -0.5644531  -0.6801758  -0.6977539
 -1.1416016  -1.1494141  -1.1533203  -1.1650391  -1.1796875  -1.3798828
 -1.390625   -1.4130859  -1.4208984  -1.4257812  -1.4287109  -1.5244141
 -1.5292969  -1.703125   -1.7050781  -1.890625   -1.8994141  -1.9414062
 -1.9443359  -1.9902344  -1.9970703  -1.9990234  -2.         -2.0214844
 -2.03125    -2.09375    -2.1015625  -2.1054688  -2.1132812  -2.1601562
 -2.1679688  -2.1796875  -2.1835938  -2.1914062  -2.203125   -2.2285156
 -2.2304688  -2.2597656  -2.2617188  -2.265625   -2.2695312  -2.3789062
 -2.390625   -2.4121094  -2.4160156  -2.4511719  -2.4570312  -2.4589844
 -2.4765625  -2.5        -2.5019531  -2.5234375  -2.5800781  -2.5878906
 -2.6191406  -2.6210938  -2.6464844  -2.6523438  -2.6660156  -2.6757812
 -2.6796875  -2.6855469  -2.7207031  -2.7265625  -2.7285156  -2.734375
 -2.765625   -2.7695312  -2.7871094  -2.7910156  -2.8125     -2.8164062
 -2.8183594  -2.8222656  -2.8378906  -2.8457031  -2.8671875  -2.875
 -2.9257812  -2.9277344  -2.9628906  -2.9648438  -2.9746094  -2.9785156
 -2.9824219  -2.9921875  -2.9960938  -3.0097656  -3.0175781  -3.0273438
 -3.0332031  -3.0351562  -3.0390625  -3.0410156  -3.0429688  -3.0742188
 -3.0859375  -3.0957031  -3.0976562  -3.0996094  -3.1015625  -3.109375
 -3.1210938  -3.125      -3.1386719  -3.1484375  -3.1601562  -3.1757812
 -3.1777344  -3.1933594  -3.1953125  -3.203125   -3.2050781  -3.2148438
 -3.21875    -3.2226562  -3.2597656  -3.2617188  -3.2675781  -3.2734375
 -3.2773438  -3.2792969  -3.28125    -3.2832031  -3.2949219  -3.3007812
 -3.3046875  -3.3066406  -3.3203125  -3.3261719  -3.328125   -3.3300781
 -3.3339844  -3.3378906  -3.3398438  -3.3417969  -3.34375    -3.3457031
 -3.3496094  -3.3632812  -3.3652344  -3.3671875  -3.3691406  -3.3710938
 -3.3730469  -3.3769531  -3.3828125  -3.3867188  -3.390625   -3.3945312
 -3.3964844  -3.4003906  -3.4023438  -3.4042969  -3.40625    -3.4082031
 -3.4121094  -3.4140625  -3.4160156  -3.4238281  -3.4257812  -3.4296875
 -3.4414062  -3.4433594  -3.453125   -3.4550781  -3.4570312  -3.4589844
 -3.4609375  -3.4726562  -3.4746094  -3.484375   -3.4921875  -3.4980469
 -3.5019531  -3.5039062  -3.5078125  -3.5253906  -3.5273438  -3.5410156
 -3.5429688  -3.5449219  -3.5488281  -3.5507812  -3.5527344  -3.5546875
 -3.5585938  -3.5722656  -3.5761719  -3.6347656 ]
