log_loss_steps: 208
eval_steps: 512
check_degradation: 0
----------------Epoch 1/1----------------
Epoch 1/1, Loss after 192 samples: 0.6998
Epoch 1/1, Loss after 400 samples: 0.6868
Mean accuracy: 0.5029, std: 0.0113, lower bound: 0.4798, upper bound: 0.5238 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 496 samples: 0.5030 with eval loss: 0.6847
Best model with eval loss 0.6846968618131453 and eval accuracy 0.5030333670374115 with 496 samples seen is saved
Epoch 1/1, Loss after 608 samples: 0.6845
Epoch 1/1, Loss after 816 samples: 0.6656
Mean accuracy: 0.8503, std: 0.0077, lower bound: 0.8357, upper bound: 0.8650 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 1008 samples: 0.8504 with eval loss: 0.3752
Best model with eval loss 0.37524898999160333 and eval accuracy 0.8503538928210314 with 1008 samples seen is saved
Epoch 1/1, Loss after 1024 samples: 0.5481
Epoch 1/1, Loss after 1232 samples: 0.4330
Epoch 1/1, Loss after 1440 samples: 0.3339
Mean accuracy: 0.8036, std: 0.0087, lower bound: 0.7872, upper bound: 0.8200 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 1520 samples: 0.8033 with eval loss: 0.4197
Epoch 1/1, Loss after 1648 samples: 0.3144
Epoch 1/1, Loss after 1856 samples: 0.3234
Mean accuracy: 0.8185, std: 0.0086, lower bound: 0.8008, upper bound: 0.8347 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 2032 samples: 0.8185 with eval loss: 0.3853
Epoch 1/1, Loss after 2064 samples: 0.2286
Epoch 1/1, Loss after 2272 samples: 0.3448
Epoch 1/1, Loss after 2480 samples: 0.2515
Mean accuracy: 0.7769, std: 0.0091, lower bound: 0.7588, upper bound: 0.7947 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 2544 samples: 0.7770 with eval loss: 0.4950
Epoch 1/1, Loss after 2688 samples: 0.2730
Epoch 1/1, Loss after 2896 samples: 0.1640
Mean accuracy: 0.7011, std: 0.0103, lower bound: 0.6810, upper bound: 0.7204 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 3056 samples: 0.7012 with eval loss: 0.8718
Epoch 1/1, Loss after 3104 samples: 0.2259
Epoch 1/1, Loss after 3312 samples: 0.2529
Epoch 1/1, Loss after 3520 samples: 0.2511
Mean accuracy: 0.7456, std: 0.0096, lower bound: 0.7270, upper bound: 0.7649 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 3568 samples: 0.7457 with eval loss: 0.6042
Epoch 1/1, Loss after 3728 samples: 0.1585
Epoch 1/1, Loss after 3936 samples: 0.1523
Mean accuracy: 0.9209, std: 0.0061, lower bound: 0.9085, upper bound: 0.9328 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 4080 samples: 0.9211 with eval loss: 0.1809
Best model with eval loss 0.18091259295901946 and eval accuracy 0.9211324570273003 with 4080 samples seen is saved
Epoch 1/1, Loss after 4144 samples: 0.2560
Epoch 1/1, Loss after 4352 samples: 0.2174
Epoch 1/1, Loss after 4560 samples: 0.1431
Mean accuracy: 0.8984, std: 0.0067, lower bound: 0.8857, upper bound: 0.9125 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 4592 samples: 0.8984 with eval loss: 0.2454
Epoch 1/1, Loss after 4768 samples: 0.1612
Epoch 1/1, Loss after 4976 samples: 0.2577
Mean accuracy: 0.7626, std: 0.0096, lower bound: 0.7427, upper bound: 0.7811 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 5104 samples: 0.7629 with eval loss: 0.4998
Epoch 1/1, Loss after 5184 samples: 0.2519
Epoch 1/1, Loss after 5392 samples: 0.2018
Epoch 1/1, Loss after 5600 samples: 0.1518
Mean accuracy: 0.8523, std: 0.0081, lower bound: 0.8367, upper bound: 0.8680 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 5616 samples: 0.8519 with eval loss: 0.3425
Epoch 1/1, Loss after 5808 samples: 0.1440
Epoch 1/1, Loss after 6016 samples: 0.1612
Mean accuracy: 0.9181, std: 0.0062, lower bound: 0.9055, upper bound: 0.9302 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 6128 samples: 0.9181 with eval loss: 0.1969
Epoch 1/1, Loss after 6224 samples: 0.2837
Epoch 1/1, Loss after 6432 samples: 0.2023
Epoch 1/1, Loss after 6640 samples: 0.1521
Mean accuracy: 0.8424, std: 0.0079, lower bound: 0.8266, upper bound: 0.8574 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 6640 samples: 0.8428 with eval loss: 0.3659
Epoch 1/1, Loss after 6848 samples: 0.1886
Epoch 1/1, Loss after 7056 samples: 0.1969
Mean accuracy: 0.8312, std: 0.0086, lower bound: 0.8139, upper bound: 0.8478 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 7152 samples: 0.8311 with eval loss: 0.3522
Epoch 1/1, Loss after 7264 samples: 0.2366
Epoch 1/1, Loss after 7472 samples: 0.1677
Mean accuracy: 0.8728, std: 0.0076, lower bound: 0.8589, upper bound: 0.8873 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 7664 samples: 0.8731 with eval loss: 0.2700
Epoch 1/1, Loss after 7680 samples: 0.1862
Epoch 1/1, Loss after 7888 samples: 0.1674
Epoch 1/1, Loss after 8096 samples: 0.1297
Mean accuracy: 0.9076, std: 0.0064, lower bound: 0.8953, upper bound: 0.9206 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 8176 samples: 0.9080 with eval loss: 0.2180
Epoch 1/1, Loss after 8304 samples: 0.1207
Epoch 1/1, Loss after 8512 samples: 0.0644
Mean accuracy: 0.8807, std: 0.0073, lower bound: 0.8655, upper bound: 0.8938 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 8688 samples: 0.8807 with eval loss: 0.3153
Epoch 1/1, Loss after 8720 samples: 0.1273
Epoch 1/1, Loss after 8928 samples: 0.0970
Epoch 1/1, Loss after 9136 samples: 0.0925
Mean accuracy: 0.8662, std: 0.0076, lower bound: 0.8514, upper bound: 0.8807 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 9200 samples: 0.8660 with eval loss: 0.3498
Epoch 1/1, Loss after 9344 samples: 0.1292
Epoch 1/1, Loss after 9552 samples: 0.1877
Mean accuracy: 0.8977, std: 0.0066, lower bound: 0.8847, upper bound: 0.9105 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 9712 samples: 0.8979 with eval loss: 0.2323
Epoch 1/1, Loss after 9760 samples: 0.2129
Epoch 1/1, Loss after 9968 samples: 0.1562
Epoch 1/1, Loss after 10176 samples: 0.1390
Mean accuracy: 0.8921, std: 0.0066, lower bound: 0.8787, upper bound: 0.9050 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 10224 samples: 0.8918 with eval loss: 0.2556
Epoch 1/1, Loss after 10384 samples: 0.1044
Epoch 1/1, Loss after 10592 samples: 0.1183
Mean accuracy: 0.8516, std: 0.0080, lower bound: 0.8372, upper bound: 0.8665 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 10736 samples: 0.8514 with eval loss: 0.3926
Epoch 1/1, Loss after 10800 samples: 0.1283
Epoch 1/1, Loss after 11008 samples: 0.1319
Epoch 1/1, Loss after 11216 samples: 0.1765
Mean accuracy: 0.8717, std: 0.0077, lower bound: 0.8574, upper bound: 0.8873 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 11248 samples: 0.8716 with eval loss: 0.3047
Epoch 1/1, Loss after 11424 samples: 0.1272
Epoch 1/1, Loss after 11632 samples: 0.1386
Mean accuracy: 0.8953, std: 0.0066, lower bound: 0.8827, upper bound: 0.9080 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 11760 samples: 0.8948 with eval loss: 0.2565
Epoch 1/1, Loss after 11840 samples: 0.0970
Epoch 1/1, Loss after 12048 samples: 0.1627
Epoch 1/1, Loss after 12256 samples: 0.1123
Mean accuracy: 0.8586, std: 0.0078, lower bound: 0.8418, upper bound: 0.8736 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 12272 samples: 0.8584 with eval loss: 0.3524
Epoch 1/1, Loss after 12464 samples: 0.1079
Epoch 1/1, Loss after 12672 samples: 0.1228
Mean accuracy: 0.8783, std: 0.0073, lower bound: 0.8640, upper bound: 0.8913 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 12784 samples: 0.8787 with eval loss: 0.2812
Epoch 1/1, Loss after 12880 samples: 0.1377
Epoch 1/1, Loss after 13088 samples: 0.1360
Epoch 1/1, Loss after 13296 samples: 0.1474
Mean accuracy: 0.9083, std: 0.0067, lower bound: 0.8948, upper bound: 0.9206 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 13296 samples: 0.9085 with eval loss: 0.2201
Epoch 1/1, Loss after 13504 samples: 0.1128
Epoch 1/1, Loss after 13712 samples: 0.0969
Mean accuracy: 0.8316, std: 0.0083, lower bound: 0.8170, upper bound: 0.8483 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 13808 samples: 0.8316 with eval loss: 0.4367
Epoch 1/1, Loss after 13920 samples: 0.0917
Epoch 1/1, Loss after 14128 samples: 0.1231
Mean accuracy: 0.8570, std: 0.0078, lower bound: 0.8412, upper bound: 0.8726 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 14320 samples: 0.8569 with eval loss: 0.3674
Epoch 1/1, Loss after 14336 samples: 0.1042
Epoch 1/1, Loss after 14544 samples: 0.1815
Epoch 1/1, Loss after 14752 samples: 0.1303
Mean accuracy: 0.8758, std: 0.0077, lower bound: 0.8610, upper bound: 0.8913 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 14832 samples: 0.8756 with eval loss: 0.3086
Epoch 1/1, Loss after 14960 samples: 0.1131
Epoch 1/1, Loss after 15168 samples: 0.1262
Mean accuracy: 0.8790, std: 0.0073, lower bound: 0.8655, upper bound: 0.8943 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 15344 samples: 0.8792 with eval loss: 0.2992
Epoch 1/1, Loss after 15376 samples: 0.1159
Epoch 1/1, Loss after 15584 samples: 0.1321
Epoch 1/1, Loss after 15792 samples: 0.1460
Training signal is False, stopping training
Training finished
Best model: {'eval_acc': 0.9211324570273003, 'nb_samples': 4080, 'eval_loss': 0.18091259295901946}
Training loss logs: [{'samples': 192, 'loss': 0.6997868464543269}, {'samples': 400, 'loss': 0.6867746206430289}, {'samples': 608, 'loss': 0.6844987135667068}, {'samples': 816, 'loss': 0.6656001164362981}, {'samples': 1024, 'loss': 0.5480649654681866}, {'samples': 1232, 'loss': 0.43304482790140003}, {'samples': 1440, 'loss': 0.3339162583534534}, {'samples': 1648, 'loss': 0.3143693174307163}, {'samples': 1856, 'loss': 0.32337045669555664}, {'samples': 2064, 'loss': 0.22859222155350906}, {'samples': 2272, 'loss': 0.34483185754372525}, {'samples': 2480, 'loss': 0.25150549526398}, {'samples': 2688, 'loss': 0.2730403175720802}, {'samples': 2896, 'loss': 0.16400432242796972}, {'samples': 3104, 'loss': 0.22594385823378196}, {'samples': 3312, 'loss': 0.2528790556467496}, {'samples': 3520, 'loss': 0.2511343640776781}, {'samples': 3728, 'loss': 0.15849882650833863}, {'samples': 3936, 'loss': 0.15230197115586355}, {'samples': 4144, 'loss': 0.2560427481165299}, {'samples': 4352, 'loss': 0.21741745219780848}, {'samples': 4560, 'loss': 0.1430783225939824}, {'samples': 4768, 'loss': 0.16120767421447313}, {'samples': 4976, 'loss': 0.2577396069581692}, {'samples': 5184, 'loss': 0.2518593823680511}, {'samples': 5392, 'loss': 0.20178361122424787}, {'samples': 5600, 'loss': 0.15180002267544085}, {'samples': 5808, 'loss': 0.14398553107793516}, {'samples': 6016, 'loss': 0.1611741236769236}, {'samples': 6224, 'loss': 0.28366218278041255}, {'samples': 6432, 'loss': 0.20226947332804018}, {'samples': 6640, 'loss': 0.15214983316568229}, {'samples': 6848, 'loss': 0.188604474067688}, {'samples': 7056, 'loss': 0.19693611504939887}, {'samples': 7264, 'loss': 0.23659661068366125}, {'samples': 7472, 'loss': 0.16773203015327454}, {'samples': 7680, 'loss': 0.18622724883831465}, {'samples': 7888, 'loss': 0.16737195734794325}, {'samples': 8096, 'loss': 0.12966943761477104}, {'samples': 8304, 'loss': 0.12072541622015145}, {'samples': 8512, 'loss': 0.06438673459566556}, {'samples': 8720, 'loss': 0.1273379480609527}, {'samples': 8928, 'loss': 0.09701339040811245}, {'samples': 9136, 'loss': 0.09248526852864486}, {'samples': 9344, 'loss': 0.1292183083983568}, {'samples': 9552, 'loss': 0.1876665044289369}, {'samples': 9760, 'loss': 0.21286449065575233}, {'samples': 9968, 'loss': 0.15620418179493684}, {'samples': 10176, 'loss': 0.1389669546714196}, {'samples': 10384, 'loss': 0.10443940644080822}, {'samples': 10592, 'loss': 0.11829767548120938}, {'samples': 10800, 'loss': 0.12826679876217476}, {'samples': 11008, 'loss': 0.1319220421405939}, {'samples': 11216, 'loss': 0.17646298958705023}, {'samples': 11424, 'loss': 0.12724976126964277}, {'samples': 11632, 'loss': 0.1385786493237202}, {'samples': 11840, 'loss': 0.09699414097345792}, {'samples': 12048, 'loss': 0.16265531113514534}, {'samples': 12256, 'loss': 0.11232689768075943}, {'samples': 12464, 'loss': 0.10786341933103707}, {'samples': 12672, 'loss': 0.12281946780589911}, {'samples': 12880, 'loss': 0.13769658253743097}, {'samples': 13088, 'loss': 0.13597500037688476}, {'samples': 13296, 'loss': 0.14744975303228086}, {'samples': 13504, 'loss': 0.11279100523545192}, {'samples': 13712, 'loss': 0.09690046081176171}, {'samples': 13920, 'loss': 0.091660958643143}, {'samples': 14128, 'loss': 0.12306688783260492}, {'samples': 14336, 'loss': 0.10416693870837872}, {'samples': 14544, 'loss': 0.181481278859652}, {'samples': 14752, 'loss': 0.13031912251160696}, {'samples': 14960, 'loss': 0.11310090525792195}, {'samples': 15168, 'loss': 0.12620778209888017}, {'samples': 15376, 'loss': 0.11591283690470916}, {'samples': 15584, 'loss': 0.1320760278747632}, {'samples': 15792, 'loss': 0.1460118184869106}]
Evaluation accuracy logs: [{'samples': 496, 'accuracy': 0.5029307381193124, 'std': 0.0112666413318149, 'lower_bound': 0.47976491405460064, 'upper_bound': 0.5237613751263903}, {'samples': 1008, 'accuracy': 0.850303842264914, 'std': 0.0077366255894156365, 'lower_bound': 0.8356926188068756, 'upper_bound': 0.8650278058645097}, {'samples': 1520, 'accuracy': 0.8035884732052578, 'std': 0.008722300210385196, 'lower_bound': 0.7871587462082912, 'upper_bound': 0.8200328614762387}, {'samples': 2032, 'accuracy': 0.818546006066734, 'std': 0.008578688528534547, 'lower_bound': 0.8008088978766431, 'upper_bound': 0.8346814964610718}, {'samples': 2544, 'accuracy': 0.7768629929221436, 'std': 0.00910894097946543, 'lower_bound': 0.7588473205257836, 'upper_bound': 0.7947421638018201}, {'samples': 3056, 'accuracy': 0.7011274014155713, 'std': 0.01025849647121679, 'lower_bound': 0.6809908998988877, 'upper_bound': 0.7204246713852376}, {'samples': 3568, 'accuracy': 0.7455576339737109, 'std': 0.00964054428277348, 'lower_bound': 0.72698432760364, 'upper_bound': 0.7649140546006067}, {'samples': 4080, 'accuracy': 0.9208852376137513, 'std': 0.0060892294220606675, 'lower_bound': 0.9084934277047523, 'upper_bound': 0.9327603640040445}, {'samples': 4592, 'accuracy': 0.8983761375126389, 'std': 0.006742485266767768, 'lower_bound': 0.8857431749241659, 'upper_bound': 0.9125379170879676}, {'samples': 5104, 'accuracy': 0.7625616784630941, 'std': 0.009578544434116756, 'lower_bound': 0.7426693629929222, 'upper_bound': 0.7810920121334681}, {'samples': 5616, 'accuracy': 0.8522694641051567, 'std': 0.00808863875359711, 'lower_bound': 0.8366911021233568, 'upper_bound': 0.8680485338725986}, {'samples': 6128, 'accuracy': 0.9181471183013145, 'std': 0.006192246796122017, 'lower_bound': 0.9054600606673407, 'upper_bound': 0.9302325581395349}, {'samples': 6640, 'accuracy': 0.8423816986855409, 'std': 0.007936692752561016, 'lower_bound': 0.8265798786653185, 'upper_bound': 0.8574443882709808}, {'samples': 7152, 'accuracy': 0.8311739130434782, 'std': 0.00864430007156323, 'lower_bound': 0.8139408493427704, 'upper_bound': 0.8478260869565217}, {'samples': 7664, 'accuracy': 0.8728442871587463, 'std': 0.007578323263607065, 'lower_bound': 0.858948432760364, 'upper_bound': 0.8872598584428716}, {'samples': 8176, 'accuracy': 0.9075692618806875, 'std': 0.006422704853909689, 'lower_bound': 0.8953488372093024, 'upper_bound': 0.9206268958543984}, {'samples': 8688, 'accuracy': 0.8806658240647118, 'std': 0.007257808289431037, 'lower_bound': 0.865520728008089, 'upper_bound': 0.8938321536905965}, {'samples': 9200, 'accuracy': 0.8662320525783621, 'std': 0.00759983647243325, 'lower_bound': 0.8513650151668352, 'upper_bound': 0.8807002022244692}, {'samples': 9712, 'accuracy': 0.8976996966632962, 'std': 0.006576911733246112, 'lower_bound': 0.884732052578362, 'upper_bound': 0.91051567239636}, {'samples': 10224, 'accuracy': 0.8921162790697675, 'std': 0.006608068698734531, 'lower_bound': 0.8786653185035389, 'upper_bound': 0.9049544994944388}, {'samples': 10736, 'accuracy': 0.8516046511627908, 'std': 0.00795193766621776, 'lower_bound': 0.8371966632962589, 'upper_bound': 0.8665318503538928}, {'samples': 11248, 'accuracy': 0.8716825075834177, 'std': 0.007683542856101252, 'lower_bound': 0.8574317492416582, 'upper_bound': 0.8872598584428716}, {'samples': 11760, 'accuracy': 0.8952810920121335, 'std': 0.006576650264353343, 'lower_bound': 0.8827098078867543, 'upper_bound': 0.908000505561173}, {'samples': 12272, 'accuracy': 0.858582406471183, 'std': 0.007820065980371103, 'lower_bound': 0.8417593528816987, 'upper_bound': 0.8736097067745198}, {'samples': 12784, 'accuracy': 0.8783154701718908, 'std': 0.007288977823868509, 'lower_bound': 0.8640040444893832, 'upper_bound': 0.8913169868554095}, {'samples': 13296, 'accuracy': 0.908278058645096, 'std': 0.006736126436065131, 'lower_bound': 0.8948432760364005, 'upper_bound': 0.920639534883721}, {'samples': 13808, 'accuracy': 0.831623356926188, 'std': 0.008256211752128184, 'lower_bound': 0.8169868554095046, 'upper_bound': 0.8483442871587462}, {'samples': 14320, 'accuracy': 0.8570182002022244, 'std': 0.007835821054886878, 'lower_bound': 0.8412411526794742, 'upper_bound': 0.8725985844287159}, {'samples': 14832, 'accuracy': 0.8757937310414561, 'std': 0.007654663637278509, 'lower_bound': 0.8609706774519716, 'upper_bound': 0.8913043478260869}, {'samples': 15344, 'accuracy': 0.8789984833164813, 'std': 0.007303788198678633, 'lower_bound': 0.865520728008089, 'upper_bound': 0.8943377148634984}]
Evaluating the best model on the test set of dataset ./fake_true_datasets/fake_true_dataset_gemma_10k...
Test metrics:
accuracy: 0.8658604651162791
precision: 0.7889488968636913
recall: 0.9989931684470866
f1_score: 0.8815892422147708
fp_rate: 0.26732767963792503
tp_rate: 0.9989931684470866
std_accuracy: 0.0075051303812828166
std_precision: 0.011239415180984692
std_recall: 0.0009918957777039755
std_f1_score: 0.007057976259291
std_fp_rate: 0.013990481370006603
std_tp_rate: 0.0009918957777039755
TP: 988.141
TN: 724.531
FP: 264.333
FN: 0.995
roc_auc: 0.9912439258537543
fpr: [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00101112 0.00101112 0.00101112
 0.00101112 0.00101112 0.00101112 0.00101112 0.00101112 0.00101112
 0.00101112 0.00101112 0.00101112 0.00101112 0.00101112 0.00101112
 0.00101112 0.00101112 0.00101112 0.00101112 0.00101112 0.00101112
 0.00101112 0.00101112 0.00101112 0.00101112 0.00101112 0.00101112
 0.00101112 0.00101112 0.00101112 0.00101112 0.00101112 0.00101112
 0.00101112 0.00101112 0.00101112 0.00101112 0.00101112 0.00101112
 0.00101112 0.00101112 0.00101112 0.00101112 0.00202224 0.00202224
 0.00202224 0.00202224 0.00202224 0.00202224 0.00202224 0.00202224
 0.00303337 0.00303337 0.00303337 0.00303337 0.00303337 0.00303337
 0.00303337 0.00303337 0.00303337 0.00303337 0.00303337 0.00404449
 0.00404449 0.00404449 0.00404449 0.00404449 0.00404449 0.00404449
 0.00404449 0.00404449 0.00404449 0.00404449 0.00404449 0.00404449
 0.00505561 0.00505561 0.00606673 0.00606673 0.00707786 0.00808898
 0.0091001  0.0091001  0.01011122 0.01011122 0.01112235 0.01112235
 0.01213347 0.01213347 0.01213347 0.01213347 0.01314459 0.01415571
 0.01415571 0.01415571 0.01415571 0.01415571 0.01415571 0.01516684
 0.01516684 0.01718908 0.01718908 0.01718908 0.01718908 0.0182002
 0.0182002  0.01921132 0.01921132 0.02022245 0.02022245 0.02123357
 0.02123357 0.02224469 0.02224469 0.02426694 0.02426694 0.02527806
 0.02527806 0.02527806 0.02527806 0.02527806 0.02527806 0.02527806
 0.0273003  0.0273003  0.02932255 0.02932255 0.03033367 0.03033367
 0.03033367 0.03033367 0.03134479 0.03134479 0.03336704 0.03336704
 0.03437816 0.03437816 0.03538928 0.03538928 0.03741153 0.03741153
 0.03842265 0.03842265 0.03943377 0.03943377 0.04044489 0.04044489
 0.04246714 0.04246714 0.04246714 0.04347826 0.04347826 0.04550051
 0.04550051 0.04651163 0.04651163 0.04954499 0.05055612 0.05055612
 0.05156724 0.05257836 0.05358948 0.05358948 0.05460061 0.05561173
 0.05561173 0.05763397 0.05763397 0.05763397 0.0586451  0.0586451
 0.06066734 0.06066734 0.06167846 0.06268959 0.06268959 0.06471183
 0.06471183 0.06572295 0.06572295 0.06673407 0.06673407 0.06875632
 0.06875632 0.07178969 0.07178969 0.07280081 0.07280081 0.07482305
 0.07482305 0.07583418 0.07583418 0.07785642 0.07785642 0.07886754
 0.08088979 0.08190091 0.08392315 0.08392315 0.08493428 0.0859454
 0.0859454  0.08796764 0.08796764 0.09302326 0.0950455  0.10313448
 0.10313448 0.10920121 0.10920121 0.11021234 0.11021234 0.11122346
 0.11223458 0.1132457  0.1132457  0.11526795 0.13447927 0.13447927
 0.13549039 0.13751264 0.14560162 0.14560162 0.14964611 0.14964611
 0.15166835 0.15166835 0.15874621 0.16076845 0.16582406 0.16582406
 0.17391304 0.17391304 0.17492417 0.17694641 0.19312437 0.19312437
 0.19615774 0.19615774 0.2173913  0.21941355 0.23458038 0.23559151
 0.24368049 0.24368049 0.26491405 0.26491405 0.30232558 0.30232558
 0.30738119 0.30940344 0.32659252 0.32861476 0.35591507 0.35793731
 0.38523761 0.3892821  0.4206269  0.42264914 0.42467139 0.42669363
 0.429727   0.44084934 0.44287159 0.44388271 0.4479272  0.46107179
 0.46309403 0.46511628 0.46713852 0.48028311 0.48331648 0.49544995
 0.49747219 0.50252781 0.5065723  0.51263903 0.51668352 0.53083923
 0.53286148 0.55005056 0.5520728  0.56218402 0.56420627 0.57128413
 0.57330637 0.59150657 0.59555106 0.6016178  0.60364004 0.61577351
 0.61779575 0.62790698 0.63195147 0.63700708 0.63902932 0.64307381
 0.6471183  0.65520728 0.65824065 0.66228514 0.66734075 0.66936299
 0.67542973 0.67745197 0.67846309 0.68048534 0.68452983 0.6875632
 0.69767442 0.69969666 0.71688574 0.71890799 0.73710819 0.73913043
 0.75530839 0.75733064 0.76238625 0.76440849 0.76946411 0.77148635
 0.77249747 0.77553084 0.78968655 0.7917088  0.79373104 0.79575329
 0.81193124 0.81395349 0.81496461 0.81698686 0.82103134 0.82406471
 0.82608696 0.84024267 0.84226491 0.84934277 0.85136502 0.85945399
 0.86248736 0.86450961 0.86552073 0.86754297 0.8685541  0.87158746
 0.87664307 0.87866532 0.88372093 0.8867543  0.89282103 0.89484328
 0.8958544  0.89787664 0.90798787 0.91001011 0.91203236 0.9140546
 0.91708797 0.92012133 0.93023256 0.9322548  0.93933266 0.94236603
 0.95045501 0.95348837 0.96258847 0.96461072 0.96663296 0.96865521
 0.97168857 0.97371082 0.97472194 0.97674419 0.9817998  0.98382204
 1.        ]
tpr: [0.         0.00101112 0.01516684 0.0182002  0.02022245 0.02224469
 0.02325581 0.02527806 0.02932255 0.03134479 0.03538928 0.03741153
 0.04246714 0.04448938 0.05055612 0.05257836 0.05662285 0.06471183
 0.06572295 0.0677452  0.06875632 0.07178969 0.07381193 0.07482305
 0.07886754 0.08088979 0.08291203 0.0859454  0.08796764 0.09201213
 0.09403438 0.09908999 0.10111223 0.10313448 0.10717897 0.10819009
 0.11021234 0.11223458 0.11627907 0.11729019 0.11931244 0.12032356
 0.1223458  0.12639029 0.13245703 0.13650152 0.13751264 0.14560162
 0.14661274 0.14863498 0.15267947 0.1536906  0.15975733 0.16076845
 0.1627907  0.16481294 0.16683519 0.16986855 0.1718908  0.17492417
 0.17795753 0.17997978 0.18200202 0.18301314 0.18907988 0.190091
 0.19312437 0.19615774 0.19817998 0.20121335 0.20323559 0.20728008
 0.21031345 0.21132457 0.21334681 0.21435794 0.21840243 0.21941355
 0.22143579 0.22446916 0.22649141 0.22952477 0.23356926 0.23559151
 0.24064712 0.24368049 0.24570273 0.24772497 0.25075834 0.25176946
 0.25379171 0.25480283 0.25682508 0.2578362  0.26188069 0.26390293
 0.26794742 0.26996967 0.27199191 0.27502528 0.27906977 0.28311426
 0.28412538 0.28816987 0.29120324 0.29524772 0.29625885 0.30131446
 0.30232558 0.30434783 0.30738119 0.30940344 0.3124368  0.31850354
 0.32052578 0.32254803 0.32457027 0.32659252 0.33164813 0.34479272
 0.34681496 0.35389282 0.35591507 0.35692619 0.35894843 0.37310415
 0.37512639 0.38321537 0.38523761 0.39635996 0.3983822  0.40444894
 0.40647118 0.41759353 0.41961577 0.42163802 0.42366026 0.42669363
 0.429727   0.43174924 0.43276036 0.43478261 0.43579373 0.4388271
 0.44287159 0.44388271 0.4479272  0.44893832 0.45096057 0.45904954
 0.46107179 0.46814965 0.47017189 0.47724975 0.47927199 0.48028311
 0.48230536 0.48331648 0.48533873 0.48736097 0.48938322 0.49848332
 0.50151668 0.5156724  0.51769464 0.51971689 0.52173913 0.52679474
 0.52982811 0.53993933 0.54196158 0.5429727  0.54499494 0.54802831
 0.55005056 0.56521739 0.56926188 0.57128413 0.57735086 0.58442872
 0.58746208 0.58948433 0.59150657 0.59150657 0.59352882 0.59555106
 0.59757331 0.60465116 0.60667341 0.6107179  0.61274014 0.63094034
 0.63296259 0.6562184  0.65824065 0.66228514 0.66430738 0.66632963
 0.66835187 0.67138524 0.67340748 0.68149646 0.68351871 0.68554095
 0.6875632  0.68857432 0.69059656 0.69160768 0.69362993 0.69464105
 0.69868554 0.69969666 0.70171891 0.70374115 0.70778564 0.70879676
 0.71183013 0.71587462 0.71688574 0.72093023 0.72194135 0.7239636
 0.73205258 0.73508595 0.73710819 0.74115268 0.74115268 0.7421638
 0.74620829 0.74924166 0.7512639  0.75631951 0.75834176 0.75935288
 0.75935288 0.76137513 0.76238625 0.76440849 0.76643074 0.76946411
 0.77047523 0.77249747 0.77350859 0.77553084 0.77957533 0.78058645
 0.78159757 0.78564206 0.78665319 0.78867543 0.79271992 0.79575329
 0.79979778 0.80384226 0.81698686 0.82103134 0.82406471 0.82608696
 0.82709808 0.83518706 0.83518706 0.83822042 0.83822042 0.83923155
 0.83923155 0.84024267 0.84024267 0.84226491 0.84226491 0.84327604
 0.84327604 0.84428716 0.8463094  0.85136502 0.85237614 0.85237614
 0.85338726 0.8554095  0.85844287 0.86046512 0.86349848 0.86349848
 0.86450961 0.86450961 0.86653185 0.8685541  0.86956522 0.87057634
 0.87866532 0.87866532 0.87967644 0.88068756 0.88169869 0.88169869
 0.88372093 0.88372093 0.88574317 0.88574317 0.8867543  0.8867543
 0.89079879 0.89282103 0.89686552 0.89888777 0.89989889 0.90192113
 0.90192113 0.90899899 0.90899899 0.91001011 0.91001011 0.91203236
 0.9140546  0.91708797 0.91708797 0.91809909 0.91809909 0.91911021
 0.91911021 0.92012133 0.92113246 0.92214358 0.92214358 0.92416582
 0.92416582 0.92517695 0.92517695 0.92821031 0.92821031 0.92922144
 0.92922144 0.93124368 0.93326593 0.93326593 0.93832154 0.93832154
 0.9413549  0.9413549  0.94337715 0.94337715 0.94438827 0.94641052
 0.94641052 0.94742164 0.94742164 0.94843276 0.94843276 0.94944388
 0.95045501 0.95045501 0.95247725 0.95449949 0.95551062 0.95652174
 0.95652174 0.95854398 0.95854398 0.95955511 0.96056623 0.96056623
 0.96157735 0.96157735 0.9635996  0.9635996  0.96562184 0.96562184
 0.96663296 0.96663296 0.96865521 0.96865521 0.96966633 0.96966633
 0.97067745 0.97067745 0.9726997  0.9726997  0.97371082 0.97472194
 0.97472194 0.97573306 0.97573306 0.97775531 0.97876643 0.97876643
 0.97977755 0.97977755 0.98281092 0.98281092 0.98281092 0.98281092
 0.98382204 0.98382204 0.98483316 0.98483316 0.98584429 0.98584429
 0.98685541 0.98685541 0.98786653 0.98786653 0.98786653 0.98887765
 0.98887765 0.98887765 0.98887765 0.98988878 0.98988878 0.9908999
 0.9908999  0.99191102 0.99191102 0.99191102 0.99191102 0.99292214
 0.99292214 0.99393327 0.99393327 0.99393327 0.99393327 0.99494439
 0.99494439 0.99595551 0.99595551 0.99595551 0.99595551 0.99696663
 0.99696663 0.99797776 0.99797776 0.99898888 0.99898888 1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.        ]
thresholds: [        inf  6.5117188   6.1171875   6.1015625   6.09375     6.0898438
  6.0859375   6.0820312   6.0585938   6.0546875   6.0351562   6.0234375
  6.0039062   6.          5.9726562   5.9609375   5.9257812   5.9101562
  5.9023438   5.8984375   5.8945312   5.890625    5.8867188   5.8828125
  5.8789062   5.8710938   5.8671875   5.8632812   5.8554688   5.8476562
  5.8398438   5.8085938   5.8007812   5.7929688   5.78125     5.7773438
  5.7695312   5.7578125   5.7539062   5.7460938   5.7421875   5.7382812
  5.734375    5.6992188   5.6875      5.6835938   5.6796875   5.6640625
  5.6523438   5.6484375   5.6445312   5.640625    5.6289062   5.625
  5.6210938   5.6054688   5.6015625   5.5976562   5.5898438   5.5664062
  5.5625      5.5546875   5.5507812   5.5429688   5.5234375   5.5195312
  5.515625    5.5039062   5.5         5.4882812   5.484375    5.4570312
  5.4453125   5.4414062   5.4375      5.4296875   5.4179688   5.4140625
  5.4101562   5.40625     5.3984375   5.390625    5.3671875   5.3632812
  5.3398438   5.3359375   5.3242188   5.3203125   5.3164062   5.3085938
  5.3046875   5.3007812   5.296875    5.2890625   5.2773438   5.2695312
  5.2539062   5.2265625   5.2226562   5.21875     5.1875      5.1679688
  5.1640625   5.1484375   5.1367188   5.1289062   5.125       5.1171875
  5.1132812   5.0976562   5.0742188   5.0703125   5.0585938   5.0273438
  5.015625    5.0078125   4.9726562   4.9648438   4.953125    4.859375
  4.8515625   4.7890625   4.7851562   4.78125     4.734375    4.609375
  4.5625      4.3945312   4.3789062   4.296875    4.2773438   4.2304688
  4.2148438   4.125       4.1210938   4.109375    4.1054688   4.1015625
  4.0898438   4.0859375   4.0742188   4.0664062   4.046875    4.0390625
  4.03125     4.0273438   4.015625    4.0078125   3.9921875   3.9453125
  3.9375      3.8867188   3.8847656   3.8457031   3.8417969   3.8398438
  3.8378906   3.828125    3.8242188   3.8066406   3.8046875   3.7675781
  3.7597656   3.6699219   3.6621094   3.6582031   3.6542969   3.6230469
  3.6191406   3.5722656   3.5703125   3.5664062   3.5644531   3.5410156
  3.5371094   3.4550781   3.4394531   3.4238281   3.390625    3.3613281
  3.359375    3.3496094   3.3457031   3.3398438   3.3378906   3.3164062
  3.3125      3.2792969   3.2753906   3.2597656   3.2578125   3.1777344
  3.1738281   3.0625      3.0488281   3.0136719   3.0117188   3.0039062
  2.9980469   2.9667969   2.9609375   2.9296875   2.9277344   2.921875
  2.9199219   2.9179688   2.9160156   2.9140625   2.9042969   2.9023438
  2.8867188   2.8847656   2.8789062   2.8730469   2.8554688   2.8535156
  2.8457031   2.8359375   2.8339844   2.8300781   2.8261719   2.8242188
  2.7871094   2.7851562   2.7773438   2.7636719   2.75        2.7480469
  2.7304688   2.7148438   2.7089844   2.6953125   2.6875      2.6816406
  2.6796875   2.6738281   2.671875    2.6640625   2.6601562   2.6582031
  2.65625     2.6503906   2.6464844   2.6445312   2.6171875   2.609375
  2.6054688   2.6015625   2.5996094   2.5976562   2.5878906   2.5859375
  2.5703125   2.5664062   2.5136719   2.5097656   2.5078125   2.5019531
  2.4941406   2.46875     2.4628906   2.4453125   2.4335938   2.4316406
  2.4296875   2.4238281   2.421875    2.4179688   2.4121094   2.4101562
  2.4042969   2.4023438   2.3984375   2.3710938   2.3691406   2.359375
  2.3574219   2.3496094   2.3359375   2.3339844   2.3164062   2.3125
  2.3066406   2.2988281   2.2871094   2.2851562   2.28125     2.2773438
  2.2304688   2.2226562   2.21875     2.2109375   2.1992188   2.1914062
  2.1835938   2.1796875   2.1757812   2.171875    2.1679688   2.1523438
  2.1367188   2.1308594   2.1210938   2.1152344   2.1035156   2.0996094
  2.0917969   2.0371094   2.0273438   2.0214844   2.0058594   1.9980469
  1.9921875   1.9814453   1.9804688   1.9746094   1.953125    1.9521484
  1.9482422   1.9365234   1.9287109   1.9267578   1.9228516   1.9169922
  1.9130859   1.9101562   1.8886719   1.8701172   1.8691406   1.8613281
  1.8417969   1.8378906   1.8173828   1.8164062   1.7988281   1.7910156
  1.7744141   1.7734375   1.7617188   1.7568359   1.7558594   1.7529297
  1.7470703   1.7441406   1.7412109   1.7236328   1.7216797   1.7207031
  1.7119141   1.6972656   1.6875      1.6855469   1.6816406   1.6777344
  1.6757812   1.6679688   1.6650391   1.6601562   1.6513672   1.6367188
  1.6328125   1.6142578   1.6005859   1.5986328   1.5947266   1.5839844
  1.5810547   1.5566406   1.5302734   1.5107422   1.5068359   1.4990234
  1.4970703   1.4833984   1.4785156   1.4755859   1.4677734   1.4638672
  1.4599609   1.4511719   1.4492188   1.4423828   1.4316406   1.4267578
  1.4042969   1.3925781   1.3574219   1.2832031   1.2802734   1.2050781
  1.2041016   1.1689453   1.1679688   1.1425781   1.1337891   1.1328125
  1.1298828   1.1103516   1.1083984   1.1064453   0.9848633   0.94384766
  0.9355469   0.93066406  0.8720703   0.8647461   0.8383789   0.8300781
  0.8203125   0.81933594  0.73339844  0.7324219   0.7104492   0.69873047
  0.65527344  0.65185547  0.6489258   0.6435547   0.53466797  0.5341797
  0.52441406  0.5102539   0.27978516  0.27856445  0.20166016  0.20019531
  0.1451416   0.14282227  0.01445007  0.01387024 -0.2607422  -0.2685547
 -0.2890625  -0.3034668  -0.41918945 -0.43530273 -0.6123047  -0.61865234
 -0.7944336  -0.80859375 -1.0205078  -1.0273438  -1.0292969  -1.03125
 -1.0341797  -1.0869141  -1.0898438  -1.0957031  -1.1054688  -1.1855469
 -1.1914062  -1.2109375  -1.2138672  -1.2929688  -1.296875   -1.3671875
 -1.3779297  -1.40625    -1.4091797  -1.4326172  -1.4570312  -1.5224609
 -1.5253906  -1.6162109  -1.6201172  -1.6855469  -1.6865234  -1.7412109
 -1.7460938  -1.8779297  -1.8867188  -1.9228516  -1.9257812  -1.9970703
 -2.0058594  -2.1132812  -2.1230469  -2.1542969  -2.1621094  -2.1816406
 -2.2070312  -2.2578125  -2.2636719  -2.2773438  -2.3046875  -2.3242188
 -2.3632812  -2.3730469  -2.3828125  -2.3867188  -2.4121094  -2.4199219
 -2.4707031  -2.4765625  -2.5761719  -2.5839844  -2.6757812  -2.6894531
 -2.8398438  -2.8457031  -2.8691406  -2.8710938  -2.9101562  -2.9121094
 -2.9140625  -2.9179688  -3.0429688  -3.0449219  -3.0644531  -3.0703125
 -3.1660156  -3.1679688  -3.1914062  -3.1953125  -3.21875    -3.2246094
 -3.2324219  -3.359375   -3.3652344  -3.4355469  -3.4375     -3.4726562
 -3.4746094  -3.4765625  -3.4980469  -3.5136719  -3.5175781  -3.5253906
 -3.5898438  -3.5957031  -3.6367188  -3.6425781  -3.6640625  -3.6660156
 -3.6816406  -3.703125   -3.8222656  -3.8242188  -3.8496094  -3.8515625
 -3.8730469  -3.8789062  -3.9765625  -3.9941406  -4.0703125  -4.0742188
 -4.171875   -4.1835938  -4.2382812  -4.2539062  -4.2734375  -4.2890625
 -4.3242188  -4.3320312  -4.3789062  -4.390625   -4.4960938  -4.5039062
 -5.1523438 ]
