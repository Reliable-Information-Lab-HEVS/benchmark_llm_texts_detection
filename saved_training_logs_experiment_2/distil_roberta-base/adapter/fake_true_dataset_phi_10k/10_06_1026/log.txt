log_loss_steps: 208
eval_steps: 512
check_degradation: 0
----------------Epoch 1/1----------------
Epoch 1/1, Loss after 192 samples: 0.6979
Epoch 1/1, Loss after 400 samples: 0.6954
Mean accuracy: 0.7795, std: 0.0094, lower bound: 0.7605, upper bound: 0.7982 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 496 samples: 0.7796 with eval loss: 0.6775
Best model with eval loss 0.67745947265625 and eval accuracy 0.7796184738955824 with 496 samples seen is saved
Epoch 1/1, Loss after 608 samples: 0.6813
Epoch 1/1, Loss after 816 samples: 0.6418
Mean accuracy: 0.8405, std: 0.0083, lower bound: 0.8243, upper bound: 0.8569 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 1008 samples: 0.8404 with eval loss: 0.3856
Best model with eval loss 0.3855964490175247 and eval accuracy 0.8403614457831325 with 1008 samples seen is saved
Epoch 1/1, Loss after 1024 samples: 0.4512
Epoch 1/1, Loss after 1232 samples: 0.3806
Epoch 1/1, Loss after 1440 samples: 0.3078
Mean accuracy: 0.8324, std: 0.0084, lower bound: 0.8153, upper bound: 0.8484 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 1520 samples: 0.8323 with eval loss: 0.3889
Epoch 1/1, Loss after 1648 samples: 0.2956
Epoch 1/1, Loss after 1856 samples: 0.2548
Mean accuracy: 0.8849, std: 0.0069, lower bound: 0.8710, upper bound: 0.8981 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 2032 samples: 0.8850 with eval loss: 0.2895
Best model with eval loss 0.28951745402812956 and eval accuracy 0.8850401606425703 with 2032 samples seen is saved
Epoch 1/1, Loss after 2064 samples: 0.1954
Epoch 1/1, Loss after 2272 samples: 0.2214
Epoch 1/1, Loss after 2480 samples: 0.2091
Mean accuracy: 0.8459, std: 0.0082, lower bound: 0.8293, upper bound: 0.8614 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 2544 samples: 0.8459 with eval loss: 0.3490
Epoch 1/1, Loss after 2688 samples: 0.2068
Epoch 1/1, Loss after 2896 samples: 0.2463
Mean accuracy: 0.8602, std: 0.0077, lower bound: 0.8449, upper bound: 0.8760 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 3056 samples: 0.8604 with eval loss: 0.3064
Epoch 1/1, Loss after 3104 samples: 0.2001
Epoch 1/1, Loss after 3312 samples: 0.1307
Epoch 1/1, Loss after 3520 samples: 0.2057
Mean accuracy: 0.9430, std: 0.0052, lower bound: 0.9327, upper bound: 0.9528 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 3568 samples: 0.9428 with eval loss: 0.1593
Best model with eval loss 0.1593276006579399 and eval accuracy 0.9427710843373494 with 3568 samples seen is saved
Epoch 1/1, Loss after 3728 samples: 0.2219
Epoch 1/1, Loss after 3936 samples: 0.1606
Mean accuracy: 0.8907, std: 0.0070, lower bound: 0.8770, upper bound: 0.9041 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 4080 samples: 0.8906 with eval loss: 0.2547
Epoch 1/1, Loss after 4144 samples: 0.1994
Epoch 1/1, Loss after 4352 samples: 0.1489
Epoch 1/1, Loss after 4560 samples: 0.1879
Mean accuracy: 0.9157, std: 0.0061, lower bound: 0.9041, upper bound: 0.9277 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 4592 samples: 0.9162 with eval loss: 0.1980
Epoch 1/1, Loss after 4768 samples: 0.1969
Epoch 1/1, Loss after 4976 samples: 0.1793
Mean accuracy: 0.9462, std: 0.0050, lower bound: 0.9362, upper bound: 0.9558 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 5104 samples: 0.9463 with eval loss: 0.1352
Best model with eval loss 0.13518625646829605 and eval accuracy 0.946285140562249 with 5104 samples seen is saved
Epoch 1/1, Loss after 5184 samples: 0.2079
Epoch 1/1, Loss after 5392 samples: 0.1555
Epoch 1/1, Loss after 5600 samples: 0.2229
Mean accuracy: 0.9445, std: 0.0052, lower bound: 0.9337, upper bound: 0.9538 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 5616 samples: 0.9443 with eval loss: 0.1312
Best model with eval loss 0.13120911598205567 and eval accuracy 0.9442771084337349 with 5616 samples seen is saved
Epoch 1/1, Loss after 5808 samples: 0.1893
Epoch 1/1, Loss after 6016 samples: 0.1500
Mean accuracy: 0.8700, std: 0.0074, lower bound: 0.8549, upper bound: 0.8840 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 6128 samples: 0.8700 with eval loss: 0.3181
Epoch 1/1, Loss after 6224 samples: 0.1658
Epoch 1/1, Loss after 6432 samples: 0.1609
Epoch 1/1, Loss after 6640 samples: 0.1630
Mean accuracy: 0.8387, std: 0.0081, lower bound: 0.8228, upper bound: 0.8544 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 6640 samples: 0.8389 with eval loss: 0.3993
Epoch 1/1, Loss after 6848 samples: 0.1595
Epoch 1/1, Loss after 7056 samples: 0.1423
Mean accuracy: 0.8784, std: 0.0072, lower bound: 0.8645, upper bound: 0.8926 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 7152 samples: 0.8785 with eval loss: 0.3208
Epoch 1/1, Loss after 7264 samples: 0.2253
Epoch 1/1, Loss after 7472 samples: 0.1324
Mean accuracy: 0.9020, std: 0.0067, lower bound: 0.8885, upper bound: 0.9137 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 7664 samples: 0.9026 with eval loss: 0.2319
Epoch 1/1, Loss after 7680 samples: 0.1843
Epoch 1/1, Loss after 7888 samples: 0.1520
Epoch 1/1, Loss after 8096 samples: 0.1368
Mean accuracy: 0.8256, std: 0.0083, lower bound: 0.8092, upper bound: 0.8419 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 8176 samples: 0.8253 with eval loss: 0.4623
Epoch 1/1, Loss after 8304 samples: 0.1771
Epoch 1/1, Loss after 8512 samples: 0.1157
Mean accuracy: 0.8954, std: 0.0071, lower bound: 0.8805, upper bound: 0.9086 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 8688 samples: 0.8951 with eval loss: 0.2629
Epoch 1/1, Loss after 8720 samples: 0.1389
Epoch 1/1, Loss after 8928 samples: 0.1486
Epoch 1/1, Loss after 9136 samples: 0.1669
Mean accuracy: 0.8273, std: 0.0084, lower bound: 0.8097, upper bound: 0.8439 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 9200 samples: 0.8273 with eval loss: 0.4314
Epoch 1/1, Loss after 9344 samples: 0.1244
Epoch 1/1, Loss after 9552 samples: 0.1026
Mean accuracy: 0.8345, std: 0.0083, lower bound: 0.8188, upper bound: 0.8504 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 9712 samples: 0.8343 with eval loss: 0.4878
Epoch 1/1, Loss after 9760 samples: 0.1424
Epoch 1/1, Loss after 9968 samples: 0.1751
Epoch 1/1, Loss after 10176 samples: 0.1093
Mean accuracy: 0.8575, std: 0.0082, lower bound: 0.8414, upper bound: 0.8735 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 10224 samples: 0.8574 with eval loss: 0.3762
Epoch 1/1, Loss after 10384 samples: 0.0808
Epoch 1/1, Loss after 10592 samples: 0.0543
Mean accuracy: 0.8571, std: 0.0076, lower bound: 0.8414, upper bound: 0.8720 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 10736 samples: 0.8569 with eval loss: 0.4918
Epoch 1/1, Loss after 10800 samples: 0.1021
Epoch 1/1, Loss after 11008 samples: 0.1816
Epoch 1/1, Loss after 11216 samples: 0.0706
Mean accuracy: 0.8725, std: 0.0073, lower bound: 0.8584, upper bound: 0.8865 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 11248 samples: 0.8730 with eval loss: 0.3238
Epoch 1/1, Loss after 11424 samples: 0.0933
Epoch 1/1, Loss after 11632 samples: 0.0898
Mean accuracy: 0.8678, std: 0.0075, lower bound: 0.8524, upper bound: 0.8815 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 11760 samples: 0.8675 with eval loss: 0.3776
Epoch 1/1, Loss after 11840 samples: 0.1443
Epoch 1/1, Loss after 12048 samples: 0.1133
Epoch 1/1, Loss after 12256 samples: 0.1508
Mean accuracy: 0.8843, std: 0.0072, lower bound: 0.8695, upper bound: 0.8981 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 12272 samples: 0.8840 with eval loss: 0.3082
Epoch 1/1, Loss after 12464 samples: 0.0873
Epoch 1/1, Loss after 12672 samples: 0.1226
Mean accuracy: 0.8268, std: 0.0084, lower bound: 0.8107, upper bound: 0.8434 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 12784 samples: 0.8273 with eval loss: 0.4848
Epoch 1/1, Loss after 12880 samples: 0.1706
Epoch 1/1, Loss after 13088 samples: 0.1723
Epoch 1/1, Loss after 13296 samples: 0.1078
Mean accuracy: 0.8525, std: 0.0076, lower bound: 0.8379, upper bound: 0.8665 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 13296 samples: 0.8529 with eval loss: 0.3600
Epoch 1/1, Loss after 13504 samples: 0.1111
Epoch 1/1, Loss after 13712 samples: 0.0693
Mean accuracy: 0.8498, std: 0.0081, lower bound: 0.8338, upper bound: 0.8650 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 13808 samples: 0.8499 with eval loss: 0.4085
Epoch 1/1, Loss after 13920 samples: 0.0653
Epoch 1/1, Loss after 14128 samples: 0.1085
Mean accuracy: 0.9050, std: 0.0064, lower bound: 0.8931, upper bound: 0.9182 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 14320 samples: 0.9051 with eval loss: 0.2551
Epoch 1/1, Loss after 14336 samples: 0.0698
Epoch 1/1, Loss after 14544 samples: 0.0990
Epoch 1/1, Loss after 14752 samples: 0.1229
Mean accuracy: 0.8930, std: 0.0069, lower bound: 0.8790, upper bound: 0.9061 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 14832 samples: 0.8931 with eval loss: 0.2941
Epoch 1/1, Loss after 14960 samples: 0.0879
Epoch 1/1, Loss after 15168 samples: 0.1236
Mean accuracy: 0.8798, std: 0.0073, lower bound: 0.8660, upper bound: 0.8941 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 15344 samples: 0.8800 with eval loss: 0.3330
Epoch 1/1, Loss after 15376 samples: 0.0865
Epoch 1/1, Loss after 15584 samples: 0.0948
Epoch 1/1, Loss after 15792 samples: 0.0770
Mean accuracy: 0.8796, std: 0.0073, lower bound: 0.8650, upper bound: 0.8946 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 15856 samples: 0.8795 with eval loss: 0.3360
Training signal is False, stopping training
Training finished
Best model: {'eval_acc': 0.9442771084337349, 'nb_samples': 5616, 'eval_loss': 0.13120911598205567}
Training loss logs: [{'samples': 192, 'loss': 0.6979417067307693}, {'samples': 400, 'loss': 0.6954228327824519}, {'samples': 608, 'loss': 0.6812626765324519}, {'samples': 816, 'loss': 0.6417623666616586}, {'samples': 1024, 'loss': 0.45124806807591367}, {'samples': 1232, 'loss': 0.38056960243445176}, {'samples': 1440, 'loss': 0.3078496398834082}, {'samples': 1648, 'loss': 0.2956109224603726}, {'samples': 1856, 'loss': 0.2547646136238025}, {'samples': 2064, 'loss': 0.1953526632143901}, {'samples': 2272, 'loss': 0.22143921599938318}, {'samples': 2480, 'loss': 0.20913463487074926}, {'samples': 2688, 'loss': 0.20681542387375465}, {'samples': 2896, 'loss': 0.24628516343923715}, {'samples': 3104, 'loss': 0.2001134157180786}, {'samples': 3312, 'loss': 0.1307292070526343}, {'samples': 3520, 'loss': 0.2056676258261387}, {'samples': 3728, 'loss': 0.2219167621089862}, {'samples': 3936, 'loss': 0.16060662556153077}, {'samples': 4144, 'loss': 0.19941653024691802}, {'samples': 4352, 'loss': 0.14890663153850114}, {'samples': 4560, 'loss': 0.18791141704871103}, {'samples': 4768, 'loss': 0.19690918463927048}, {'samples': 4976, 'loss': 0.1792676288347978}, {'samples': 5184, 'loss': 0.20791257574008062}, {'samples': 5392, 'loss': 0.1555158868432045}, {'samples': 5600, 'loss': 0.22292206436395645}, {'samples': 5808, 'loss': 0.18931004519645983}, {'samples': 6016, 'loss': 0.1500457485134785}, {'samples': 6224, 'loss': 0.16581094494232765}, {'samples': 6432, 'loss': 0.16088785976171494}, {'samples': 6640, 'loss': 0.16295141268234986}, {'samples': 6848, 'loss': 0.15945417892474395}, {'samples': 7056, 'loss': 0.14225432104789293}, {'samples': 7264, 'loss': 0.2252659694506572}, {'samples': 7472, 'loss': 0.13241912539188677}, {'samples': 7680, 'loss': 0.18431033996435311}, {'samples': 7888, 'loss': 0.15196358928313622}, {'samples': 8096, 'loss': 0.13682489097118378}, {'samples': 8304, 'loss': 0.17709847940848425}, {'samples': 8512, 'loss': 0.11573671778807273}, {'samples': 8720, 'loss': 0.1389244353541961}, {'samples': 8928, 'loss': 0.14856072859122202}, {'samples': 9136, 'loss': 0.16686705213326675}, {'samples': 9344, 'loss': 0.12440162438612717}, {'samples': 9552, 'loss': 0.10259783096038379}, {'samples': 9760, 'loss': 0.14238973305775568}, {'samples': 9968, 'loss': 0.17506861285521433}, {'samples': 10176, 'loss': 0.1093487676519614}, {'samples': 10384, 'loss': 0.08076629157249744}, {'samples': 10592, 'loss': 0.05434264414585554}, {'samples': 10800, 'loss': 0.10209912405564235}, {'samples': 11008, 'loss': 0.1815701086933796}, {'samples': 11216, 'loss': 0.0705514859694701}, {'samples': 11424, 'loss': 0.09329361984362969}, {'samples': 11632, 'loss': 0.08975622516412002}, {'samples': 11840, 'loss': 0.14429167199593324}, {'samples': 12048, 'loss': 0.11333447866714917}, {'samples': 12256, 'loss': 0.15080352528737143}, {'samples': 12464, 'loss': 0.08731162949250294}, {'samples': 12672, 'loss': 0.122579014645173}, {'samples': 12880, 'loss': 0.170623867557599}, {'samples': 13088, 'loss': 0.17230685169880205}, {'samples': 13296, 'loss': 0.10780653414817956}, {'samples': 13504, 'loss': 0.11107156196465859}, {'samples': 13712, 'loss': 0.06931556360079692}, {'samples': 13920, 'loss': 0.06530465758763827}, {'samples': 14128, 'loss': 0.1084549822486364}, {'samples': 14336, 'loss': 0.06975431109850223}, {'samples': 14544, 'loss': 0.09899852596796475}, {'samples': 14752, 'loss': 0.12293705286887976}, {'samples': 14960, 'loss': 0.08792858570814133}, {'samples': 15168, 'loss': 0.12363244363894829}, {'samples': 15376, 'loss': 0.08652805880858348}, {'samples': 15584, 'loss': 0.09479031310631679}, {'samples': 15792, 'loss': 0.0770095970768195}]
Evaluation accuracy logs: [{'samples': 496, 'accuracy': 0.7794964859437751, 'std': 0.009371078893764104, 'lower_bound': 0.7605421686746988, 'upper_bound': 0.7981927710843374}, {'samples': 1008, 'accuracy': 0.8404583333333334, 'std': 0.008267628044309736, 'lower_bound': 0.8242971887550201, 'upper_bound': 0.8569402610441766}, {'samples': 1520, 'accuracy': 0.8324126506024098, 'std': 0.008436400599480687, 'lower_bound': 0.8152610441767069, 'upper_bound': 0.8483935742971888}, {'samples': 2032, 'accuracy': 0.8849221887550202, 'std': 0.0069148171548595976, 'lower_bound': 0.8709839357429718, 'upper_bound': 0.8980923694779116}, {'samples': 2544, 'accuracy': 0.845875, 'std': 0.008171338667719666, 'lower_bound': 0.829304718875502, 'upper_bound': 0.8614457831325302}, {'samples': 3056, 'accuracy': 0.860187751004016, 'std': 0.007669262718390818, 'lower_bound': 0.8448795180722891, 'upper_bound': 0.876004016064257}, {'samples': 3568, 'accuracy': 0.9430165662650601, 'std': 0.005155353124085567, 'lower_bound': 0.9327309236947792, 'upper_bound': 0.9528112449799196}, {'samples': 4080, 'accuracy': 0.8906852409638555, 'std': 0.006966507377063263, 'lower_bound': 0.8770080321285141, 'upper_bound': 0.9041164658634538}, {'samples': 4592, 'accuracy': 0.9156676706827309, 'std': 0.006137977709849309, 'lower_bound': 0.9041164658634538, 'upper_bound': 0.927710843373494}, {'samples': 5104, 'accuracy': 0.9461661646586346, 'std': 0.004968125501804217, 'lower_bound': 0.9362449799196787, 'upper_bound': 0.9558232931726908}, {'samples': 5616, 'accuracy': 0.9444733935742972, 'std': 0.005196016403393202, 'lower_bound': 0.9337349397590361, 'upper_bound': 0.9538278112449798}, {'samples': 6128, 'accuracy': 0.8699814257028113, 'std': 0.007419213221970858, 'lower_bound': 0.8549196787148594, 'upper_bound': 0.8840361445783133}, {'samples': 6640, 'accuracy': 0.8387058232931727, 'std': 0.008051232239048108, 'lower_bound': 0.8227911646586346, 'upper_bound': 0.854430220883534}, {'samples': 7152, 'accuracy': 0.8784302208835341, 'std': 0.00719388389134059, 'lower_bound': 0.8644578313253012, 'upper_bound': 0.892570281124498}, {'samples': 7664, 'accuracy': 0.9019769076305221, 'std': 0.0066776759721787265, 'lower_bound': 0.8885416666666666, 'upper_bound': 0.9136546184738956}, {'samples': 8176, 'accuracy': 0.8256114457831325, 'std': 0.008331398805310435, 'lower_bound': 0.8092243975903614, 'upper_bound': 0.8418674698795181}, {'samples': 8688, 'accuracy': 0.8954236947791164, 'std': 0.0070512410598321746, 'lower_bound': 0.8805220883534136, 'upper_bound': 0.9086345381526104}, {'samples': 9200, 'accuracy': 0.8272881526104416, 'std': 0.008440041089953484, 'lower_bound': 0.8097389558232931, 'upper_bound': 0.8438755020080321}, {'samples': 9712, 'accuracy': 0.8344638554216868, 'std': 0.00825711650055047, 'lower_bound': 0.8187751004016064, 'upper_bound': 0.8504016064257028}, {'samples': 10224, 'accuracy': 0.8574523092369478, 'std': 0.00818262274486848, 'lower_bound': 0.8413654618473896, 'upper_bound': 0.8734939759036144}, {'samples': 10736, 'accuracy': 0.8571370481927711, 'std': 0.007647925734605934, 'lower_bound': 0.8413654618473896, 'upper_bound': 0.8719879518072289}, {'samples': 11248, 'accuracy': 0.8725100401606426, 'std': 0.007338876694928769, 'lower_bound': 0.858433734939759, 'upper_bound': 0.8865461847389559}, {'samples': 11760, 'accuracy': 0.8678373493975903, 'std': 0.0075205792296742175, 'lower_bound': 0.8523970883534137, 'upper_bound': 0.8815261044176707}, {'samples': 12272, 'accuracy': 0.884304718875502, 'std': 0.007242929978039245, 'lower_bound': 0.8694779116465864, 'upper_bound': 0.8981049196787148}, {'samples': 12784, 'accuracy': 0.8268092369477912, 'std': 0.008384375543953413, 'lower_bound': 0.8107304216867469, 'upper_bound': 0.8433734939759037}, {'samples': 13296, 'accuracy': 0.8525346385542169, 'std': 0.007557863868790788, 'lower_bound': 0.8378514056224899, 'upper_bound': 0.8664784136546184}, {'samples': 13808, 'accuracy': 0.8497515060240963, 'std': 0.008077307286947433, 'lower_bound': 0.8338227911646586, 'upper_bound': 0.8649598393574297}, {'samples': 14320, 'accuracy': 0.9050070281124498, 'std': 0.00642705424731758, 'lower_bound': 0.8930722891566265, 'upper_bound': 0.9181852409638553}, {'samples': 14832, 'accuracy': 0.8930180722891566, 'std': 0.006939238996765292, 'lower_bound': 0.8790160642570282, 'upper_bound': 0.9061244979919679}, {'samples': 15344, 'accuracy': 0.8798092369477911, 'std': 0.00727083034818122, 'lower_bound': 0.8659638554216867, 'upper_bound': 0.8940888554216867}, {'samples': 15856, 'accuracy': 0.8795557228915664, 'std': 0.007327587114242163, 'lower_bound': 0.8649598393574297, 'upper_bound': 0.8945783132530121}]
Evaluating the best model on the test set of dataset ./fake_true_datasets/fake_true_dataset_phi_10k...
Test metrics:
accuracy: 0.8796265060240964
precision: 0.8085473915634205
recall: 0.9950350247657745
f1_score: 0.8921050191803143
fp_rate: 0.23594279428839543
tp_rate: 0.9950350247657745
std_accuracy: 0.007355665628728909
std_precision: 0.011463076244415822
std_recall: 0.0023073878546277277
std_f1_score: 0.007086550485907024
std_fp_rate: 0.013273185811732974
std_tp_rate: 0.0023073878546277277
TP: 991.81
TN: 760.406
FP: 234.836
FN: 4.948
roc_auc: 0.992461815132014
fpr: [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00100402 0.00100402 0.00100402 0.00100402
 0.00100402 0.00100402 0.00100402 0.00100402 0.00100402 0.00100402
 0.00100402 0.00100402 0.00100402 0.00100402 0.00100402 0.00100402
 0.00100402 0.00100402 0.00100402 0.00100402 0.00100402 0.00100402
 0.00100402 0.00100402 0.00100402 0.00100402 0.00100402 0.00100402
 0.00100402 0.00100402 0.00100402 0.00100402 0.00100402 0.00100402
 0.00100402 0.00100402 0.00100402 0.00100402 0.00100402 0.00100402
 0.00100402 0.00200803 0.00200803 0.00200803 0.00200803 0.00200803
 0.00200803 0.00200803 0.00200803 0.00200803 0.00200803 0.00200803
 0.00200803 0.00200803 0.00200803 0.00200803 0.00200803 0.00200803
 0.00200803 0.00200803 0.00200803 0.00200803 0.00200803 0.00200803
 0.00200803 0.00200803 0.00200803 0.00200803 0.00200803 0.00200803
 0.00200803 0.00301205 0.00301205 0.00301205 0.00301205 0.00301205
 0.00301205 0.00301205 0.00301205 0.00301205 0.00301205 0.00301205
 0.00301205 0.00301205 0.00301205 0.00301205 0.00301205 0.00301205
 0.00301205 0.00301205 0.00301205 0.00301205 0.00401606 0.00502008
 0.00502008 0.0060241  0.0060241  0.0060241  0.0060241  0.0060241
 0.0060241  0.0060241  0.0060241  0.0060241  0.00702811 0.00702811
 0.00803213 0.00803213 0.00903614 0.00903614 0.00903614 0.00903614
 0.00903614 0.00903614 0.01004016 0.01004016 0.01104418 0.01104418
 0.01204819 0.01204819 0.01305221 0.01305221 0.01506024 0.01506024
 0.01606426 0.01606426 0.01606426 0.01606426 0.01706827 0.01807229
 0.01907631 0.01907631 0.01907631 0.01907631 0.01907631 0.01907631
 0.02008032 0.02208835 0.02208835 0.02208835 0.02208835 0.02208835
 0.02309237 0.02309237 0.02409639 0.02409639 0.0251004  0.0251004
 0.02610442 0.02610442 0.02610442 0.02811245 0.02811245 0.02911647
 0.02911647 0.03012048 0.03012048 0.03313253 0.03313253 0.03514056
 0.03514056 0.03714859 0.03815261 0.03815261 0.03815261 0.03815261
 0.03915663 0.03915663 0.04016064 0.04116466 0.04216867 0.04216867
 0.04317269 0.04417671 0.04518072 0.04518072 0.04618474 0.04618474
 0.04718876 0.04819277 0.0502008  0.0502008  0.05120482 0.05120482
 0.05220884 0.05220884 0.05421687 0.05421687 0.05722892 0.05823293
 0.06325301 0.06325301 0.06626506 0.06626506 0.06827309 0.07028112
 0.07228916 0.07228916 0.07630522 0.07730924 0.08032129 0.08032129
 0.0813253  0.0813253  0.08433735 0.08433735 0.08835341 0.08835341
 0.09437751 0.09437751 0.09538153 0.09538153 0.10341365 0.10341365
 0.11144578 0.11144578 0.1184739  0.12048193 0.12751004 0.12751004
 0.13955823 0.14156627 0.14759036 0.14759036 0.16064257 0.16064257
 0.26104418 0.26104418 0.27911647 0.2811245  0.30823293 0.31024096
 0.31526104 0.31726908 0.3564257  0.35843373 0.36646586 0.36646586
 0.36746988 0.36947791 0.37048193 0.37248996 0.37851406 0.38052209
 0.39457831 0.39658635 0.39759036 0.40060241 0.40461847 0.40662651
 0.41365462 0.41566265 0.4186747  0.4186747  0.42871486 0.43072289
 0.46285141 0.46485944 0.47389558 0.47590361 0.49297189 0.49297189
 0.50702811 0.51004016 0.51104418 0.51305221 0.53514056 0.53915663
 0.54317269 0.54518072 0.54819277 0.55220884 0.55823293 0.56124498
 0.58835341 0.59036145 0.59136546 0.59538153 0.59839357 0.60240964
 0.61947791 0.62148594 0.62951807 0.6315261  0.64257028 0.64457831
 0.64959839 0.65160643 0.65963855 0.6626506  0.66465863 0.66666667
 0.67269076 0.67570281 0.67771084 0.67871486 0.68172691 0.68674699
 0.68875502 0.69277108 0.69477912 0.69678715 0.70080321 0.70983936
 0.71184739 0.71385542 0.71686747 0.7248996  0.72690763 0.73092369
 0.73293173 0.74899598 0.75100402 0.75200803 0.75200803 0.75803213
 0.76004016 0.76706827 0.76907631 0.77008032 0.77409639 0.77811245
 0.78212851 0.78514056 0.78915663 0.79116466 0.80120482 0.80321285
 0.80522088 0.80823293 0.81024096 0.812249   0.81325301 0.81526104
 0.81726908 0.81927711 0.82128514 0.82329317 0.82630522 0.82831325
 0.8313253  0.83333333 0.84538153 0.84738956 0.85542169 0.85742972
 0.86044177 0.8624498  0.86345382 0.86546185 0.86947791 0.87148594
 0.87851406 0.88855422 0.89056225 0.89257028 0.8935743  0.89658635
 0.89959839 0.90361446 0.90763052 0.91064257 0.9126506  0.91767068
 0.92068273 0.92168675 0.92369478 0.92871486 0.93072289 0.93172691
 0.93373494 0.93875502 0.94076305 0.94578313 0.94779116 0.9497992
 0.95381526 0.95481928 0.95682731 0.95783133 0.95983936 0.96385542
 0.96586345 0.9688755  0.97088353 0.98192771 0.98393574 0.98594378
 0.98895582 0.99497992 0.99698795 1.        ]
tpr: [0.         0.00100402 0.00200803 0.00401606 0.00903614 0.01104418
 0.01204819 0.01606426 0.02108434 0.02309237 0.02610442 0.03413655
 0.03714859 0.04016064 0.04216867 0.04518072 0.04718876 0.04819277
 0.0502008  0.05220884 0.0562249  0.05923695 0.062249   0.06425703
 0.06726908 0.07128514 0.07831325 0.08032129 0.08333333 0.08634538
 0.08935743 0.09437751 0.09738956 0.10040161 0.10240964 0.10742972
 0.10943775 0.11345382 0.11646586 0.1184739  0.12048193 0.12349398
 0.12951807 0.13353414 0.1375502  0.14056225 0.14257028 0.1435743
 0.14558233 0.14658635 0.14859438 0.14959839 0.15361446 0.15562249
 0.15863454 0.16064257 0.16365462 0.16566265 0.1686747  0.17068273
 0.17369478 0.17570281 0.17971888 0.18172691 0.18273092 0.18473896
 0.18975904 0.19176707 0.19578313 0.19879518 0.20080321 0.20481928
 0.20682731 0.20783133 0.21084337 0.21385542 0.21686747 0.21787149
 0.21987952 0.22991968 0.23192771 0.23393574 0.23594378 0.23995984
 0.24497992 0.24799197 0.24899598 0.25200803 0.2560241  0.25903614
 0.26104418 0.26305221 0.26706827 0.26907631 0.27108434 0.27409639
 0.27610442 0.27710843 0.27911647 0.2811245  0.28413655 0.28915663
 0.29116466 0.29417671 0.3002008  0.30220884 0.30522088 0.31124498
 0.31526104 0.31626506 0.31827309 0.32228916 0.32429719 0.32931727
 0.33333333 0.3373494  0.33935743 0.34136546 0.34437751 0.34538153
 0.34738956 0.34939759 0.35240964 0.35341365 0.35542169 0.3564257
 0.35943775 0.36144578 0.36646586 0.36947791 0.37148594 0.37449799
 0.37650602 0.37751004 0.37951807 0.3815261  0.38353414 0.38654618
 0.38955823 0.39156627 0.3935743  0.39457831 0.39658635 0.39959839
 0.40160643 0.40261044 0.40562249 0.40763052 0.40963855 0.4126506
 0.41465863 0.41566265 0.41767068 0.42570281 0.42771084 0.42871486
 0.43072289 0.43172691 0.43373494 0.437751   0.43975904 0.44076305
 0.44277108 0.4437751  0.44678715 0.45080321 0.45281124 0.45381526
 0.45582329 0.45783133 0.45983936 0.46385542 0.46787149 0.4688755
 0.47088353 0.47389558 0.47791165 0.47891566 0.48393574 0.48493976
 0.49297189 0.49497992 0.49799197 0.5        0.5060241  0.50803213
 0.51004016 0.51104418 0.51405622 0.51506024 0.51706827 0.51807229
 0.52008032 0.52610442 0.52811245 0.5311245  0.53514056 0.53714859
 0.53815261 0.54116466 0.54317269 0.54718876 0.5502008  0.55220884
 0.55321285 0.55522088 0.55722892 0.56024096 0.56425703 0.56626506
 0.56927711 0.57128514 0.57228916 0.57630522 0.57730924 0.57931727
 0.58032129 0.58232932 0.58333333 0.58534137 0.58935743 0.59236948
 0.59337349 0.59638554 0.60040161 0.60240964 0.60341365 0.60542169
 0.6064257  0.61044177 0.61144578 0.61345382 0.61445783 0.61646586
 0.61746988 0.62048193 0.62349398 0.62650602 0.62851406 0.6375502
 0.64156627 0.6435743  0.64658635 0.64759036 0.65060241 0.65461847
 0.65662651 0.65763052 0.65963855 0.66164659 0.6626506  0.66666667
 0.6686747  0.6746988  0.67771084 0.67971888 0.68373494 0.687751
 0.68875502 0.69076305 0.69176707 0.69578313 0.70883534 0.71084337
 0.71285141 0.71485944 0.7188755  0.72088353 0.72188755 0.72389558
 0.72590361 0.72891566 0.73493976 0.73694779 0.73995984 0.74196787
 0.74497992 0.74497992 0.75903614 0.76104418 0.76706827 0.77108434
 0.77309237 0.7751004  0.78212851 0.78514056 0.78915663 0.79317269
 0.79417671 0.79618474 0.79919679 0.80120482 0.80722892 0.80923695
 0.81024096 0.81425703 0.81827309 0.81927711 0.81927711 0.82028112
 0.82931727 0.82931727 0.83433735 0.8373494  0.83935743 0.84036145
 0.84437751 0.85140562 0.85341365 0.85441767 0.85441767 0.8624498
 0.86345382 0.87751004 0.87751004 0.88052209 0.88253012 0.88353414
 0.8875502  0.88955823 0.88955823 0.89457831 0.89457831 0.89558233
 0.89558233 0.90060241 0.90060241 0.90461847 0.90461847 0.90662651
 0.90662651 0.90763052 0.90963855 0.91064257 0.91064257 0.91164659
 0.91164659 0.9126506  0.91465863 0.91566265 0.91767068 0.9186747
 0.9186747  0.9186747  0.9246988  0.92670683 0.92771084 0.92971888
 0.92971888 0.93072289 0.93072289 0.93273092 0.93273092 0.93373494
 0.93473896 0.93674699 0.93875502 0.93875502 0.94277108 0.94277108
 0.94477912 0.94477912 0.94779116 0.94779116 0.94879518 0.94879518
 0.95080321 0.95080321 0.95080321 0.95381526 0.95582329 0.95682731
 0.95682731 0.95783133 0.95883534 0.95883534 0.95983936 0.96285141
 0.96285141 0.96385542 0.96385542 0.96485944 0.96485944 0.96586345
 0.96586345 0.96686747 0.96686747 0.96787149 0.96787149 0.9688755
 0.9688755  0.97188755 0.97188755 0.97289157 0.97289157 0.97389558
 0.97389558 0.9748996  0.9748996  0.97590361 0.97590361 0.97590361
 0.97590361 0.97891566 0.97891566 0.97991968 0.97991968 0.98092369
 0.98192771 0.98293173 0.98293173 0.98594378 0.98594378 0.98694779
 0.98694779 0.98795181 0.98795181 0.98895582 0.98895582 0.99096386
 0.99096386 0.99196787 0.99196787 0.99196787 0.99196787 0.99297189
 0.99297189 0.99297189 0.99297189 0.9939759  0.9939759  0.99497992
 0.99497992 0.99598394 0.99598394 0.99598394 0.99598394 0.99598394
 0.99598394 0.99598394 0.99598394 0.99598394 0.99598394 0.99698795
 0.99698795 0.99698795 0.99698795 0.99698795 0.99698795 0.99698795
 0.99698795 0.99698795 0.99698795 0.99698795 0.99698795 0.99698795
 0.99698795 0.99698795 0.99698795 0.99799197 0.99799197 0.99799197
 0.99799197 0.99799197 0.99799197 0.99799197 0.99799197 0.99899598
 0.99899598 0.99899598 0.99899598 0.99899598 0.99899598 0.99899598
 0.99899598 0.99899598 0.99899598 0.99899598 0.99899598 0.99899598
 0.99899598 0.99899598 0.99899598 0.99899598 0.99899598 0.99899598
 0.99899598 0.99899598 0.99899598 0.99899598 0.99899598 0.99899598
 0.99899598 0.99899598 0.99899598 0.99899598 0.99899598 0.99899598
 0.99899598 0.99899598 0.99899598 0.99899598 0.99899598 0.99899598
 0.99899598 0.99899598 0.99899598 0.99899598 0.99899598 0.99899598
 0.99899598 0.99899598 0.99899598 0.99899598 0.99899598 0.99899598
 0.99899598 0.99899598 0.99899598 0.99899598 1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.        ]
thresholds: [        inf  6.5546875   6.5273438   6.5078125   6.3828125   6.375
  6.3554688   6.3398438   6.3125      6.296875    6.2695312   6.25
  6.2460938   6.2226562   6.2148438   6.1875      6.1835938   6.1757812
  6.171875    6.1523438   6.1367188   6.1210938   6.1171875   6.1132812
  6.0898438   6.0859375   6.078125    6.0664062   6.0625      6.0507812
  6.0429688   6.0390625   6.0234375   6.0195312   6.015625    5.9960938
  5.9921875   5.9648438   5.9609375   5.9414062   5.9375      5.9335938
  5.9179688   5.9140625   5.8984375   5.8945312   5.890625    5.8867188
  5.8789062   5.875       5.8710938   5.8671875   5.859375    5.8515625
  5.8476562   5.828125    5.8242188   5.8046875   5.8007812   5.796875
  5.7929688   5.7890625   5.765625    5.7617188   5.7578125   5.7539062
  5.7304688   5.7226562   5.71875     5.6953125   5.6914062   5.6679688
  5.6601562   5.65625     5.6445312   5.6289062   5.625       5.6210938
  5.6171875   5.5703125   5.5664062   5.5546875   5.546875    5.5429688
  5.5234375   5.5195312   5.5117188   5.5039062   5.4960938   5.484375
  5.4609375   5.453125    5.4335938   5.421875    5.4140625   5.390625
  5.3867188   5.3828125   5.3710938   5.3554688   5.3476562   5.34375
  5.3359375   5.3320312   5.3046875   5.3007812   5.296875    5.2851562
  5.28125     5.2773438   5.2734375   5.2539062   5.25        5.21875
  5.2109375   5.1796875   5.171875    5.15625     5.1523438   5.1484375
  5.140625    5.1328125   5.1289062   5.125       5.1171875   5.109375
  5.1054688   5.1015625   5.0859375   5.0585938   5.0507812   5.0351562
  5.03125     5.0273438   5.0195312   5.0039062   4.9960938   4.984375
  4.9804688   4.9648438   4.9570312   4.953125    4.9492188   4.9453125
  4.9414062   4.9375      4.9296875   4.9179688   4.9101562   4.8984375
  4.8945312   4.890625    4.8828125   4.8398438   4.8320312   4.828125
  4.8242188   4.8203125   4.8125      4.8007812   4.796875    4.7929688
  4.7890625   4.7851562   4.78125     4.7578125   4.7539062   4.75
  4.7460938   4.734375    4.7304688   4.7109375   4.703125    4.6992188
  4.6914062   4.6875      4.671875    4.6679688   4.6640625   4.6601562
  4.640625    4.625       4.6210938   4.6132812   4.6015625   4.59375
  4.5859375   4.5820312   4.578125    4.5742188   4.5703125   4.5664062
  4.5625      4.5351562   4.53125     4.5234375   4.515625    4.5117188
  4.4960938   4.4921875   4.4882812   4.484375    4.4570312   4.453125
  4.4414062   4.4335938   4.4296875   4.4257812   4.421875    4.4179688
  4.4023438   4.3945312   4.390625    4.3828125   4.3789062   4.375
  4.3710938   4.3671875   4.359375    4.3554688   4.3359375   4.3320312
  4.3164062   4.3125      4.3046875   4.2929688   4.2890625   4.2851562
  4.2773438   4.265625    4.2617188   4.2578125   4.2539062   4.2460938
  4.2382812   4.234375    4.2109375   4.2070312   4.203125    4.1367188
  4.1328125   4.125       4.1210938   4.1132812   4.109375    4.0859375
  4.078125    4.0742188   4.0664062   4.0625      4.0585938   4.0546875
  4.046875    4.0195312   4.015625    4.0078125   3.9960938   3.9902344
  3.9863281   3.9765625   3.96875     3.9355469   3.875       3.8710938
  3.8613281   3.8222656   3.8027344   3.7949219   3.7890625   3.7832031
  3.7714844   3.7695312   3.7460938   3.7382812   3.7207031   3.7109375
  3.6972656   3.6914062   3.6191406   3.6074219   3.5839844   3.5664062
  3.5527344   3.5488281   3.5039062   3.5019531   3.484375    3.4550781
  3.4472656   3.4414062   3.4238281   3.421875    3.4003906   3.3984375
  3.390625    3.3808594   3.3671875   3.3535156   3.3515625   3.3496094
  3.2929688   3.2910156   3.2597656   3.25        3.2421875   3.2265625
  3.1992188   3.1601562   3.1523438   3.1503906   3.1445312   3.0644531
  3.0507812   2.9648438   2.9550781   2.9394531   2.9277344   2.921875
  2.9101562   2.8945312   2.890625    2.8417969   2.8125      2.8105469
  2.8085938   2.7597656   2.7480469   2.6875      2.6738281   2.6601562
  2.6503906   2.6464844   2.6445312   2.640625    2.6347656   2.6152344
  2.5996094   2.5917969   2.578125    2.5761719   2.5664062   2.5546875
  2.546875    2.5410156   2.5         2.4921875   2.4882812   2.4726562
  2.46875     2.4414062   2.4355469   2.4160156   2.3984375   2.3964844
  2.3886719   2.3828125   2.3789062   2.3613281   2.3476562   2.3359375
  2.3027344   2.2988281   2.2597656   2.2519531   2.25        2.2363281
  2.2246094   2.2089844   2.203125    2.1738281   2.171875    2.1601562
  2.1523438   2.1484375   2.1464844   2.1289062   2.1210938   2.0722656
  2.0703125   2.0566406   2.0527344   2.0507812   2.0429688   2.0390625
  2.03125     2.0097656   1.9990234   1.9960938   1.9951172   1.9570312
  1.9560547   1.9335938   1.9199219   1.9160156   1.8886719   1.8808594
  1.8408203   1.8330078   1.796875    1.7919922   1.7675781   1.7373047
  1.7294922   1.7119141   1.6738281   1.6728516   1.6279297   1.625
  1.6132812   1.6025391   1.5888672   1.5595703   1.5029297   1.4833984
  1.4179688   1.4082031   1.40625     1.3886719   1.3105469   1.2900391
  1.2070312   1.1943359   1.1259766   1.1220703   1.0283203   1.0273438
  0.9238281   0.90478516  0.8359375   0.8334961   0.67333984  0.65527344
 -0.24121094 -0.24572754 -0.36328125 -0.3659668  -0.5698242  -0.57421875
 -0.64404297 -0.66552734 -0.90185547 -0.9038086  -0.94091797 -0.9423828
 -0.94970703 -0.9638672  -0.9794922  -0.9951172  -1.0488281  -1.0546875
 -1.1630859  -1.1738281  -1.1787109  -1.1796875  -1.2226562  -1.2236328
 -1.2832031  -1.2890625  -1.3066406  -1.3076172  -1.3808594  -1.3837891
 -1.5917969  -1.59375    -1.6425781  -1.6445312  -1.7744141  -1.7792969
 -1.8457031  -1.8525391  -1.8535156  -1.875      -2.0097656  -2.0175781
 -2.0371094  -2.0390625  -2.0585938  -2.0683594  -2.09375    -2.109375
 -2.2695312  -2.2753906  -2.2773438  -2.2851562  -2.3105469  -2.328125
 -2.421875   -2.4277344  -2.4921875  -2.5        -2.5839844  -2.5878906
 -2.6191406  -2.6210938  -2.6914062  -2.6953125  -2.7050781  -2.7128906
 -2.7324219  -2.734375   -2.7363281  -2.7578125  -2.7675781  -2.7871094
 -2.7910156  -2.8066406  -2.8085938  -2.8164062  -2.8242188  -2.8769531
 -2.8789062  -2.8886719  -2.8925781  -2.9257812  -2.9335938  -2.953125
 -2.9550781  -3.0351562  -3.0410156  -3.0449219  -3.046875   -3.0742188
 -3.09375    -3.1347656  -3.1367188  -3.140625   -3.1503906  -3.1601562
 -3.1738281  -3.1894531  -3.2050781  -3.2070312  -3.2519531  -3.2558594
 -3.2695312  -3.2773438  -3.2890625  -3.296875   -3.2988281  -3.3066406
 -3.3203125  -3.3222656  -3.3320312  -3.3398438  -3.3496094  -3.3671875
 -3.3789062  -3.3886719  -3.4414062  -3.4433594  -3.4921875  -3.4960938
 -3.5058594  -3.515625   -3.5234375  -3.5273438  -3.5507812  -3.5546875
 -3.5996094  -3.6191406  -3.6367188  -3.6425781  -3.6445312  -3.6542969
 -3.6679688  -3.6796875  -3.6933594  -3.703125   -3.7089844  -3.7246094
 -3.7265625  -3.7285156  -3.734375   -3.7617188  -3.7636719  -3.765625
 -3.7792969  -3.8046875  -3.8164062  -3.8496094  -3.8515625  -3.8789062
 -3.9042969  -3.90625    -3.9101562  -3.9335938  -3.9433594  -3.9648438
 -3.96875    -3.9863281  -3.9882812  -4.0976562  -4.1328125  -4.15625
 -4.1679688  -4.2695312  -4.2773438  -4.3476562 ]
