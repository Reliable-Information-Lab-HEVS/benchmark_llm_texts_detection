log_loss_steps: 208
eval_steps: 512
check_degradation: 0
----------------Epoch 1/1----------------
Epoch 1/1, Loss after 192 samples: 0.6928
Epoch 1/1, Loss after 400 samples: 0.6851
Mean accuracy: 0.5400, std: 0.0115, lower bound: 0.5172, upper bound: 0.5619 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 496 samples: 0.5401 with eval loss: 0.6795
Best model with eval loss 0.6795221144153226 and eval accuracy 0.5400608519269777 with 496 samples seen is saved
Epoch 1/1, Loss after 608 samples: 0.6835
Epoch 1/1, Loss after 816 samples: 0.6343
Mean accuracy: 0.7961, std: 0.0095, lower bound: 0.7764, upper bound: 0.8139 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 1008 samples: 0.7961 with eval loss: 0.4011
Best model with eval loss 0.4010956287384033 and eval accuracy 0.7961460446247465 with 1008 samples seen is saved
Epoch 1/1, Loss after 1024 samples: 0.4943
Epoch 1/1, Loss after 1232 samples: 0.4903
Epoch 1/1, Loss after 1440 samples: 0.4679
Mean accuracy: 0.7612, std: 0.0098, lower bound: 0.7414, upper bound: 0.7799 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 1520 samples: 0.7612 with eval loss: 0.4453
Epoch 1/1, Loss after 1648 samples: 0.3788
Epoch 1/1, Loss after 1856 samples: 0.3302
Mean accuracy: 0.8381, std: 0.0082, lower bound: 0.8220, upper bound: 0.8540 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 2032 samples: 0.8377 with eval loss: 0.3309
Best model with eval loss 0.3308951346383941 and eval accuracy 0.8377281947261663 with 2032 samples seen is saved
Epoch 1/1, Loss after 2064 samples: 0.3236
Epoch 1/1, Loss after 2272 samples: 0.2984
Epoch 1/1, Loss after 2480 samples: 0.2429
Mean accuracy: 0.8186, std: 0.0090, lower bound: 0.8012, upper bound: 0.8352 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 2544 samples: 0.8185 with eval loss: 0.4154
Epoch 1/1, Loss after 2688 samples: 0.2793
Epoch 1/1, Loss after 2896 samples: 0.2326
Mean accuracy: 0.7566, std: 0.0097, lower bound: 0.7378, upper bound: 0.7754 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 3056 samples: 0.7566 with eval loss: 0.5158
Epoch 1/1, Loss after 3104 samples: 0.2760
Epoch 1/1, Loss after 3312 samples: 0.2634
Epoch 1/1, Loss after 3520 samples: 0.3322
Mean accuracy: 0.9071, std: 0.0064, lower bound: 0.8940, upper bound: 0.9194 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 3568 samples: 0.9067 with eval loss: 0.2290
Best model with eval loss 0.22897242826800193 and eval accuracy 0.9066937119675457 with 3568 samples seen is saved
Epoch 1/1, Loss after 3728 samples: 0.2468
Epoch 1/1, Loss after 3936 samples: 0.2357
Mean accuracy: 0.9126, std: 0.0064, lower bound: 0.9001, upper bound: 0.9255 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 4080 samples: 0.9128 with eval loss: 0.2102
Best model with eval loss 0.21023676207950037 and eval accuracy 0.9127789046653144 with 4080 samples seen is saved
Epoch 1/1, Loss after 4144 samples: 0.2963
Epoch 1/1, Loss after 4352 samples: 0.2678
Epoch 1/1, Loss after 4560 samples: 0.1866
Mean accuracy: 0.8747, std: 0.0073, lower bound: 0.8600, upper bound: 0.8889 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 4592 samples: 0.8747 with eval loss: 0.3016
Epoch 1/1, Loss after 4768 samples: 0.2075
Epoch 1/1, Loss after 4976 samples: 0.2800
Mean accuracy: 0.8847, std: 0.0072, lower bound: 0.8702, upper bound: 0.8981 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 5104 samples: 0.8849 with eval loss: 0.2531
Epoch 1/1, Loss after 5184 samples: 0.2201
Epoch 1/1, Loss after 5392 samples: 0.2365
Epoch 1/1, Loss after 5600 samples: 0.2637
Mean accuracy: 0.8982, std: 0.0067, lower bound: 0.8849, upper bound: 0.9118 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 5616 samples: 0.8981 with eval loss: 0.2317
Epoch 1/1, Loss after 5808 samples: 0.2645
Epoch 1/1, Loss after 6016 samples: 0.2226
Mean accuracy: 0.8695, std: 0.0075, lower bound: 0.8540, upper bound: 0.8839 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 6128 samples: 0.8697 with eval loss: 0.2928
Epoch 1/1, Loss after 6224 samples: 0.2797
Epoch 1/1, Loss after 6432 samples: 0.2089
Epoch 1/1, Loss after 6640 samples: 0.1793
Mean accuracy: 0.8461, std: 0.0082, lower bound: 0.8296, upper bound: 0.8626 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 6640 samples: 0.8458 with eval loss: 0.3501
Epoch 1/1, Loss after 6848 samples: 0.2268
Epoch 1/1, Loss after 7056 samples: 0.1612
Mean accuracy: 0.8968, std: 0.0068, lower bound: 0.8839, upper bound: 0.9102 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 7152 samples: 0.8971 with eval loss: 0.2245
Epoch 1/1, Loss after 7264 samples: 0.2509
Epoch 1/1, Loss after 7472 samples: 0.1876
Mean accuracy: 0.9110, std: 0.0064, lower bound: 0.8976, upper bound: 0.9229 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 7664 samples: 0.9108 with eval loss: 0.1907
Best model with eval loss 0.19069203091484885 and eval accuracy 0.9107505070993914 with 7664 samples seen is saved
Epoch 1/1, Loss after 7680 samples: 0.2083
Epoch 1/1, Loss after 7888 samples: 0.2398
Epoch 1/1, Loss after 8096 samples: 0.2506
Mean accuracy: 0.8999, std: 0.0066, lower bound: 0.8864, upper bound: 0.9123 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 8176 samples: 0.8996 with eval loss: 0.2189
Epoch 1/1, Loss after 8304 samples: 0.2573
Epoch 1/1, Loss after 8512 samples: 0.1908
Mean accuracy: 0.8253, std: 0.0084, lower bound: 0.8093, upper bound: 0.8418 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 8688 samples: 0.8256 with eval loss: 0.3848
Epoch 1/1, Loss after 8720 samples: 0.2273
Epoch 1/1, Loss after 8928 samples: 0.2125
Epoch 1/1, Loss after 9136 samples: 0.1709
Mean accuracy: 0.9122, std: 0.0064, lower bound: 0.9001, upper bound: 0.9239 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 9200 samples: 0.9118 with eval loss: 0.1983
Epoch 1/1, Loss after 9344 samples: 0.1612
Epoch 1/1, Loss after 9552 samples: 0.2225
Mean accuracy: 0.8808, std: 0.0074, lower bound: 0.8661, upper bound: 0.8945 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 9712 samples: 0.8808 with eval loss: 0.2544
Epoch 1/1, Loss after 9760 samples: 0.1904
Epoch 1/1, Loss after 9968 samples: 0.2032
Epoch 1/1, Loss after 10176 samples: 0.1847
Mean accuracy: 0.8372, std: 0.0082, lower bound: 0.8215, upper bound: 0.8534 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 10224 samples: 0.8367 with eval loss: 0.3820
Epoch 1/1, Loss after 10384 samples: 0.1925
Epoch 1/1, Loss after 10592 samples: 0.1420
Mean accuracy: 0.9007, std: 0.0066, lower bound: 0.8874, upper bound: 0.9133 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 10736 samples: 0.9011 with eval loss: 0.2239
Epoch 1/1, Loss after 10800 samples: 0.2101
Epoch 1/1, Loss after 11008 samples: 0.2031
Epoch 1/1, Loss after 11216 samples: 0.1477
Mean accuracy: 0.8846, std: 0.0070, lower bound: 0.8712, upper bound: 0.8986 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 11248 samples: 0.8844 with eval loss: 0.2551
Epoch 1/1, Loss after 11424 samples: 0.2324
Epoch 1/1, Loss after 11632 samples: 0.1713
Mean accuracy: 0.8332, std: 0.0086, lower bound: 0.8159, upper bound: 0.8499 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 11760 samples: 0.8337 with eval loss: 0.3981
Epoch 1/1, Loss after 11840 samples: 0.1709
Epoch 1/1, Loss after 12048 samples: 0.1767
Epoch 1/1, Loss after 12256 samples: 0.2170
Mean accuracy: 0.8373, std: 0.0086, lower bound: 0.8205, upper bound: 0.8540 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 12272 samples: 0.8377 with eval loss: 0.3730
Epoch 1/1, Loss after 12464 samples: 0.1418
Epoch 1/1, Loss after 12672 samples: 0.1512
Mean accuracy: 0.8751, std: 0.0074, lower bound: 0.8605, upper bound: 0.8895 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 12784 samples: 0.8753 with eval loss: 0.2827
Epoch 1/1, Loss after 12880 samples: 0.1840
Epoch 1/1, Loss after 13088 samples: 0.1487
Epoch 1/1, Loss after 13296 samples: 0.1696
Mean accuracy: 0.8918, std: 0.0070, lower bound: 0.8778, upper bound: 0.9047 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 13296 samples: 0.8920 with eval loss: 0.2390
Epoch 1/1, Loss after 13504 samples: 0.1112
Epoch 1/1, Loss after 13712 samples: 0.1965
Mean accuracy: 0.8376, std: 0.0081, lower bound: 0.8220, upper bound: 0.8535 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 13808 samples: 0.8377 with eval loss: 0.3839
Epoch 1/1, Loss after 13920 samples: 0.1206
Epoch 1/1, Loss after 14128 samples: 0.1910
Mean accuracy: 0.8327, std: 0.0087, lower bound: 0.8149, upper bound: 0.8489 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 14320 samples: 0.8327 with eval loss: 0.3999
Epoch 1/1, Loss after 14336 samples: 0.1827
Epoch 1/1, Loss after 14544 samples: 0.1255
Epoch 1/1, Loss after 14752 samples: 0.1857
Mean accuracy: 0.8572, std: 0.0074, lower bound: 0.8423, upper bound: 0.8712 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 14832 samples: 0.8570 with eval loss: 0.3257
Epoch 1/1, Loss after 14960 samples: 0.1765
Epoch 1/1, Loss after 15168 samples: 0.1681
Mean accuracy: 0.8552, std: 0.0077, lower bound: 0.8403, upper bound: 0.8697 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 15344 samples: 0.8555 with eval loss: 0.3288
Epoch 1/1, Loss after 15376 samples: 0.2121
Epoch 1/1, Loss after 15584 samples: 0.1669
Training signal is False, stopping training
Training finished
Best model: {'eval_acc': 0.9107505070993914, 'nb_samples': 7664, 'eval_loss': 0.19069203091484885}
Training loss logs: [{'samples': 192, 'loss': 0.6928147536057693}, {'samples': 400, 'loss': 0.6850562462439904}, {'samples': 608, 'loss': 0.6834658109224759}, {'samples': 816, 'loss': 0.6343319232647235}, {'samples': 1024, 'loss': 0.49433760459606463}, {'samples': 1232, 'loss': 0.49031539490589726}, {'samples': 1440, 'loss': 0.46794666006014896}, {'samples': 1648, 'loss': 0.3787561357021332}, {'samples': 1856, 'loss': 0.33017802696961623}, {'samples': 2064, 'loss': 0.3236147715495183}, {'samples': 2272, 'loss': 0.2983506562618109}, {'samples': 2480, 'loss': 0.24293272311870867}, {'samples': 2688, 'loss': 0.2793039089212051}, {'samples': 2896, 'loss': 0.23257536326463407}, {'samples': 3104, 'loss': 0.2759559532770744}, {'samples': 3312, 'loss': 0.2634342943246548}, {'samples': 3520, 'loss': 0.33218113619547623}, {'samples': 3728, 'loss': 0.24678705288813665}, {'samples': 3936, 'loss': 0.23574555894503227}, {'samples': 4144, 'loss': 0.29629624921541947}, {'samples': 4352, 'loss': 0.267824232005156}, {'samples': 4560, 'loss': 0.18659836512345535}, {'samples': 4768, 'loss': 0.2074757762826406}, {'samples': 4976, 'loss': 0.28004398540808606}, {'samples': 5184, 'loss': 0.22005911286060625}, {'samples': 5392, 'loss': 0.23653038648458627}, {'samples': 5600, 'loss': 0.2636899690215404}, {'samples': 5808, 'loss': 0.26454473172242826}, {'samples': 6016, 'loss': 0.2225651924426739}, {'samples': 6224, 'loss': 0.2796935989306523}, {'samples': 6432, 'loss': 0.20894016783971053}, {'samples': 6640, 'loss': 0.17929697380616114}, {'samples': 6848, 'loss': 0.22679842435396635}, {'samples': 7056, 'loss': 0.16118698165966913}, {'samples': 7264, 'loss': 0.2508941607979628}, {'samples': 7472, 'loss': 0.18759951282006043}, {'samples': 7680, 'loss': 0.2083147896023897}, {'samples': 7888, 'loss': 0.23982090159104422}, {'samples': 8096, 'loss': 0.25056997400063735}, {'samples': 8304, 'loss': 0.25729400091446364}, {'samples': 8512, 'loss': 0.1907613489490289}, {'samples': 8720, 'loss': 0.22727043009721315}, {'samples': 8928, 'loss': 0.21250351346456087}, {'samples': 9136, 'loss': 0.170913383937799}, {'samples': 9344, 'loss': 0.16115538030862808}, {'samples': 9552, 'loss': 0.22249083908704612}, {'samples': 9760, 'loss': 0.19043138050116026}, {'samples': 9968, 'loss': 0.20318331512121054}, {'samples': 10176, 'loss': 0.1846834449813916}, {'samples': 10384, 'loss': 0.19249609055427405}, {'samples': 10592, 'loss': 0.14201794163538858}, {'samples': 10800, 'loss': 0.21005302552993482}, {'samples': 11008, 'loss': 0.2031440855218814}, {'samples': 11216, 'loss': 0.14768633303733972}, {'samples': 11424, 'loss': 0.23239784343884543}, {'samples': 11632, 'loss': 0.171298504448854}, {'samples': 11840, 'loss': 0.17088274715038446}, {'samples': 12048, 'loss': 0.17668811575724527}, {'samples': 12256, 'loss': 0.21700270302020586}, {'samples': 12464, 'loss': 0.14179948029609826}, {'samples': 12672, 'loss': 0.15122072799847677}, {'samples': 12880, 'loss': 0.18398310301395562}, {'samples': 13088, 'loss': 0.1487077296926425}, {'samples': 13296, 'loss': 0.16961470189002845}, {'samples': 13504, 'loss': 0.1112297263282996}, {'samples': 13712, 'loss': 0.1964794615140328}, {'samples': 13920, 'loss': 0.12061745788042362}, {'samples': 14128, 'loss': 0.19095421238587454}, {'samples': 14336, 'loss': 0.18274832860781595}, {'samples': 14544, 'loss': 0.1254604859994008}, {'samples': 14752, 'loss': 0.1856655289347355}, {'samples': 14960, 'loss': 0.17650691477152017}, {'samples': 15168, 'loss': 0.16809790925337717}, {'samples': 15376, 'loss': 0.2121161956053514}, {'samples': 15584, 'loss': 0.16688664257526398}]
Evaluation accuracy logs: [{'samples': 496, 'accuracy': 0.5400299188640973, 'std': 0.011508109601665073, 'lower_bound': 0.5172287018255578, 'upper_bound': 0.5618661257606491}, {'samples': 1008, 'accuracy': 0.7961495943204868, 'std': 0.009523361013121227, 'lower_bound': 0.776369168356998, 'upper_bound': 0.813894523326572}, {'samples': 1520, 'accuracy': 0.761226673427992, 'std': 0.009823153700263614, 'lower_bound': 0.7413793103448276, 'upper_bound': 0.7799188640973631}, {'samples': 2032, 'accuracy': 0.8381206896551723, 'std': 0.008207508595110225, 'lower_bound': 0.8219954361054767, 'upper_bound': 0.8539553752535497}, {'samples': 2544, 'accuracy': 0.8186293103448277, 'std': 0.00902675989707454, 'lower_bound': 0.8012170385395537, 'upper_bound': 0.8351926977687627}, {'samples': 3056, 'accuracy': 0.7565649087221096, 'std': 0.009747409102368973, 'lower_bound': 0.7378296146044625, 'upper_bound': 0.7753549695740365}, {'samples': 3568, 'accuracy': 0.9070912778904665, 'std': 0.006446893672684535, 'lower_bound': 0.8940162271805274, 'upper_bound': 0.9193711967545639}, {'samples': 4080, 'accuracy': 0.9126369168356998, 'std': 0.00644278906995424, 'lower_bound': 0.9001014198782962, 'upper_bound': 0.9254563894523327}, {'samples': 4592, 'accuracy': 0.8747094320486816, 'std': 0.007328799828609126, 'lower_bound': 0.8600278904665314, 'upper_bound': 0.8889452332657201}, {'samples': 5104, 'accuracy': 0.8846648073022312, 'std': 0.007196927054944111, 'lower_bound': 0.8701825557809331, 'upper_bound': 0.8980856997971602}, {'samples': 5616, 'accuracy': 0.8981546653144017, 'std': 0.006680581330302593, 'lower_bound': 0.8848884381338742, 'upper_bound': 0.9117647058823529}, {'samples': 6128, 'accuracy': 0.869461967545639, 'std': 0.007494712147622875, 'lower_bound': 0.8539553752535497, 'upper_bound': 0.8838742393509128}, {'samples': 6640, 'accuracy': 0.8460892494929005, 'std': 0.008232299938308487, 'lower_bound': 0.8296019269776875, 'upper_bound': 0.8625887423935091}, {'samples': 7152, 'accuracy': 0.8968331643002029, 'std': 0.0067807925776009696, 'lower_bound': 0.8838615618661257, 'upper_bound': 0.9102434077079108}, {'samples': 7664, 'accuracy': 0.911026369168357, 'std': 0.006408220929040912, 'lower_bound': 0.8975659229208925, 'upper_bound': 0.922920892494929}, {'samples': 8176, 'accuracy': 0.8998717038539554, 'std': 0.00663474765105935, 'lower_bound': 0.8864097363083164, 'upper_bound': 0.9122718052738337}, {'samples': 8688, 'accuracy': 0.8252799188640975, 'std': 0.008439140942791105, 'lower_bound': 0.8093052738336715, 'upper_bound': 0.8417976673427993}, {'samples': 9200, 'accuracy': 0.9121637931034483, 'std': 0.0064135871611773415, 'lower_bound': 0.9001014198782962, 'upper_bound': 0.9239350912778904}, {'samples': 9712, 'accuracy': 0.8808306288032454, 'std': 0.007413716725978241, 'lower_bound': 0.8661130831643001, 'upper_bound': 0.8945360040567951}, {'samples': 10224, 'accuracy': 0.8371587221095335, 'std': 0.008197177684812472, 'lower_bound': 0.8214883367139959, 'upper_bound': 0.853448275862069}, {'samples': 10736, 'accuracy': 0.9007063894523326, 'std': 0.006636839480119478, 'lower_bound': 0.8874239350912779, 'upper_bound': 0.9132860040567952}, {'samples': 11248, 'accuracy': 0.8845882352941177, 'std': 0.007010445712656303, 'lower_bound': 0.8711840770791075, 'upper_bound': 0.8985801217038539}, {'samples': 11760, 'accuracy': 0.8331587221095336, 'std': 0.008598511009081988, 'lower_bound': 0.815922920892495, 'upper_bound': 0.8498985801217038}, {'samples': 12272, 'accuracy': 0.8372860040567951, 'std': 0.008606294149340839, 'lower_bound': 0.8204868154158215, 'upper_bound': 0.8539553752535497}, {'samples': 12784, 'accuracy': 0.8751293103448277, 'std': 0.007426452409504916, 'lower_bound': 0.8605476673427992, 'upper_bound': 0.8894523326572008}, {'samples': 13296, 'accuracy': 0.8918204868154157, 'std': 0.007018063214777492, 'lower_bound': 0.8777890466531441, 'upper_bound': 0.9046653144016227}, {'samples': 13808, 'accuracy': 0.8375638945233266, 'std': 0.008148394923707898, 'lower_bound': 0.8220081135902637, 'upper_bound': 0.853460953346856}, {'samples': 14320, 'accuracy': 0.8326962474645031, 'std': 0.00868599342205394, 'lower_bound': 0.8148960446247464, 'upper_bound': 0.8488970588235294}, {'samples': 14832, 'accuracy': 0.857169878296146, 'std': 0.007411046887477165, 'lower_bound': 0.8422794117647059, 'upper_bound': 0.8711967545638946}, {'samples': 15344, 'accuracy': 0.8551774847870184, 'std': 0.007663476533076894, 'lower_bound': 0.8402510141987829, 'upper_bound': 0.8696881338742394}]
Evaluating the best model on the test set of dataset ./fake_true_datasets/fake_true_dataset_mistral_10k...
Test metrics:
accuracy: 0.862447261663286
precision: 0.7867051316943856
recall: 0.9939743192640825
f1_score: 0.8782289520575364
fp_rate: 0.2686888180496695
tp_rate: 0.9939743192640825
std_accuracy: 0.0076682329215975835
std_precision: 0.011595235832043945
std_recall: 0.0024654732543760148
std_f1_score: 0.007328059140406786
std_fp_rate: 0.013799472995939043
std_tp_rate: 0.0024654732543760148
TP: 978.627
TN: 722.119
FP: 265.322
FN: 5.932
roc_auc: 0.9852869174528593
fpr: [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.0010142  0.0010142  0.0010142  0.0010142
 0.0020284  0.0020284  0.0020284  0.0020284  0.0020284  0.0020284
 0.0020284  0.0020284  0.0020284  0.0020284  0.0030426  0.0030426
 0.0030426  0.0030426  0.0030426  0.0040568  0.0040568  0.00507099
 0.00507099 0.00507099 0.00507099 0.00507099 0.00507099 0.00507099
 0.00507099 0.00507099 0.00507099 0.00507099 0.00507099 0.00507099
 0.00507099 0.00507099 0.00507099 0.00507099 0.00507099 0.00608519
 0.00608519 0.00608519 0.00608519 0.00608519 0.00608519 0.00709939
 0.00709939 0.00811359 0.00811359 0.00811359 0.01014199 0.01014199
 0.01014199 0.01014199 0.01014199 0.01014199 0.01014199 0.01014199
 0.01014199 0.01014199 0.01014199 0.01115619 0.01115619 0.01115619
 0.01217039 0.01217039 0.01217039 0.01217039 0.01217039 0.01318458
 0.01318458 0.01318458 0.01318458 0.01419878 0.01419878 0.01419878
 0.01521298 0.01521298 0.01521298 0.01521298 0.01622718 0.01622718
 0.01724138 0.01724138 0.01926978 0.01926978 0.02028398 0.02028398
 0.02028398 0.02028398 0.02028398 0.02028398 0.02129817 0.02129817
 0.02231237 0.02231237 0.02332657 0.02535497 0.02535497 0.02535497
 0.02535497 0.02636917 0.02636917 0.02738337 0.02738337 0.02738337
 0.02839757 0.02839757 0.03144016 0.03144016 0.03144016 0.03245436
 0.03245436 0.03346856 0.03346856 0.03448276 0.03448276 0.03549696
 0.03651116 0.03651116 0.03955375 0.03955375 0.03955375 0.04056795
 0.04158215 0.04158215 0.04259635 0.04259635 0.04462475 0.04462475
 0.04462475 0.04665314 0.04665314 0.04766734 0.04766734 0.04969574
 0.04969574 0.05679513 0.05679513 0.05780933 0.05780933 0.05983773
 0.06085193 0.06085193 0.06186613 0.06186613 0.06490872 0.06490872
 0.06997972 0.07200811 0.07200811 0.07505071 0.07505071 0.07606491
 0.07707911 0.07707911 0.07910751 0.07910751 0.0801217  0.0801217
 0.0811359  0.0811359  0.0821501  0.0821501  0.0841785  0.0841785
 0.0851927  0.0851927  0.0862069  0.0862069  0.0872211  0.0872211
 0.08924949 0.08924949 0.09127789 0.09432049 0.09432049 0.09533469
 0.09736308 0.09736308 0.09837728 0.09837728 0.09939148 0.09939148
 0.10141988 0.10344828 0.10344828 0.10446247 0.10446247 0.10547667
 0.10649087 0.10649087 0.10953347 0.10953347 0.11054767 0.11054767
 0.11359026 0.11359026 0.12271805 0.12271805 0.12474645 0.12474645
 0.12981744 0.12981744 0.13184584 0.13184584 0.13286004 0.13286004
 0.13590264 0.13590264 0.14503043 0.14503043 0.14908722 0.14908722
 0.15415822 0.15415822 0.15720081 0.15720081 0.16227181 0.1643002
 0.1643002  0.1653144  0.1653144  0.168357   0.168357   0.17748479
 0.17748479 0.18864097 0.18864097 0.19979716 0.19979716 0.20182556
 0.20182556 0.20283976 0.20283976 0.20993915 0.20993915 0.21399594
 0.21399594 0.23833671 0.23833671 0.26977688 0.26977688 0.29817444
 0.30020284 0.35496957 0.35699797 0.37119675 0.37322515 0.37829615
 0.38032454 0.39655172 0.39655172 0.4188641  0.4188641  0.42292089
 0.42494929 0.43813387 0.44016227 0.44219067 0.44421907 0.46653144
 0.47058824 0.47667343 0.47971602 0.47971602 0.5030426  0.50608519
 0.51217039 0.51419878 0.53752535 0.53955375 0.54158215 0.54361055
 0.54665314 0.54868154 0.55882353 0.56085193 0.58924949 0.59127789
 0.59229209 0.59432049 0.60141988 0.60344828 0.60649087 0.60851927
 0.62474645 0.62677485 0.62981744 0.63184584 0.63387424 0.63387424
 0.63995943 0.63995943 0.64300203 0.64503043 0.6663286  0.668357
 0.67545639 0.67748479 0.67951318 0.68154158 0.68559838 0.68762677
 0.69168357 0.69371197 0.70182556 0.70385396 0.71095335 0.71298174
 0.72616633 0.72819473 0.75557809 0.75760649 0.76369168 0.76572008
 0.76774848 0.76977688 0.77281947 0.77383367 0.77586207 0.77687627
 0.77890467 0.78701826 0.78904665 0.79107505 0.79310345 0.80628803
 0.80831643 0.81135903 0.81338742 0.81643002 0.81845842 0.81947262
 0.82150101 0.82352941 0.82758621 0.8296146  0.8336714  0.84584178
 0.84989858 0.85192698 0.85395538 0.85801217 0.86004057 0.86206897
 0.86409736 0.86815416 0.87018256 0.87322515 0.87525355 0.88843813
 0.89046653 0.90770791 0.9137931  0.9158215  0.9178499  0.92089249
 0.92292089 0.92596349 0.92799189 0.94219067 0.94523327 0.94624746
 0.95030426 0.97261663 0.97464503 0.97667343 0.97870183 1.        ]
tpr: [0.         0.0010142  0.00507099 0.00709939 0.00912779 0.01217039
 0.01419878 0.01724138 0.01926978 0.02231237 0.02434077 0.02636917
 0.03549696 0.04056795 0.04158215 0.04361055 0.04462475 0.04766734
 0.04969574 0.05578093 0.05679513 0.06288032 0.06490872 0.07200811
 0.07707911 0.07809331 0.0801217  0.0841785  0.0872211  0.08823529
 0.09127789 0.09736308 0.10040568 0.10141988 0.10547667 0.11054767
 0.11460446 0.11967546 0.12068966 0.12373225 0.12576065 0.13083164
 0.13387424 0.13793103 0.13894523 0.14300203 0.14807302 0.15010142
 0.15517241 0.16125761 0.163286   0.1663286  0.1693712  0.17139959
 0.18154158 0.18356998 0.18762677 0.19168357 0.19269777 0.19878296
 0.20385396 0.20588235 0.20892495 0.21298174 0.21602434 0.21805274
 0.22008114 0.22413793 0.22515213 0.22718053 0.22819473 0.23225152
 0.23732252 0.24137931 0.24340771 0.2484787  0.25456389 0.25557809
 0.26166329 0.26267748 0.26572008 0.26876268 0.27180527 0.27281947
 0.27586207 0.27890467 0.28093306 0.28397566 0.28498986 0.28803245
 0.29208925 0.29310345 0.29513185 0.29614604 0.29817444 0.30527383
 0.30730223 0.32048682 0.32251521 0.32454361 0.32657201 0.3306288
 0.3336714  0.34584178 0.34787018 0.36105477 0.36308316 0.36511156
 0.36713996 0.38336714 0.38539554 0.38945233 0.39148073 0.39350913
 0.39553753 0.39655172 0.39858012 0.40162272 0.40365112 0.40669371
 0.41075051 0.4127789  0.4158215  0.4188641  0.42089249 0.42596349
 0.43002028 0.43204868 0.43407708 0.43914807 0.44117647 0.44523327
 0.44726166 0.44827586 0.45030426 0.45233266 0.45436105 0.45740365
 0.46348884 0.46653144 0.46957404 0.47058824 0.47363083 0.47565923
 0.48377282 0.48985801 0.49087221 0.49290061 0.49492901 0.4989858
 0.5        0.5020284  0.51014199 0.51217039 0.51724138 0.51926978
 0.53651116 0.54056795 0.54462475 0.54665314 0.54868154 0.55070994
 0.55780933 0.56186613 0.57505071 0.57707911 0.58823529 0.59026369
 0.59432049 0.59634888 0.59939148 0.60141988 0.60344828 0.60953347
 0.61156187 0.61764706 0.61764706 0.62271805 0.62474645 0.62677485
 0.62677485 0.62880325 0.63083164 0.63691684 0.63894523 0.64604462
 0.64807302 0.65010142 0.65314402 0.65517241 0.65618661 0.65821501
 0.65922921 0.66227181 0.6663286  0.6663286  0.668357   0.668357
 0.6703854  0.67241379 0.67647059 0.68052738 0.69269777 0.69472617
 0.69574037 0.69776876 0.70791075 0.70993915 0.71399594 0.71805274
 0.72312373 0.72515213 0.73935091 0.74137931 0.74239351 0.74239351
 0.74543611 0.7494929  0.7515213  0.76166329 0.76774848 0.76774848
 0.76977688 0.76977688 0.77180527 0.77281947 0.77281947 0.77383367
 0.77586207 0.77890467 0.78296146 0.78397566 0.78600406 0.79006085
 0.79208925 0.79614604 0.79817444 0.80121704 0.80324544 0.80831643
 0.80831643 0.81034483 0.81338742 0.81541582 0.81845842 0.81845842
 0.82251521 0.82555781 0.82758621 0.82758621 0.8296146  0.8336714
 0.8356998  0.84279919 0.84482759 0.84787018 0.84787018 0.84888438
 0.84888438 0.84989858 0.84989858 0.85091278 0.85091278 0.85192698
 0.85395538 0.85496957 0.85699797 0.86105477 0.86105477 0.86409736
 0.86409736 0.86612576 0.86612576 0.86815416 0.86916836 0.87119675
 0.87423935 0.87525355 0.87626775 0.87626775 0.87829615 0.87931034
 0.87931034 0.88336714 0.88336714 0.88438134 0.88640974 0.88742394
 0.88843813 0.88843813 0.89148073 0.89148073 0.89655172 0.89655172
 0.89756592 0.90567951 0.90567951 0.90770791 0.90872211 0.90973631
 0.90973631 0.91075051 0.91075051 0.9158215  0.9158215  0.9178499
 0.9188641  0.9188641  0.92494929 0.92494929 0.92697769 0.92697769
 0.92799189 0.92799189 0.93002028 0.93002028 0.93306288 0.93306288
 0.93407708 0.93509128 0.93509128 0.93610548 0.93610548 0.93711968
 0.93711968 0.93711968 0.93813387 0.93813387 0.93914807 0.93914807
 0.94016227 0.94117647 0.94117647 0.94219067 0.94219067 0.94421907
 0.94421907 0.94523327 0.94523327 0.94624746 0.94624746 0.94827586
 0.94827586 0.94929006 0.94929006 0.95030426 0.95030426 0.95131846
 0.95131846 0.95334686 0.95334686 0.95334686 0.95537525 0.95537525
 0.95537525 0.95638945 0.95638945 0.95740365 0.95841785 0.96146045
 0.96146045 0.96146045 0.96247465 0.96247465 0.96348884 0.96450304
 0.96450304 0.96551724 0.96551724 0.96653144 0.96653144 0.96754564
 0.96754564 0.96855984 0.96855984 0.96957404 0.96957404 0.97058824
 0.97058824 0.97363083 0.97363083 0.97464503 0.97464503 0.97565923
 0.97565923 0.97667343 0.97667343 0.97768763 0.97768763 0.97870183
 0.97870183 0.97971602 0.97971602 0.98073022 0.98073022 0.98073022
 0.98174442 0.98174442 0.98275862 0.98275862 0.98377282 0.98377282
 0.98478702 0.98478702 0.98681542 0.98681542 0.98782961 0.98782961
 0.98884381 0.98884381 0.98985801 0.98985801 0.99087221 0.99087221
 0.99290061 0.99290061 0.99391481 0.99391481 0.99492901 0.99492901
 0.99492901 0.99492901 0.99492901 0.99492901 0.99492901 0.99492901
 0.99492901 0.99492901 0.9959432  0.9959432  0.9969574  0.9969574
 0.9969574  0.9969574  0.9969574  0.9969574  0.9969574  0.9969574
 0.9969574  0.9969574  0.9969574  0.9979716  0.9979716  0.9979716
 0.9979716  0.9979716  0.9979716  0.9979716  0.9979716  0.9979716
 0.9979716  0.9979716  0.9979716  0.9979716  0.9979716  0.9979716
 0.9979716  0.9979716  0.9979716  0.9979716  0.9979716  0.9979716
 0.9979716  0.9979716  0.9979716  0.9979716  0.9979716  0.9989858
 0.9989858  1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.        ]
thresholds: [        inf  6.3789062   6.328125    6.3125      6.2734375   6.2695312
  6.265625    6.2421875   6.2382812   6.234375    6.2148438   6.2070312
  6.1953125   6.1914062   6.1796875   6.1757812   6.171875    6.1640625
  6.1601562   6.1523438   6.1484375   6.140625    6.1367188   6.1328125
  6.1289062   6.125       6.1210938   6.1171875   6.1132812   6.109375
  6.1054688   6.1015625   6.0976562   6.09375     6.0859375   6.0820312
  6.0742188   6.0703125   6.0664062   6.0625      6.0585938   6.0546875
  6.0507812   6.046875    6.0429688   6.0351562   6.03125     6.0273438
  6.0234375   6.015625    6.0117188   6.          5.9960938   5.9921875
  5.984375    5.9804688   5.9765625   5.96875     5.9648438   5.9570312
  5.953125    5.9492188   5.9453125   5.9414062   5.9375      5.9335938
  5.921875    5.9140625   5.9101562   5.90625     5.9023438   5.890625
  5.8867188   5.8828125   5.8789062   5.8710938   5.8632812   5.859375
  5.8476562   5.84375     5.8320312   5.8203125   5.8164062   5.8125
  5.8085938   5.796875    5.7929688   5.7890625   5.7851562   5.78125
  5.7695312   5.765625    5.7617188   5.7539062   5.7460938   5.6835938
  5.6796875   5.546875    5.5351562   5.5039062   5.4882812   5.4257812
  5.40625     5.1875      5.1757812   4.9960938   4.9804688   4.96875
  4.9609375   4.7929688   4.7890625   4.7382812   4.734375    4.7148438
  4.6679688   4.6640625   4.625       4.5585938   4.5429688   4.5078125
  4.5         4.4765625   4.4726562   4.4453125   4.4179688   4.3710938
  4.3632812   4.3515625   4.3476562   4.3125      4.3046875   4.2617188
  4.2460938   4.2421875   4.2382812   4.2226562   4.2109375   4.1835938
  4.1640625   4.1445312   4.140625    4.1367188   4.125       4.1210938
  4.0585938   4.0273438   4.0234375   4.0039062   3.984375    3.9726562
  3.96875     3.9648438   3.90625     3.9042969   3.8828125   3.8808594
  3.7988281   3.78125     3.7519531   3.7421875   3.7382812   3.7363281
  3.6972656   3.6933594   3.6230469   3.6171875   3.5234375   3.5117188
  3.4765625   3.4726562   3.4707031   3.4550781   3.4472656   3.4003906
  3.3945312   3.3574219   3.3535156   3.3164062   3.3144531   3.3046875
  3.3007812   3.2929688   3.2851562   3.265625    3.2578125   3.2246094
  3.2207031   3.2128906   3.2011719   3.1972656   3.1953125   3.1914062
  3.1875      3.1777344   3.1621094   3.1503906   3.1347656   3.1328125
  3.1191406   3.1171875   3.1074219   3.1054688   3.0332031   3.03125
  3.0292969   3.0234375   2.9746094   2.9433594   2.9140625   2.9082031
  2.8886719   2.8867188   2.8085938   2.8046875   2.7988281   2.7929688
  2.7773438   2.7695312   2.765625    2.71875     2.6777344   2.6699219
  2.6660156   2.6621094   2.65625     2.6542969   2.6289062   2.6269531
  2.625       2.6152344   2.6132812   2.6054688   2.5917969   2.5644531
  2.5527344   2.53125     2.5234375   2.5214844   2.5195312   2.4941406
  2.4648438   2.4589844   2.4550781   2.453125    2.4296875   2.4238281
  2.4023438   2.4003906   2.390625    2.3867188   2.3828125   2.3476562
  2.34375     2.3066406   2.2929688   2.2792969   2.2773438   2.2753906
  2.2714844   2.2558594   2.2402344   2.2363281   2.234375    2.2304688
  2.2265625   2.2089844   2.2070312   2.1777344   2.1699219   2.1660156
  2.1445312   2.1191406   2.1171875   2.1074219   2.1015625   2.0976562
  2.0644531   2.0488281   2.0449219   2.0410156   2.0371094   2.0351562
  2.0292969   2.0136719   1.9960938   1.9912109   1.9853516   1.9824219
  1.9716797   1.9648438   1.9472656   1.9433594   1.9355469   1.9326172
  1.9306641   1.875       1.8535156   1.8427734   1.8388672   1.8076172
  1.8066406   1.8056641   1.7949219   1.7480469   1.7402344   1.7382812
  1.7314453   1.7138672   1.6826172   1.6777344   1.671875    1.6445312
  1.6425781   1.5986328   1.5869141   1.5849609   1.5380859   1.5205078
  1.5039062   1.4970703   1.4960938   1.4912109   1.46875     1.4628906
  1.4462891   1.4423828   1.4375      1.4013672   1.3925781   1.390625
  1.3681641   1.3671875   1.3613281   1.359375    1.3583984   1.3525391
  1.3408203   1.3330078   1.3271484   1.3261719   1.3203125   1.3105469
  1.3056641   1.2988281   1.2949219   1.2929688   1.2763672   1.2753906
  1.2695312   1.2636719   1.2587891   1.2265625   1.2255859   1.2216797
  1.21875     1.2060547   1.2041016   1.2021484   1.2011719   1.1953125
  1.1757812   1.1552734   1.1494141   1.1425781   1.1376953   1.1318359
  1.1308594   1.1269531   1.1044922   1.1015625   1.0908203   1.0791016
  1.0449219   1.0341797   0.94921875  0.9472656   0.9135742   0.90722656
  0.87646484  0.8618164   0.8544922   0.8540039   0.8466797   0.8442383
  0.8251953   0.8100586   0.7607422   0.7578125   0.74365234  0.73876953
  0.72558594  0.7192383   0.7133789   0.7080078   0.6796875   0.6777344
  0.671875    0.66845703  0.6665039   0.64208984  0.6308594   0.57421875
  0.5649414   0.5180664   0.50439453  0.42407227  0.42260742  0.4008789
  0.3955078   0.39282227  0.39135742  0.3527832   0.34838867  0.32958984
  0.3161621   0.18896484  0.18762207  0.0072937  -0.00994873 -0.17797852
 -0.17883301 -0.48876953 -0.49365234 -0.5444336  -0.54541016 -0.5541992
 -0.55615234 -0.6333008  -0.6357422  -0.75390625 -0.75927734 -0.7675781
 -0.7739258  -0.82666016 -0.8305664  -0.83447266 -0.8544922  -0.9091797
 -0.9169922  -0.93896484 -0.9404297  -0.9458008  -1.0068359  -1.0146484
 -1.0302734  -1.0380859  -1.125      -1.1308594  -1.1337891  -1.1347656
 -1.1484375  -1.1494141  -1.1865234  -1.1923828  -1.3222656  -1.3271484
 -1.3349609  -1.3359375  -1.3603516  -1.3632812  -1.3769531  -1.3798828
 -1.4462891  -1.4570312  -1.4677734  -1.4794922  -1.4824219  -1.4833984
 -1.5117188  -1.515625   -1.5537109  -1.5556641  -1.6416016  -1.6464844
 -1.6767578  -1.6835938  -1.6884766  -1.6894531  -1.6992188  -1.7021484
 -1.7158203  -1.7167969  -1.7412109  -1.75       -1.7753906  -1.78125
 -1.8515625  -1.8564453  -2.0019531  -2.0039062  -2.0253906  -2.03125
 -2.0351562  -2.0410156  -2.0488281  -2.0507812  -2.0527344  -2.0683594
 -2.0722656  -2.1347656  -2.1386719  -2.1445312  -2.1484375  -2.2324219
 -2.2421875  -2.2539062  -2.2558594  -2.265625   -2.2675781  -2.2734375
 -2.2792969  -2.3027344  -2.3125     -2.3261719  -2.3496094  -2.4316406
 -2.4375     -2.4433594  -2.4492188  -2.4667969  -2.46875    -2.4804688
 -2.4941406  -2.5136719  -2.5273438  -2.5429688  -2.5644531  -2.640625
 -2.6484375  -2.7246094  -2.7480469  -2.7578125  -2.7695312  -2.7910156
 -2.7949219  -2.8125     -2.8164062  -2.9355469  -2.9394531  -2.9414062
 -2.9550781  -3.2226562  -3.2304688  -3.2480469  -3.2636719  -3.9746094 ]
