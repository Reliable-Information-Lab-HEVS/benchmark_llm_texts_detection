log_loss_steps: 256
eval_steps: 512
check_degradation: 0
----------------Epoch 1/1----------------
Epoch 1/1, Loss after 192 samples: 0.6937
Epoch 1/1, Loss after 448 samples: 0.6695
Mean accuracy: 0.6960, std: 0.0101, lower bound: 0.6757, upper bound: 0.7149 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 448 samples: 0.6963 with eval loss: 0.6375
Best model with eval loss 0.6375097036361694 and eval accuracy 0.696285140562249 with 448 samples seen is saved
Epoch 1/1, Loss after 704 samples: 0.6288
Epoch 1/1, Loss after 960 samples: 0.5654
Mean accuracy: 0.8484, std: 0.0082, lower bound: 0.8333, upper bound: 0.8655 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 960 samples: 0.8484 with eval loss: 0.4677
Best model with eval loss 0.46773117780685425 and eval accuracy 0.8483935742971888 with 960 samples seen is saved
Epoch 1/1, Loss after 1216 samples: 0.5163
Epoch 1/1, Loss after 1472 samples: 0.4198
Mean accuracy: 0.8873, std: 0.0067, lower bound: 0.8745, upper bound: 0.8996 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 1472 samples: 0.8876 with eval loss: 0.3386
Best model with eval loss 0.3386444989591837 and eval accuracy 0.8875502008032129 with 1472 samples seen is saved
Epoch 1/1, Loss after 1728 samples: 0.3962
Epoch 1/1, Loss after 1984 samples: 0.3489
Mean accuracy: 0.8782, std: 0.0075, lower bound: 0.8630, upper bound: 0.8936 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 1984 samples: 0.8780 with eval loss: 0.2984
Best model with eval loss 0.2984266700223088 and eval accuracy 0.8780120481927711 with 1984 samples seen is saved
Epoch 1/1, Loss after 2240 samples: 0.3501
Epoch 1/1, Loss after 2496 samples: 0.3343
Mean accuracy: 0.8526, std: 0.0078, lower bound: 0.8373, upper bound: 0.8670 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 2496 samples: 0.8524 with eval loss: 0.3191
Epoch 1/1, Loss after 2752 samples: 0.3287
Epoch 1/1, Loss after 3008 samples: 0.3832
Mean accuracy: 0.8891, std: 0.0068, lower bound: 0.8765, upper bound: 0.9016 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 3008 samples: 0.8891 with eval loss: 0.2568
Best model with eval loss 0.2568259034305811 and eval accuracy 0.8890562248995983 with 3008 samples seen is saved
Epoch 1/1, Loss after 3264 samples: 0.2438
Epoch 1/1, Loss after 3520 samples: 0.2809
Mean accuracy: 0.7996, std: 0.0089, lower bound: 0.7821, upper bound: 0.8168 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 3520 samples: 0.8002 with eval loss: 0.4311
Epoch 1/1, Loss after 3776 samples: 0.3368
Epoch 1/1, Loss after 4032 samples: 0.3328
Mean accuracy: 0.7661, std: 0.0097, lower bound: 0.7480, upper bound: 0.7847 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 4032 samples: 0.7661 with eval loss: 0.5146
Epoch 1/1, Loss after 4288 samples: 0.4138
Epoch 1/1, Loss after 4544 samples: 0.3879
Mean accuracy: 0.7405, std: 0.0097, lower bound: 0.7214, upper bound: 0.7600 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 4544 samples: 0.7410 with eval loss: 0.5663
Epoch 1/1, Loss after 4800 samples: 0.3748
Epoch 1/1, Loss after 5056 samples: 0.3128
Mean accuracy: 0.8014, std: 0.0089, lower bound: 0.7841, upper bound: 0.8193 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 5056 samples: 0.8017 with eval loss: 0.4146
Epoch 1/1, Loss after 5312 samples: 0.2965
Epoch 1/1, Loss after 5568 samples: 0.3088
Mean accuracy: 0.7929, std: 0.0091, lower bound: 0.7741, upper bound: 0.8097 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 5568 samples: 0.7932 with eval loss: 0.4238
Epoch 1/1, Loss after 5824 samples: 0.3709
Epoch 1/1, Loss after 6080 samples: 0.2967
Mean accuracy: 0.8645, std: 0.0074, lower bound: 0.8494, upper bound: 0.8780 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 6080 samples: 0.8645 with eval loss: 0.2929
Epoch 1/1, Loss after 6336 samples: 0.2944
Epoch 1/1, Loss after 6592 samples: 0.3049
Mean accuracy: 0.8030, std: 0.0088, lower bound: 0.7846, upper bound: 0.8198 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 6592 samples: 0.8032 with eval loss: 0.4049
Epoch 1/1, Loss after 6848 samples: 0.2717
Epoch 1/1, Loss after 7104 samples: 0.3527
Mean accuracy: 0.8358, std: 0.0081, lower bound: 0.8198, upper bound: 0.8514 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 7104 samples: 0.8358 with eval loss: 0.3422
Epoch 1/1, Loss after 7360 samples: 0.2926
Epoch 1/1, Loss after 7616 samples: 0.3262
Mean accuracy: 0.8658, std: 0.0078, lower bound: 0.8504, upper bound: 0.8805 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 7616 samples: 0.8660 with eval loss: 0.2912
Epoch 1/1, Loss after 7872 samples: 0.3276
Epoch 1/1, Loss after 8128 samples: 0.3161
Mean accuracy: 0.8952, std: 0.0070, lower bound: 0.8810, upper bound: 0.9081 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 8128 samples: 0.8951 with eval loss: 0.2476
Best model with eval loss 0.24756259424611926 and eval accuracy 0.8950803212851406 with 8128 samples seen is saved
Epoch 1/1, Loss after 8384 samples: 0.3319
Epoch 1/1, Loss after 8640 samples: 0.2835
Mean accuracy: 0.8679, std: 0.0077, lower bound: 0.8514, upper bound: 0.8820 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 8640 samples: 0.8680 with eval loss: 0.2867
Epoch 1/1, Loss after 8896 samples: 0.3333
Epoch 1/1, Loss after 9152 samples: 0.2905
Mean accuracy: 0.8487, std: 0.0082, lower bound: 0.8328, upper bound: 0.8650 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 9152 samples: 0.8484 with eval loss: 0.3207
Epoch 1/1, Loss after 9408 samples: 0.2680
Epoch 1/1, Loss after 9664 samples: 0.3029
Mean accuracy: 0.8258, std: 0.0084, lower bound: 0.8097, upper bound: 0.8419 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 9664 samples: 0.8258 with eval loss: 0.3579
Epoch 1/1, Loss after 9920 samples: 0.2746
Epoch 1/1, Loss after 10176 samples: 0.2645
Mean accuracy: 0.8435, std: 0.0079, lower bound: 0.8283, upper bound: 0.8589 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 10176 samples: 0.8434 with eval loss: 0.3247
Epoch 1/1, Loss after 10432 samples: 0.3223
Epoch 1/1, Loss after 10688 samples: 0.3072
Mean accuracy: 0.9069, std: 0.0066, lower bound: 0.8941, upper bound: 0.9192 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 10688 samples: 0.9071 with eval loss: 0.2232
Best model with eval loss 0.2232201350852847 and eval accuracy 0.9071285140562249 with 10688 samples seen is saved
Epoch 1/1, Loss after 10944 samples: 0.3013
Epoch 1/1, Loss after 11200 samples: 0.2940
Mean accuracy: 0.8728, std: 0.0076, lower bound: 0.8579, upper bound: 0.8870 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 11200 samples: 0.8730 with eval loss: 0.2793
Epoch 1/1, Loss after 11456 samples: 0.2216
Epoch 1/1, Loss after 11712 samples: 0.2798
Mean accuracy: 0.8418, std: 0.0079, lower bound: 0.8268, upper bound: 0.8564 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 11712 samples: 0.8414 with eval loss: 0.3331
Epoch 1/1, Loss after 11968 samples: 0.2713
Epoch 1/1, Loss after 12224 samples: 0.3301
Mean accuracy: 0.8528, std: 0.0079, lower bound: 0.8378, upper bound: 0.8680 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 12224 samples: 0.8524 with eval loss: 0.3147
Epoch 1/1, Loss after 12480 samples: 0.2416
Epoch 1/1, Loss after 12736 samples: 0.2726
Mean accuracy: 0.8506, std: 0.0078, lower bound: 0.8358, upper bound: 0.8655 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 12736 samples: 0.8504 with eval loss: 0.3198
Epoch 1/1, Loss after 12992 samples: 0.2990
Epoch 1/1, Loss after 13248 samples: 0.2983
Mean accuracy: 0.8292, std: 0.0086, lower bound: 0.8117, upper bound: 0.8464 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 13248 samples: 0.8288 with eval loss: 0.3555
Epoch 1/1, Loss after 13504 samples: 0.2936
Epoch 1/1, Loss after 13760 samples: 0.2619
Mean accuracy: 0.8694, std: 0.0075, lower bound: 0.8549, upper bound: 0.8845 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 13760 samples: 0.8695 with eval loss: 0.2849
Epoch 1/1, Loss after 14016 samples: 0.2389
Epoch 1/1, Loss after 14272 samples: 0.2820
Mean accuracy: 0.8633, std: 0.0077, lower bound: 0.8479, upper bound: 0.8780 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 14272 samples: 0.8635 with eval loss: 0.2961
Epoch 1/1, Loss after 14528 samples: 0.2610
Epoch 1/1, Loss after 14784 samples: 0.3203
Mean accuracy: 0.8724, std: 0.0070, lower bound: 0.8589, upper bound: 0.8860 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 14784 samples: 0.8725 with eval loss: 0.2811
Epoch 1/1, Loss after 15040 samples: 0.2220
Epoch 1/1, Loss after 15296 samples: 0.2552
Mean accuracy: 0.8443, std: 0.0082, lower bound: 0.8273, upper bound: 0.8594 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 15296 samples: 0.8444 with eval loss: 0.3306
Epoch 1/1, Loss after 15552 samples: 0.3187
Epoch 1/1, Loss after 15808 samples: 0.3024
Mean accuracy: 0.8451, std: 0.0082, lower bound: 0.8288, upper bound: 0.8614 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 15808 samples: 0.8454 with eval loss: 0.3288
Training signal is False, stopping training
Training finished
Best model: {'eval_acc': 0.9071285140562249, 'nb_samples': 10688, 'eval_loss': 0.2232201350852847}
Training loss logs: [{'samples': 192, 'loss': 0.6937007904052734}, {'samples': 448, 'loss': 0.6695470809936523}, {'samples': 704, 'loss': 0.6288042068481445}, {'samples': 960, 'loss': 0.5654406547546387}, {'samples': 1216, 'loss': 0.5163286924362183}, {'samples': 1472, 'loss': 0.4197530150413513}, {'samples': 1728, 'loss': 0.3961979001760483}, {'samples': 1984, 'loss': 0.3488945960998535}, {'samples': 2240, 'loss': 0.3501380681991577}, {'samples': 2496, 'loss': 0.33430660516023636}, {'samples': 2752, 'loss': 0.32874801754951477}, {'samples': 3008, 'loss': 0.383151151239872}, {'samples': 3264, 'loss': 0.24375193193554878}, {'samples': 3520, 'loss': 0.28093448281288147}, {'samples': 3776, 'loss': 0.3368116393685341}, {'samples': 4032, 'loss': 0.33275962620973587}, {'samples': 4288, 'loss': 0.4138118550181389}, {'samples': 4544, 'loss': 0.3879416212439537}, {'samples': 4800, 'loss': 0.37478576228022575}, {'samples': 5056, 'loss': 0.31280405819416046}, {'samples': 5312, 'loss': 0.2965451776981354}, {'samples': 5568, 'loss': 0.3088031932711601}, {'samples': 5824, 'loss': 0.3708588033914566}, {'samples': 6080, 'loss': 0.2967378869652748}, {'samples': 6336, 'loss': 0.2944178506731987}, {'samples': 6592, 'loss': 0.30486060306429863}, {'samples': 6848, 'loss': 0.2716897577047348}, {'samples': 7104, 'loss': 0.3527027294039726}, {'samples': 7360, 'loss': 0.29260503500699997}, {'samples': 7616, 'loss': 0.32624416798353195}, {'samples': 7872, 'loss': 0.3276209831237793}, {'samples': 8128, 'loss': 0.31614646315574646}, {'samples': 8384, 'loss': 0.3319384939968586}, {'samples': 8640, 'loss': 0.28346074372529984}, {'samples': 8896, 'loss': 0.3332894369959831}, {'samples': 9152, 'loss': 0.2905239462852478}, {'samples': 9408, 'loss': 0.2680109664797783}, {'samples': 9664, 'loss': 0.3029216080904007}, {'samples': 9920, 'loss': 0.2745526283979416}, {'samples': 10176, 'loss': 0.26454292610287666}, {'samples': 10432, 'loss': 0.32225240767002106}, {'samples': 10688, 'loss': 0.3071863278746605}, {'samples': 10944, 'loss': 0.30133212730288506}, {'samples': 11200, 'loss': 0.29397960752248764}, {'samples': 11456, 'loss': 0.22161249443888664}, {'samples': 11712, 'loss': 0.2798452079296112}, {'samples': 11968, 'loss': 0.2713281363248825}, {'samples': 12224, 'loss': 0.3301299512386322}, {'samples': 12480, 'loss': 0.2415892668068409}, {'samples': 12736, 'loss': 0.2726057544350624}, {'samples': 12992, 'loss': 0.2989935800433159}, {'samples': 13248, 'loss': 0.2982960641384125}, {'samples': 13504, 'loss': 0.2936268448829651}, {'samples': 13760, 'loss': 0.26185866817831993}, {'samples': 14016, 'loss': 0.23886598646640778}, {'samples': 14272, 'loss': 0.2820492312312126}, {'samples': 14528, 'loss': 0.2610177621245384}, {'samples': 14784, 'loss': 0.3203289210796356}, {'samples': 15040, 'loss': 0.2220468781888485}, {'samples': 15296, 'loss': 0.25524088367819786}, {'samples': 15552, 'loss': 0.3187370039522648}, {'samples': 15808, 'loss': 0.3023664765059948}]
Evaluation accuracy logs: [{'samples': 448, 'accuracy': 0.6959723895582328, 'std': 0.01008802435271457, 'lower_bound': 0.6757028112449799, 'upper_bound': 0.7148719879518072}, {'samples': 960, 'accuracy': 0.8483955823293172, 'std': 0.008169507634258623, 'lower_bound': 0.8333333333333334, 'upper_bound': 0.8654618473895582}, {'samples': 1472, 'accuracy': 0.8873303212851406, 'std': 0.006688701895332779, 'lower_bound': 0.8744979919678715, 'upper_bound': 0.8995983935742972}, {'samples': 1984, 'accuracy': 0.8782294176706826, 'std': 0.007483010449408815, 'lower_bound': 0.8629518072289156, 'upper_bound': 0.893574297188755}, {'samples': 2496, 'accuracy': 0.8525722891566265, 'std': 0.00778551217139704, 'lower_bound': 0.8373493975903614, 'upper_bound': 0.8669804216867469}, {'samples': 3008, 'accuracy': 0.8890532128514055, 'std': 0.006762887876715484, 'lower_bound': 0.8765060240963856, 'upper_bound': 0.9016064257028112}, {'samples': 3520, 'accuracy': 0.7996204819277108, 'std': 0.00892408291189938, 'lower_bound': 0.7821285140562249, 'upper_bound': 0.8167670682730924}, {'samples': 4032, 'accuracy': 0.7661485943775102, 'std': 0.009726498263582647, 'lower_bound': 0.7479919678714859, 'upper_bound': 0.7846511044176706}, {'samples': 4544, 'accuracy': 0.740535140562249, 'std': 0.00972329667892937, 'lower_bound': 0.7213855421686747, 'upper_bound': 0.7600401606425703}, {'samples': 5056, 'accuracy': 0.801370983935743, 'std': 0.008932265452450827, 'lower_bound': 0.7841239959839357, 'upper_bound': 0.8192771084337349}, {'samples': 5568, 'accuracy': 0.7928684738955822, 'std': 0.009071060998290968, 'lower_bound': 0.7740963855421686, 'upper_bound': 0.8097389558232931}, {'samples': 6080, 'accuracy': 0.8645130522088353, 'std': 0.007421639955236804, 'lower_bound': 0.8493975903614458, 'upper_bound': 0.8780245983935743}, {'samples': 6592, 'accuracy': 0.8030210843373495, 'std': 0.008797410765586017, 'lower_bound': 0.7846385542168675, 'upper_bound': 0.8197791164658634}, {'samples': 7104, 'accuracy': 0.8357971887550201, 'std': 0.008077050834397954, 'lower_bound': 0.8197791164658634, 'upper_bound': 0.8514056224899599}, {'samples': 7616, 'accuracy': 0.8658067269076305, 'std': 0.007847800720685657, 'lower_bound': 0.8503890562248996, 'upper_bound': 0.8805346385542168}, {'samples': 8128, 'accuracy': 0.8951862449799196, 'std': 0.007010097375088486, 'lower_bound': 0.8810240963855421, 'upper_bound': 0.9081325301204819}, {'samples': 8640, 'accuracy': 0.8679317269076305, 'std': 0.00765353352235531, 'lower_bound': 0.8514056224899599, 'upper_bound': 0.8820406626506023}, {'samples': 9152, 'accuracy': 0.8487414658634539, 'std': 0.008236493239231438, 'lower_bound': 0.8328187751004016, 'upper_bound': 0.8649598393574297}, {'samples': 9664, 'accuracy': 0.8258353413654619, 'std': 0.00837318554226668, 'lower_bound': 0.8097264056224899, 'upper_bound': 0.8418674698795181}, {'samples': 10176, 'accuracy': 0.8434518072289157, 'std': 0.007881968370677759, 'lower_bound': 0.8283132530120482, 'upper_bound': 0.8589482931726907}, {'samples': 10688, 'accuracy': 0.9068549196787149, 'std': 0.006559268049470223, 'lower_bound': 0.8940763052208835, 'upper_bound': 0.9191767068273092}, {'samples': 11200, 'accuracy': 0.8727821285140562, 'std': 0.007576385251690981, 'lower_bound': 0.8579317269076305, 'upper_bound': 0.8870481927710844}, {'samples': 11712, 'accuracy': 0.8417786144578313, 'std': 0.00792803519330851, 'lower_bound': 0.8267946787148593, 'upper_bound': 0.8564382530120481}, {'samples': 12224, 'accuracy': 0.852750502008032, 'std': 0.007865810038067205, 'lower_bound': 0.8378388554216867, 'upper_bound': 0.867984437751004}, {'samples': 12736, 'accuracy': 0.8505913654618473, 'std': 0.007844140413197262, 'lower_bound': 0.8358433734939759, 'upper_bound': 0.8654618473895582}, {'samples': 13248, 'accuracy': 0.8291877510040162, 'std': 0.008648091014432034, 'lower_bound': 0.8117469879518072, 'upper_bound': 0.8463855421686747}, {'samples': 13760, 'accuracy': 0.8693815261044177, 'std': 0.00748771756007446, 'lower_bound': 0.8549196787148594, 'upper_bound': 0.8845381526104418}, {'samples': 14272, 'accuracy': 0.8632801204819277, 'std': 0.007725156339851875, 'lower_bound': 0.8478915662650602, 'upper_bound': 0.8780245983935743}, {'samples': 14784, 'accuracy': 0.8724011044176707, 'std': 0.007045941080587996, 'lower_bound': 0.8589357429718876, 'upper_bound': 0.8860441767068273}, {'samples': 15296, 'accuracy': 0.8443293172690763, 'std': 0.008184375127391664, 'lower_bound': 0.8272966867469879, 'upper_bound': 0.8594377510040161}, {'samples': 15808, 'accuracy': 0.8451380522088353, 'std': 0.008169288472491834, 'lower_bound': 0.8288152610441767, 'upper_bound': 0.8614457831325302}]
Evaluating the best model on the test set of dataset ./fake_true_datasets/fake_true_dataset_phi_10k...
Test metrics:
accuracy: 0.8522640562248996
precision: 0.7775938051939199
recall: 0.9870513816255232
f1_score: 0.8698427077828078
fp_rate: 0.2827216764633605
tp_rate: 0.9870513816255232
std_accuracy: 0.007874678874079956
std_precision: 0.01166627808055648
std_recall: 0.0035837007944730746
std_f1_score: 0.007556452759664948
std_fp_rate: 0.013779781658961819
std_tp_rate: 0.0035837007944730746
TP: 983.852
TN: 713.858
FP: 281.384
FN: 12.906
roc_auc: 0.9745639183238979
fpr: [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00100402 0.00100402 0.00100402 0.00100402 0.00100402 0.00100402
 0.00100402 0.00100402 0.00100402 0.00100402 0.00100402 0.00100402
 0.00100402 0.00100402 0.00100402 0.00100402 0.00100402 0.00100402
 0.00100402 0.00100402 0.00100402 0.00100402 0.00100402 0.00200803
 0.00200803 0.00200803 0.00200803 0.00200803 0.00200803 0.00200803
 0.00200803 0.00200803 0.00200803 0.00200803 0.00200803 0.00200803
 0.00200803 0.00200803 0.00200803 0.00200803 0.00200803 0.00200803
 0.00301205 0.00301205 0.00301205 0.00301205 0.00301205 0.00301205
 0.00301205 0.00301205 0.00301205 0.00301205 0.00301205 0.00301205
 0.00301205 0.00301205 0.00301205 0.00301205 0.00301205 0.00301205
 0.00301205 0.00301205 0.00301205 0.00301205 0.00301205 0.00301205
 0.00301205 0.00301205 0.00401606 0.00401606 0.00401606 0.00401606
 0.00401606 0.00401606 0.00401606 0.00401606 0.00502008 0.00502008
 0.00502008 0.00502008 0.00502008 0.00502008 0.00502008 0.00502008
 0.00502008 0.00502008 0.00502008 0.00502008 0.00502008 0.00502008
 0.00502008 0.00502008 0.0060241  0.0060241  0.0060241  0.00702811
 0.00702811 0.00803213 0.00803213 0.00803213 0.00803213 0.00803213
 0.00803213 0.00803213 0.00903614 0.00903614 0.00903614 0.00903614
 0.01004016 0.01004016 0.01004016 0.01004016 0.01004016 0.01004016
 0.01004016 0.01004016 0.01004016 0.01004016 0.01004016 0.01004016
 0.01004016 0.01004016 0.01004016 0.01104418 0.01104418 0.01104418
 0.01204819 0.01204819 0.01204819 0.01305221 0.01305221 0.01506024
 0.01506024 0.01606426 0.01606426 0.01706827 0.01706827 0.01807229
 0.01807229 0.01907631 0.01907631 0.02008032 0.02008032 0.02008032
 0.02008032 0.02008032 0.02008032 0.02008032 0.02008032 0.02008032
 0.02008032 0.02208835 0.02208835 0.02208835 0.02208835 0.02309237
 0.02309237 0.02309237 0.02309237 0.02409639 0.02409639 0.02409639
 0.02409639 0.02409639 0.0251004  0.0251004  0.02710843 0.02710843
 0.02811245 0.02811245 0.02911647 0.02911647 0.02911647 0.02911647
 0.0311245  0.0311245  0.03212851 0.03212851 0.03313253 0.03313253
 0.03413655 0.03413655 0.03413655 0.03514056 0.03514056 0.03514056
 0.03514056 0.03614458 0.03614458 0.03714859 0.03815261 0.03915663
 0.03915663 0.04016064 0.04016064 0.04016064 0.04116466 0.04116466
 0.04116466 0.04116466 0.04317269 0.04518072 0.04518072 0.04718876
 0.04718876 0.04819277 0.04819277 0.04819277 0.04819277 0.04919679
 0.0502008  0.0502008  0.05220884 0.05220884 0.05421687 0.05522088
 0.05522088 0.0562249  0.0562249  0.05823293 0.05823293 0.05823293
 0.05923695 0.06024096 0.06124498 0.06124498 0.06425703 0.06425703
 0.06526104 0.06526104 0.06626506 0.06726908 0.06726908 0.06827309
 0.06827309 0.07128514 0.07228916 0.07429719 0.0753012  0.0753012
 0.07630522 0.07630522 0.07730924 0.08032129 0.08032129 0.08232932
 0.08232932 0.08634538 0.0873494  0.0873494  0.08835341 0.08835341
 0.08935743 0.09136546 0.09337349 0.09337349 0.09437751 0.09437751
 0.09538153 0.09538153 0.09638554 0.09638554 0.10140562 0.10341365
 0.10341365 0.10742972 0.10742972 0.10943775 0.10943775 0.11345382
 0.11345382 0.11646586 0.11646586 0.12048193 0.12148594 0.12248996
 0.12248996 0.12550201 0.12650602 0.13353414 0.13453815 0.14457831
 0.14457831 0.14759036 0.14759036 0.15160643 0.15160643 0.15662651
 0.15662651 0.15863454 0.15963855 0.15963855 0.16465863 0.16465863
 0.16767068 0.16767068 0.17269076 0.17269076 0.1746988  0.1746988
 0.17570281 0.17670683 0.17670683 0.18875502 0.18875502 0.18975904
 0.18975904 0.19076305 0.19076305 0.19879518 0.1997992  0.20180723
 0.20180723 0.21084337 0.21084337 0.21184739 0.21184739 0.21586345
 0.21586345 0.22088353 0.22088353 0.22188755 0.22188755 0.23694779
 0.23895582 0.23995984 0.23995984 0.24698795 0.24698795 0.25301205
 0.25301205 0.25502008 0.25502008 0.25702811 0.25702811 0.25903614
 0.25903614 0.28514056 0.28514056 0.29518072 0.29518072 0.29718876
 0.29718876 0.3002008  0.3002008  0.3373494  0.33935743 0.39558233
 0.39558233 0.39959839 0.40160643 0.41566265 0.41767068 0.42269076
 0.4246988  0.42771084 0.42771084 0.44277108 0.44477912 0.44779116
 0.44779116 0.44879518 0.44879518 0.45983936 0.46084337 0.46184739
 0.46184739 0.48795181 0.48995984 0.51405622 0.51606426 0.52710843
 0.52911647 0.53413655 0.53614458 0.55421687 0.5562249  0.55722892
 0.55923695 0.58032129 0.58232932 0.58634538 0.58835341 0.59036145
 0.59036145 0.59337349 0.59538153 0.59738956 0.59939759 0.60240964
 0.60441767 0.60742972 0.61044177 0.61144578 0.61345382 0.62751004
 0.62951807 0.64859438 0.65461847 0.67068273 0.67269076 0.6746988
 0.67670683 0.70180723 0.70381526 0.70783133 0.70983936 0.71285141
 0.71485944 0.71586345 0.71787149 0.7248996  0.72690763 0.73594378
 0.73795181 0.74799197 0.75       0.75803213 0.76004016 0.76606426
 0.76807229 0.76907631 0.77108434 0.78012048 0.78413655 0.79718876
 0.79919679 0.81325301 0.81526104 0.81927711 0.82228916 0.83333333
 0.83534137 0.84036145 0.84236948 0.84738956 0.84939759 0.85843373
 0.86044177 0.90461847 0.90662651 0.90863454 0.91064257 0.91164659
 0.91164659 0.937751   0.93975904 0.95281124 0.95481928 0.95682731
 0.95983936 0.97088353 0.97289157 0.98192771 0.98393574 0.98594378
 0.98795181 0.99297189 0.99297189 1.        ]
tpr: [0.         0.00100402 0.03413655 0.03614458 0.04016064 0.04216867
 0.04718876 0.04919679 0.05220884 0.05421687 0.05522088 0.05823293
 0.06024096 0.06425703 0.06827309 0.06927711 0.07128514 0.0753012
 0.07831325 0.08032129 0.08232932 0.08935743 0.09136546 0.09236948
 0.09437751 0.10742972 0.11144578 0.11646586 0.1184739  0.11947791
 0.12148594 0.12650602 0.12851406 0.12951807 0.1315261  0.1435743
 0.14558233 0.14658635 0.14959839 0.15361446 0.15662651 0.15763052
 0.15963855 0.16164659 0.16465863 0.16767068 0.16967871 0.17269076
 0.1746988  0.17670683 0.17871486 0.18473896 0.18674699 0.18975904
 0.19176707 0.1937751  0.19678715 0.19779116 0.1997992  0.20180723
 0.20381526 0.20481928 0.20682731 0.20883534 0.21084337 0.21184739
 0.21385542 0.21686747 0.2188755  0.22590361 0.22791165 0.23393574
 0.24297189 0.2439759  0.24598394 0.25200803 0.2560241  0.25702811
 0.26506024 0.26807229 0.27108434 0.27309237 0.27409639 0.27610442
 0.27710843 0.2811245  0.28413655 0.28614458 0.29016064 0.29317269
 0.29919679 0.3002008  0.30421687 0.30823293 0.31024096 0.31124498
 0.31325301 0.31526104 0.32128514 0.32228916 0.32429719 0.32730924
 0.32931727 0.33333333 0.33935743 0.34036145 0.34236948 0.35240964
 0.35240964 0.35542169 0.35943775 0.36044177 0.3624498  0.37349398
 0.37550201 0.37851406 0.38654618 0.38855422 0.39056225 0.39257028
 0.3935743  0.39759036 0.39859438 0.40261044 0.40863454 0.41164659
 0.41365462 0.42570281 0.42771084 0.43172691 0.43574297 0.43674699
 0.43875502 0.43975904 0.44277108 0.45281124 0.45682731 0.46184739
 0.46485944 0.46586345 0.46987952 0.47289157 0.4748996  0.47791165
 0.47991968 0.48192771 0.48393574 0.48594378 0.48995984 0.49096386
 0.49096386 0.49297189 0.49497992 0.50100402 0.50301205 0.5060241
 0.50903614 0.51004016 0.51204819 0.51305221 0.51506024 0.51606426
 0.51807229 0.52008032 0.52208835 0.52409639 0.52710843 0.52811245
 0.53212851 0.53313253 0.53714859 0.53815261 0.54016064 0.54216867
 0.54518072 0.54718876 0.54819277 0.54919679 0.55120482 0.562249
 0.56425703 0.56726908 0.57128514 0.5753012  0.5753012  0.57730924
 0.57931727 0.58232932 0.58333333 0.58534137 0.59136546 0.59337349
 0.59839357 0.60040161 0.60140562 0.60341365 0.60843373 0.61044177
 0.61345382 0.61546185 0.61546185 0.61746988 0.61947791 0.61947791
 0.62349398 0.62449799 0.62951807 0.6315261  0.63353414 0.63554217
 0.63654618 0.63855422 0.64056225 0.64257028 0.64558233 0.64759036
 0.64759036 0.65261044 0.65461847 0.65763052 0.65963855 0.66164659
 0.66365462 0.66465863 0.66666667 0.6686747  0.67269076 0.67369478
 0.67771084 0.67871486 0.68072289 0.68072289 0.68273092 0.70381526
 0.70481928 0.70783133 0.71084337 0.71084337 0.71586345 0.71586345
 0.71686747 0.71686747 0.7188755  0.7188755  0.72991968 0.72991968
 0.73393574 0.73393574 0.73493976 0.73493976 0.73694779 0.73895582
 0.73995984 0.74196787 0.75200803 0.75502008 0.75803213 0.76004016
 0.76204819 0.76204819 0.76606426 0.76807229 0.77208835 0.77309237
 0.77409639 0.77610442 0.77710843 0.77710843 0.77911647 0.78614458
 0.78815261 0.79518072 0.79518072 0.80421687 0.8062249  0.80823293
 0.80823293 0.80923695 0.81024096 0.81726908 0.81927711 0.82028112
 0.82128514 0.8253012  0.8253012  0.82630522 0.82730924 0.82831325
 0.82831325 0.83333333 0.83534137 0.83534137 0.84036145 0.84236948
 0.84638554 0.84638554 0.85240964 0.85441767 0.85441767 0.85542169
 0.85843373 0.85943775 0.86144578 0.86546185 0.86546185 0.87148594
 0.87349398 0.87449799 0.87449799 0.87449799 0.87751004 0.87751004
 0.88052209 0.88052209 0.8815261  0.88353414 0.8875502  0.8875502
 0.88855422 0.89056225 0.89056225 0.89759036 0.89759036 0.89759036
 0.90060241 0.90060241 0.90160643 0.90160643 0.90361446 0.90461847
 0.90461847 0.90562249 0.90562249 0.90662651 0.90662651 0.90863454
 0.90863454 0.90963855 0.9126506  0.9126506  0.91365462 0.91365462
 0.91566265 0.91566265 0.91666667 0.91666667 0.91666667 0.91767068
 0.91767068 0.91967871 0.92168675 0.92168675 0.92269076 0.92269076
 0.9246988  0.9246988  0.92570281 0.92670683 0.92670683 0.92771084
 0.92871486 0.92871486 0.92871486 0.93072289 0.93072289 0.93172691
 0.93172691 0.93273092 0.93273092 0.93373494 0.93373494 0.93373494
 0.93473896 0.93473896 0.93674699 0.93674699 0.937751   0.937751
 0.93875502 0.93875502 0.94176707 0.94176707 0.94277108 0.94277108
 0.94477912 0.94477912 0.94578313 0.94578313 0.94678715 0.94678715
 0.94879518 0.94879518 0.9497992  0.9497992  0.95080321 0.95080321
 0.95582329 0.95582329 0.95682731 0.95783133 0.95783133 0.95983936
 0.95983936 0.96084337 0.96084337 0.96285141 0.96285141 0.96385542
 0.96385542 0.96485944 0.96686747 0.96686747 0.96787149 0.96787149
 0.9688755  0.9688755  0.96987952 0.96987952 0.97088353 0.97088353
 0.97188755 0.97188755 0.97289157 0.97289157 0.97389558 0.97389558
 0.9748996  0.9748996  0.97590361 0.97590361 0.97791165 0.97791165
 0.97791165 0.97791165 0.97891566 0.97891566 0.98092369 0.98092369
 0.98192771 0.98192771 0.98293173 0.98293173 0.98594378 0.98594378
 0.98694779 0.98694779 0.98795181 0.98795181 0.98895582 0.98895582
 0.98995984 0.98995984 0.99096386 0.99096386 0.99096386 0.99096386
 0.99196787 0.99196787 0.99196787 0.99196787 0.99196787 0.99196787
 0.99196787 0.99196787 0.99297189 0.99297189 0.99297189 0.99297189
 0.9939759  0.9939759  0.99497992 0.99497992 0.99598394 0.99598394
 0.99698795 0.99698795 0.99698795 0.99698795 0.99698795 0.99698795
 0.99698795 0.99698795 0.99698795 0.99698795 0.99698795 0.99698795
 0.99698795 0.99698795 0.99698795 0.99698795 0.99698795 0.99698795
 0.99799197 0.99799197 0.99799197 0.99799197 0.99799197 0.99799197
 0.99799197 0.99799197 0.99799197 0.99799197 0.99799197 0.99799197
 0.99799197 0.99799197 0.99799197 0.99799197 0.99799197 0.99799197
 0.99799197 0.99799197 0.99799197 0.99799197 0.99799197 0.99799197
 0.99799197 0.99799197 0.99799197 0.99799197 0.99799197 0.99799197
 0.99799197 0.99799197 0.99799197 0.99799197 0.99799197 0.99799197
 0.99799197 0.99799197 0.99799197 0.99799197 0.99799197 0.99799197
 0.99799197 0.99799197 0.99799197 0.99799197 0.99799197 0.99799197
 0.99799197 0.99799197 0.99799197 0.99799197 0.99799197 0.99799197
 0.99799197 0.99799197 0.99799197 0.99799197 0.99799197 0.99799197
 0.99899598 0.99899598 0.99899598 0.99899598 0.99899598 0.99899598
 0.99899598 0.99899598 0.99899598 0.99899598 0.99899598 0.99899598
 0.99899598 0.99899598 1.         1.        ]
thresholds: [        inf  5.6835938   4.421875    4.4140625   4.3671875   4.3476562
  4.3125      4.3046875   4.2539062   4.25        4.2421875   4.2304688
  4.1953125   4.1445312   4.125       4.1132812   4.1015625   4.0507812
  4.0429688   4.0234375   4.0195312   3.9511719   3.9355469   3.9277344
  3.9160156   3.8085938   3.8046875   3.7910156   3.7558594   3.7519531
  3.7441406   3.7207031   3.7167969   3.7050781   3.6914062   3.6425781
  3.6367188   3.6347656   3.6328125   3.609375    3.5917969   3.5839844
  3.5820312   3.5742188   3.5722656   3.5585938   3.5566406   3.5410156
  3.5332031   3.5214844   3.5175781   3.4824219   3.4726562   3.4589844
  3.4570312   3.4511719   3.4472656   3.4433594   3.4355469   3.4257812
  3.4238281   3.40625     3.4023438   3.3945312   3.3925781   3.390625
  3.3886719   3.3769531   3.375       3.3535156   3.3515625   3.3144531
  3.2988281   3.296875    3.2890625   3.265625    3.2578125   3.2558594
  3.2402344   3.234375    3.2265625   3.2246094   3.21875     3.2167969
  3.2109375   3.2050781   3.1914062   3.1894531   3.1640625   3.1621094
  3.1484375   3.1464844   3.140625    3.1210938   3.1171875   3.1132812
  3.1074219   3.1015625   3.0898438   3.0878906   3.0859375   3.078125
  3.0761719   3.0625      3.0488281   3.046875    3.0429688   2.9980469
  2.9921875   2.96875     2.9628906   2.9609375   2.9589844   2.9140625
  2.9121094   2.9101562   2.8867188   2.8847656   2.8789062   2.8769531
  2.875       2.8691406   2.8632812   2.8574219   2.8378906   2.8222656
  2.8203125   2.7792969   2.7773438   2.7421875   2.7382812   2.7363281
  2.734375    2.7324219   2.7285156   2.6972656   2.6933594   2.671875
  2.6660156   2.6640625   2.6582031   2.6503906   2.6464844   2.6367188
  2.6347656   2.625       2.6210938   2.6152344   2.6015625   2.5976562
  2.5957031   2.5878906   2.5839844   2.5605469   2.5585938   2.5488281
  2.5429688   2.5390625   2.5371094   2.5292969   2.5253906   2.5234375
  2.5175781   2.5078125   2.5019531   2.4980469   2.4941406   2.4921875
  2.4824219   2.4785156   2.4726562   2.4550781   2.4511719   2.4375
  2.4355469   2.4316406   2.4296875   2.4277344   2.421875    2.375
  2.3710938   2.3632812   2.3574219   2.3417969   2.3378906   2.3320312
  2.328125    2.3261719   2.3183594   2.3164062   2.3046875   2.2988281
  2.2832031   2.2792969   2.2773438   2.2753906   2.2675781   2.2597656
  2.2578125   2.25        2.2480469   2.2441406   2.2363281   2.234375
  2.2148438   2.2128906   2.1914062   2.1875      2.1816406   2.1796875
  2.1777344   2.1738281   2.171875    2.1660156   2.1601562   2.1464844
  2.1425781   2.125       2.1230469   2.1171875   2.1152344   2.109375
  2.1074219   2.1054688   2.1035156   2.09375     2.0898438   2.0839844
  2.078125    2.0761719   2.0683594   2.0644531   2.0625      1.9833984
  1.9824219   1.9755859   1.9726562   1.9707031   1.9560547   1.9541016
  1.9482422   1.9443359   1.9384766   1.9365234   1.9072266   1.9033203
  1.8916016   1.8876953   1.8808594   1.8798828   1.875       1.8730469
  1.8710938   1.8701172   1.8291016   1.8261719   1.8144531   1.8095703
  1.7958984   1.7910156   1.7851562   1.7841797   1.7783203   1.7763672
  1.7724609   1.7695312   1.7666016   1.7646484   1.7529297   1.7177734
  1.7167969   1.6972656   1.6953125   1.6689453   1.6582031   1.6542969
  1.6503906   1.6484375   1.6425781   1.6269531   1.6201172   1.6181641
  1.6171875   1.5996094   1.5986328   1.5976562   1.5898438   1.5878906
  1.5859375   1.5664062   1.5654297   1.5625      1.5449219   1.5429688
  1.5322266   1.5302734   1.5107422   1.5078125   1.5068359   1.4980469
  1.484375    1.4824219   1.4785156   1.4541016   1.4511719   1.4355469
  1.4257812   1.4179688   1.4169922   1.4111328   1.4042969   1.3984375
  1.3847656   1.3837891   1.3798828   1.3769531   1.3447266   1.3378906
  1.3359375   1.3271484   1.3251953   1.2929688   1.2919922   1.28125
  1.2773438   1.2753906   1.2734375   1.2607422   1.2558594   1.2519531
  1.2490234   1.2480469   1.2470703   1.2460938   1.21875     1.2119141
  1.2060547   1.2050781   1.1992188   1.1923828   1.1904297   1.1806641
  1.1728516   1.1679688   1.1650391   1.1630859   1.1591797   1.15625
  1.1464844   1.1367188   1.1347656   1.1113281   1.109375    1.1044922
  1.1035156   1.0957031   1.0947266   1.0917969   1.0898438   1.0869141
  1.0849609   1.0839844   1.0771484   1.0605469   1.0556641   1.0546875
  1.0537109   1.0527344   1.0517578   1.0322266   1.0107422   1.0087891
  1.0048828   1.          0.98535156  0.9555664   0.95214844  0.93652344
  0.9301758   0.92626953  0.9169922   0.91015625  0.9091797   0.9086914
  0.9038086   0.89697266  0.89453125  0.86328125  0.8623047   0.828125
  0.8129883   0.80810547  0.8066406   0.7832031   0.7709961   0.7421875
  0.7348633   0.7319336   0.73046875  0.7285156   0.70214844  0.68359375
  0.6699219   0.6660156   0.6586914   0.64941406  0.640625    0.6401367
  0.63916016  0.6376953   0.6347656   0.6074219   0.6040039   0.60253906
  0.59716797  0.5942383   0.59033203  0.5644531   0.5541992   0.5493164
  0.5488281   0.5205078   0.5151367   0.5136719   0.50390625  0.49780273
  0.48999023  0.46923828  0.46289062  0.46240234  0.44091797  0.40722656
  0.3984375   0.39819336  0.39746094  0.38867188  0.38378906  0.37036133
  0.36547852  0.34594727  0.34448242  0.33813477  0.3330078   0.32006836
  0.31933594  0.24035645  0.23291016  0.20043945  0.19177246  0.18725586
  0.18664551  0.17834473  0.17578125  0.06817627  0.06750488 -0.13830566
 -0.13989258 -0.1496582  -0.1505127  -0.19299316 -0.20227051 -0.21777344
 -0.21862793 -0.2310791  -0.23413086 -0.27978516 -0.2854004  -0.30004883
 -0.3005371  -0.3010254  -0.30908203 -0.3408203  -0.34106445 -0.34375
 -0.3474121  -0.4038086  -0.40698242 -0.45385742 -0.4560547  -0.48266602
 -0.48388672 -0.4987793  -0.50341797 -0.5498047  -0.55126953 -0.55566406
 -0.5629883  -0.6147461  -0.61816406 -0.6245117  -0.625      -0.63183594
 -0.6347656  -0.6484375  -0.6557617  -0.6635742  -0.6660156  -0.67285156
 -0.67333984 -0.6870117  -0.6879883  -0.6899414  -0.69189453 -0.7290039
 -0.72998047 -0.7963867  -0.7988281  -0.83496094 -0.8359375  -0.8408203
 -0.8466797  -0.9140625  -0.91503906 -0.92871094 -0.93066406 -0.9394531
 -0.9423828  -0.9428711  -0.9506836  -0.9692383  -0.9741211  -0.9980469
 -0.9995117  -1.0332031  -1.0341797  -1.0683594  -1.078125   -1.1015625
 -1.1064453  -1.1083984  -1.1123047  -1.1494141  -1.1542969  -1.2324219
 -1.2333984  -1.3007812  -1.3046875  -1.3398438  -1.3466797  -1.3886719
 -1.4023438  -1.4130859  -1.4208984  -1.4287109  -1.4326172  -1.4658203
 -1.4726562  -1.6894531  -1.703125   -1.7080078  -1.7167969  -1.7207031
 -1.7373047  -1.9570312  -1.9580078  -2.0859375  -2.0878906  -2.1445312
 -2.1640625  -2.3847656  -2.4023438  -2.5527344  -2.5605469  -2.578125
 -2.6035156  -2.78125    -2.8613281  -4.203125  ]
