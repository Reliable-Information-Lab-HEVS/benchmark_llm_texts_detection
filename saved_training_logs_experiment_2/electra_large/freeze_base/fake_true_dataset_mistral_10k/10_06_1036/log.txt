log_loss_steps: 256
eval_steps: 512
check_degradation: 0
----------------Epoch 1/1----------------
Epoch 1/1, Loss after 192 samples: 0.6969
Epoch 1/1, Loss after 448 samples: 0.6754
Mean accuracy: 0.6193, std: 0.0110, lower bound: 0.5968, upper bound: 0.6405 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 448 samples: 0.6187 with eval loss: 0.6584
Best model with eval loss 0.6583754016507056 and eval accuracy 0.6186612576064908 with 448 samples seen is saved
Epoch 1/1, Loss after 704 samples: 0.6566
Epoch 1/1, Loss after 960 samples: 0.6413
Mean accuracy: 0.6982, std: 0.0103, lower bound: 0.6780, upper bound: 0.7181 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 960 samples: 0.6978 with eval loss: 0.5737
Best model with eval loss 0.5736762054504887 and eval accuracy 0.6977687626774848 with 960 samples seen is saved
Epoch 1/1, Loss after 1216 samples: 0.5737
Epoch 1/1, Loss after 1472 samples: 0.5337
Mean accuracy: 0.7718, std: 0.0093, lower bound: 0.7535, upper bound: 0.7891 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 1472 samples: 0.7723 with eval loss: 0.4854
Best model with eval loss 0.4854423192239577 and eval accuracy 0.7723123732251521 with 1472 samples seen is saved
Epoch 1/1, Loss after 1728 samples: 0.4910
Epoch 1/1, Loss after 1984 samples: 0.4303
Mean accuracy: 0.7727, std: 0.0096, lower bound: 0.7535, upper bound: 0.7906 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 1984 samples: 0.7728 with eval loss: 0.4547
Best model with eval loss 0.45471287062091215 and eval accuracy 0.7728194726166329 with 1984 samples seen is saved
Epoch 1/1, Loss after 2240 samples: 0.4579
Epoch 1/1, Loss after 2496 samples: 0.4214
Mean accuracy: 0.7449, std: 0.0101, lower bound: 0.7236, upper bound: 0.7637 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 2496 samples: 0.7444 with eval loss: 0.4968
Epoch 1/1, Loss after 2752 samples: 0.4491
Epoch 1/1, Loss after 3008 samples: 0.4236
Mean accuracy: 0.8068, std: 0.0089, lower bound: 0.7896, upper bound: 0.8246 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 3008 samples: 0.8068 with eval loss: 0.4033
Best model with eval loss 0.40331752069534793 and eval accuracy 0.8067951318458418 with 3008 samples seen is saved
Epoch 1/1, Loss after 3264 samples: 0.4089
Epoch 1/1, Loss after 3520 samples: 0.4425
Mean accuracy: 0.7133, std: 0.0101, lower bound: 0.6937, upper bound: 0.7333 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 3520 samples: 0.7140 with eval loss: 0.5760
Epoch 1/1, Loss after 3776 samples: 0.3685
Epoch 1/1, Loss after 4032 samples: 0.4345
Mean accuracy: 0.8372, std: 0.0085, lower bound: 0.8205, upper bound: 0.8535 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 4032 samples: 0.8367 with eval loss: 0.3628
Best model with eval loss 0.3627908085623095 and eval accuracy 0.8367139959432048 with 4032 samples seen is saved
Epoch 1/1, Loss after 4288 samples: 0.4563
Epoch 1/1, Loss after 4544 samples: 0.4886
Mean accuracy: 0.8258, std: 0.0086, lower bound: 0.8093, upper bound: 0.8418 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 4544 samples: 0.8261 with eval loss: 0.3852
Epoch 1/1, Loss after 4800 samples: 0.4602
Epoch 1/1, Loss after 5056 samples: 0.3973
Mean accuracy: 0.7711, std: 0.0092, lower bound: 0.7535, upper bound: 0.7890 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 5056 samples: 0.7713 with eval loss: 0.4585
Epoch 1/1, Loss after 5312 samples: 0.3986
Epoch 1/1, Loss after 5568 samples: 0.4285
Mean accuracy: 0.8290, std: 0.0085, lower bound: 0.8114, upper bound: 0.8458 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 5568 samples: 0.8291 with eval loss: 0.3750
Epoch 1/1, Loss after 5824 samples: 0.4586
Epoch 1/1, Loss after 6080 samples: 0.4378
Mean accuracy: 0.8320, std: 0.0085, lower bound: 0.8149, upper bound: 0.8479 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 6080 samples: 0.8322 with eval loss: 0.3685
Epoch 1/1, Loss after 6336 samples: 0.3859
Epoch 1/1, Loss after 6592 samples: 0.4742
Mean accuracy: 0.8535, std: 0.0079, lower bound: 0.8372, upper bound: 0.8687 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 6592 samples: 0.8534 with eval loss: 0.3439
Best model with eval loss 0.34386822196745104 and eval accuracy 0.853448275862069 with 6592 samples seen is saved
Epoch 1/1, Loss after 6848 samples: 0.3876
Epoch 1/1, Loss after 7104 samples: 0.4452
Mean accuracy: 0.8542, std: 0.0079, lower bound: 0.8377, upper bound: 0.8687 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 7104 samples: 0.8540 with eval loss: 0.3375
Best model with eval loss 0.33745050814843947 and eval accuracy 0.8539553752535497 with 7104 samples seen is saved
Epoch 1/1, Loss after 7360 samples: 0.4666
Epoch 1/1, Loss after 7616 samples: 0.3588
Mean accuracy: 0.8507, std: 0.0081, lower bound: 0.8347, upper bound: 0.8666 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 7616 samples: 0.8504 with eval loss: 0.3468
Epoch 1/1, Loss after 7872 samples: 0.4495
Epoch 1/1, Loss after 8128 samples: 0.4048
Mean accuracy: 0.8404, std: 0.0084, lower bound: 0.8235, upper bound: 0.8565 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 8128 samples: 0.8403 with eval loss: 0.3551
Epoch 1/1, Loss after 8384 samples: 0.3926
Epoch 1/1, Loss after 8640 samples: 0.3925
Mean accuracy: 0.7906, std: 0.0091, lower bound: 0.7728, upper bound: 0.8083 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 8640 samples: 0.7911 with eval loss: 0.4131
Epoch 1/1, Loss after 8896 samples: 0.3797
Epoch 1/1, Loss after 9152 samples: 0.3993
Mean accuracy: 0.7905, std: 0.0091, lower bound: 0.7733, upper bound: 0.8088 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 9152 samples: 0.7906 with eval loss: 0.4206
Epoch 1/1, Loss after 9408 samples: 0.3358
Epoch 1/1, Loss after 9664 samples: 0.3904
Mean accuracy: 0.7802, std: 0.0093, lower bound: 0.7627, upper bound: 0.7967 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 9664 samples: 0.7799 with eval loss: 0.4359
Epoch 1/1, Loss after 9920 samples: 0.3684
Epoch 1/1, Loss after 10176 samples: 0.3252
Mean accuracy: 0.7532, std: 0.0096, lower bound: 0.7353, upper bound: 0.7728 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 10176 samples: 0.7530 with eval loss: 0.4812
Epoch 1/1, Loss after 10432 samples: 0.4005
Epoch 1/1, Loss after 10688 samples: 0.4153
Mean accuracy: 0.8270, std: 0.0086, lower bound: 0.8098, upper bound: 0.8433 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 10688 samples: 0.8271 with eval loss: 0.3762
Epoch 1/1, Loss after 10944 samples: 0.3473
Epoch 1/1, Loss after 11200 samples: 0.3761
Mean accuracy: 0.8000, std: 0.0088, lower bound: 0.7835, upper bound: 0.8169 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 11200 samples: 0.7997 with eval loss: 0.4107
Epoch 1/1, Loss after 11456 samples: 0.3660
Epoch 1/1, Loss after 11712 samples: 0.3841
Mean accuracy: 0.7805, std: 0.0095, lower bound: 0.7622, upper bound: 0.7987 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 11712 samples: 0.7809 with eval loss: 0.4389
Epoch 1/1, Loss after 11968 samples: 0.3656
Epoch 1/1, Loss after 12224 samples: 0.3836
Mean accuracy: 0.7953, std: 0.0091, lower bound: 0.7774, upper bound: 0.8129 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 12224 samples: 0.7956 with eval loss: 0.4162
Epoch 1/1, Loss after 12480 samples: 0.3417
Epoch 1/1, Loss after 12736 samples: 0.4013
Mean accuracy: 0.8316, std: 0.0085, lower bound: 0.8154, upper bound: 0.8479 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 12736 samples: 0.8316 with eval loss: 0.3575
Epoch 1/1, Loss after 12992 samples: 0.3953
Epoch 1/1, Loss after 13248 samples: 0.4052
Mean accuracy: 0.7356, std: 0.0101, lower bound: 0.7165, upper bound: 0.7556 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 13248 samples: 0.7358 with eval loss: 0.5341
Epoch 1/1, Loss after 13504 samples: 0.3441
Epoch 1/1, Loss after 13760 samples: 0.4047
Mean accuracy: 0.8128, std: 0.0091, lower bound: 0.7951, upper bound: 0.8306 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 13760 samples: 0.8124 with eval loss: 0.3875
Epoch 1/1, Loss after 14016 samples: 0.3716
Epoch 1/1, Loss after 14272 samples: 0.3695
Mean accuracy: 0.7881, std: 0.0092, lower bound: 0.7693, upper bound: 0.8058 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 14272 samples: 0.7880 with eval loss: 0.4292
Epoch 1/1, Loss after 14528 samples: 0.3655
Epoch 1/1, Loss after 14784 samples: 0.3659
Mean accuracy: 0.7883, std: 0.0092, lower bound: 0.7698, upper bound: 0.8063 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 14784 samples: 0.7880 with eval loss: 0.4301
Epoch 1/1, Loss after 15040 samples: 0.4195
Epoch 1/1, Loss after 15296 samples: 0.3888
Mean accuracy: 0.7689, std: 0.0095, lower bound: 0.7500, upper bound: 0.7865 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 15296 samples: 0.7688 with eval loss: 0.4590
Epoch 1/1, Loss after 15552 samples: 0.3591
Training signal is False, stopping training
Training finished
Best model: {'eval_acc': 0.8539553752535497, 'nb_samples': 7104, 'eval_loss': 0.33745050814843947}
Training loss logs: [{'samples': 192, 'loss': 0.6969470977783203}, {'samples': 448, 'loss': 0.6754217147827148}, {'samples': 704, 'loss': 0.6566228866577148}, {'samples': 960, 'loss': 0.6412515640258789}, {'samples': 1216, 'loss': 0.5737102031707764}, {'samples': 1472, 'loss': 0.5337052345275879}, {'samples': 1728, 'loss': 0.49095118045806885}, {'samples': 1984, 'loss': 0.43034422397613525}, {'samples': 2240, 'loss': 0.4578876942396164}, {'samples': 2496, 'loss': 0.42140601575374603}, {'samples': 2752, 'loss': 0.44911210238933563}, {'samples': 3008, 'loss': 0.42364174127578735}, {'samples': 3264, 'loss': 0.4089133068919182}, {'samples': 3520, 'loss': 0.4424938037991524}, {'samples': 3776, 'loss': 0.3685028702020645}, {'samples': 4032, 'loss': 0.4345203712582588}, {'samples': 4288, 'loss': 0.45625007152557373}, {'samples': 4544, 'loss': 0.48857282102108}, {'samples': 4800, 'loss': 0.4601762518286705}, {'samples': 5056, 'loss': 0.3973164036870003}, {'samples': 5312, 'loss': 0.39857272058725357}, {'samples': 5568, 'loss': 0.4285437688231468}, {'samples': 5824, 'loss': 0.4585944414138794}, {'samples': 6080, 'loss': 0.4378392919898033}, {'samples': 6336, 'loss': 0.3859093561768532}, {'samples': 6592, 'loss': 0.4741591438651085}, {'samples': 6848, 'loss': 0.387615866959095}, {'samples': 7104, 'loss': 0.4451768398284912}, {'samples': 7360, 'loss': 0.4665694460272789}, {'samples': 7616, 'loss': 0.358775295317173}, {'samples': 7872, 'loss': 0.44950416684150696}, {'samples': 8128, 'loss': 0.40481558442115784}, {'samples': 8384, 'loss': 0.3926073759794235}, {'samples': 8640, 'loss': 0.3924596384167671}, {'samples': 8896, 'loss': 0.3797379806637764}, {'samples': 9152, 'loss': 0.3992609307169914}, {'samples': 9408, 'loss': 0.3358265981078148}, {'samples': 9664, 'loss': 0.3904265910387039}, {'samples': 9920, 'loss': 0.36844920367002487}, {'samples': 10176, 'loss': 0.32515085488557816}, {'samples': 10432, 'loss': 0.4004852771759033}, {'samples': 10688, 'loss': 0.4153474271297455}, {'samples': 10944, 'loss': 0.34725797921419144}, {'samples': 11200, 'loss': 0.37612804770469666}, {'samples': 11456, 'loss': 0.36598241329193115}, {'samples': 11712, 'loss': 0.38414810597896576}, {'samples': 11968, 'loss': 0.36562301218509674}, {'samples': 12224, 'loss': 0.3836275637149811}, {'samples': 12480, 'loss': 0.3416823074221611}, {'samples': 12736, 'loss': 0.4013091027736664}, {'samples': 12992, 'loss': 0.39534566923975945}, {'samples': 13248, 'loss': 0.405248261988163}, {'samples': 13504, 'loss': 0.3440501391887665}, {'samples': 13760, 'loss': 0.4047490656375885}, {'samples': 14016, 'loss': 0.37164467573165894}, {'samples': 14272, 'loss': 0.3694886267185211}, {'samples': 14528, 'loss': 0.36550093442201614}, {'samples': 14784, 'loss': 0.3659333661198616}, {'samples': 15040, 'loss': 0.4195118844509125}, {'samples': 15296, 'loss': 0.38878749310970306}, {'samples': 15552, 'loss': 0.3591237887740135}]
Evaluation accuracy logs: [{'samples': 448, 'accuracy': 0.6192814401622718, 'std': 0.010969619127287543, 'lower_bound': 0.5968433062880324, 'upper_bound': 0.6404665314401623}, {'samples': 960, 'accuracy': 0.6981886409736309, 'std': 0.010285126020373148, 'lower_bound': 0.6779918864097363, 'upper_bound': 0.718052738336714}, {'samples': 1472, 'accuracy': 0.7717733265720081, 'std': 0.009277882359478705, 'lower_bound': 0.7535496957403651, 'upper_bound': 0.7890593306288033}, {'samples': 1984, 'accuracy': 0.7727114604462474, 'std': 0.009588063206512136, 'lower_bound': 0.7535370182555781, 'upper_bound': 0.7905679513184585}, {'samples': 2496, 'accuracy': 0.7449183569979716, 'std': 0.010116048967171906, 'lower_bound': 0.723630831643002, 'upper_bound': 0.7637043610547668}, {'samples': 3008, 'accuracy': 0.8067723123732252, 'std': 0.008930119270034552, 'lower_bound': 0.789553752535497, 'upper_bound': 0.8245562880324544}, {'samples': 3520, 'accuracy': 0.7133199797160243, 'std': 0.010053187704624376, 'lower_bound': 0.6936992900608518, 'upper_bound': 0.7332657200811359}, {'samples': 4032, 'accuracy': 0.837171906693712, 'std': 0.008516546565023932, 'lower_bound': 0.8204868154158215, 'upper_bound': 0.853460953346856}, {'samples': 4544, 'accuracy': 0.8257707910750507, 'std': 0.008580978814861168, 'lower_bound': 0.8093306288032455, 'upper_bound': 0.8417976673427993}, {'samples': 5056, 'accuracy': 0.771118154158215, 'std': 0.009172403013590479, 'lower_bound': 0.7535496957403651, 'upper_bound': 0.7890466531440162}, {'samples': 5568, 'accuracy': 0.8289665314401623, 'std': 0.008479495132098346, 'lower_bound': 0.8113590263691683, 'upper_bound': 0.845841784989858}, {'samples': 6080, 'accuracy': 0.8319787018255578, 'std': 0.008479328943586871, 'lower_bound': 0.8149087221095335, 'upper_bound': 0.847882860040568}, {'samples': 6592, 'accuracy': 0.8535207910750507, 'std': 0.007871328526775898, 'lower_bound': 0.8372210953346856, 'upper_bound': 0.8686612576064908}, {'samples': 7104, 'accuracy': 0.8542210953346855, 'std': 0.007936476865633582, 'lower_bound': 0.8377281947261663, 'upper_bound': 0.8686612576064908}, {'samples': 7616, 'accuracy': 0.8507474645030426, 'std': 0.00807046290221085, 'lower_bound': 0.834685598377282, 'upper_bound': 0.866645537525355}, {'samples': 8128, 'accuracy': 0.8404193711967546, 'std': 0.00841147439044503, 'lower_bound': 0.8235294117647058, 'upper_bound': 0.8565035496957404}, {'samples': 8640, 'accuracy': 0.7905943204868153, 'std': 0.009139238030498976, 'lower_bound': 0.7728194726166329, 'upper_bound': 0.808316430020284}, {'samples': 9152, 'accuracy': 0.7905314401622718, 'std': 0.00910523492535409, 'lower_bound': 0.7733265720081136, 'upper_bound': 0.8088362068965518}, {'samples': 9664, 'accuracy': 0.7801947261663286, 'std': 0.009304079434318886, 'lower_bound': 0.7626648073022312, 'upper_bound': 0.7966658215010142}, {'samples': 10176, 'accuracy': 0.7531952332657201, 'std': 0.009569523108776514, 'lower_bound': 0.7352814401622718, 'upper_bound': 0.7728194726166329}, {'samples': 10688, 'accuracy': 0.8269543610547666, 'std': 0.008625155209000914, 'lower_bound': 0.8098250507099392, 'upper_bound': 0.8433062880324543}, {'samples': 11200, 'accuracy': 0.8000289046653145, 'std': 0.008834209208412824, 'lower_bound': 0.7834685598377282, 'upper_bound': 0.8169371196754563}, {'samples': 11712, 'accuracy': 0.7805116632860041, 'std': 0.009502485677200375, 'lower_bound': 0.7621703853955375, 'upper_bound': 0.7986815415821501}, {'samples': 12224, 'accuracy': 0.7953138945233266, 'std': 0.009080621298839657, 'lower_bound': 0.7773833671399595, 'upper_bound': 0.8128930020283975}, {'samples': 12736, 'accuracy': 0.8316348884381339, 'std': 0.008514888035724652, 'lower_bound': 0.8154031440162272, 'upper_bound': 0.847870182555781}, {'samples': 13248, 'accuracy': 0.7355725152129818, 'std': 0.010105583739334609, 'lower_bound': 0.7165314401622718, 'upper_bound': 0.755578093306288}, {'samples': 13760, 'accuracy': 0.8128387423935091, 'std': 0.009146859670009283, 'lower_bound': 0.795131845841785, 'upper_bound': 0.8306288032454361}, {'samples': 14272, 'accuracy': 0.7881125760649088, 'std': 0.009196426576359685, 'lower_bound': 0.7692570993914807, 'upper_bound': 0.8057809330628803}, {'samples': 14784, 'accuracy': 0.7883351926977687, 'std': 0.009229718671142334, 'lower_bound': 0.7697768762677485, 'upper_bound': 0.8062880324543611}, {'samples': 15296, 'accuracy': 0.7689117647058823, 'std': 0.009510291353033887, 'lower_bound': 0.7499873225152129, 'upper_bound': 0.7865238336713997}]
Evaluating the best model on the test set of dataset ./fake_true_datasets/fake_true_dataset_mistral_10k...
Test metrics:
accuracy: 0.7901850912778905
precision: 0.7079661353115089
recall: 0.9868060186254717
f1_score: 0.8243864036620747
fp_rate: 0.40585477386686547
tp_rate: 0.9868060186254717
std_accuracy: 0.00899947812793705
std_precision: 0.012173403883296404
std_recall: 0.0036451593413609085
std_f1_score: 0.008410400104050843
std_fp_rate: 0.01520089616298983
std_tp_rate: 0.0036451593413609085
TP: 971.566
TN: 586.679
FP: 400.762
FN: 12.993
roc_auc: 0.9464665561265423
fpr: [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.0010142  0.0010142  0.0010142  0.0010142  0.0010142  0.0010142
 0.0010142  0.0020284  0.0020284  0.0020284  0.0020284  0.0020284
 0.0020284  0.0020284  0.0020284  0.0020284  0.0020284  0.0020284
 0.0020284  0.0020284  0.0020284  0.0030426  0.0030426  0.0030426
 0.0030426  0.0040568  0.0040568  0.0040568  0.0040568  0.0040568
 0.0040568  0.0040568  0.0040568  0.0040568  0.0040568  0.0040568
 0.00507099 0.00507099 0.00608519 0.00608519 0.00608519 0.00709939
 0.00709939 0.00709939 0.00709939 0.00709939 0.00709939 0.00709939
 0.00709939 0.00709939 0.00709939 0.00709939 0.00709939 0.00709939
 0.00811359 0.00811359 0.00811359 0.00811359 0.00811359 0.00811359
 0.00811359 0.00811359 0.00811359 0.00811359 0.00811359 0.00811359
 0.00811359 0.00811359 0.00811359 0.00912779 0.00912779 0.01014199
 0.01217039 0.01217039 0.01318458 0.01318458 0.01318458 0.01318458
 0.01318458 0.01318458 0.01318458 0.01318458 0.01521298 0.01521298
 0.01521298 0.01521298 0.01622718 0.01622718 0.01724138 0.01724138
 0.01724138 0.01825558 0.01825558 0.01926978 0.01926978 0.01926978
 0.01926978 0.02028398 0.02028398 0.02231237 0.02231237 0.02332657
 0.02332657 0.02332657 0.02332657 0.02332657 0.02332657 0.02434077
 0.02434077 0.02434077 0.02434077 0.02434077 0.02434077 0.02434077
 0.02434077 0.02636917 0.02636917 0.02738337 0.02738337 0.02738337
 0.02738337 0.02839757 0.03042596 0.03144016 0.03144016 0.03245436
 0.03245436 0.03245436 0.03245436 0.03245436 0.03245436 0.03245436
 0.03245436 0.03245436 0.03245436 0.03245436 0.03245436 0.03245436
 0.03245436 0.03245436 0.03346856 0.03346856 0.03448276 0.03448276
 0.03549696 0.03549696 0.03549696 0.03549696 0.03549696 0.03549696
 0.03651116 0.03651116 0.03752535 0.03752535 0.03752535 0.03752535
 0.03752535 0.03752535 0.03752535 0.03752535 0.03853955 0.03853955
 0.03853955 0.03955375 0.03955375 0.04056795 0.04056795 0.04056795
 0.04158215 0.04158215 0.04259635 0.04259635 0.04259635 0.04462475
 0.04563895 0.04563895 0.04665314 0.04665314 0.04766734 0.04868154
 0.04868154 0.04868154 0.04868154 0.04969574 0.04969574 0.05172414
 0.05172414 0.05273834 0.05273834 0.05375254 0.05375254 0.05476673
 0.05578093 0.05578093 0.05679513 0.05679513 0.05780933 0.05780933
 0.05882353 0.05882353 0.05983773 0.05983773 0.06186613 0.06186613
 0.06592292 0.06693712 0.06693712 0.06795132 0.06795132 0.06997972
 0.06997972 0.07099391 0.07099391 0.07200811 0.07302231 0.07505071
 0.07505071 0.07505071 0.07606491 0.07606491 0.0811359  0.0821501
 0.0821501  0.0831643  0.0862069  0.0872211  0.08823529 0.08823529
 0.08924949 0.08924949 0.08924949 0.09026369 0.09026369 0.09127789
 0.09127789 0.09229209 0.09229209 0.09229209 0.09330629 0.09330629
 0.09432049 0.09432049 0.09533469 0.09533469 0.09634888 0.09634888
 0.09736308 0.09736308 0.09837728 0.09837728 0.09837728 0.09939148
 0.09939148 0.10141988 0.10243408 0.10446247 0.10446247 0.10547667
 0.10547667 0.10547667 0.10649087 0.10649087 0.10649087 0.10750507
 0.10750507 0.10953347 0.10953347 0.11156187 0.11156187 0.11359026
 0.11359026 0.11460446 0.11460446 0.11967546 0.11967546 0.12170385
 0.12373225 0.12373225 0.12474645 0.12474645 0.12576065 0.12576065
 0.12778905 0.12778905 0.12880325 0.12880325 0.12981744 0.12981744
 0.13083164 0.13083164 0.13691684 0.13691684 0.13793103 0.13793103
 0.13894523 0.13894523 0.14401623 0.14401623 0.15010142 0.15010142
 0.15111562 0.15111562 0.15415822 0.15415822 0.15618661 0.15618661
 0.15720081 0.15720081 0.15922921 0.15922921 0.16024341 0.16227181
 0.16227181 0.163286   0.163286   0.1653144  0.1673428  0.1673428
 0.168357   0.168357   0.1703854  0.1703854  0.17139959 0.17139959
 0.17647059 0.17647059 0.17849899 0.17849899 0.17951318 0.17951318
 0.18154158 0.18458418 0.18965517 0.18965517 0.19168357 0.19168357
 0.19371197 0.19472617 0.19574037 0.19574037 0.19675456 0.19675456
 0.19776876 0.19878296 0.20182556 0.20182556 0.20283976 0.20283976
 0.20385396 0.20486815 0.20791075 0.20791075 0.21095335 0.21095335
 0.21703854 0.21703854 0.22008114 0.22008114 0.22210953 0.22210953
 0.22515213 0.22515213 0.22718053 0.22718053 0.22819473 0.22819473
 0.22920892 0.22920892 0.23022312 0.23022312 0.23123732 0.23123732
 0.23529412 0.23529412 0.23630832 0.23630832 0.23732252 0.23732252
 0.24340771 0.24340771 0.24543611 0.24543611 0.2505071  0.2505071
 0.2515213  0.2515213  0.25659229 0.25659229 0.26774848 0.26774848
 0.26876268 0.26876268 0.27281947 0.27281947 0.27890467 0.27890467
 0.28600406 0.28600406 0.29310345 0.29310345 0.29817444 0.29817444
 0.30324544 0.30324544 0.30730223 0.30730223 0.31237323 0.31237323
 0.31338742 0.31338742 0.31440162 0.31440162 0.32150101 0.32150101
 0.32352941 0.32352941 0.3306288  0.331643   0.33975659 0.33975659
 0.34381339 0.34381339 0.34482759 0.34482759 0.34888438 0.34888438
 0.35091278 0.35091278 0.36004057 0.36004057 0.36511156 0.36511156
 0.37322515 0.37322515 0.37626775 0.37626775 0.38945233 0.38945233
 0.39452333 0.39452333 0.40365112 0.40365112 0.40973631 0.40973631
 0.4178499  0.4198783  0.43509128 0.43509128 0.46044625 0.46348884
 0.46653144 0.46855984 0.4959432  0.4959432  0.5010142  0.5010142
 0.5020284  0.5040568  0.51115619 0.51318458 0.51622718 0.51622718
 0.52129817 0.52332657 0.52434077 0.52636917 0.53346856 0.53448276
 0.54766734 0.54766734 0.54969574 0.54969574 0.55070994 0.55375254
 0.56288032 0.56288032 0.56795132 0.56795132 0.5811359  0.5811359
 0.59229209 0.59432049 0.59634888 0.59634888 0.60040568 0.60141988
 0.60344828 0.63083164 0.63286004 0.64097363 0.64097363 0.64705882
 0.64908722 0.65720081 0.65922921 0.6703854  0.67241379 0.68154158
 0.68356998 0.68458418 0.68762677 0.70993915 0.71196755 0.71906694
 0.72109533 0.73225152 0.73427992 0.73529412 0.73833671 0.74036511
 0.74239351 0.7515213  0.7535497  0.76064909 0.76267748 0.76673428
 0.76876268 0.77383367 0.77789047 0.78296146 0.78498986 0.79107505
 0.79310345 0.79513185 0.79918864 0.80020284 0.80223124 0.80831643
 0.81034483 0.81845842 0.82251521 0.82758621 0.8296146  0.8326572
 0.85091278 0.85294118 0.85598377 0.85801217 0.87119675 0.87525355
 0.87728195 0.88032454 0.89959432 0.89959432 0.9158215  0.9178499
 0.9198783  0.92190669 0.92292089 0.92494929 0.94117647 0.94320487
 0.94929006 0.95131846 0.96855984 0.97058824 0.97565923 0.97768763
 0.97870183 0.98073022 0.98985801 0.99188641 1.        ]
tpr: [0.         0.0010142  0.01318458 0.01521298 0.01825558 0.02028398
 0.02941176 0.03144016 0.05476673 0.05679513 0.06896552 0.07099391
 0.07200811 0.07606491 0.0811359  0.0831643  0.0841785  0.0862069
 0.09330629 0.09736308 0.10953347 0.11156187 0.11460446 0.11663286
 0.11764706 0.11967546 0.12880325 0.13184584 0.13286004 0.13488844
 0.13793103 0.14198783 0.14401623 0.14705882 0.14908722 0.15111562
 0.15212982 0.15415822 0.15720081 0.15922921 0.17139959 0.17342799
 0.17545639 0.17748479 0.18762677 0.19066937 0.19269777 0.19574037
 0.19776876 0.20283976 0.20486815 0.20892495 0.21298174 0.21602434
 0.21805274 0.22109533 0.22718053 0.22819473 0.23022312 0.23123732
 0.23326572 0.23935091 0.24137931 0.24239351 0.24442191 0.2494929
 0.2515213  0.25557809 0.25760649 0.25963489 0.26166329 0.26673428
 0.26876268 0.27079108 0.27281947 0.27586207 0.27789047 0.27991886
 0.28093306 0.28296146 0.28904665 0.29310345 0.29513185 0.29918864
 0.30223124 0.30223124 0.30324544 0.30628803 0.30831643 0.31034483
 0.31237323 0.32758621 0.3296146  0.331643   0.3336714  0.3356998
 0.33772819 0.34077079 0.35598377 0.35598377 0.36206897 0.36409736
 0.37322515 0.37322515 0.37728195 0.37931034 0.38032454 0.38235294
 0.38336714 0.38640974 0.38843813 0.39148073 0.39350913 0.40060852
 0.40060852 0.40263692 0.40263692 0.40669371 0.40872211 0.40973631
 0.41176471 0.42697769 0.43306288 0.43610548 0.43813387 0.44016227
 0.44320487 0.44421907 0.44624746 0.44726166 0.44929006 0.45638945
 0.45740365 0.45943205 0.46146045 0.46247465 0.46450304 0.47058824
 0.47464503 0.47565923 0.47768763 0.48275862 0.48580122 0.48985801
 0.49188641 0.4959432  0.4979716  0.4989858  0.50709939 0.50709939
 0.50912779 0.51115619 0.51115619 0.51926978 0.52129817 0.52231237
 0.52434077 0.52738337 0.52941176 0.53448276 0.53448276 0.53853955
 0.54056795 0.54361055 0.54361055 0.54766734 0.54868154 0.55375254
 0.55578093 0.55578093 0.55679513 0.55780933 0.55983773 0.56186613
 0.56693712 0.56795132 0.56896552 0.56896552 0.57302231 0.57302231
 0.57505071 0.57707911 0.57809331 0.5801217  0.5821501  0.5841785
 0.58823529 0.59229209 0.59330629 0.59533469 0.59634888 0.59837728
 0.60040568 0.60243408 0.60547667 0.60649087 0.60953347 0.61156187
 0.61866126 0.61866126 0.61866126 0.61866126 0.61967546 0.61967546
 0.62068966 0.62271805 0.62373225 0.62778905 0.63286004 0.63488844
 0.63590264 0.64097363 0.64401623 0.64705882 0.64908722 0.65212982
 0.65415822 0.65821501 0.65821501 0.66024341 0.66125761 0.67241379
 0.67241379 0.67545639 0.67849899 0.68052738 0.68154158 0.68356998
 0.68356998 0.68661258 0.68661258 0.68864097 0.69066937 0.69371197
 0.69574037 0.69776876 0.69979716 0.70283976 0.70283976 0.70486815
 0.70588235 0.70588235 0.70791075 0.70892495 0.71095335 0.71298174
 0.71298174 0.71906694 0.71906694 0.72109533 0.72210953 0.72210953
 0.72312373 0.72616633 0.72616633 0.73326572 0.73326572 0.73427992
 0.74036511 0.74239351 0.74442191 0.74442191 0.74543611 0.74543611
 0.7484787  0.7484787  0.7494929  0.7494929  0.7535497  0.7535497
 0.75456389 0.75760649 0.75760649 0.75862069 0.75862069 0.76064909
 0.76064909 0.76369168 0.76369168 0.76572008 0.76572008 0.77383367
 0.77383367 0.77484787 0.77687627 0.77687627 0.77890467 0.77890467
 0.78194726 0.78194726 0.78498986 0.78498986 0.78701826 0.78701826
 0.78904665 0.79208925 0.79208925 0.79513185 0.79513185 0.79614604
 0.79716024 0.79817444 0.79817444 0.79918864 0.79918864 0.80324544
 0.80324544 0.80527383 0.80730223 0.80831643 0.81237323 0.81338742
 0.81845842 0.81845842 0.81947262 0.82150101 0.82150101 0.82251521
 0.82251521 0.82657201 0.82657201 0.82758621 0.82860041 0.8296146
 0.8296146  0.8336714  0.8336714  0.836714   0.83772819 0.83874239
 0.84077079 0.84077079 0.84178499 0.84178499 0.84381339 0.84381339
 0.84482759 0.84685598 0.84685598 0.84787018 0.85091278 0.85091278
 0.85192698 0.85192698 0.85294118 0.85294118 0.85395538 0.85395538
 0.85699797 0.85699797 0.85801217 0.85801217 0.86004057 0.86004057
 0.86004057 0.86105477 0.86105477 0.86308316 0.86308316 0.86409736
 0.86409736 0.86511156 0.86612576 0.86815416 0.86815416 0.87018256
 0.87018256 0.87221095 0.87221095 0.87322515 0.87423935 0.87525355
 0.87525355 0.87626775 0.87626775 0.88235294 0.88235294 0.88336714
 0.88336714 0.88640974 0.88640974 0.88742394 0.88742394 0.88843813
 0.88843813 0.88945233 0.88945233 0.89148073 0.89148073 0.89350913
 0.89452333 0.89553753 0.89655172 0.89655172 0.89655172 0.89858012
 0.89858012 0.89959432 0.89959432 0.90060852 0.90060852 0.90263692
 0.90263692 0.90567951 0.90567951 0.90669371 0.90669371 0.90770791
 0.90770791 0.90770791 0.90770791 0.90872211 0.90872211 0.91176471
 0.91176471 0.9127789  0.9127789  0.9137931  0.9137931  0.9158215
 0.9158215  0.9168357  0.9168357  0.9178499  0.9178499  0.9198783
 0.9198783  0.92089249 0.92089249 0.92190669 0.92190669 0.92292089
 0.92292089 0.92494929 0.92494929 0.92697769 0.92697769 0.92799189
 0.92799189 0.92900609 0.92900609 0.93002028 0.93002028 0.93103448
 0.93103448 0.93306288 0.93306288 0.93407708 0.93407708 0.93914807
 0.93914807 0.94117647 0.94117647 0.94219067 0.94219067 0.94320487
 0.94320487 0.94624746 0.94624746 0.94827586 0.94827586 0.94929006
 0.94929006 0.95030426 0.95030426 0.95131846 0.95131846 0.95233266
 0.95233266 0.95334686 0.95334686 0.95436105 0.95436105 0.95841785
 0.95841785 0.96044625 0.96044625 0.96146045 0.96146045 0.96247465
 0.96247465 0.96348884 0.96348884 0.96450304 0.96450304 0.96551724
 0.96551724 0.96653144 0.96653144 0.96754564 0.96754564 0.96855984
 0.96855984 0.96957404 0.96957404 0.97058824 0.97058824 0.97160243
 0.97160243 0.97261663 0.97261663 0.97464503 0.97464503 0.97565923
 0.97565923 0.97768763 0.97768763 0.97971602 0.97971602 0.98073022
 0.98073022 0.98174442 0.98174442 0.98275862 0.98275862 0.98377282
 0.98377282 0.98478702 0.98478702 0.98580122 0.98580122 0.98681542
 0.98681542 0.98681542 0.98681542 0.98782961 0.98782961 0.98782961
 0.98782961 0.98782961 0.98782961 0.98884381 0.98884381 0.98985801
 0.98985801 0.98985801 0.98985801 0.98985801 0.98985801 0.99087221
 0.99087221 0.99087221 0.99087221 0.99087221 0.99087221 0.99188641
 0.99188641 0.99290061 0.99290061 0.99391481 0.99391481 0.99391481
 0.99391481 0.99492901 0.99492901 0.9959432  0.9959432  0.9969574
 0.9969574  0.9969574  0.9969574  0.9979716  0.9979716  0.9979716
 0.9979716  0.9979716  0.9979716  0.9979716  0.9989858  0.9989858
 0.9989858  0.9989858  0.9989858  0.9989858  0.9989858  0.9989858
 0.9989858  0.9989858  0.9989858  0.9989858  0.9989858  0.9989858
 0.9989858  0.9989858  0.9989858  0.9989858  0.9989858  0.9989858
 0.9989858  0.9989858  0.9989858  0.9989858  0.9989858  0.9989858
 0.9989858  0.9989858  0.9989858  0.9989858  0.9989858  0.9989858
 0.9989858  0.9989858  0.9989858  0.9989858  0.9989858  0.9989858
 0.9989858  0.9989858  0.9989858  0.9989858  0.9989858  0.9989858
 0.9989858  0.9989858  0.9989858  0.9989858  0.9989858  0.9989858
 0.9989858  0.9989858  0.9989858  1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.        ]
thresholds: [            inf  5.46093750e+00  3.67773438e+00  3.67578125e+00
  3.62109375e+00  3.58593750e+00  3.39648438e+00  3.39257812e+00
  3.10546875e+00  3.10351562e+00  2.93164062e+00  2.92773438e+00
  2.92578125e+00  2.91992188e+00  2.86132812e+00  2.85546875e+00
  2.85351562e+00  2.83398438e+00  2.78710938e+00  2.76757812e+00
  2.66601562e+00  2.66210938e+00  2.63281250e+00  2.62890625e+00
  2.61718750e+00  2.61132812e+00  2.53515625e+00  2.52539062e+00
  2.52148438e+00  2.51953125e+00  2.50781250e+00  2.50195312e+00
  2.48632812e+00  2.48437500e+00  2.47265625e+00  2.47070312e+00
  2.46484375e+00  2.46289062e+00  2.44921875e+00  2.44726562e+00
  2.38476562e+00  2.38085938e+00  2.36914062e+00  2.36523438e+00
  2.33007812e+00  2.32812500e+00  2.32031250e+00  2.31250000e+00
  2.31054688e+00  2.28125000e+00  2.27929688e+00  2.26953125e+00
  2.25976562e+00  2.22460938e+00  2.22265625e+00  2.21484375e+00
  2.20898438e+00  2.20703125e+00  2.20507812e+00  2.20312500e+00
  2.19140625e+00  2.16601562e+00  2.16210938e+00  2.15820312e+00
  2.15625000e+00  2.14257812e+00  2.14062500e+00  2.12304688e+00
  2.12109375e+00  2.10546875e+00  2.10351562e+00  2.08593750e+00
  2.08398438e+00  2.07617188e+00  2.06835938e+00  2.06054688e+00
  2.05859375e+00  2.05273438e+00  2.05078125e+00  2.04687500e+00
  2.01757812e+00  2.01171875e+00  2.00000000e+00  1.99609375e+00
  1.99023438e+00  1.98144531e+00  1.97363281e+00  1.97265625e+00
  1.97167969e+00  1.96484375e+00  1.96191406e+00  1.88183594e+00
  1.87304688e+00  1.87011719e+00  1.86816406e+00  1.86328125e+00
  1.86132812e+00  1.86035156e+00  1.81054688e+00  1.80761719e+00
  1.79785156e+00  1.79687500e+00  1.76953125e+00  1.76757812e+00
  1.75390625e+00  1.75000000e+00  1.74707031e+00  1.74218750e+00
  1.73828125e+00  1.73632812e+00  1.73242188e+00  1.72265625e+00
  1.71875000e+00  1.70019531e+00  1.69531250e+00  1.69238281e+00
  1.69042969e+00  1.68750000e+00  1.68457031e+00  1.68359375e+00
  1.68261719e+00  1.65136719e+00  1.63867188e+00  1.63476562e+00
  1.63183594e+00  1.62597656e+00  1.62500000e+00  1.62304688e+00
  1.62011719e+00  1.61914062e+00  1.61816406e+00  1.59863281e+00
  1.59667969e+00  1.59277344e+00  1.58984375e+00  1.58886719e+00
  1.58300781e+00  1.57031250e+00  1.56347656e+00  1.56250000e+00
  1.56054688e+00  1.54785156e+00  1.54687500e+00  1.53808594e+00
  1.53710938e+00  1.51953125e+00  1.51757812e+00  1.51660156e+00
  1.49511719e+00  1.48925781e+00  1.48437500e+00  1.48242188e+00
  1.47656250e+00  1.44824219e+00  1.44531250e+00  1.44433594e+00
  1.43945312e+00  1.42871094e+00  1.42773438e+00  1.41601562e+00
  1.41210938e+00  1.39648438e+00  1.39257812e+00  1.37988281e+00
  1.37890625e+00  1.37500000e+00  1.37402344e+00  1.36425781e+00
  1.36328125e+00  1.36230469e+00  1.35937500e+00  1.35742188e+00
  1.34765625e+00  1.34472656e+00  1.32812500e+00  1.32714844e+00
  1.32617188e+00  1.32128906e+00  1.31445312e+00  1.31152344e+00
  1.30761719e+00  1.30468750e+00  1.30371094e+00  1.30175781e+00
  1.29003906e+00  1.28906250e+00  1.27343750e+00  1.27050781e+00
  1.26464844e+00  1.26367188e+00  1.25976562e+00  1.25878906e+00
  1.25390625e+00  1.25292969e+00  1.25195312e+00  1.24804688e+00
  1.24511719e+00  1.24218750e+00  1.22460938e+00  1.22265625e+00
  1.22167969e+00  1.21582031e+00  1.21484375e+00  1.21386719e+00
  1.21191406e+00  1.20898438e+00  1.20703125e+00  1.20507812e+00
  1.19140625e+00  1.18945312e+00  1.18750000e+00  1.18652344e+00
  1.17968750e+00  1.17773438e+00  1.17480469e+00  1.17285156e+00
  1.16406250e+00  1.15527344e+00  1.15332031e+00  1.13867188e+00
  1.13769531e+00  1.10449219e+00  1.10253906e+00  1.09570312e+00
  1.09082031e+00  1.08984375e+00  1.08886719e+00  1.08593750e+00
  1.08203125e+00  1.07910156e+00  1.07812500e+00  1.07324219e+00
  1.07226562e+00  1.06933594e+00  1.06835938e+00  1.05761719e+00
  1.05664062e+00  1.05175781e+00  1.05078125e+00  1.04882812e+00
  1.04492188e+00  1.04296875e+00  1.04101562e+00  1.03710938e+00
  1.03320312e+00  1.03222656e+00  1.03027344e+00  1.01855469e+00
  1.01660156e+00  1.01367188e+00  1.01269531e+00  1.01074219e+00
  1.00781250e+00  1.00195312e+00  9.99511719e-01  9.85839844e-01
  9.83398438e-01  9.81933594e-01  9.60449219e-01  9.59472656e-01
  9.52148438e-01  9.49707031e-01  9.48730469e-01  9.44335938e-01
  9.31152344e-01  9.28710938e-01  9.28222656e-01  9.23339844e-01
  8.97949219e-01  8.92578125e-01  8.92089844e-01  8.83789062e-01
  8.78906250e-01  8.78417969e-01  8.71582031e-01  8.67675781e-01
  8.64746094e-01  8.51074219e-01  8.48632812e-01  8.39843750e-01
  8.38378906e-01  8.16406250e-01  8.08105469e-01  8.07617188e-01
  8.06152344e-01  8.05664062e-01  8.05175781e-01  8.00781250e-01
  7.88085938e-01  7.86621094e-01  7.81250000e-01  7.76855469e-01
  7.71972656e-01  7.69531250e-01  7.69042969e-01  7.65625000e-01
  7.64648438e-01  7.54394531e-01  7.42187500e-01  7.39746094e-01
  7.38281250e-01  7.36328125e-01  7.30468750e-01  7.27050781e-01
  7.26562500e-01  7.12402344e-01  7.10449219e-01  7.06542969e-01
  7.06054688e-01  7.05566406e-01  7.00683594e-01  7.00195312e-01
  6.93359375e-01  6.92382812e-01  6.89453125e-01  6.86035156e-01
  6.84082031e-01  6.82128906e-01  6.78710938e-01  6.72363281e-01
  6.69433594e-01  6.67968750e-01  6.66992188e-01  6.63574219e-01
  6.56738281e-01  6.46484375e-01  6.44042969e-01  6.41601562e-01
  6.41113281e-01  6.40136719e-01  6.37695312e-01  6.27929688e-01
  6.26464844e-01  6.22558594e-01  6.15234375e-01  6.13769531e-01
  6.11816406e-01  6.09375000e-01  6.06933594e-01  6.04492188e-01
  5.94726562e-01  5.92773438e-01  5.92285156e-01  5.84960938e-01
  5.78125000e-01  5.74707031e-01  5.73730469e-01  5.71289062e-01
  5.65429688e-01  5.63964844e-01  5.61035156e-01  5.49316406e-01
  5.40039062e-01  5.32714844e-01  5.32226562e-01  5.31738281e-01
  5.29296875e-01  5.22949219e-01  5.20996094e-01  5.20507812e-01
  5.16601562e-01  5.15625000e-01  5.08300781e-01  5.05859375e-01
  5.02441406e-01  5.00976562e-01  5.00488281e-01  4.98046875e-01
  4.86572266e-01  4.81933594e-01  4.81201172e-01  4.79736328e-01
  4.79492188e-01  4.76806641e-01  4.68505859e-01  4.57031250e-01
  4.45068359e-01  4.44091797e-01  4.42871094e-01  4.37500000e-01
  4.32861328e-01  4.25781250e-01  4.20898438e-01  4.20654297e-01
  4.17724609e-01  4.17236328e-01  4.15283203e-01  4.12353516e-01
  4.05761719e-01  4.00146484e-01  3.98437500e-01  3.94775391e-01
  3.93310547e-01  3.91113281e-01  3.90380859e-01  3.88183594e-01
  3.87939453e-01  3.85253906e-01  3.79150391e-01  3.78662109e-01
  3.78173828e-01  3.72314453e-01  3.53271484e-01  3.48388672e-01
  3.46679688e-01  3.45947266e-01  3.45214844e-01  3.42529297e-01
  3.37646484e-01  3.33740234e-01  3.22509766e-01  3.21533203e-01
  3.16894531e-01  3.12500000e-01  3.09814453e-01  3.04687500e-01
  3.03955078e-01  2.98583984e-01  2.86865234e-01  2.83691406e-01
  2.83447266e-01  2.76611328e-01  2.72705078e-01  2.71972656e-01
  2.70507812e-01  2.68310547e-01  2.64160156e-01  2.63427734e-01
  2.59765625e-01  2.55126953e-01  2.52929688e-01  2.50732422e-01
  2.32543945e-01  2.31567383e-01  2.27783203e-01  2.26928711e-01
  2.26562500e-01  2.24853516e-01  2.22778320e-01  2.21923828e-01
  2.21435547e-01  2.16918945e-01  2.14721680e-01  2.12280273e-01
  2.11547852e-01  2.05322266e-01  2.00439453e-01  1.99462891e-01
  1.97753906e-01  1.93603516e-01  1.87011719e-01  1.84204102e-01
  1.83227539e-01  1.81396484e-01  1.79443359e-01  1.74316406e-01
  1.63940430e-01  1.50146484e-01  1.46362305e-01  1.40136719e-01
  1.22131348e-01  1.19201660e-01  1.15722656e-01  1.10168457e-01
  1.03271484e-01  9.91821289e-02  7.92846680e-02  7.70874023e-02
  7.61108398e-02  6.56127930e-02  5.85327148e-02  5.42602539e-02
  4.70275879e-02  3.65905762e-02  2.36816406e-02  1.98669434e-02
  1.96075439e-03  1.09004974e-03 -4.29534912e-03 -1.03454590e-02
 -3.13720703e-02 -3.22265625e-02 -4.22973633e-02 -4.29992676e-02
 -5.55725098e-02 -5.99670410e-02 -6.21948242e-02 -6.25000000e-02
 -7.25708008e-02 -7.41577148e-02 -8.75854492e-02 -8.77075195e-02
 -8.90502930e-02 -9.11865234e-02 -1.05102539e-01 -1.10778809e-01
 -1.22558594e-01 -1.22680664e-01 -1.31347656e-01 -1.31591797e-01
 -1.32812500e-01 -1.35498047e-01 -1.36718750e-01 -1.37451172e-01
 -1.43066406e-01 -1.47949219e-01 -1.64062500e-01 -1.69921875e-01
 -1.80297852e-01 -1.82495117e-01 -2.02148438e-01 -2.03735352e-01
 -2.11547852e-01 -2.11914062e-01 -2.27783203e-01 -2.29858398e-01
 -2.37182617e-01 -2.44995117e-01 -2.66845703e-01 -2.68310547e-01
 -2.76123047e-01 -2.77587891e-01 -3.03466797e-01 -3.10302734e-01
 -3.29833984e-01 -3.31054688e-01 -3.97949219e-01 -4.01367188e-01
 -4.07714844e-01 -4.08691406e-01 -4.74853516e-01 -4.77539062e-01
 -4.93896484e-01 -4.94628906e-01 -4.96582031e-01 -5.00488281e-01
 -5.07812500e-01 -5.14648438e-01 -5.19042969e-01 -5.20507812e-01
 -5.28808594e-01 -5.29296875e-01 -5.32714844e-01 -5.33203125e-01
 -5.48828125e-01 -5.49804688e-01 -5.83984375e-01 -5.84472656e-01
 -5.85449219e-01 -5.87402344e-01 -5.88378906e-01 -5.89355469e-01
 -6.08886719e-01 -6.09375000e-01 -6.20605469e-01 -6.21093750e-01
 -6.48925781e-01 -6.49414062e-01 -6.66503906e-01 -6.66992188e-01
 -6.82617188e-01 -6.85546875e-01 -6.88476562e-01 -6.90917969e-01
 -6.96289062e-01 -7.66113281e-01 -7.67578125e-01 -7.91503906e-01
 -7.92968750e-01 -8.06152344e-01 -8.06640625e-01 -8.25683594e-01
 -8.26171875e-01 -8.54980469e-01 -8.55957031e-01 -8.79394531e-01
 -8.83789062e-01 -8.84765625e-01 -8.90136719e-01 -9.36035156e-01
 -9.37500000e-01 -9.56542969e-01 -9.63867188e-01 -9.97558594e-01
 -9.99511719e-01 -1.00390625e+00 -1.00781250e+00 -1.01269531e+00
 -1.02050781e+00 -1.05859375e+00 -1.05957031e+00 -1.08300781e+00
 -1.08398438e+00 -1.08886719e+00 -1.09765625e+00 -1.13183594e+00
 -1.13378906e+00 -1.14843750e+00 -1.14941406e+00 -1.16796875e+00
 -1.17285156e+00 -1.18066406e+00 -1.18554688e+00 -1.18750000e+00
 -1.18847656e+00 -1.20117188e+00 -1.20214844e+00 -1.22753906e+00
 -1.23730469e+00 -1.25585938e+00 -1.26367188e+00 -1.26757812e+00
 -1.33203125e+00 -1.33300781e+00 -1.36035156e+00 -1.36425781e+00
 -1.40234375e+00 -1.41113281e+00 -1.41796875e+00 -1.41894531e+00
 -1.51855469e+00 -1.52441406e+00 -1.61621094e+00 -1.61816406e+00
 -1.63183594e+00 -1.63378906e+00 -1.63574219e+00 -1.63769531e+00
 -1.73046875e+00 -1.73437500e+00 -1.76660156e+00 -1.76757812e+00
 -1.96386719e+00 -1.97851562e+00 -2.04882812e+00 -2.05468750e+00
 -2.06640625e+00 -2.08007812e+00 -2.31640625e+00 -2.36132812e+00
 -2.94140625e+00]
