log_loss_steps: 256
eval_steps: 512
check_degradation: 0
----------------Epoch 1/1----------------
Epoch 1/1, Loss after 192 samples: 0.7671
Epoch 1/1, Loss after 448 samples: 0.7528
Mean accuracy: 0.5004, std: 0.0116, lower bound: 0.4782, upper bound: 0.5258 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 448 samples: 0.5000 with eval loss: 0.7146
Best model with eval loss 0.7145943910844864 and eval accuracy 0.5 with 448 samples seen is saved
Epoch 1/1, Loss after 704 samples: 0.6771
Epoch 1/1, Loss after 960 samples: 0.6470
Mean accuracy: 0.5124, std: 0.0115, lower bound: 0.4904, upper bound: 0.5349 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 960 samples: 0.5121 with eval loss: 0.6394
Best model with eval loss 0.639360249042511 and eval accuracy 0.5121334681496461 with 960 samples seen is saved
Epoch 1/1, Loss after 1216 samples: 0.6107
Epoch 1/1, Loss after 1472 samples: 0.5561
Mean accuracy: 0.8214, std: 0.0088, lower bound: 0.8033, upper bound: 0.8372 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 1472 samples: 0.8220 with eval loss: 0.5183
Best model with eval loss 0.5183353366390351 and eval accuracy 0.8220424671385238 with 1472 samples seen is saved
Epoch 1/1, Loss after 1728 samples: 0.5471
Epoch 1/1, Loss after 1984 samples: 0.4650
Mean accuracy: 0.8541, std: 0.0078, lower bound: 0.8387, upper bound: 0.8691 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 1984 samples: 0.8544 with eval loss: 0.4483
Best model with eval loss 0.4483081688804011 and eval accuracy 0.8543983822042467 with 1984 samples seen is saved
Epoch 1/1, Loss after 2240 samples: 0.4849
Epoch 1/1, Loss after 2496 samples: 0.4176
Mean accuracy: 0.8636, std: 0.0076, lower bound: 0.8483, upper bound: 0.8777 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 2496 samples: 0.8635 with eval loss: 0.3800
Best model with eval loss 0.38000528370180436 and eval accuracy 0.8634984833164813 with 2496 samples seen is saved
Epoch 1/1, Loss after 2752 samples: 0.3804
Epoch 1/1, Loss after 3008 samples: 0.3813
Mean accuracy: 0.7443, std: 0.0099, lower bound: 0.7245, upper bound: 0.7634 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 3008 samples: 0.7447 with eval loss: 0.4900
Epoch 1/1, Loss after 3264 samples: 0.3808
Epoch 1/1, Loss after 3520 samples: 0.3701
Mean accuracy: 0.7728, std: 0.0095, lower bound: 0.7538, upper bound: 0.7912 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 3520 samples: 0.7725 with eval loss: 0.4387
Epoch 1/1, Loss after 3776 samples: 0.3669
Epoch 1/1, Loss after 4032 samples: 0.4039
Mean accuracy: 0.8388, std: 0.0080, lower bound: 0.8225, upper bound: 0.8544 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 4032 samples: 0.8387 with eval loss: 0.3400
Best model with eval loss 0.34001556134993033 and eval accuracy 0.8387259858442871 with 4032 samples seen is saved
Epoch 1/1, Loss after 4288 samples: 0.3641
Epoch 1/1, Loss after 4544 samples: 0.3498
Mean accuracy: 0.8856, std: 0.0071, lower bound: 0.8721, upper bound: 0.8989 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 4544 samples: 0.8852 with eval loss: 0.2852
Best model with eval loss 0.2851850981673887 and eval accuracy 0.8852376137512639 with 4544 samples seen is saved
Epoch 1/1, Loss after 4800 samples: 0.3736
Epoch 1/1, Loss after 5056 samples: 0.4262
Mean accuracy: 0.8807, std: 0.0075, lower bound: 0.8655, upper bound: 0.8948 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 5056 samples: 0.8807 with eval loss: 0.2795
Best model with eval loss 0.2795208115731516 and eval accuracy 0.8806875631951466 with 5056 samples seen is saved
Epoch 1/1, Loss after 5312 samples: 0.3891
Epoch 1/1, Loss after 5568 samples: 0.3322
Mean accuracy: 0.8773, std: 0.0074, lower bound: 0.8625, upper bound: 0.8918 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 5568 samples: 0.8771 with eval loss: 0.2958
Epoch 1/1, Loss after 5824 samples: 0.3426
Epoch 1/1, Loss after 6080 samples: 0.3685
Mean accuracy: 0.8431, std: 0.0081, lower bound: 0.8271, upper bound: 0.8589 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 6080 samples: 0.8428 with eval loss: 0.3357
Epoch 1/1, Loss after 6336 samples: 0.3272
Epoch 1/1, Loss after 6592 samples: 0.3428
Mean accuracy: 0.8056, std: 0.0090, lower bound: 0.7877, upper bound: 0.8221 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 6592 samples: 0.8064 with eval loss: 0.3817
Epoch 1/1, Loss after 6848 samples: 0.3494
Epoch 1/1, Loss after 7104 samples: 0.3442
Mean accuracy: 0.8775, std: 0.0071, lower bound: 0.8635, upper bound: 0.8908 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 7104 samples: 0.8777 with eval loss: 0.2931
Epoch 1/1, Loss after 7360 samples: 0.4192
Epoch 1/1, Loss after 7616 samples: 0.2825
Mean accuracy: 0.8764, std: 0.0073, lower bound: 0.8625, upper bound: 0.8908 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 7616 samples: 0.8766 with eval loss: 0.2900
Epoch 1/1, Loss after 7872 samples: 0.3655
Epoch 1/1, Loss after 8128 samples: 0.2649
Mean accuracy: 0.8721, std: 0.0075, lower bound: 0.8569, upper bound: 0.8862 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 8128 samples: 0.8721 with eval loss: 0.2945
Epoch 1/1, Loss after 8384 samples: 0.2638
Epoch 1/1, Loss after 8640 samples: 0.2733
Mean accuracy: 0.8384, std: 0.0082, lower bound: 0.8220, upper bound: 0.8539 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 8640 samples: 0.8382 with eval loss: 0.3439
Epoch 1/1, Loss after 8896 samples: 0.2877
Epoch 1/1, Loss after 9152 samples: 0.2782
Mean accuracy: 0.8101, std: 0.0086, lower bound: 0.7932, upper bound: 0.8266 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 9152 samples: 0.8099 with eval loss: 0.3811
Epoch 1/1, Loss after 9408 samples: 0.2854
Epoch 1/1, Loss after 9664 samples: 0.2884
Mean accuracy: 0.8175, std: 0.0090, lower bound: 0.7998, upper bound: 0.8347 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 9664 samples: 0.8175 with eval loss: 0.3699
Epoch 1/1, Loss after 9920 samples: 0.2970
Epoch 1/1, Loss after 10176 samples: 0.2888
Mean accuracy: 0.8571, std: 0.0075, lower bound: 0.8413, upper bound: 0.8721 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 10176 samples: 0.8574 with eval loss: 0.3162
Epoch 1/1, Loss after 10432 samples: 0.3149
Epoch 1/1, Loss after 10688 samples: 0.3684
Mean accuracy: 0.8491, std: 0.0080, lower bound: 0.8332, upper bound: 0.8645 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 10688 samples: 0.8498 with eval loss: 0.3290
Epoch 1/1, Loss after 10944 samples: 0.2910
Epoch 1/1, Loss after 11200 samples: 0.3039
Mean accuracy: 0.8170, std: 0.0088, lower bound: 0.7998, upper bound: 0.8337 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 11200 samples: 0.8170 with eval loss: 0.3698
Epoch 1/1, Loss after 11456 samples: 0.3251
Epoch 1/1, Loss after 11712 samples: 0.3262
Mean accuracy: 0.8782, std: 0.0075, lower bound: 0.8635, upper bound: 0.8923 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 11712 samples: 0.8782 with eval loss: 0.2784
Best model with eval loss 0.27835074547798405 and eval accuracy 0.878159757330637 with 11712 samples seen is saved
Epoch 1/1, Loss after 11968 samples: 0.3066
Epoch 1/1, Loss after 12224 samples: 0.2983
Mean accuracy: 0.8794, std: 0.0074, lower bound: 0.8650, upper bound: 0.8933 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 12224 samples: 0.8797 with eval loss: 0.2738
Best model with eval loss 0.2737833256683042 and eval accuracy 0.8796764408493428 with 12224 samples seen is saved
Epoch 1/1, Loss after 12480 samples: 0.2668
Epoch 1/1, Loss after 12736 samples: 0.3340
Mean accuracy: 0.8645, std: 0.0075, lower bound: 0.8493, upper bound: 0.8782 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 12736 samples: 0.8640 with eval loss: 0.3028
Epoch 1/1, Loss after 12992 samples: 0.3281
Epoch 1/1, Loss after 13248 samples: 0.2602
Mean accuracy: 0.8311, std: 0.0087, lower bound: 0.8140, upper bound: 0.8468 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 13248 samples: 0.8306 with eval loss: 0.3558
Epoch 1/1, Loss after 13504 samples: 0.2957
Epoch 1/1, Loss after 13760 samples: 0.3114
Mean accuracy: 0.8389, std: 0.0084, lower bound: 0.8231, upper bound: 0.8554 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 13760 samples: 0.8387 with eval loss: 0.3387
Epoch 1/1, Loss after 14016 samples: 0.3060
Epoch 1/1, Loss after 14272 samples: 0.2972
Mean accuracy: 0.8622, std: 0.0078, lower bound: 0.8473, upper bound: 0.8771 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 14272 samples: 0.8625 with eval loss: 0.3001
Epoch 1/1, Loss after 14528 samples: 0.2923
Epoch 1/1, Loss after 14784 samples: 0.2808
Mean accuracy: 0.8533, std: 0.0084, lower bound: 0.8362, upper bound: 0.8701 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 14784 samples: 0.8529 with eval loss: 0.3222
Epoch 1/1, Loss after 15040 samples: 0.2646
Epoch 1/1, Loss after 15296 samples: 0.2969
Mean accuracy: 0.8424, std: 0.0080, lower bound: 0.8276, upper bound: 0.8574 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 15296 samples: 0.8423 with eval loss: 0.3337
Epoch 1/1, Loss after 15552 samples: 0.2990
Epoch 1/1, Loss after 15808 samples: 0.3684
Mean accuracy: 0.8370, std: 0.0083, lower bound: 0.8210, upper bound: 0.8529 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 15808 samples: 0.8372 with eval loss: 0.3411
Training signal is False, stopping training
Training finished
Best model: {'eval_acc': 0.8796764408493428, 'nb_samples': 12224, 'eval_loss': 0.2737833256683042}
Training loss logs: [{'samples': 192, 'loss': 0.767059326171875}, {'samples': 448, 'loss': 0.7528290748596191}, {'samples': 704, 'loss': 0.6770539283752441}, {'samples': 960, 'loss': 0.6470398902893066}, {'samples': 1216, 'loss': 0.6107406616210938}, {'samples': 1472, 'loss': 0.5560848712921143}, {'samples': 1728, 'loss': 0.5470767766237259}, {'samples': 1984, 'loss': 0.46496596187353134}, {'samples': 2240, 'loss': 0.48491282761096954}, {'samples': 2496, 'loss': 0.4175722077488899}, {'samples': 2752, 'loss': 0.38038933277130127}, {'samples': 3008, 'loss': 0.38127943128347397}, {'samples': 3264, 'loss': 0.38083232194185257}, {'samples': 3520, 'loss': 0.3701319769024849}, {'samples': 3776, 'loss': 0.3669001832604408}, {'samples': 4032, 'loss': 0.40394145995378494}, {'samples': 4288, 'loss': 0.36407943814992905}, {'samples': 4544, 'loss': 0.3497537225484848}, {'samples': 4800, 'loss': 0.37358834594488144}, {'samples': 5056, 'loss': 0.42617931216955185}, {'samples': 5312, 'loss': 0.38905031979084015}, {'samples': 5568, 'loss': 0.3322029486298561}, {'samples': 5824, 'loss': 0.342570997774601}, {'samples': 6080, 'loss': 0.3685467764735222}, {'samples': 6336, 'loss': 0.3272335007786751}, {'samples': 6592, 'loss': 0.3427770063281059}, {'samples': 6848, 'loss': 0.34939027577638626}, {'samples': 7104, 'loss': 0.3442310392856598}, {'samples': 7360, 'loss': 0.41920723021030426}, {'samples': 7616, 'loss': 0.28252290189266205}, {'samples': 7872, 'loss': 0.3654981032013893}, {'samples': 8128, 'loss': 0.26489758491516113}, {'samples': 8384, 'loss': 0.26379435509443283}, {'samples': 8640, 'loss': 0.2733270563185215}, {'samples': 8896, 'loss': 0.2877219319343567}, {'samples': 9152, 'loss': 0.278169147670269}, {'samples': 9408, 'loss': 0.2853681221604347}, {'samples': 9664, 'loss': 0.288390826433897}, {'samples': 9920, 'loss': 0.296977736055851}, {'samples': 10176, 'loss': 0.2888360321521759}, {'samples': 10432, 'loss': 0.3148512542247772}, {'samples': 10688, 'loss': 0.3683623969554901}, {'samples': 10944, 'loss': 0.2910304218530655}, {'samples': 11200, 'loss': 0.3039472699165344}, {'samples': 11456, 'loss': 0.3251008242368698}, {'samples': 11712, 'loss': 0.3262043371796608}, {'samples': 11968, 'loss': 0.30655108392238617}, {'samples': 12224, 'loss': 0.2982802987098694}, {'samples': 12480, 'loss': 0.26677846908569336}, {'samples': 12736, 'loss': 0.3340294286608696}, {'samples': 12992, 'loss': 0.32810768485069275}, {'samples': 13248, 'loss': 0.26023563742637634}, {'samples': 13504, 'loss': 0.2956903353333473}, {'samples': 13760, 'loss': 0.31140249222517014}, {'samples': 14016, 'loss': 0.30598147213459015}, {'samples': 14272, 'loss': 0.2972376421093941}, {'samples': 14528, 'loss': 0.29228124767541885}, {'samples': 14784, 'loss': 0.28077933192253113}, {'samples': 15040, 'loss': 0.26459581032395363}, {'samples': 15296, 'loss': 0.2968949191272259}, {'samples': 15552, 'loss': 0.2990116849541664}, {'samples': 15808, 'loss': 0.368446908891201}]
Evaluation accuracy logs: [{'samples': 448, 'accuracy': 0.5003594539939332, 'std': 0.011637876314977744, 'lower_bound': 0.4782482305358949, 'upper_bound': 0.5257836198179979}, {'samples': 960, 'accuracy': 0.5123847320525783, 'std': 0.011454123971269594, 'lower_bound': 0.490381698685541, 'upper_bound': 0.5348837209302325}, {'samples': 1472, 'accuracy': 0.8214100101112235, 'std': 0.00876591173534774, 'lower_bound': 0.8033367037411526, 'upper_bound': 0.837221941354904}, {'samples': 1984, 'accuracy': 0.8541218402426694, 'std': 0.007811104853493326, 'lower_bound': 0.8387259858442871, 'upper_bound': 0.869072295247725}, {'samples': 2496, 'accuracy': 0.863587462082912, 'std': 0.0075640640130735, 'lower_bound': 0.8483316481294236, 'upper_bound': 0.8776541961577351}, {'samples': 3008, 'accuracy': 0.7442593528816988, 'std': 0.009934759119921725, 'lower_bound': 0.7244565217391304, 'upper_bound': 0.7634100101112234}, {'samples': 3520, 'accuracy': 0.7727649140546006, 'std': 0.009450927072132968, 'lower_bound': 0.7537917087967644, 'upper_bound': 0.7912032355915066}, {'samples': 4032, 'accuracy': 0.838819514661274, 'std': 0.008029909053329076, 'lower_bound': 0.8225480283114257, 'upper_bound': 0.8543983822042467}, {'samples': 4544, 'accuracy': 0.8856405460060667, 'std': 0.007131276647831866, 'lower_bound': 0.8720803842264914, 'upper_bound': 0.8988877654196158}, {'samples': 5056, 'accuracy': 0.8806729019211325, 'std': 0.00750958597280608, 'lower_bound': 0.865520728008089, 'upper_bound': 0.8948432760364005}, {'samples': 5568, 'accuracy': 0.8772740141557128, 'std': 0.0073925162120892654, 'lower_bound': 0.8624873609706775, 'upper_bound': 0.8918099089989889}, {'samples': 6080, 'accuracy': 0.8431112234580384, 'std': 0.008147188872517249, 'lower_bound': 0.8270854398382204, 'upper_bound': 0.858948432760364}, {'samples': 6592, 'accuracy': 0.8056112234580385, 'std': 0.008988047321187253, 'lower_bound': 0.7876643073811931, 'upper_bound': 0.8220551061678464}, {'samples': 7104, 'accuracy': 0.8774878665318504, 'std': 0.007086747971981899, 'lower_bound': 0.8634984833164813, 'upper_bound': 0.890798786653185}, {'samples': 7616, 'accuracy': 0.8763579373104146, 'std': 0.007343385309893898, 'lower_bound': 0.8624873609706775, 'upper_bound': 0.890798786653185}, {'samples': 8128, 'accuracy': 0.8720581395348838, 'std': 0.007497631377738741, 'lower_bound': 0.8569135490394337, 'upper_bound': 0.8862487360970678}, {'samples': 8640, 'accuracy': 0.8384064711830131, 'std': 0.008172636408075447, 'lower_bound': 0.8220298281092012, 'upper_bound': 0.8538928210313448}, {'samples': 9152, 'accuracy': 0.810118806875632, 'std': 0.008620919116976739, 'lower_bound': 0.7932128412537917, 'upper_bound': 0.826592517694641}, {'samples': 9664, 'accuracy': 0.8175404448938322, 'std': 0.00902895985863165, 'lower_bound': 0.7997977755308392, 'upper_bound': 0.8346814964610718}, {'samples': 10176, 'accuracy': 0.857149646107179, 'std': 0.007525268056101302, 'lower_bound': 0.8412537917087968, 'upper_bound': 0.8721056622851365}, {'samples': 10688, 'accuracy': 0.8491046511627908, 'std': 0.00804171541960828, 'lower_bound': 0.833164812942366, 'upper_bound': 0.8645096056622852}, {'samples': 11200, 'accuracy': 0.816984833164813, 'std': 0.008795572830515382, 'lower_bound': 0.7997851365015166, 'upper_bound': 0.8336703741152679}, {'samples': 11712, 'accuracy': 0.8781981799797774, 'std': 0.00748210422488114, 'lower_bound': 0.8634984833164813, 'upper_bound': 0.8923281092012134}, {'samples': 12224, 'accuracy': 0.8793827098078868, 'std': 0.00744414473929626, 'lower_bound': 0.8650025278058645, 'upper_bound': 0.8933392315470172}, {'samples': 12736, 'accuracy': 0.8644706774519717, 'std': 0.007486660645239089, 'lower_bound': 0.8493427704752275, 'upper_bound': 0.878159757330637}, {'samples': 13248, 'accuracy': 0.8310692618806875, 'std': 0.008704054171523546, 'lower_bound': 0.813953488372093, 'upper_bound': 0.8468276036400405}, {'samples': 13760, 'accuracy': 0.8388852376137513, 'std': 0.008378656868584064, 'lower_bound': 0.8230535894843276, 'upper_bound': 0.8554095045500506}, {'samples': 14272, 'accuracy': 0.8621905965621839, 'std': 0.007769612255406277, 'lower_bound': 0.8473205257836198, 'upper_bound': 0.8771486349848332}, {'samples': 14784, 'accuracy': 0.8532896865520728, 'std': 0.008404150519945515, 'lower_bound': 0.8361981799797775, 'upper_bound': 0.8700834175935288}, {'samples': 15296, 'accuracy': 0.8424459049544994, 'std': 0.007997872871449414, 'lower_bound': 0.8275910010111223, 'upper_bound': 0.8574443882709808}, {'samples': 15808, 'accuracy': 0.836974216380182, 'std': 0.008290105339897253, 'lower_bound': 0.8210187057633973, 'upper_bound': 0.852881698685541}]
Evaluating the best model on the test set of dataset ./fake_true_datasets/fake_true_dataset_gemma_10k...
Test metrics:
accuracy: 0.8510601617795753
precision: 0.7743930663648496
recall: 0.9908288860114441
f1_score: 0.8692899350574275
fp_rate: 0.2887521766241472
tp_rate: 0.9908288860114441
std_accuracy: 0.008249596445058191
std_precision: 0.012016622541417556
std_recall: 0.002979444491124829
std_f1_score: 0.00773181226431187
std_fp_rate: 0.014962303912720827
std_tp_rate: 0.002979444491124829
TP: 980.067
TN: 703.33
FP: 285.534
FN: 9.069
roc_auc: 0.972915416395313
fpr: [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00101112 0.00101112 0.00101112 0.00101112 0.00101112
 0.00202224 0.00202224 0.00202224 0.00202224 0.00202224 0.00202224
 0.00202224 0.00303337 0.00303337 0.00303337 0.00303337 0.00303337
 0.00303337 0.00303337 0.00303337 0.00303337 0.00303337 0.00303337
 0.00303337 0.00303337 0.00303337 0.00303337 0.00303337 0.00303337
 0.00303337 0.00303337 0.00404449 0.00404449 0.00404449 0.00404449
 0.00404449 0.00404449 0.00404449 0.00404449 0.00404449 0.00404449
 0.00404449 0.00404449 0.00404449 0.00404449 0.00404449 0.00404449
 0.00404449 0.00404449 0.00404449 0.00404449 0.00404449 0.00404449
 0.00404449 0.00505561 0.00505561 0.00505561 0.00505561 0.00606673
 0.00606673 0.00606673 0.00606673 0.00606673 0.00606673 0.00606673
 0.00707786 0.00707786 0.00707786 0.00707786 0.00707786 0.00707786
 0.00707786 0.00707786 0.00707786 0.00707786 0.00808898 0.00808898
 0.00808898 0.0091001  0.0091001  0.01011122 0.01011122 0.01011122
 0.01112235 0.01112235 0.01112235 0.01112235 0.01213347 0.01213347
 0.01415571 0.01415571 0.01415571 0.01415571 0.01516684 0.01516684
 0.01617796 0.01617796 0.01718908 0.01718908 0.01718908 0.01718908
 0.01718908 0.01718908 0.01718908 0.01718908 0.01718908 0.01718908
 0.01718908 0.01718908 0.01921132 0.01921132 0.02022245 0.02022245
 0.02325581 0.02325581 0.02527806 0.02527806 0.02628918 0.02628918
 0.0273003  0.0273003  0.02831143 0.02831143 0.02831143 0.02831143
 0.03033367 0.03033367 0.03033367 0.03134479 0.03134479 0.03235592
 0.03235592 0.03235592 0.03437816 0.03437816 0.03538928 0.03538928
 0.03538928 0.03538928 0.0364004  0.0364004  0.03741153 0.03741153
 0.03842265 0.04044489 0.04044489 0.04044489 0.04044489 0.04145602
 0.04145602 0.04246714 0.04246714 0.04246714 0.04347826 0.04448938
 0.04448938 0.04550051 0.04550051 0.04550051 0.04651163 0.04651163
 0.04651163 0.05055612 0.05055612 0.05055612 0.05156724 0.05156724
 0.05257836 0.05257836 0.05257836 0.05460061 0.05460061 0.05662285
 0.05662285 0.0586451  0.0586451  0.05965622 0.05965622 0.06370071
 0.06370071 0.06471183 0.06471183 0.06471183 0.06572295 0.06572295
 0.06976744 0.06976744 0.07178969 0.07178969 0.07178969 0.07381193
 0.07381193 0.07482305 0.07482305 0.0768453  0.0768453  0.07785642
 0.07785642 0.07886754 0.07886754 0.07987867 0.07987867 0.08088979
 0.08088979 0.08291203 0.08392315 0.08392315 0.08392315 0.08392315
 0.08392315 0.08392315 0.0859454  0.0859454  0.08695652 0.08695652
 0.08796764 0.08796764 0.08897877 0.08897877 0.09201213 0.09201213
 0.09302326 0.09302326 0.09403438 0.09403438 0.09706775 0.09706775
 0.10010111 0.10010111 0.10111223 0.10111223 0.10212336 0.10212336
 0.10313448 0.10313448 0.10515672 0.10515672 0.10717897 0.10717897
 0.11021234 0.11223458 0.1132457  0.1132457  0.11425683 0.11425683
 0.11627907 0.11627907 0.11830131 0.11830131 0.12032356 0.12032356
 0.12133468 0.12133468 0.1223458  0.1223458  0.12436805 0.12436805
 0.12537917 0.12537917 0.12639029 0.12639029 0.12740142 0.12740142
 0.12841254 0.12841254 0.12942366 0.12942366 0.13447927 0.13447927
 0.13650152 0.13650152 0.14155713 0.14155713 0.1445905  0.1445905
 0.14560162 0.14560162 0.14560162 0.14762386 0.14762386 0.15065723
 0.15065723 0.1536906  0.1536906  0.15470172 0.15470172 0.15672396
 0.15672396 0.15975733 0.15975733 0.16076845 0.1627907  0.16380182
 0.16380182 0.16481294 0.16683519 0.16986855 0.16986855 0.1718908
 0.1718908  0.17391304 0.17391304 0.17795753 0.17795753 0.17896866
 0.17896866 0.18200202 0.18200202 0.18705763 0.18705763 0.18907988
 0.18907988 0.190091   0.19211325 0.19514661 0.19514661 0.19615774
 0.19615774 0.21638018 0.21638018 0.23255814 0.23356926 0.24266936
 0.24469161 0.24469161 0.24772497 0.24772497 0.25480283 0.25581395
 0.2578362  0.2578362  0.26086957 0.26086957 0.26592518 0.26592518
 0.26895854 0.26895854 0.28311426 0.28311426 0.28614762 0.28614762
 0.28715875 0.28715875 0.31951466 0.31951466 0.32355915 0.3255814
 0.33670374 0.33670374 0.34681496 0.34681496 0.35894843 0.36097068
 0.3619818  0.3619818  0.36501517 0.36501517 0.39029323 0.39231547
 0.43579373 0.43579373 0.43781598 0.43781598 0.44489383 0.44691608
 0.4479272  0.44994944 0.47118301 0.47320526 0.48129424 0.48331648
 0.48837209 0.49039434 0.50758342 0.50960566 0.51061678 0.51263903
 0.54499494 0.54701719 0.56319515 0.56319515 0.58847321 0.59049545
 0.6016178  0.60364004 0.60970677 0.61274014 0.62487361 0.62689585
 0.63700708 0.63902932 0.64105157 0.64307381 0.64914055 0.65116279
 0.66026289 0.66228514 0.67340748 0.67542973 0.67644085 0.67846309
 0.69565217 0.69868554 0.72598584 0.73003033 0.74620829 0.74823054
 0.760364   0.76238625 0.76440849 0.76845298 0.76946411 0.77350859
 0.77553084 0.77856421 0.78058645 0.7826087  0.78361982 0.78564206
 0.79271992 0.79474216 0.79676441 0.8008089  0.80384226 0.82002022
 0.82204247 0.82305359 0.82709808 0.8281092  0.83013145 0.83114257
 0.83316481 0.83518706 0.8372093  0.84125379 0.84327604 0.8867543
 0.88877654 0.89282103 0.89484328 0.90091001 0.90293225 0.93023256
 0.9322548  0.94539939 0.94742164 0.96663296 0.96865521 0.9726997
 0.97472194 1.        ]
tpr: [0.         0.00101112 0.00202224 0.00404449 0.0182002  0.02022245
 0.02325581 0.02527806 0.02831143 0.03033367 0.0364004  0.03842265
 0.04145602 0.04347826 0.04651163 0.04853387 0.05156724 0.05358948
 0.06572295 0.0677452  0.08493428 0.08695652 0.08796764 0.08998989
 0.09908999 0.10212336 0.11122346 0.1132457  0.12133468 0.12335693
 0.13346815 0.13549039 0.13650152 0.13852376 0.13953488 0.14155713
 0.14863498 0.15065723 0.15773509 0.15975733 0.16076845 0.1627907
 0.16582406 0.16784631 0.17492417 0.17694641 0.17896866 0.1809909
 0.18604651 0.18806876 0.19110212 0.19312437 0.21031345 0.21233569
 0.21840243 0.22042467 0.22244692 0.22446916 0.23761375 0.239636
 0.24064712 0.24368049 0.25176946 0.25379171 0.26086957 0.26289181
 0.26895854 0.27098079 0.27199191 0.27401416 0.28311426 0.28715875
 0.28816987 0.29019211 0.2942366  0.29828109 0.31041456 0.3124368
 0.31445905 0.31648129 0.33063701 0.33265925 0.33771486 0.34074823
 0.34175935 0.34580384 0.34782609 0.35085945 0.35187058 0.35389282
 0.35793731 0.3619818  0.36299292 0.36501517 0.36602629 0.36905966
 0.37310415 0.37613751 0.37917088 0.38119312 0.38220425 0.38422649
 0.38523761 0.38725986 0.38827098 0.39130435 0.39534884 0.40546006
 0.40748231 0.40950455 0.41152679 0.41557128 0.41860465 0.4206269
 0.42366026 0.42467139 0.42669363 0.42770475 0.43073812 0.43478261
 0.43478261 0.44084934 0.44489383 0.44691608 0.45197169 0.45399393
 0.45500506 0.45601618 0.4570273  0.45904954 0.46006067 0.4661274
 0.47118301 0.47320526 0.4752275  0.47724975 0.47826087 0.48028311
 0.48533873 0.48837209 0.49241658 0.49747219 0.49949444 0.50353893
 0.50556117 0.50859454 0.50859454 0.50960566 0.51365015 0.5156724
 0.51870576 0.5247725  0.52679474 0.52982811 0.53286148 0.53488372
 0.53589484 0.53791709 0.54095046 0.5429727  0.54600607 0.54802831
 0.54903943 0.55106168 0.5520728  0.55409505 0.55712841 0.55915066
 0.56016178 0.56016178 0.56825076 0.570273   0.59049545 0.59150657
 0.59757331 0.60364004 0.60768453 0.60970677 0.61172902 0.61476239
 0.61476239 0.61577351 0.61880688 0.62184024 0.62386249 0.62992922
 0.63195147 0.63498483 0.63700708 0.64004044 0.64004044 0.64206269
 0.65116279 0.65116279 0.6653185  0.6653185  0.67239636 0.6744186
 0.6744186  0.67745197 0.67947422 0.68554095 0.68554095 0.68857432
 0.68857432 0.69059656 0.69969666 0.70171891 0.70374115 0.7057634
 0.70677452 0.71284125 0.71284125 0.71991911 0.72194135 0.72295248
 0.72497472 0.73205258 0.73407482 0.74418605 0.74620829 0.75025278
 0.75429727 0.75733064 0.75733064 0.75834176 0.75834176 0.76339737
 0.76339737 0.76643074 0.76643074 0.76845298 0.76845298 0.77047523
 0.77047523 0.77350859 0.77451972 0.77553084 0.77755308 0.77957533
 0.77957533 0.78159757 0.78361982 0.78361982 0.78564206 0.78665319
 0.78867543 0.7917088  0.7917088  0.79676441 0.79676441 0.79878665
 0.8008089  0.80283114 0.80283114 0.80586451 0.80586451 0.80788675
 0.80889788 0.80889788 0.809909   0.81193124 0.81294237 0.81294237
 0.81698686 0.8190091  0.82002022 0.82204247 0.82204247 0.82305359
 0.82608696 0.82608696 0.82709808 0.82912032 0.83013145 0.83114257
 0.83316481 0.83316481 0.83417594 0.83619818 0.8372093  0.83822042
 0.83822042 0.84024267 0.84226491 0.84226491 0.84428716 0.8463094
 0.84732053 0.84732053 0.84833165 0.84833165 0.85338726 0.85338726
 0.8554095  0.8554095  0.85743175 0.85844287 0.85844287 0.86349848
 0.86349848 0.86653185 0.86653185 0.87057634 0.87259858 0.87259858
 0.87360971 0.87462083 0.87563195 0.87563195 0.87664307 0.8776542
 0.87967644 0.88068756 0.88169869 0.88169869 0.88372093 0.88372093
 0.88473205 0.8867543  0.8867543  0.89180991 0.89383215 0.89484328
 0.89686552 0.89888777 0.89888777 0.90192113 0.90192113 0.90293225
 0.90293225 0.90394338 0.90394338 0.90596562 0.90596562 0.91203236
 0.91203236 0.91304348 0.91304348 0.9140546  0.9140546  0.91506572
 0.91506572 0.91607685 0.91607685 0.91708797 0.91708797 0.91911021
 0.91911021 0.92012133 0.92012133 0.92113246 0.92113246 0.92214358
 0.92214358 0.92214358 0.92214358 0.92416582 0.92416582 0.92517695
 0.92517695 0.92618807 0.92618807 0.92821031 0.92821031 0.92922144
 0.92922144 0.93124368 0.93124368 0.9322548  0.9322548  0.93427705
 0.93427705 0.93528817 0.93528817 0.93832154 0.93832154 0.94034378
 0.94034378 0.94236603 0.94236603 0.94337715 0.94337715 0.94438827
 0.94438827 0.94539939 0.94539939 0.94843276 0.94843276 0.94944388
 0.94944388 0.95045501 0.95247725 0.95247725 0.95348837 0.95348837
 0.95449949 0.95449949 0.95551062 0.95551062 0.95652174 0.95652174
 0.95753286 0.95753286 0.95955511 0.95955511 0.96056623 0.96056623
 0.96157735 0.96157735 0.96157735 0.96157735 0.96461072 0.96461072
 0.96562184 0.96562184 0.96663296 0.96663296 0.96764408 0.96764408
 0.96865521 0.96865521 0.97067745 0.97067745 0.97168857 0.97168857
 0.97371082 0.97472194 0.97472194 0.97472194 0.97573306 0.97573306
 0.97674419 0.97674419 0.97876643 0.97876643 0.97977755 0.97977755
 0.97977755 0.98078868 0.98078868 0.9817998  0.9817998  0.98281092
 0.98281092 0.98382204 0.98382204 0.98483316 0.98483316 0.98584429
 0.98584429 0.98685541 0.98685541 0.98887765 0.98887765 0.98988878
 0.98988878 0.9908999  0.9908999  0.99191102 0.99191102 0.99191102
 0.99191102 0.99393327 0.99393327 0.99494439 0.99494439 0.99494439
 0.99494439 0.99595551 0.99595551 0.99696663 0.99696663 0.99696663
 0.99696663 0.99797776 0.99797776 0.99898888 0.99898888 0.99898888
 0.99898888 0.99898888 0.99898888 0.99898888 0.99898888 0.99898888
 0.99898888 0.99898888 0.99898888 0.99898888 0.99898888 0.99898888
 0.99898888 0.99898888 0.99898888 1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.        ]
thresholds: [           inf  6.9804688e+00  6.6015625e+00  6.5820312e+00
  5.5976562e+00  5.5078125e+00  5.3984375e+00  5.3789062e+00
  5.3320312e+00  5.2617188e+00  5.0625000e+00  5.0507812e+00
  4.9882812e+00  4.9804688e+00  4.9375000e+00  4.9296875e+00
  4.9101562e+00  4.8906250e+00  4.6210938e+00  4.6054688e+00
  4.3281250e+00  4.3242188e+00  4.2812500e+00  4.2539062e+00
  4.1210938e+00  4.1132812e+00  4.0156250e+00  4.0078125e+00
  3.9023438e+00  3.9003906e+00  3.6875000e+00  3.6855469e+00
  3.6816406e+00  3.6757812e+00  3.6718750e+00  3.6660156e+00
  3.6289062e+00  3.6230469e+00  3.5175781e+00  3.4902344e+00
  3.4882812e+00  3.4863281e+00  3.4570312e+00  3.4375000e+00
  3.3906250e+00  3.3769531e+00  3.3632812e+00  3.3613281e+00
  3.3359375e+00  3.3339844e+00  3.3164062e+00  3.3125000e+00
  3.2050781e+00  3.2031250e+00  3.1640625e+00  3.1523438e+00
  3.1484375e+00  3.1386719e+00  3.0292969e+00  3.0214844e+00
  3.0175781e+00  3.0156250e+00  2.9785156e+00  2.9707031e+00
  2.9375000e+00  2.9355469e+00  2.8789062e+00  2.8769531e+00
  2.8750000e+00  2.8691406e+00  2.8281250e+00  2.8085938e+00
  2.8007812e+00  2.7988281e+00  2.7832031e+00  2.7617188e+00
  2.6894531e+00  2.6875000e+00  2.6816406e+00  2.6796875e+00
  2.6113281e+00  2.6054688e+00  2.5781250e+00  2.5761719e+00
  2.5742188e+00  2.5703125e+00  2.5585938e+00  2.5566406e+00
  2.5488281e+00  2.5468750e+00  2.5371094e+00  2.5273438e+00
  2.5253906e+00  2.5234375e+00  2.5195312e+00  2.5156250e+00
  2.5039062e+00  2.5019531e+00  2.4902344e+00  2.4824219e+00
  2.4804688e+00  2.4785156e+00  2.4707031e+00  2.4687500e+00
  2.4648438e+00  2.4589844e+00  2.4550781e+00  2.4218750e+00
  2.4179688e+00  2.4121094e+00  2.4101562e+00  2.3867188e+00
  2.3847656e+00  2.3730469e+00  2.3593750e+00  2.3574219e+00
  2.3535156e+00  2.3515625e+00  2.3496094e+00  2.3183594e+00
  2.3144531e+00  2.3007812e+00  2.2910156e+00  2.2890625e+00
  2.2636719e+00  2.2597656e+00  2.2558594e+00  2.2460938e+00
  2.2441406e+00  2.2421875e+00  2.2402344e+00  2.2343750e+00
  2.2187500e+00  2.2148438e+00  2.2109375e+00  2.2089844e+00
  2.2031250e+00  2.2011719e+00  2.1894531e+00  2.1855469e+00
  2.1816406e+00  2.1601562e+00  2.1562500e+00  2.1464844e+00
  2.1425781e+00  2.1367188e+00  2.1347656e+00  2.1328125e+00
  2.1269531e+00  2.1171875e+00  2.1093750e+00  2.0878906e+00
  2.0859375e+00  2.0781250e+00  2.0664062e+00  2.0585938e+00
  2.0566406e+00  2.0546875e+00  2.0410156e+00  2.0312500e+00
  2.0156250e+00  2.0117188e+00  2.0097656e+00  2.0039062e+00
  2.0019531e+00  1.9931641e+00  1.9892578e+00  1.9882812e+00
  1.9843750e+00  1.9804688e+00  1.9589844e+00  1.9570312e+00
  1.8916016e+00  1.8896484e+00  1.8847656e+00  1.8710938e+00
  1.8681641e+00  1.8632812e+00  1.8613281e+00  1.8554688e+00
  1.8544922e+00  1.8515625e+00  1.8505859e+00  1.8427734e+00
  1.8408203e+00  1.8037109e+00  1.7998047e+00  1.7939453e+00
  1.7919922e+00  1.7832031e+00  1.7812500e+00  1.7792969e+00
  1.7548828e+00  1.7529297e+00  1.7089844e+00  1.7080078e+00
  1.6933594e+00  1.6904297e+00  1.6894531e+00  1.6855469e+00
  1.6835938e+00  1.6689453e+00  1.6679688e+00  1.6611328e+00
  1.6513672e+00  1.6494141e+00  1.6191406e+00  1.6162109e+00
  1.6142578e+00  1.6093750e+00  1.6064453e+00  1.5898438e+00
  1.5820312e+00  1.5605469e+00  1.5595703e+00  1.5576172e+00
  1.5566406e+00  1.5439453e+00  1.5371094e+00  1.5039062e+00
  1.5029297e+00  1.4902344e+00  1.4863281e+00  1.4785156e+00
  1.4736328e+00  1.4707031e+00  1.4697266e+00  1.4453125e+00
  1.4414062e+00  1.4257812e+00  1.4218750e+00  1.4130859e+00
  1.4082031e+00  1.4033203e+00  1.3925781e+00  1.3876953e+00
  1.3867188e+00  1.3828125e+00  1.3808594e+00  1.3750000e+00
  1.3710938e+00  1.3691406e+00  1.3652344e+00  1.3613281e+00
  1.3583984e+00  1.3574219e+00  1.3564453e+00  1.3515625e+00
  1.3476562e+00  1.3369141e+00  1.3339844e+00  1.3203125e+00
  1.3134766e+00  1.3085938e+00  1.3066406e+00  1.2968750e+00
  1.2919922e+00  1.2880859e+00  1.2861328e+00  1.2822266e+00
  1.2792969e+00  1.2763672e+00  1.2734375e+00  1.2714844e+00
  1.2607422e+00  1.2568359e+00  1.2558594e+00  1.2548828e+00
  1.2509766e+00  1.2460938e+00  1.2265625e+00  1.2207031e+00
  1.2158203e+00  1.1982422e+00  1.1962891e+00  1.1943359e+00
  1.1933594e+00  1.1835938e+00  1.1816406e+00  1.1777344e+00
  1.1767578e+00  1.1757812e+00  1.1728516e+00  1.1699219e+00
  1.1669922e+00  1.1572266e+00  1.1542969e+00  1.1494141e+00
  1.1464844e+00  1.1425781e+00  1.1376953e+00  1.1367188e+00
  1.1289062e+00  1.1123047e+00  1.1093750e+00  1.1005859e+00
  1.0957031e+00  1.0917969e+00  1.0888672e+00  1.0771484e+00
  1.0644531e+00  1.0576172e+00  1.0546875e+00  1.0458984e+00
  1.0429688e+00  1.0380859e+00  1.0361328e+00  1.0322266e+00
  1.0273438e+00  1.0253906e+00  1.0117188e+00  1.0078125e+00
  1.0039062e+00  1.0019531e+00  1.0009766e+00  9.8193359e-01
  9.7998047e-01  9.7949219e-01  9.6533203e-01  9.6142578e-01
  9.6093750e-01  9.4970703e-01  9.4775391e-01  9.4433594e-01
  9.4189453e-01  9.3945312e-01  9.3798828e-01  9.3017578e-01
  9.2626953e-01  9.2529297e-01  9.2333984e-01  9.2041016e-01
  9.1894531e-01  9.1748047e-01  9.0966797e-01  8.9843750e-01
  8.8671875e-01  8.8037109e-01  8.7792969e-01  8.7304688e-01
  8.6523438e-01  8.6083984e-01  8.4472656e-01  8.4179688e-01
  8.3740234e-01  8.3300781e-01  8.3056641e-01  8.2617188e-01
  8.2519531e-01  8.2128906e-01  8.2080078e-01  8.2031250e-01
  8.1835938e-01  8.1640625e-01  8.0957031e-01  8.0664062e-01
  8.0371094e-01  7.9833984e-01  7.9589844e-01  7.9492188e-01
  7.8613281e-01  7.8417969e-01  7.8173828e-01  7.7441406e-01
  7.6464844e-01  7.6269531e-01  7.6220703e-01  7.5976562e-01
  7.5878906e-01  7.5781250e-01  7.5341797e-01  7.3974609e-01
  7.3730469e-01  7.3095703e-01  7.2705078e-01  7.2216797e-01
  7.1533203e-01  7.0751953e-01  7.0361328e-01  6.9677734e-01
  6.9628906e-01  6.9580078e-01  6.8945312e-01  6.8603516e-01
  6.8408203e-01  6.8164062e-01  6.7041016e-01  6.5820312e-01
  6.4941406e-01  6.4843750e-01  6.4404297e-01  6.4160156e-01
  6.3720703e-01  6.2255859e-01  6.2158203e-01  6.0009766e-01
  5.9960938e-01  5.8935547e-01  5.8105469e-01  5.8056641e-01
  5.7519531e-01  5.6054688e-01  5.5908203e-01  5.3759766e-01
  5.3173828e-01  5.2685547e-01  5.2343750e-01  5.2050781e-01
  5.2001953e-01  5.1855469e-01  5.1757812e-01  5.0146484e-01
  4.9462891e-01  4.9218750e-01  4.8828125e-01  4.8168945e-01
  4.8071289e-01  4.7509766e-01  4.7460938e-01  4.7265625e-01
  4.7192383e-01  4.6557617e-01  4.6142578e-01  4.5092773e-01
  4.4824219e-01  4.4311523e-01  4.3798828e-01  4.3383789e-01
  4.3164062e-01  4.3066406e-01  4.3017578e-01  4.2773438e-01
  4.2553711e-01  3.4765625e-01  3.4252930e-01  2.9589844e-01
  2.9394531e-01  2.6928711e-01  2.6879883e-01  2.6611328e-01
  2.5195312e-01  2.4816895e-01  2.4011230e-01  2.3522949e-01
  2.2766113e-01  2.2656250e-01  2.1789551e-01  2.1130371e-01
  2.0153809e-01  1.9750977e-01  1.9079590e-01  1.8957520e-01
  1.4855957e-01  1.4099121e-01  1.2963867e-01  1.2365723e-01
  1.2347412e-01  1.2274170e-01  1.7883301e-02  1.7715454e-02
  1.3694763e-02  1.1497498e-02 -2.6302338e-03 -7.7896118e-03
 -3.8452148e-02 -3.9947510e-02 -6.3903809e-02 -6.4270020e-02
 -6.5368652e-02 -6.7626953e-02 -6.9580078e-02 -6.9885254e-02
 -1.2744141e-01 -1.2792969e-01 -2.4450684e-01 -2.4572754e-01
 -2.5317383e-01 -2.5390625e-01 -2.6489258e-01 -2.6538086e-01
 -2.6635742e-01 -2.6806641e-01 -3.2250977e-01 -3.2275391e-01
 -3.4350586e-01 -3.4887695e-01 -3.5668945e-01 -3.5791016e-01
 -4.0380859e-01 -4.0844727e-01 -4.1137695e-01 -4.1259766e-01
 -5.0341797e-01 -5.0537109e-01 -5.5371094e-01 -5.5908203e-01
 -6.3085938e-01 -6.3183594e-01 -6.6406250e-01 -6.6503906e-01
 -6.9091797e-01 -6.9335938e-01 -7.1728516e-01 -7.1777344e-01
 -7.5341797e-01 -7.5390625e-01 -7.6220703e-01 -7.6367188e-01
 -7.8222656e-01 -7.8369141e-01 -8.1640625e-01 -8.1738281e-01
 -8.4619141e-01 -8.4716797e-01 -8.4863281e-01 -8.5400391e-01
 -9.0625000e-01 -9.0673828e-01 -1.0253906e+00 -1.0400391e+00
 -1.0869141e+00 -1.0917969e+00 -1.1337891e+00 -1.1367188e+00
 -1.1494141e+00 -1.1562500e+00 -1.1582031e+00 -1.1738281e+00
 -1.1914062e+00 -1.1933594e+00 -1.1972656e+00 -1.2001953e+00
 -1.2050781e+00 -1.2089844e+00 -1.2490234e+00 -1.2519531e+00
 -1.2578125e+00 -1.2587891e+00 -1.2597656e+00 -1.3359375e+00
 -1.3408203e+00 -1.3427734e+00 -1.3476562e+00 -1.3496094e+00
 -1.3505859e+00 -1.3515625e+00 -1.3525391e+00 -1.3554688e+00
 -1.3593750e+00 -1.3828125e+00 -1.3847656e+00 -1.6357422e+00
 -1.6386719e+00 -1.6640625e+00 -1.6650391e+00 -1.6943359e+00
 -1.7011719e+00 -1.9091797e+00 -1.9101562e+00 -2.0449219e+00
 -2.0605469e+00 -2.3183594e+00 -2.3242188e+00 -2.3730469e+00
 -2.3789062e+00 -3.7636719e+00]
