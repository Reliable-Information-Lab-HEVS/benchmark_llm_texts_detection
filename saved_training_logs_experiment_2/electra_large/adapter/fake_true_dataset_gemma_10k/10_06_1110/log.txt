log_loss_steps: 200
eval_steps: 504
check_degradation: 0
----------------Epoch 1/1----------------
Epoch 1/1, Loss after 192 samples: 0.6890
Epoch 1/1, Loss after 392 samples: 0.6711
Mean accuracy: 0.7015, std: 0.0102, lower bound: 0.6810, upper bound: 0.7214 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 496 samples: 0.7012 with eval loss: 0.6423
Best model with eval loss 0.6423012518113659 and eval accuracy 0.7012133468149646 with 496 samples seen is saved
Epoch 1/1, Loss after 592 samples: 0.6494
Epoch 1/1, Loss after 792 samples: 0.5985
Epoch 1/1, Loss after 992 samples: 0.4175
Mean accuracy: 0.8746, std: 0.0075, lower bound: 0.8605, upper bound: 0.8883 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 1000 samples: 0.8746 with eval loss: 0.2880
Best model with eval loss 0.28800713871755906 and eval accuracy 0.8746208291203236 with 1000 samples seen is saved
Epoch 1/1, Loss after 1192 samples: 0.3306
Epoch 1/1, Loss after 1392 samples: 0.2549
Mean accuracy: 0.9039, std: 0.0063, lower bound: 0.8913, upper bound: 0.9161 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 1504 samples: 0.9039 with eval loss: 0.2470
Best model with eval loss 0.24701518325075025 and eval accuracy 0.9039433771486349 with 1504 samples seen is saved
Epoch 1/1, Loss after 1592 samples: 0.3431
Epoch 1/1, Loss after 1792 samples: 0.2885
Epoch 1/1, Loss after 1992 samples: 0.1742
Mean accuracy: 0.8534, std: 0.0080, lower bound: 0.8377, upper bound: 0.8681 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 2008 samples: 0.8534 with eval loss: 0.3838
Epoch 1/1, Loss after 2192 samples: 0.1551
Epoch 1/1, Loss after 2392 samples: 0.2435
Mean accuracy: 0.9432, std: 0.0053, lower bound: 0.9328, upper bound: 0.9535 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 2512 samples: 0.9434 with eval loss: 0.1518
Best model with eval loss 0.1517629132515961 and eval accuracy 0.9433771486349848 with 2512 samples seen is saved
Epoch 1/1, Loss after 2592 samples: 0.2346
Epoch 1/1, Loss after 2792 samples: 0.1788
Epoch 1/1, Loss after 2992 samples: 0.1409
Mean accuracy: 0.9381, std: 0.0056, lower bound: 0.9267, upper bound: 0.9484 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 3016 samples: 0.9383 with eval loss: 0.1531
Epoch 1/1, Loss after 3192 samples: 0.1483
Epoch 1/1, Loss after 3392 samples: 0.2100
Mean accuracy: 0.8652, std: 0.0077, lower bound: 0.8498, upper bound: 0.8802 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 3520 samples: 0.8650 with eval loss: 0.3626
Epoch 1/1, Loss after 3592 samples: 0.1604
Epoch 1/1, Loss after 3792 samples: 0.1682
Epoch 1/1, Loss after 3992 samples: 0.1684
Mean accuracy: 0.9307, std: 0.0058, lower bound: 0.9186, upper bound: 0.9419 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 4024 samples: 0.9307 with eval loss: 0.1783
Epoch 1/1, Loss after 4192 samples: 0.2311
Epoch 1/1, Loss after 4392 samples: 0.2502
Mean accuracy: 0.8933, std: 0.0069, lower bound: 0.8797, upper bound: 0.9065 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 4528 samples: 0.8933 with eval loss: 0.2538
Epoch 1/1, Loss after 4592 samples: 0.1355
Epoch 1/1, Loss after 4792 samples: 0.1069
Epoch 1/1, Loss after 4992 samples: 0.1883
Mean accuracy: 0.9361, std: 0.0055, lower bound: 0.9257, upper bound: 0.9464 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 5032 samples: 0.9363 with eval loss: 0.1615
Epoch 1/1, Loss after 5192 samples: 0.1691
Epoch 1/1, Loss after 5392 samples: 0.1044
Mean accuracy: 0.9514, std: 0.0049, lower bound: 0.9414, upper bound: 0.9606 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 5536 samples: 0.9515 with eval loss: 0.1259
Best model with eval loss 0.12588101030597765 and eval accuracy 0.9514661274014156 with 5536 samples seen is saved
Epoch 1/1, Loss after 5592 samples: 0.1298
Epoch 1/1, Loss after 5792 samples: 0.1213
Epoch 1/1, Loss after 5992 samples: 0.1306
Mean accuracy: 0.8986, std: 0.0069, lower bound: 0.8847, upper bound: 0.9115 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 6040 samples: 0.8984 with eval loss: 0.3044
Epoch 1/1, Loss after 6192 samples: 0.1591
Epoch 1/1, Loss after 6392 samples: 0.1549
Mean accuracy: 0.9597, std: 0.0045, lower bound: 0.9509, upper bound: 0.9681 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 6544 samples: 0.9596 with eval loss: 0.1129
Best model with eval loss 0.11292359274962256 and eval accuracy 0.9595551061678463 with 6544 samples seen is saved
Epoch 1/1, Loss after 6592 samples: 0.1550
Epoch 1/1, Loss after 6792 samples: 0.1642
Epoch 1/1, Loss after 6992 samples: 0.1241
Mean accuracy: 0.9531, std: 0.0047, lower bound: 0.9444, upper bound: 0.9621 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 7048 samples: 0.9530 with eval loss: 0.1272
Epoch 1/1, Loss after 7192 samples: 0.1444
Epoch 1/1, Loss after 7392 samples: 0.1633
Mean accuracy: 0.9658, std: 0.0043, lower bound: 0.9575, upper bound: 0.9737 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 7552 samples: 0.9656 with eval loss: 0.0956
Best model with eval loss 0.09556810888311555 and eval accuracy 0.9656218402426694 with 7552 samples seen is saved
Epoch 1/1, Loss after 7592 samples: 0.0936
Epoch 1/1, Loss after 7792 samples: 0.0705
Epoch 1/1, Loss after 7992 samples: 0.1876
Mean accuracy: 0.9438, std: 0.0052, lower bound: 0.9338, upper bound: 0.9540 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 8056 samples: 0.9439 with eval loss: 0.1432
Epoch 1/1, Loss after 8192 samples: 0.0896
Epoch 1/1, Loss after 8392 samples: 0.1354
Mean accuracy: 0.9295, std: 0.0059, lower bound: 0.9181, upper bound: 0.9408 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 8560 samples: 0.9292 with eval loss: 0.1856
Epoch 1/1, Loss after 8592 samples: 0.0563
Epoch 1/1, Loss after 8792 samples: 0.1141
Epoch 1/1, Loss after 8992 samples: 0.0871
Mean accuracy: 0.9596, std: 0.0043, lower bound: 0.9515, upper bound: 0.9681 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 9064 samples: 0.9596 with eval loss: 0.1053
Epoch 1/1, Loss after 9192 samples: 0.0653
Epoch 1/1, Loss after 9392 samples: 0.0825
Mean accuracy: 0.9135, std: 0.0063, lower bound: 0.9009, upper bound: 0.9257 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 9568 samples: 0.9135 with eval loss: 0.2607
Epoch 1/1, Loss after 9592 samples: 0.0740
Epoch 1/1, Loss after 9792 samples: 0.1765
Epoch 1/1, Loss after 9992 samples: 0.1113
Mean accuracy: 0.9559, std: 0.0045, lower bound: 0.9469, upper bound: 0.9646 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 10072 samples: 0.9560 with eval loss: 0.1214
Epoch 1/1, Loss after 10192 samples: 0.0821
Epoch 1/1, Loss after 10392 samples: 0.1107
Mean accuracy: 0.9284, std: 0.0060, lower bound: 0.9161, upper bound: 0.9408 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 10576 samples: 0.9282 with eval loss: 0.1876
Epoch 1/1, Loss after 10592 samples: 0.1184
Epoch 1/1, Loss after 10792 samples: 0.0839
Epoch 1/1, Loss after 10992 samples: 0.0509
Mean accuracy: 0.9279, std: 0.0059, lower bound: 0.9171, upper bound: 0.9398 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 11080 samples: 0.9277 with eval loss: 0.2096
Epoch 1/1, Loss after 11192 samples: 0.0891
Epoch 1/1, Loss after 11392 samples: 0.0909
Mean accuracy: 0.9291, std: 0.0057, lower bound: 0.9186, upper bound: 0.9403 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 11584 samples: 0.9292 with eval loss: 0.1968
Epoch 1/1, Loss after 11592 samples: 0.1249
Epoch 1/1, Loss after 11792 samples: 0.1236
Epoch 1/1, Loss after 11992 samples: 0.0935
Mean accuracy: 0.9481, std: 0.0050, lower bound: 0.9383, upper bound: 0.9575 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 12088 samples: 0.9479 with eval loss: 0.1452
Epoch 1/1, Loss after 12192 samples: 0.1012
Epoch 1/1, Loss after 12392 samples: 0.0707
Epoch 1/1, Loss after 12592 samples: 0.0982
Mean accuracy: 0.9448, std: 0.0050, lower bound: 0.9348, upper bound: 0.9540 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 12592 samples: 0.9449 with eval loss: 0.1522
Epoch 1/1, Loss after 12792 samples: 0.1356
Epoch 1/1, Loss after 12992 samples: 0.0682
Mean accuracy: 0.9202, std: 0.0060, lower bound: 0.9085, upper bound: 0.9317 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 13096 samples: 0.9201 with eval loss: 0.2284
Epoch 1/1, Loss after 13192 samples: 0.0513
Epoch 1/1, Loss after 13392 samples: 0.0977
Epoch 1/1, Loss after 13592 samples: 0.1016
Mean accuracy: 0.9420, std: 0.0052, lower bound: 0.9312, upper bound: 0.9515 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 13600 samples: 0.9419 with eval loss: 0.1652
Epoch 1/1, Loss after 13792 samples: 0.0764
Epoch 1/1, Loss after 13992 samples: 0.0462
Mean accuracy: 0.9576, std: 0.0046, lower bound: 0.9489, upper bound: 0.9661 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 14104 samples: 0.9575 with eval loss: 0.1243
Epoch 1/1, Loss after 14192 samples: 0.1209
Epoch 1/1, Loss after 14392 samples: 0.1254
Epoch 1/1, Loss after 14592 samples: 0.1158
Mean accuracy: 0.9567, std: 0.0045, lower bound: 0.9474, upper bound: 0.9656 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 14608 samples: 0.9565 with eval loss: 0.1257
Epoch 1/1, Loss after 14792 samples: 0.0667
Epoch 1/1, Loss after 14992 samples: 0.0712
Mean accuracy: 0.9466, std: 0.0051, lower bound: 0.9363, upper bound: 0.9560 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 15112 samples: 0.9464 with eval loss: 0.1562
Epoch 1/1, Loss after 15192 samples: 0.0636
Epoch 1/1, Loss after 15392 samples: 0.0930
Epoch 1/1, Loss after 15592 samples: 0.0819
Mean accuracy: 0.9551, std: 0.0048, lower bound: 0.9449, upper bound: 0.9641 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 15616 samples: 0.9550 with eval loss: 0.1338
Epoch 1/1, Loss after 15792 samples: 0.1122
Training signal is False, stopping training
Training finished
Best model: {'eval_acc': 0.9656218402426694, 'nb_samples': 7552, 'eval_loss': 0.09556810888311555}
Training loss logs: [{'samples': 192, 'loss': 0.689013671875}, {'samples': 392, 'loss': 0.67106689453125}, {'samples': 592, 'loss': 0.649371337890625}, {'samples': 792, 'loss': 0.5984872436523437}, {'samples': 992, 'loss': 0.41745079040527344}, {'samples': 1192, 'loss': 0.3305929613113403}, {'samples': 1392, 'loss': 0.25490499019622803}, {'samples': 1592, 'loss': 0.34310184955596923}, {'samples': 1792, 'loss': 0.2884877610206604}, {'samples': 1992, 'loss': 0.17424435734748842}, {'samples': 2192, 'loss': 0.15505513846874236}, {'samples': 2392, 'loss': 0.24352351307868958}, {'samples': 2592, 'loss': 0.2346048855781555}, {'samples': 2792, 'loss': 0.17880432605743407}, {'samples': 2992, 'loss': 0.14093969166278839}, {'samples': 3192, 'loss': 0.1483074003458023}, {'samples': 3392, 'loss': 0.20997466564178466}, {'samples': 3592, 'loss': 0.16036103367805482}, {'samples': 3792, 'loss': 0.16815114498138428}, {'samples': 3992, 'loss': 0.16838725447654723}, {'samples': 4192, 'loss': 0.23111214518547057}, {'samples': 4392, 'loss': 0.25024973511695864}, {'samples': 4592, 'loss': 0.13550667643547057}, {'samples': 4792, 'loss': 0.10685275256633758}, {'samples': 4992, 'loss': 0.18825413882732392}, {'samples': 5192, 'loss': 0.16914989531040192}, {'samples': 5392, 'loss': 0.10441250383853912}, {'samples': 5592, 'loss': 0.12984548330307008}, {'samples': 5792, 'loss': 0.12129811525344848}, {'samples': 5992, 'loss': 0.13059282779693604}, {'samples': 6192, 'loss': 0.1590990275144577}, {'samples': 6392, 'loss': 0.1549163717031479}, {'samples': 6592, 'loss': 0.15495981395244598}, {'samples': 6792, 'loss': 0.16418614983558655}, {'samples': 6992, 'loss': 0.1240588092803955}, {'samples': 7192, 'loss': 0.14435831010341643}, {'samples': 7392, 'loss': 0.16327156007289886}, {'samples': 7592, 'loss': 0.09361712396144867}, {'samples': 7792, 'loss': 0.07051213204860687}, {'samples': 7992, 'loss': 0.18763997316360473}, {'samples': 8192, 'loss': 0.0895789235830307}, {'samples': 8392, 'loss': 0.13543859720230103}, {'samples': 8592, 'loss': 0.05633294641971588}, {'samples': 8792, 'loss': 0.11408794403076172}, {'samples': 8992, 'loss': 0.08707394242286683}, {'samples': 9192, 'loss': 0.06530946552753449}, {'samples': 9392, 'loss': 0.08248388826847076}, {'samples': 9592, 'loss': 0.07404381394386292}, {'samples': 9792, 'loss': 0.17653946876525878}, {'samples': 9992, 'loss': 0.11126901805400849}, {'samples': 10192, 'loss': 0.0820880764722824}, {'samples': 10392, 'loss': 0.11072184503078461}, {'samples': 10592, 'loss': 0.11842487633228302}, {'samples': 10792, 'loss': 0.08389948070049286}, {'samples': 10992, 'loss': 0.050920591950416566}, {'samples': 11192, 'loss': 0.08911890983581543}, {'samples': 11392, 'loss': 0.09088272333145142}, {'samples': 11592, 'loss': 0.12487115025520325}, {'samples': 11792, 'loss': 0.12362059473991394}, {'samples': 11992, 'loss': 0.09348142683506012}, {'samples': 12192, 'loss': 0.10115957200527191}, {'samples': 12392, 'loss': 0.0706681990623474}, {'samples': 12592, 'loss': 0.0982185059785843}, {'samples': 12792, 'loss': 0.13556700050830842}, {'samples': 12992, 'loss': 0.06818516492843628}, {'samples': 13192, 'loss': 0.05130596101284027}, {'samples': 13392, 'loss': 0.09773244142532349}, {'samples': 13592, 'loss': 0.10156580984592438}, {'samples': 13792, 'loss': 0.07639941275119781}, {'samples': 13992, 'loss': 0.0461975371837616}, {'samples': 14192, 'loss': 0.12090822994709015}, {'samples': 14392, 'loss': 0.12535577058792113}, {'samples': 14592, 'loss': 0.11583593904972077}, {'samples': 14792, 'loss': 0.06672626674175262}, {'samples': 14992, 'loss': 0.07116279542446137}, {'samples': 15192, 'loss': 0.0635974657535553}, {'samples': 15392, 'loss': 0.09296992182731628}, {'samples': 15592, 'loss': 0.08189435422420502}, {'samples': 15792, 'loss': 0.11222169637680053}]
Evaluation accuracy logs: [{'samples': 496, 'accuracy': 0.7014863498483317, 'std': 0.010231918674592012, 'lower_bound': 0.6809908998988877, 'upper_bound': 0.7214357937310415}, {'samples': 1000, 'accuracy': 0.8746304347826087, 'std': 0.007543485003489262, 'lower_bound': 0.8604651162790697, 'upper_bound': 0.888283619817998}, {'samples': 1504, 'accuracy': 0.9038513650151667, 'std': 0.006338512919156162, 'lower_bound': 0.8912917087967643, 'upper_bound': 0.916076845298281}, {'samples': 2008, 'accuracy': 0.853447927199191, 'std': 0.007995248153985305, 'lower_bound': 0.8377148634984833, 'upper_bound': 0.8680611729019212}, {'samples': 2512, 'accuracy': 0.9431921132457026, 'std': 0.005290403435914305, 'lower_bound': 0.9327603640040445, 'upper_bound': 0.9534883720930233}, {'samples': 3016, 'accuracy': 0.9381435793731041, 'std': 0.0055612670274229475, 'lower_bound': 0.9266936299292214, 'upper_bound': 0.948432760364004}, {'samples': 3520, 'accuracy': 0.8651769464105157, 'std': 0.007666241342206672, 'lower_bound': 0.8498483316481295, 'upper_bound': 0.8801820020222447}, {'samples': 4024, 'accuracy': 0.9306663296258847, 'std': 0.005826948066593775, 'lower_bound': 0.9186046511627907, 'upper_bound': 0.9418604651162791}, {'samples': 4528, 'accuracy': 0.893309403437816, 'std': 0.006948199324312182, 'lower_bound': 0.8796638018200202, 'upper_bound': 0.9064711830131446}, {'samples': 5032, 'accuracy': 0.9361122345803842, 'std': 0.005530411752426577, 'lower_bound': 0.925669868554095, 'upper_bound': 0.9464105156723963}, {'samples': 5536, 'accuracy': 0.9514484327603641, 'std': 0.004856066389588135, 'lower_bound': 0.9413549039433772, 'upper_bound': 0.9605662285136501}, {'samples': 6040, 'accuracy': 0.8986208291203235, 'std': 0.0068712294833110435, 'lower_bound': 0.884732052578362, 'upper_bound': 0.9115267947421638}, {'samples': 6544, 'accuracy': 0.9597052578361981, 'std': 0.004515846922530171, 'lower_bound': 0.9509479271991911, 'upper_bound': 0.968149646107179}, {'samples': 7048, 'accuracy': 0.9531324570273003, 'std': 0.0046769842003912205, 'lower_bound': 0.9443882709807887, 'upper_bound': 0.962082912032356}, {'samples': 7552, 'accuracy': 0.9657724974721941, 'std': 0.0042659057645035685, 'lower_bound': 0.9575328614762386, 'upper_bound': 0.9737108190091001}, {'samples': 8056, 'accuracy': 0.9438397371081901, 'std': 0.005181392230268503, 'lower_bound': 0.9337714863498483, 'upper_bound': 0.9539939332659252}, {'samples': 8560, 'accuracy': 0.929463599595551, 'std': 0.005876777530495199, 'lower_bound': 0.9180990899898888, 'upper_bound': 0.9408493427704753}, {'samples': 9064, 'accuracy': 0.959566734074823, 'std': 0.004342369391908409, 'lower_bound': 0.9514661274014156, 'upper_bound': 0.968149646107179}, {'samples': 9568, 'accuracy': 0.9134817997977754, 'std': 0.006336222826694522, 'lower_bound': 0.9009100101112234, 'upper_bound': 0.9256825075834176}, {'samples': 10072, 'accuracy': 0.955896865520728, 'std': 0.004463114650780731, 'lower_bound': 0.9469160768452983, 'upper_bound': 0.9646107178968655}, {'samples': 10576, 'accuracy': 0.9284256825075834, 'std': 0.0059768904386312514, 'lower_bound': 0.916076845298281, 'upper_bound': 0.9408493427704753}, {'samples': 11080, 'accuracy': 0.9278574317492416, 'std': 0.005865485693324678, 'lower_bound': 0.917087967644085, 'upper_bound': 0.9398382204246714}, {'samples': 11584, 'accuracy': 0.9290591506572295, 'std': 0.005678691181622698, 'lower_bound': 0.9186046511627907, 'upper_bound': 0.9403437815975733}, {'samples': 12088, 'accuracy': 0.9480894843276038, 'std': 0.005047565867590529, 'lower_bound': 0.9383215369059656, 'upper_bound': 0.9575328614762386}, {'samples': 12592, 'accuracy': 0.9448083923154702, 'std': 0.005038353733873593, 'lower_bound': 0.9347699696663296, 'upper_bound': 0.9539939332659252}, {'samples': 13096, 'accuracy': 0.9201830131445905, 'std': 0.005988352694295926, 'lower_bound': 0.9084807886754297, 'upper_bound': 0.9317492416582407}, {'samples': 13600, 'accuracy': 0.9420166835187058, 'std': 0.005161963252684602, 'lower_bound': 0.9312436804853387, 'upper_bound': 0.9514661274014156}, {'samples': 14104, 'accuracy': 0.9575722952477249, 'std': 0.004604713663670947, 'lower_bound': 0.9489383215369059, 'upper_bound': 0.9661274014155713}, {'samples': 14608, 'accuracy': 0.956665318503539, 'std': 0.00452761980333678, 'lower_bound': 0.9474216380182002, 'upper_bound': 0.9656218402426694}, {'samples': 15112, 'accuracy': 0.9466137512639029, 'std': 0.0051326883890290146, 'lower_bound': 0.9362992922143579, 'upper_bound': 0.9560288169868555}, {'samples': 15616, 'accuracy': 0.9550596562184024, 'std': 0.004826251660944385, 'lower_bound': 0.9448938321536906, 'upper_bound': 0.9641051567239636}]
Evaluating the best model on the test set of dataset ./fake_true_datasets/fake_true_dataset_gemma_10k...
Test metrics:
accuracy: 0.9476870576339738
precision: 0.9083070630348098
recall: 0.9959267202041754
f1_score: 0.9500799193779251
fp_rate: 0.10057272852911904
tp_rate: 0.9959267202041754
std_accuracy: 0.004835432299329893
std_precision: 0.008415339692227842
std_recall: 0.0020395942879471492
std_f1_score: 0.0047189558239598564
std_fp_rate: 0.009267007072404779
std_tp_rate: 0.0020395942879471492
TP: 985.105
TN: 889.42
FP: 99.444
FN: 4.031
roc_auc: 0.9961911665325661
fpr: [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00101112 0.00101112 0.00101112
 0.00101112 0.00101112 0.00101112 0.00101112 0.00101112 0.00101112
 0.00101112 0.00101112 0.00101112 0.00101112 0.00101112 0.00101112
 0.00101112 0.00101112 0.00101112 0.00101112 0.00101112 0.00101112
 0.00101112 0.00101112 0.00101112 0.00101112 0.00101112 0.00101112
 0.00101112 0.00202224 0.00202224 0.00202224 0.00202224 0.00202224
 0.00202224 0.00202224 0.00202224 0.00202224 0.00202224 0.00202224
 0.00202224 0.00202224 0.00202224 0.00202224 0.00202224 0.00202224
 0.00202224 0.00303337 0.00303337 0.00303337 0.00303337 0.00303337
 0.00303337 0.00303337 0.00303337 0.00303337 0.00303337 0.00303337
 0.00303337 0.00303337 0.00404449 0.00505561 0.00505561 0.00505561
 0.00505561 0.00505561 0.00505561 0.00505561 0.00505561 0.00505561
 0.00505561 0.00505561 0.00606673 0.00606673 0.00606673 0.00606673
 0.00606673 0.00606673 0.00707786 0.00707786 0.0091001  0.0091001
 0.0091001  0.0091001  0.0091001  0.0091001  0.0091001  0.01011122
 0.01011122 0.01011122 0.01011122 0.01011122 0.01112235 0.01112235
 0.01112235 0.01112235 0.01213347 0.01314459 0.01314459 0.01415571
 0.01415571 0.01516684 0.01516684 0.01617796 0.01617796 0.01718908
 0.01718908 0.0182002  0.0182002  0.01921132 0.01921132 0.02022245
 0.02022245 0.02325581 0.02325581 0.02527806 0.02527806 0.02628918
 0.02628918 0.0273003  0.0273003  0.02831143 0.02831143 0.03033367
 0.03033367 0.03235592 0.03235592 0.03437816 0.03437816 0.0364004
 0.0364004  0.03842265 0.03842265 0.03943377 0.03943377 0.04044489
 0.04044489 0.04550051 0.04550051 0.04853387 0.04853387 0.05055612
 0.05055612 0.06167846 0.06167846 0.06673407 0.06673407 0.07280081
 0.07280081 0.07482305 0.07482305 0.08796764 0.08796764 0.09605662
 0.09605662 0.10515672 0.10515672 0.11526795 0.11526795 0.11830131
 0.11830131 0.14054601 0.14054601 0.16380182 0.16582406 0.239636
 0.24165824 0.25278059 0.25682508 0.26794742 0.26996967 0.27199191
 0.27401416 0.2760364  0.27805865 0.27906977 0.28109201 0.28210313
 0.2851365  0.30637007 0.30839232 0.31041456 0.3124368  0.31344793
 0.31547017 0.34074823 0.3437816  0.34782609 0.35894843 0.36097068
 0.36501517 0.36804853 0.38422649 0.38624874 0.39737108 0.39939333
 0.40444894 0.40647118 0.41253792 0.41354904 0.41557128 0.41860465
 0.4206269  0.42871587 0.43276036 0.43579373 0.43781598 0.44388271
 0.44691608 0.44893832 0.45399393 0.4570273  0.46006067 0.46208291
 0.46309403 0.46713852 0.46814965 0.47017189 0.47118301 0.47320526
 0.47927199 0.48129424 0.48230536 0.48533873 0.49241658 0.49443883
 0.49949444 0.50151668 0.50252781 0.50455005 0.5065723  0.50960566
 0.51061678 0.51263903 0.51668352 0.51870576 0.52173913 0.52376138
 0.52578362 0.52780586 0.52982811 0.53589484 0.54802831 0.55409505
 0.56622851 0.56825076 0.570273   0.57229525 0.57431749 0.57633974
 0.58038423 0.58240647 0.58341759 0.58645096 0.58847321 0.59150657
 0.59555106 0.59858443 0.60364004 0.60566229 0.60970677 0.61274014
 0.61678463 0.61880688 0.62082912 0.62285137 0.62386249 0.63195147
 0.63296259 0.63599596 0.63700708 0.64307381 0.64408493 0.64610718
 0.6471183  0.65015167 0.65217391 0.65419616 0.65520728 0.65722952
 0.66127401 0.66329626 0.6653185  0.66835187 0.67037412 0.67239636
 0.67340748 0.67542973 0.67745197 0.67947422 0.68250758 0.68452983
 0.68554095 0.68857432 0.68958544 0.69160768 0.69464105 0.70070779
 0.70171891 0.70475228 0.7057634  0.70778564 0.71284125 0.71688574
 0.71890799 0.72093023 0.72800809 0.73003033 0.73407482 0.73609707
 0.73710819 0.73913043 0.74014156 0.7421638  0.7512639  0.75530839
 0.75733064 0.75935288 0.76744186 0.77047523 0.77249747 0.77755308
 0.77957533 0.78361982 0.78665319 0.78867543 0.7917088  0.79373104
 0.80182002 0.80384226 0.81193124 0.81496461 0.81597573 0.81799798
 0.8190091  0.82406471 0.82507583 0.82709808 0.83114257 0.83215369
 0.83518706 0.83822042 0.84327604 0.84529828 0.8463094  0.84934277
 0.85136502 0.85237614 0.85642063 0.85743175 0.86147624 0.86248736
 0.86450961 0.86552073 0.87057634 0.87360971 0.87563195 0.87866532
 0.89484328 0.89787664 0.89989889 0.90293225 0.90394338 0.90697674
 0.90899899 0.91911021 0.92214358 0.92416582 0.92922144 0.93023256
 0.9322548  0.93326593 0.93528817 0.93629929 0.93832154 0.9413549
 0.94742164 0.94843276 0.95247725 0.95753286 0.96056623 0.9635996
 0.96562184 0.96865521 0.97067745 0.9726997  0.97573306 0.97775531
 0.97977755 0.98382204 0.98584429 1.        ]
tpr: [0.         0.00101112 0.00606673 0.00808898 0.01011122 0.01617796
 0.01718908 0.02022245 0.02224469 0.02325581 0.02628918 0.0273003
 0.03033367 0.03437816 0.03741153 0.03943377 0.04145602 0.04651163
 0.04853387 0.05358948 0.05662285 0.0586451  0.06471183 0.06673407
 0.07077856 0.07178969 0.07482305 0.0768453  0.07886754 0.07987867
 0.08291203 0.08493428 0.08796764 0.08998989 0.09403438 0.09605662
 0.09807887 0.10212336 0.1041456  0.10819009 0.10920121 0.11223458
 0.1132457  0.11729019 0.12032356 0.12335693 0.12537917 0.12740142
 0.13043478 0.13447927 0.13650152 0.13751264 0.13953488 0.14661274
 0.14863498 0.15065723 0.15470172 0.15672396 0.16076845 0.1627907
 0.16683519 0.17087968 0.1718908  0.17391304 0.17492417 0.17694641
 0.17896866 0.18200202 0.18503539 0.18806876 0.19211325 0.19615774
 0.19716886 0.20020222 0.20424671 0.21334681 0.21941355 0.22244692
 0.22446916 0.22548028 0.22952477 0.23356926 0.23761375 0.239636
 0.24064712 0.24469161 0.24570273 0.2487361  0.24974722 0.25176946
 0.25278059 0.2578362  0.25985844 0.26289181 0.2669363  0.27098079
 0.2760364  0.27704752 0.28109201 0.28311426 0.28715875 0.28816987
 0.29019211 0.29322548 0.29625885 0.30030334 0.30637007 0.30940344
 0.31344793 0.31547017 0.31749242 0.31850354 0.32254803 0.32659252
 0.32861476 0.33164813 0.33367037 0.33771486 0.34175935 0.3437816
 0.34681496 0.34883721 0.35085945 0.35389282 0.35591507 0.35692619
 0.36097068 0.36602629 0.3710819  0.37310415 0.37613751 0.37917088
 0.38119312 0.38321537 0.38523761 0.38725986 0.38827098 0.39231547
 0.39332659 0.39534884 0.39635996 0.39939333 0.40141557 0.40343782
 0.40444894 0.40647118 0.40950455 0.41152679 0.41354904 0.41759353
 0.41961577 0.42163802 0.42467139 0.42669363 0.42871587 0.43174924
 0.43478261 0.43781598 0.44084934 0.4479272  0.45096057 0.45298281
 0.4570273  0.45803842 0.46006067 0.46107179 0.46309403 0.4661274
 0.47421638 0.4752275  0.47927199 0.48736097 0.48938322 0.49544995
 0.49747219 0.49949444 0.50151668 0.50353893 0.5065723  0.50960566
 0.5156724  0.52173913 0.52275025 0.52578362 0.52780586 0.53083923
 0.53185035 0.5338726  0.53791709 0.53892821 0.54095046 0.5429727
 0.54499494 0.54600607 0.55611729 0.55712841 0.55915066 0.5611729
 0.56420627 0.570273   0.57128413 0.57431749 0.57633974 0.57836198
 0.58240647 0.59656218 0.59959555 0.60262892 0.60465116 0.60667341
 0.60869565 0.61172902 0.61476239 0.61678463 0.619818   0.62184024
 0.62487361 0.6289181  0.63195147 0.63296259 0.63397371 0.6380182
 0.63902932 0.64307381 0.65015167 0.65217391 0.66026289 0.66228514
 0.67138524 0.67340748 0.67947422 0.68149646 0.68452983 0.68655207
 0.69059656 0.69261881 0.69362993 0.6966633  0.69868554 0.69969666
 0.70273003 0.70374115 0.7057634  0.70879676 0.71081901 0.71890799
 0.72093023 0.72497472 0.72598584 0.72800809 0.73104146 0.73508595
 0.73811931 0.74014156 0.74418605 0.74620829 0.75733064 0.76137513
 0.76440849 0.76643074 0.77047523 0.77350859 0.77755308 0.78361982
 0.78766431 0.78766431 0.78867543 0.79069767 0.79474216 0.79676441
 0.79777553 0.79979778 0.80586451 0.809909   0.81092012 0.81294237
 0.82204247 0.82406471 0.82406471 0.82507583 0.82709808 0.82912032
 0.83114257 0.83215369 0.83417594 0.83822042 0.84024267 0.8554095
 0.85743175 0.86046512 0.86046512 0.86147624 0.86349848 0.86653185
 0.8685541  0.86956522 0.86956522 0.87360971 0.87360971 0.87563195
 0.88270981 0.88473205 0.8958544  0.89787664 0.89888777 0.89888777
 0.90091001 0.90192113 0.90394338 0.91304348 0.91304348 0.91708797
 0.91911021 0.92113246 0.92214358 0.92214358 0.92618807 0.92618807
 0.92821031 0.92821031 0.93124368 0.93124368 0.94034378 0.94034378
 0.94236603 0.94236603 0.94438827 0.94438827 0.94742164 0.94742164
 0.95449949 0.95449949 0.95652174 0.95652174 0.96056623 0.96056623
 0.9635996  0.9635996  0.97067745 0.97067745 0.97168857 0.97168857
 0.9726997  0.9726997  0.97371082 0.97371082 0.97472194 0.97472194
 0.97573306 0.97573306 0.97876643 0.97876643 0.97977755 0.97977755
 0.98281092 0.98281092 0.98382204 0.98382204 0.98685541 0.98685541
 0.98887765 0.98887765 0.98988878 0.98988878 0.9908999  0.9908999
 0.99191102 0.99191102 0.99393327 0.99393327 0.99494439 0.99494439
 0.99595551 0.99595551 0.99696663 0.99696663 0.99797776 0.99797776
 0.99898888 0.99898888 1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.        ]
thresholds: [        inf  5.4023438   5.2421875   5.234375    5.21875     5.171875
  5.1679688   5.15625     5.140625    5.1328125   5.1289062   5.1210938
  5.1171875   5.0859375   5.0820312   5.0664062   5.0585938   5.0234375
  5.015625    5.0078125   5.0039062   5.          4.9648438   4.9492188
  4.9453125   4.9414062   4.9257812   4.9101562   4.90625     4.9023438
  4.8984375   4.8945312   4.875       4.8710938   4.8671875   4.8515625
  4.8476562   4.796875    4.7851562   4.7773438   4.7734375   4.765625
  4.7617188   4.7539062   4.7460938   4.7226562   4.7148438   4.7070312
  4.703125    4.6601562   4.6523438   4.6484375   4.6367188   4.59375
  4.5898438   4.578125    4.5664062   4.5507812   4.5234375   4.515625
  4.4648438   4.4609375   4.453125    4.4492188   4.4414062   4.4375
  4.4296875   4.4257812   4.4140625   4.40625     4.3828125   4.359375
  4.3515625   4.34375     4.3242188   4.2734375   4.2539062   4.2421875
  4.2382812   4.234375    4.21875     4.2070312   4.203125    4.1953125
  4.1914062   4.1796875   4.1640625   4.1601562   4.15625     4.1523438
  4.1484375   4.140625    4.1367188   4.1289062   4.1210938   4.1132812
  4.109375    4.1054688   4.1015625   4.0820312   4.0742188   4.0703125
  4.0664062   4.0507812   4.046875    4.0234375   4.0117188   4.0039062
  3.9980469   3.9902344   3.9882812   3.9863281   3.9804688   3.96875
  3.9570312   3.9492188   3.9453125   3.9355469   3.9316406   3.9277344
  3.9257812   3.9238281   3.9082031   3.90625     3.9003906   3.8964844
  3.8828125   3.8691406   3.8652344   3.8613281   3.8554688   3.8535156
  3.84375     3.8417969   3.8300781   3.828125    3.8261719   3.8203125
  3.8183594   3.8164062   3.8125      3.8027344   3.7949219   3.7929688
  3.7910156   3.7890625   3.765625    3.7636719   3.7460938   3.7363281
  3.7304688   3.7285156   3.7265625   3.7226562   3.7167969   3.7050781
  3.703125    3.6933594   3.6914062   3.6464844   3.6445312   3.6386719
  3.6367188   3.6328125   3.6269531   3.6210938   3.6191406   3.6171875
  3.6015625   3.5996094   3.5976562   3.5664062   3.5644531   3.5585938
  3.5546875   3.5527344   3.546875    3.5449219   3.5390625   3.5371094
  3.53125     3.5175781   3.515625    3.5136719   3.5039062   3.4980469
  3.4824219   3.4804688   3.4726562   3.4707031   3.46875     3.4648438
  3.4609375   3.4589844   3.4453125   3.4414062   3.4375      3.4277344
  3.4257812   3.4140625   3.4101562   3.40625     3.4042969   3.3945312
  3.3867188   3.3417969   3.3398438   3.328125    3.3261719   3.3203125
  3.3183594   3.3164062   3.2988281   3.296875    3.2792969   3.2773438
  3.2675781   3.265625    3.2558594   3.25        3.2421875   3.2226562
  3.21875     3.1992188   3.1835938   3.1796875   3.1464844   3.140625
  3.1132812   3.1054688   3.0820312   3.0800781   3.0585938   3.0546875
  3.046875    3.0449219   3.0429688   3.0410156   3.0390625   3.03125
  3.0273438   3.0234375   3.0214844   3.0058594   3.0019531   2.9628906
  2.953125    2.9511719   2.9492188   2.9375      2.9257812   2.9160156
  2.9082031   2.8964844   2.8828125   2.8789062   2.828125    2.8203125
  2.8105469   2.8046875   2.7832031   2.7792969   2.7539062   2.7226562
  2.7128906   2.7070312   2.7050781   2.7011719   2.6835938   2.6816406
  2.6796875   2.6757812   2.6464844   2.640625    2.6386719   2.6328125
  2.5722656   2.5703125   2.5625      2.5546875   2.5507812   2.5410156
  2.5371094   2.5351562   2.5332031   2.5         2.4980469   2.4082031
  2.4023438   2.3769531   2.3652344   2.3457031   2.34375     2.3222656
  2.3183594   2.2988281   2.296875    2.2675781   2.2578125   2.2558594
  2.2265625   2.2207031   2.1347656   2.1269531   2.1152344   2.0917969
  2.0878906   2.0859375   2.0546875   2.0136719   2.0058594   1.9521484
  1.9492188   1.9404297   1.9248047   1.8964844   1.8632812   1.8623047
  1.8554688   1.8486328   1.8330078   1.8320312   1.7929688   1.7841797
  1.7421875   1.7373047   1.7285156   1.7197266   1.6669922   1.6337891
  1.5625      1.5029297   1.4882812   1.4667969   1.4238281   1.4189453
  1.3974609   1.390625    1.2851562   1.2822266   1.2802734   1.2773438
  1.2568359   1.2431641   1.2216797   1.1982422   1.1962891   1.1699219
  1.1660156   1.1484375   1.1308594   1.1152344   1.1132812   1.1123047
  1.0166016   0.93603516  0.91308594  0.86279297  0.7963867   0.7866211
  0.7729492   0.6279297   0.625       0.5644531   0.5629883   0.50341797
  0.48120117  0.4272461   0.40185547  0.07513428  0.07177734 -0.05990601
 -0.06866455 -0.22814941 -0.23376465 -0.43310547 -0.43725586 -0.4951172
 -0.49902344 -0.8203125  -0.8232422  -1.1386719  -1.171875   -1.7802734
 -1.8115234  -1.8720703  -1.8837891  -1.9765625  -1.9873047  -2.0097656
 -2.0136719  -2.0566406  -2.0625     -2.0683594  -2.0722656  -2.0742188
 -2.0800781  -2.265625   -2.2773438  -2.28125    -2.2851562  -2.2910156
 -2.3027344  -2.4726562  -2.4746094  -2.4863281  -2.6132812  -2.6171875
 -2.6464844  -2.6523438  -2.7617188  -2.765625   -2.8417969  -2.8515625
 -2.8867188  -2.9023438  -2.9121094  -2.9277344  -2.9296875  -2.953125
 -2.9589844  -3.0039062  -3.0195312  -3.0214844  -3.0234375  -3.0585938
 -3.0703125  -3.0742188  -3.1113281  -3.1132812  -3.125      -3.1269531
 -3.1308594  -3.1425781  -3.1503906  -3.1542969  -3.1582031  -3.1601562
 -3.1796875  -3.1894531  -3.2050781  -3.2246094  -3.25       -3.2519531
 -3.2734375  -3.2753906  -3.2792969  -3.2871094  -3.2988281  -3.3027344
 -3.3046875  -3.3066406  -3.3242188  -3.3261719  -3.3339844  -3.3359375
 -3.3398438  -3.3417969  -3.3535156  -3.359375   -3.421875   -3.4335938
 -3.484375   -3.4882812  -3.4941406  -3.4960938  -3.5058594  -3.5136719
 -3.5332031  -3.5351562  -3.5371094  -3.5390625  -3.5429688  -3.5449219
 -3.5585938  -3.5625     -3.5898438  -3.5917969  -3.6171875  -3.6191406
 -3.6484375  -3.6503906  -3.65625    -3.6601562  -3.6621094  -3.6816406
 -3.6835938  -3.6875     -3.6894531  -3.7011719  -3.703125   -3.7070312
 -3.7128906  -3.7167969  -3.734375   -3.7363281  -3.7382812  -3.7402344
 -3.7519531  -3.7539062  -3.7832031  -3.7871094  -3.7910156  -3.7929688
 -3.7949219  -3.7988281  -3.8144531  -3.8164062  -3.8320312  -3.8339844
 -3.8359375  -3.8457031  -3.8515625  -3.8554688  -3.8632812  -3.8730469
 -3.8769531  -3.8808594  -3.8828125  -3.8847656  -3.9023438  -3.9082031
 -3.9179688  -3.9257812  -3.9433594  -3.9453125  -3.9667969  -3.9707031
 -3.9726562  -3.9746094  -3.9785156  -3.9804688  -4.015625   -4.0195312
 -4.0273438  -4.03125    -4.0390625  -4.0429688  -4.046875   -4.0507812
 -4.0546875  -4.0820312  -4.0859375  -4.0898438  -4.09375    -4.1015625
 -4.125      -4.1328125  -4.1523438  -4.15625    -4.1601562  -4.1640625
 -4.1679688  -4.171875   -4.1796875  -4.1914062  -4.1953125  -4.1992188
 -4.2070312  -4.2226562  -4.2265625  -4.2304688  -4.2382812  -4.2421875
 -4.2460938  -4.25       -4.2539062  -4.2578125  -4.265625   -4.2695312
 -4.2734375  -4.2851562  -4.2929688  -4.3007812  -4.3046875  -4.3085938
 -4.3476562  -4.3515625  -4.3554688  -4.3632812  -4.3671875  -4.3710938
 -4.3789062  -4.3867188  -4.3945312  -4.3984375  -4.4023438  -4.40625
 -4.4101562  -4.4140625  -4.4179688  -4.421875   -4.4257812  -4.4375
 -4.4492188  -4.453125   -4.4609375  -4.4882812  -4.4960938  -4.5117188
 -4.5195312  -4.53125    -4.5351562  -4.5585938  -4.5625     -4.5703125
 -4.5820312  -4.6054688  -4.6132812  -4.8554688 ]
