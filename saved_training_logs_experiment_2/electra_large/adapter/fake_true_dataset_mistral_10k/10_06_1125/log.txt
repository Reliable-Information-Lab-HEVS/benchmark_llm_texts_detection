log_loss_steps: 200
eval_steps: 504
check_degradation: 0
----------------Epoch 1/1----------------
Epoch 1/1, Loss after 192 samples: 0.7084
Epoch 1/1, Loss after 392 samples: 0.6842
Mean accuracy: 0.7285, std: 0.0098, lower bound: 0.7099, upper bound: 0.7475 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 496 samples: 0.7287 with eval loss: 0.6654
Best model with eval loss 0.6654495053928391 and eval accuracy 0.7287018255578094 with 496 samples seen is saved
Epoch 1/1, Loss after 592 samples: 0.6755
Epoch 1/1, Loss after 792 samples: 0.6404
Epoch 1/1, Loss after 992 samples: 0.5071
Mean accuracy: 0.8238, std: 0.0085, lower bound: 0.8073, upper bound: 0.8408 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 1000 samples: 0.8240 with eval loss: 0.3805
Best model with eval loss 0.3804556271325239 and eval accuracy 0.8240365111561866 with 1000 samples seen is saved
Epoch 1/1, Loss after 1192 samples: 0.3563
Epoch 1/1, Loss after 1392 samples: 0.4024
Mean accuracy: 0.8931, std: 0.0070, lower bound: 0.8788, upper bound: 0.9062 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 1504 samples: 0.8930 with eval loss: 0.2503
Best model with eval loss 0.2503062414254254 and eval accuracy 0.8930020283975659 with 1504 samples seen is saved
Epoch 1/1, Loss after 1592 samples: 0.2820
Epoch 1/1, Loss after 1792 samples: 0.3388
Epoch 1/1, Loss after 1992 samples: 0.2326
Mean accuracy: 0.7956, std: 0.0090, lower bound: 0.7784, upper bound: 0.8134 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 2008 samples: 0.7956 with eval loss: 0.4773
Epoch 1/1, Loss after 2192 samples: 0.2551
Epoch 1/1, Loss after 2392 samples: 0.2955
Mean accuracy: 0.8723, std: 0.0076, lower bound: 0.8560, upper bound: 0.8864 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 2512 samples: 0.8727 with eval loss: 0.2749
Epoch 1/1, Loss after 2592 samples: 0.2244
Epoch 1/1, Loss after 2792 samples: 0.1932
Epoch 1/1, Loss after 2992 samples: 0.2222
Mean accuracy: 0.8690, std: 0.0072, lower bound: 0.8560, upper bound: 0.8834 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 3016 samples: 0.8687 with eval loss: 0.3060
Epoch 1/1, Loss after 3192 samples: 0.2239
Epoch 1/1, Loss after 3392 samples: 0.1713
Mean accuracy: 0.9216, std: 0.0065, lower bound: 0.9077, upper bound: 0.9336 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 3520 samples: 0.9214 with eval loss: 0.1943
Best model with eval loss 0.19428410896888146 and eval accuracy 0.9213995943204868 with 3520 samples seen is saved
Epoch 1/1, Loss after 3592 samples: 0.2048
Epoch 1/1, Loss after 3792 samples: 0.2272
Epoch 1/1, Loss after 3992 samples: 0.2466
Mean accuracy: 0.8690, std: 0.0077, lower bound: 0.8534, upper bound: 0.8839 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 4024 samples: 0.8692 with eval loss: 0.2752
Epoch 1/1, Loss after 4192 samples: 0.3577
Epoch 1/1, Loss after 4392 samples: 0.2819
Mean accuracy: 0.8551, std: 0.0081, lower bound: 0.8377, upper bound: 0.8707 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 4528 samples: 0.8550 with eval loss: 0.3260
Epoch 1/1, Loss after 4592 samples: 0.1682
Epoch 1/1, Loss after 4792 samples: 0.2084
Epoch 1/1, Loss after 4992 samples: 0.1799
Mean accuracy: 0.9272, std: 0.0060, lower bound: 0.9153, upper bound: 0.9386 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 5032 samples: 0.9275 with eval loss: 0.1728
Best model with eval loss 0.17283377352996393 and eval accuracy 0.9274847870182555 with 5032 samples seen is saved
Epoch 1/1, Loss after 5192 samples: 0.0921
Epoch 1/1, Loss after 5392 samples: 0.1928
Mean accuracy: 0.9030, std: 0.0068, lower bound: 0.8895, upper bound: 0.9158 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 5536 samples: 0.9031 with eval loss: 0.2140
Epoch 1/1, Loss after 5592 samples: 0.2871
Epoch 1/1, Loss after 5792 samples: 0.1805
Epoch 1/1, Loss after 5992 samples: 0.1871
Mean accuracy: 0.8929, std: 0.0070, lower bound: 0.8783, upper bound: 0.9062 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 6040 samples: 0.8930 with eval loss: 0.2470
Epoch 1/1, Loss after 6192 samples: 0.2524
Epoch 1/1, Loss after 6392 samples: 0.1659
Mean accuracy: 0.8928, std: 0.0072, lower bound: 0.8783, upper bound: 0.9062 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 6544 samples: 0.8930 with eval loss: 0.2404
Epoch 1/1, Loss after 6592 samples: 0.1761
Epoch 1/1, Loss after 6792 samples: 0.1567
Epoch 1/1, Loss after 6992 samples: 0.1477
Mean accuracy: 0.8990, std: 0.0068, lower bound: 0.8854, upper bound: 0.9123 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 7048 samples: 0.8991 with eval loss: 0.2356
Epoch 1/1, Loss after 7192 samples: 0.1098
Epoch 1/1, Loss after 7392 samples: 0.1277
Mean accuracy: 0.8751, std: 0.0074, lower bound: 0.8595, upper bound: 0.8895 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 7552 samples: 0.8753 with eval loss: 0.3435
Epoch 1/1, Loss after 7592 samples: 0.1711
Epoch 1/1, Loss after 7792 samples: 0.2105
Epoch 1/1, Loss after 7992 samples: 0.1932
Mean accuracy: 0.8674, std: 0.0075, lower bound: 0.8524, upper bound: 0.8813 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 8056 samples: 0.8671 with eval loss: 0.3240
Epoch 1/1, Loss after 8192 samples: 0.1155
Epoch 1/1, Loss after 8392 samples: 0.1463
Mean accuracy: 0.8858, std: 0.0072, lower bound: 0.8717, upper bound: 0.8996 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 8560 samples: 0.8859 with eval loss: 0.2936
Epoch 1/1, Loss after 8592 samples: 0.1467
Epoch 1/1, Loss after 8792 samples: 0.1196
Epoch 1/1, Loss after 8992 samples: 0.1544
Mean accuracy: 0.8693, std: 0.0077, lower bound: 0.8544, upper bound: 0.8834 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 9064 samples: 0.8697 with eval loss: 0.3411
Epoch 1/1, Loss after 9192 samples: 0.0724
Epoch 1/1, Loss after 9392 samples: 0.1389
Mean accuracy: 0.8844, std: 0.0070, lower bound: 0.8712, upper bound: 0.8976 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 9568 samples: 0.8849 with eval loss: 0.3185
Epoch 1/1, Loss after 9592 samples: 0.1197
Epoch 1/1, Loss after 9792 samples: 0.2421
Epoch 1/1, Loss after 9992 samples: 0.1676
Mean accuracy: 0.9132, std: 0.0066, lower bound: 0.9006, upper bound: 0.9260 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 10072 samples: 0.9133 with eval loss: 0.2043
Epoch 1/1, Loss after 10192 samples: 0.1697
Epoch 1/1, Loss after 10392 samples: 0.1726
Mean accuracy: 0.8940, std: 0.0070, lower bound: 0.8788, upper bound: 0.9072 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 10576 samples: 0.8940 with eval loss: 0.2438
Epoch 1/1, Loss after 10592 samples: 0.1356
Epoch 1/1, Loss after 10792 samples: 0.1048
Epoch 1/1, Loss after 10992 samples: 0.1236
Mean accuracy: 0.9018, std: 0.0069, lower bound: 0.8879, upper bound: 0.9148 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 11080 samples: 0.9016 with eval loss: 0.2533
Epoch 1/1, Loss after 11192 samples: 0.1298
Epoch 1/1, Loss after 11392 samples: 0.1918
Mean accuracy: 0.9228, std: 0.0060, lower bound: 0.9102, upper bound: 0.9336 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 11584 samples: 0.9229 with eval loss: 0.1825
Epoch 1/1, Loss after 11592 samples: 0.1278
Epoch 1/1, Loss after 11792 samples: 0.1145
Epoch 1/1, Loss after 11992 samples: 0.1316
Mean accuracy: 0.9143, std: 0.0065, lower bound: 0.9011, upper bound: 0.9270 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 12088 samples: 0.9143 with eval loss: 0.2096
Epoch 1/1, Loss after 12192 samples: 0.1278
Epoch 1/1, Loss after 12392 samples: 0.1377
Epoch 1/1, Loss after 12592 samples: 0.1201
Mean accuracy: 0.8850, std: 0.0074, lower bound: 0.8697, upper bound: 0.8991 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 12592 samples: 0.8849 with eval loss: 0.2987
Epoch 1/1, Loss after 12792 samples: 0.1571
Epoch 1/1, Loss after 12992 samples: 0.1538
Mean accuracy: 0.9054, std: 0.0066, lower bound: 0.8935, upper bound: 0.9184 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 13096 samples: 0.9057 with eval loss: 0.2215
Epoch 1/1, Loss after 13192 samples: 0.0902
Epoch 1/1, Loss after 13392 samples: 0.1762
Epoch 1/1, Loss after 13592 samples: 0.0925
Mean accuracy: 0.9108, std: 0.0065, lower bound: 0.8981, upper bound: 0.9229 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 13600 samples: 0.9108 with eval loss: 0.2179
Epoch 1/1, Loss after 13792 samples: 0.1149
Epoch 1/1, Loss after 13992 samples: 0.1014
Mean accuracy: 0.9187, std: 0.0064, lower bound: 0.9057, upper bound: 0.9315 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 14104 samples: 0.9189 with eval loss: 0.1988
Epoch 1/1, Loss after 14192 samples: 0.1553
Epoch 1/1, Loss after 14392 samples: 0.1869
Epoch 1/1, Loss after 14592 samples: 0.1126
Mean accuracy: 0.9208, std: 0.0061, lower bound: 0.9087, upper bound: 0.9331 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 14608 samples: 0.9204 with eval loss: 0.1891
Epoch 1/1, Loss after 14792 samples: 0.1595
Epoch 1/1, Loss after 14992 samples: 0.1062
Mean accuracy: 0.8936, std: 0.0072, lower bound: 0.8803, upper bound: 0.9077 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 15112 samples: 0.8935 with eval loss: 0.2695
Epoch 1/1, Loss after 15192 samples: 0.1658
Epoch 1/1, Loss after 15392 samples: 0.1762
Epoch 1/1, Loss after 15592 samples: 0.1180
Mean accuracy: 0.8993, std: 0.0068, lower bound: 0.8864, upper bound: 0.9128 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 15616 samples: 0.8996 with eval loss: 0.2483
Training signal is False, stopping training
Training finished
Best model: {'eval_acc': 0.9274847870182555, 'nb_samples': 5032, 'eval_loss': 0.17283377352996393}
Training loss logs: [{'samples': 192, 'loss': 0.708448486328125}, {'samples': 392, 'loss': 0.68424560546875}, {'samples': 592, 'loss': 0.675489501953125}, {'samples': 792, 'loss': 0.640382080078125}, {'samples': 992, 'loss': 0.50707275390625}, {'samples': 1192, 'loss': 0.3563471126556397}, {'samples': 1392, 'loss': 0.40237667083740236}, {'samples': 1592, 'loss': 0.2820351791381836}, {'samples': 1792, 'loss': 0.3387649726867676}, {'samples': 1992, 'loss': 0.23258048057556152}, {'samples': 2192, 'loss': 0.25507297515869143}, {'samples': 2392, 'loss': 0.29549417495727537}, {'samples': 2592, 'loss': 0.2243556022644043}, {'samples': 2792, 'loss': 0.19318464279174805}, {'samples': 2992, 'loss': 0.22223840713500975}, {'samples': 3192, 'loss': 0.22393468379974366}, {'samples': 3392, 'loss': 0.171305251121521}, {'samples': 3592, 'loss': 0.20483877658843994}, {'samples': 3792, 'loss': 0.22718658447265624}, {'samples': 3992, 'loss': 0.246642107963562}, {'samples': 4192, 'loss': 0.35771058082580565}, {'samples': 4392, 'loss': 0.28190664291381834}, {'samples': 4592, 'loss': 0.16821521282196045}, {'samples': 4792, 'loss': 0.20840794324874878}, {'samples': 4992, 'loss': 0.1799069833755493}, {'samples': 5192, 'loss': 0.09214442014694214}, {'samples': 5392, 'loss': 0.19275495111942292}, {'samples': 5592, 'loss': 0.28713136553764346}, {'samples': 5792, 'loss': 0.18048271656036377}, {'samples': 5992, 'loss': 0.1871203899383545}, {'samples': 6192, 'loss': 0.2524289977550507}, {'samples': 6392, 'loss': 0.1658812928199768}, {'samples': 6592, 'loss': 0.17608126878738403}, {'samples': 6792, 'loss': 0.15665785431861878}, {'samples': 6992, 'loss': 0.14774062871932983}, {'samples': 7192, 'loss': 0.10981648206710816}, {'samples': 7392, 'loss': 0.12770393788814544}, {'samples': 7592, 'loss': 0.17106442987918855}, {'samples': 7792, 'loss': 0.2104936110973358}, {'samples': 7992, 'loss': 0.19316662073135377}, {'samples': 8192, 'loss': 0.11545693397521972}, {'samples': 8392, 'loss': 0.1462603235244751}, {'samples': 8592, 'loss': 0.14667110323905944}, {'samples': 8792, 'loss': 0.11957952976226807}, {'samples': 8992, 'loss': 0.15436640977859498}, {'samples': 9192, 'loss': 0.07242637395858764}, {'samples': 9392, 'loss': 0.13894805669784546}, {'samples': 9592, 'loss': 0.11968485951423645}, {'samples': 9792, 'loss': 0.24207546770572663}, {'samples': 9992, 'loss': 0.16763607263565064}, {'samples': 10192, 'loss': 0.1697447943687439}, {'samples': 10392, 'loss': 0.17255528688430785}, {'samples': 10592, 'loss': 0.13560794353485106}, {'samples': 10792, 'loss': 0.10482606172561645}, {'samples': 10992, 'loss': 0.12360932648181916}, {'samples': 11192, 'loss': 0.1298471999168396}, {'samples': 11392, 'loss': 0.19179925620555877}, {'samples': 11592, 'loss': 0.12776121199131013}, {'samples': 11792, 'loss': 0.11452767372131348}, {'samples': 11992, 'loss': 0.1315944916009903}, {'samples': 12192, 'loss': 0.12783363223075866}, {'samples': 12392, 'loss': 0.1376880669593811}, {'samples': 12592, 'loss': 0.12010867536067962}, {'samples': 12792, 'loss': 0.15713903903961182}, {'samples': 12992, 'loss': 0.15379496455192565}, {'samples': 13192, 'loss': 0.09018894433975219}, {'samples': 13392, 'loss': 0.17620507776737213}, {'samples': 13592, 'loss': 0.09247319757938385}, {'samples': 13792, 'loss': 0.11486163556575775}, {'samples': 13992, 'loss': 0.10144672334194184}, {'samples': 14192, 'loss': 0.1553015160560608}, {'samples': 14392, 'loss': 0.18692853569984436}, {'samples': 14592, 'loss': 0.11264856100082397}, {'samples': 14792, 'loss': 0.15953575372695922}, {'samples': 14992, 'loss': 0.10623720526695252}, {'samples': 15192, 'loss': 0.16575951278209686}, {'samples': 15392, 'loss': 0.17622093319892884}, {'samples': 15592, 'loss': 0.118040811419487}]
Evaluation accuracy logs: [{'samples': 496, 'accuracy': 0.7285481744421907, 'std': 0.009827282557474438, 'lower_bound': 0.7099264705882352, 'upper_bound': 0.7474645030425964}, {'samples': 1000, 'accuracy': 0.8238235294117646, 'std': 0.008530123241907186, 'lower_bound': 0.8073022312373225, 'upper_bound': 0.8407834685598378}, {'samples': 1504, 'accuracy': 0.8931090263691683, 'std': 0.007004628323346832, 'lower_bound': 0.8788032454361054, 'upper_bound': 0.9061866125760649}, {'samples': 2008, 'accuracy': 0.795618154158215, 'std': 0.008952958868865721, 'lower_bound': 0.7783975659229209, 'upper_bound': 0.8134127789046652}, {'samples': 2512, 'accuracy': 0.8723377281947262, 'std': 0.007639978684702767, 'lower_bound': 0.8559837728194726, 'upper_bound': 0.8864224137931035}, {'samples': 3016, 'accuracy': 0.8690451318458418, 'std': 0.007200846195241848, 'lower_bound': 0.8559837728194726, 'upper_bound': 0.8833798174442191}, {'samples': 3520, 'accuracy': 0.9215649087221095, 'std': 0.006482898801374416, 'lower_bound': 0.907707910750507, 'upper_bound': 0.9335699797160243}, {'samples': 4024, 'accuracy': 0.8689675456389453, 'std': 0.007710465574247079, 'lower_bound': 0.853448275862069, 'upper_bound': 0.8838869168356999}, {'samples': 4528, 'accuracy': 0.855107505070994, 'std': 0.008141433210244237, 'lower_bound': 0.8377281947261663, 'upper_bound': 0.8706896551724138}, {'samples': 5032, 'accuracy': 0.9272231237322516, 'std': 0.005978866211243129, 'lower_bound': 0.915314401622718, 'upper_bound': 0.9386409736308317}, {'samples': 5536, 'accuracy': 0.903026369168357, 'std': 0.006774137590149012, 'lower_bound': 0.8894523326572008, 'upper_bound': 0.9158215010141988}, {'samples': 6040, 'accuracy': 0.8928549695740364, 'std': 0.007017858021428844, 'lower_bound': 0.8782961460446247, 'upper_bound': 0.9061866125760649}, {'samples': 6544, 'accuracy': 0.8928088235294117, 'std': 0.007164505778299488, 'lower_bound': 0.8782961460446247, 'upper_bound': 0.9061866125760649}, {'samples': 7048, 'accuracy': 0.8990273833671399, 'std': 0.006803113519436227, 'lower_bound': 0.8853955375253549, 'upper_bound': 0.9122718052738337}, {'samples': 7552, 'accuracy': 0.8751262677484787, 'std': 0.0073722206328068075, 'lower_bound': 0.8595334685598377, 'upper_bound': 0.8894523326572008}, {'samples': 8056, 'accuracy': 0.8674112576064908, 'std': 0.007479376947655221, 'lower_bound': 0.8524213995943204, 'upper_bound': 0.8813387423935092}, {'samples': 8560, 'accuracy': 0.8858062880324544, 'std': 0.00721233507814762, 'lower_bound': 0.8716784989858013, 'upper_bound': 0.8996069979716025}, {'samples': 9064, 'accuracy': 0.8693022312373224, 'std': 0.00769432559617463, 'lower_bound': 0.8544497971602434, 'upper_bound': 0.8833671399594321}, {'samples': 9568, 'accuracy': 0.884353448275862, 'std': 0.006996390872267262, 'lower_bound': 0.8711840770791075, 'upper_bound': 0.8975786004056795}, {'samples': 10072, 'accuracy': 0.9132058823529412, 'std': 0.006644028457623224, 'lower_bound': 0.9006085192697769, 'upper_bound': 0.9259634888438134}, {'samples': 10576, 'accuracy': 0.8940339756592292, 'std': 0.007046999065914244, 'lower_bound': 0.8788032454361054, 'upper_bound': 0.9072008113590264}, {'samples': 11080, 'accuracy': 0.9018118661257606, 'std': 0.006862759334694147, 'lower_bound': 0.8879183569979716, 'upper_bound': 0.9148073022312373}, {'samples': 11584, 'accuracy': 0.9228230223123732, 'std': 0.006005601686248603, 'lower_bound': 0.9102434077079108, 'upper_bound': 0.9335826572008113}, {'samples': 12088, 'accuracy': 0.9143326572008114, 'std': 0.006543663363757833, 'lower_bound': 0.9011156186612576, 'upper_bound': 0.9269776876267748}, {'samples': 12592, 'accuracy': 0.8849954361054767, 'std': 0.0074077567702822466, 'lower_bound': 0.8696754563894523, 'upper_bound': 0.8990872210953347}, {'samples': 13096, 'accuracy': 0.9053595334685598, 'std': 0.006599559868778687, 'lower_bound': 0.8935091277890467, 'upper_bound': 0.9183696754563895}, {'samples': 13600, 'accuracy': 0.9107692697768763, 'std': 0.00653075568002585, 'lower_bound': 0.8980730223123732, 'upper_bound': 0.9229335699797161}, {'samples': 14104, 'accuracy': 0.9186698782961461, 'std': 0.006357261203970719, 'lower_bound': 0.9056795131845842, 'upper_bound': 0.9315415821501014}, {'samples': 14608, 'accuracy': 0.9207576064908721, 'std': 0.006092947307133062, 'lower_bound': 0.9087221095334685, 'upper_bound': 0.9330628803245437}, {'samples': 15112, 'accuracy': 0.8936206896551723, 'std': 0.007185217237426398, 'lower_bound': 0.8803245436105477, 'upper_bound': 0.907707910750507}, {'samples': 15616, 'accuracy': 0.8993265720081135, 'std': 0.006789400265082172, 'lower_bound': 0.8863970588235294, 'upper_bound': 0.9127915821501015}]
Evaluating the best model on the test set of dataset ./fake_true_datasets/fake_true_dataset_mistral_10k...
Test metrics:
accuracy: 0.9010765720081135
precision: 0.8374778918837124
recall: 0.9949411155526
f1_score: 0.9094055518921689
fp_rate: 0.19250980781606958
tp_rate: 0.9949411155526
std_accuracy: 0.0067022172092768804
std_precision: 0.010811982981562007
std_recall: 0.0022075178567932067
std_f1_score: 0.006468258972978677
std_fp_rate: 0.012452990123999159
std_tp_rate: 0.0022075178567932067
TP: 979.578
TN: 797.345
FP: 190.096
FN: 4.981
roc_auc: 0.9918714950483236
fpr: [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.0010142  0.0010142
 0.0010142  0.0010142  0.0010142  0.0010142  0.0010142  0.0010142
 0.0010142  0.0010142  0.0010142  0.0010142  0.0010142  0.0010142
 0.0010142  0.0010142  0.0010142  0.0010142  0.0010142  0.0010142
 0.0010142  0.0010142  0.0010142  0.0010142  0.0010142  0.0010142
 0.0010142  0.0010142  0.0010142  0.0010142  0.0010142  0.0010142
 0.0010142  0.0010142  0.0010142  0.0010142  0.0010142  0.0010142
 0.0010142  0.0010142  0.0010142  0.0010142  0.0010142  0.0010142
 0.0010142  0.0010142  0.0010142  0.0010142  0.0010142  0.0010142
 0.0010142  0.0020284  0.0020284  0.0020284  0.0020284  0.0020284
 0.0020284  0.0020284  0.0020284  0.0020284  0.0020284  0.0020284
 0.0020284  0.0020284  0.0020284  0.0020284  0.0020284  0.0020284
 0.0020284  0.0020284  0.0020284  0.0020284  0.0020284  0.0020284
 0.0020284  0.0020284  0.0020284  0.0020284  0.0020284  0.0020284
 0.0020284  0.0020284  0.0020284  0.0020284  0.0020284  0.0020284
 0.0020284  0.0020284  0.0020284  0.0020284  0.0020284  0.0020284
 0.0020284  0.0020284  0.0030426  0.0030426  0.0040568  0.0040568
 0.0040568  0.0040568  0.0040568  0.00507099 0.00507099 0.00507099
 0.00507099 0.00507099 0.00507099 0.00709939 0.00709939 0.00709939
 0.00811359 0.00912779 0.00912779 0.00912779 0.01014199 0.01014199
 0.01014199 0.01014199 0.01014199 0.01115619 0.01115619 0.01217039
 0.01217039 0.01318458 0.01318458 0.01318458 0.01419878 0.01419878
 0.01419878 0.01419878 0.01419878 0.01419878 0.01419878 0.01521298
 0.01521298 0.01521298 0.01521298 0.01521298 0.01521298 0.01622718
 0.01622718 0.01724138 0.01724138 0.01825558 0.01825558 0.02028398
 0.02028398 0.02028398 0.02129817 0.02129817 0.02129817 0.02332657
 0.02332657 0.02332657 0.02434077 0.02434077 0.02434077 0.02535497
 0.02535497 0.02636917 0.02636917 0.02738337 0.02738337 0.02941176
 0.02941176 0.03042596 0.03042596 0.03144016 0.03144016 0.03346856
 0.03346856 0.03448276 0.03448276 0.03549696 0.03549696 0.03651116
 0.03651116 0.03752535 0.03752535 0.03853955 0.03853955 0.04158215
 0.04158215 0.04259635 0.04462475 0.04462475 0.05172414 0.05172414
 0.05476673 0.05476673 0.05578093 0.05578093 0.05679513 0.05679513
 0.05780933 0.05780933 0.05882353 0.05882353 0.06186613 0.06186613
 0.06186613 0.06389452 0.06389452 0.06592292 0.06592292 0.06592292
 0.06795132 0.06795132 0.07200811 0.07200811 0.07302231 0.07302231
 0.07403651 0.07403651 0.07505071 0.07505071 0.0801217  0.0801217
 0.0862069  0.0862069  0.09026369 0.09026369 0.09127789 0.09127789
 0.09330629 0.09330629 0.09634888 0.09634888 0.09736308 0.09736308
 0.10141988 0.10141988 0.10750507 0.10750507 0.10851927 0.10851927
 0.11359026 0.11359026 0.11561866 0.11561866 0.11663286 0.11663286
 0.11866126 0.11866126 0.12677485 0.12677485 0.14401623 0.14604462
 0.1703854  0.1703854  0.19371197 0.19371197 0.19472617 0.19472617
 0.20892495 0.20892495 0.2474645  0.2494929  0.25557809 0.25760649
 0.27789047 0.27789047 0.28904665 0.29107505 0.29310345 0.29513185
 0.29817444 0.30020284 0.31135903 0.31135903 0.31440162 0.31643002
 0.32657201 0.32860041 0.3356998  0.33772819 0.34989858 0.35192698
 0.39756592 0.39959432 0.40567951 0.40770791 0.4137931  0.4178499
 0.43711968 0.44016227 0.51521298 0.51926978 0.52028398 0.52434077
 0.52535497 0.52738337 0.53042596 0.53448276 0.53955375 0.54158215
 0.54259635 0.54462475 0.56592292 0.56896552 0.57606491 0.57809331
 0.5841785  0.5872211  0.58823529 0.59026369 0.59432049 0.59736308
 0.60243408 0.60547667 0.60649087 0.60851927 0.61764706 0.62170385
 0.63387424 0.63590264 0.64097363 0.64300203 0.65415822 0.65618661
 0.6643002  0.6663286  0.67139959 0.67342799 0.67545639 0.67951318
 0.68356998 0.68559838 0.68965517 0.69168357 0.69472617 0.69675456
 0.69776876 0.69979716 0.70283976 0.70588235 0.70993915 0.71399594
 0.71703854 0.72210953 0.72413793 0.73022312 0.73225152 0.73427992
 0.73732252 0.74137931 0.74543611 0.7484787  0.75456389 0.75659229
 0.75862069 0.76166329 0.76572008 0.76876268 0.77180527 0.77484787
 0.77687627 0.77789047 0.77991886 0.78194726 0.78498986 0.79107505
 0.79310345 0.80020284 0.80223124 0.80425963 0.80628803 0.81135903
 0.81744422 0.81947262 0.82352941 0.82454361 0.82657201 0.82758621
 0.8356998  0.83874239 0.84077079 0.84381339 0.84584178 0.84685598
 0.84989858 0.85091278 0.85395538 0.85902637 0.86105477 0.86206897
 0.86409736 0.86815416 0.87018256 0.87119675 0.87322515 0.87626775
 0.87829615 0.88032454 0.88336714 0.88742394 0.88945233 0.89350913
 0.89655172 0.90060852 0.90669371 0.90872211 0.90973631 0.9137931
 0.9178499  0.9198783  0.92697769 0.92900609 0.93306288 0.93711968
 0.93914807 0.94219067 0.94421907 0.94624746 0.94827586 0.95233266
 0.95537525 0.95638945 0.95841785 0.95943205 0.96146045 0.96247465
 0.96653144 0.96855984 0.97058824 0.97363083 0.97565923 1.        ]
tpr: [0.         0.0010142  0.0030426  0.00507099 0.00912779 0.01115619
 0.01318458 0.01521298 0.01724138 0.01825558 0.02231237 0.02434077
 0.02636917 0.03448276 0.03549696 0.03955375 0.04056795 0.04462475
 0.04766734 0.04969574 0.05476673 0.05679513 0.06085193 0.06288032
 0.06693712 0.06997972 0.07200811 0.07606491 0.0811359  0.0872211
 0.09432049 0.09634888 0.10243408 0.10547667 0.10750507 0.11156187
 0.11561866 0.11866126 0.12068966 0.12373225 0.12576065 0.13083164
 0.13488844 0.13691684 0.14503043 0.15010142 0.15314402 0.15821501
 0.1643002  0.1653144  0.17139959 0.17444219 0.18052738 0.18965517
 0.19066937 0.19269777 0.19979716 0.20385396 0.20791075 0.21298174
 0.21703854 0.22109533 0.22413793 0.22515213 0.22920892 0.23022312
 0.23529412 0.23732252 0.24137931 0.24239351 0.2484787  0.2505071
 0.25456389 0.25862069 0.26064909 0.26267748 0.26369168 0.26977688
 0.27281947 0.27789047 0.28194726 0.28600406 0.28904665 0.29006085
 0.29208925 0.29817444 0.30020284 0.30223124 0.30527383 0.30730223
 0.31135903 0.31440162 0.31947262 0.32150101 0.32251521 0.32860041
 0.3346856  0.33874239 0.33975659 0.34178499 0.35192698 0.35496957
 0.36206897 0.36511156 0.36916836 0.37018256 0.37322515 0.37423935
 0.37829615 0.38032454 0.38235294 0.38742394 0.38945233 0.39350913
 0.39756592 0.40365112 0.40567951 0.40770791 0.40973631 0.4127789
 0.4168357  0.4188641  0.42697769 0.42900609 0.43002028 0.43407708
 0.44219067 0.44421907 0.44523327 0.44827586 0.45131846 0.45436105
 0.45638945 0.45740365 0.45943205 0.46247465 0.46450304 0.46653144
 0.46855984 0.47261663 0.47565923 0.47768763 0.47971602 0.48073022
 0.48478702 0.48681542 0.48884381 0.49188641 0.49391481 0.4959432
 0.5        0.5020284  0.5040568  0.50608519 0.50709939 0.50912779
 0.51318458 0.51724138 0.51926978 0.52535497 0.52839757 0.52941176
 0.53144016 0.53448276 0.53853955 0.53955375 0.54158215 0.54361055
 0.54563895 0.54766734 0.54969574 0.55375254 0.55679513 0.56186613
 0.56490872 0.56592292 0.57200811 0.57403651 0.57606491 0.57707911
 0.57910751 0.5811359  0.5851927  0.58823529 0.59229209 0.59533469
 0.59736308 0.60141988 0.60344828 0.60446247 0.60649087 0.60851927
 0.61054767 0.61156187 0.61561866 0.61663286 0.62271805 0.62474645
 0.63083164 0.63184584 0.63488844 0.63793103 0.63995943 0.64198783
 0.64401623 0.64604462 0.65618661 0.65922921 0.66125761 0.663286
 0.6653144  0.67139959 0.67241379 0.67748479 0.67849899 0.68052738
 0.68154158 0.68356998 0.68458418 0.68762677 0.69168357 0.69574037
 0.69979716 0.70283976 0.70588235 0.70791075 0.71399594 0.71805274
 0.72312373 0.72718053 0.73427992 0.73833671 0.74036511 0.74239351
 0.74340771 0.7474645  0.7515213  0.7535497  0.76369168 0.76673428
 0.79411765 0.79614604 0.79614604 0.79817444 0.79817444 0.80020284
 0.80628803 0.80933063 0.81135903 0.81237323 0.81643002 0.81845842
 0.82150101 0.82352941 0.82454361 0.82454361 0.82657201 0.82758621
 0.82860041 0.82860041 0.8306288  0.8326572  0.8326572  0.8356998
 0.83975659 0.84178499 0.84381339 0.84381339 0.84584178 0.84685598
 0.85395538 0.85395538 0.85699797 0.85902637 0.85902637 0.86206897
 0.86308316 0.86511156 0.86815416 0.87119675 0.87221095 0.87221095
 0.87931034 0.88336714 0.88742394 0.89148073 0.89756592 0.89756592
 0.89858012 0.89858012 0.90060852 0.90162272 0.90365112 0.90365112
 0.90567951 0.90770791 0.90770791 0.90973631 0.91075051 0.91075051
 0.91176471 0.9137931  0.9137931  0.9158215  0.9178499  0.9178499
 0.9198783  0.92089249 0.92190669 0.92190669 0.92292089 0.92292089
 0.92799189 0.92799189 0.92900609 0.92900609 0.93002028 0.93002028
 0.93407708 0.93407708 0.93509128 0.93509128 0.93610548 0.93610548
 0.93813387 0.93813387 0.93914807 0.93914807 0.94016227 0.94016227
 0.94117647 0.94219067 0.94219067 0.94421907 0.94421907 0.94624746
 0.94624746 0.95030426 0.95030426 0.95131846 0.95233266 0.95334686
 0.95334686 0.95436105 0.95537525 0.95638945 0.95638945 0.95943205
 0.96146045 0.96146045 0.96348884 0.96348884 0.96551724 0.96653144
 0.96653144 0.96957404 0.96957404 0.97058824 0.97058824 0.97160243
 0.97160243 0.97261663 0.97261663 0.97464503 0.97464503 0.97565923
 0.97565923 0.97667343 0.97667343 0.97768763 0.97768763 0.97870183
 0.97870183 0.97971602 0.97971602 0.98073022 0.98073022 0.98275862
 0.98275862 0.98377282 0.98377282 0.98478702 0.98478702 0.98580122
 0.98580122 0.98985801 0.98985801 0.99087221 0.99087221 0.99188641
 0.99188641 0.99290061 0.99290061 0.99391481 0.99391481 0.99391481
 0.99391481 0.99492901 0.99492901 0.9959432  0.9959432  0.9969574
 0.9969574  0.9979716  0.9979716  0.9979716  0.9979716  0.9979716
 0.9979716  0.9989858  0.9989858  0.9989858  0.9989858  0.9989858
 0.9989858  0.9989858  0.9989858  1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.        ]
thresholds: [           inf  5.1093750e+00  5.1015625e+00  5.0937500e+00
  5.0820312e+00  5.0703125e+00  5.0664062e+00  5.0585938e+00
  5.0507812e+00  5.0468750e+00  5.0429688e+00  5.0351562e+00
  5.0273438e+00  5.0117188e+00  5.0078125e+00  4.9960938e+00
  4.9921875e+00  4.9843750e+00  4.9804688e+00  4.9765625e+00
  4.9726562e+00  4.9687500e+00  4.9648438e+00  4.9609375e+00
  4.9570312e+00  4.9531250e+00  4.9492188e+00  4.9453125e+00
  4.9414062e+00  4.9375000e+00  4.9335938e+00  4.9296875e+00
  4.9257812e+00  4.9218750e+00  4.9179688e+00  4.9140625e+00
  4.9062500e+00  4.9023438e+00  4.8984375e+00  4.8945312e+00
  4.8906250e+00  4.8867188e+00  4.8828125e+00  4.8789062e+00
  4.8710938e+00  4.8671875e+00  4.8632812e+00  4.8593750e+00
  4.8515625e+00  4.8476562e+00  4.8437500e+00  4.8398438e+00
  4.8359375e+00  4.8242188e+00  4.8203125e+00  4.8164062e+00
  4.8125000e+00  4.8085938e+00  4.8007812e+00  4.7968750e+00
  4.7929688e+00  4.7851562e+00  4.7812500e+00  4.7773438e+00
  4.7656250e+00  4.7617188e+00  4.7578125e+00  4.7539062e+00
  4.7460938e+00  4.7421875e+00  4.7304688e+00  4.7226562e+00
  4.7148438e+00  4.7070312e+00  4.6992188e+00  4.6953125e+00
  4.6875000e+00  4.6718750e+00  4.6601562e+00  4.6328125e+00
  4.6250000e+00  4.5937500e+00  4.5859375e+00  4.5820312e+00
  4.5781250e+00  4.5664062e+00  4.5507812e+00  4.5468750e+00
  4.5273438e+00  4.5234375e+00  4.4804688e+00  4.4765625e+00
  4.4492188e+00  4.4414062e+00  4.4375000e+00  4.4140625e+00
  4.3398438e+00  4.3203125e+00  4.3085938e+00  4.2968750e+00
  4.1953125e+00  4.1914062e+00  4.1484375e+00  4.1250000e+00
  4.0937500e+00  4.0898438e+00  4.0859375e+00  4.0781250e+00
  4.0703125e+00  4.0507812e+00  4.0468750e+00  4.0195312e+00
  4.0156250e+00  3.9804688e+00  3.9707031e+00  3.9257812e+00
  3.9199219e+00  3.9121094e+00  3.9082031e+00  3.9062500e+00
  3.8906250e+00  3.8867188e+00  3.8496094e+00  3.8457031e+00
  3.8359375e+00  3.8300781e+00  3.7695312e+00  3.7675781e+00
  3.7656250e+00  3.7539062e+00  3.7421875e+00  3.7402344e+00
  3.7324219e+00  3.7285156e+00  3.7207031e+00  3.7109375e+00
  3.7050781e+00  3.7011719e+00  3.6992188e+00  3.6835938e+00
  3.6796875e+00  3.6660156e+00  3.6621094e+00  3.6601562e+00
  3.6562500e+00  3.6503906e+00  3.6406250e+00  3.6328125e+00
  3.6289062e+00  3.6132812e+00  3.5957031e+00  3.5917969e+00
  3.5878906e+00  3.5859375e+00  3.5820312e+00  3.5800781e+00
  3.5703125e+00  3.5625000e+00  3.5566406e+00  3.5273438e+00
  3.5175781e+00  3.5156250e+00  3.5136719e+00  3.5019531e+00
  3.4960938e+00  3.4941406e+00  3.4902344e+00  3.4843750e+00
  3.4824219e+00  3.4785156e+00  3.4765625e+00  3.4648438e+00
  3.4570312e+00  3.4453125e+00  3.4433594e+00  3.4414062e+00
  3.4042969e+00  3.3984375e+00  3.3964844e+00  3.3886719e+00
  3.3867188e+00  3.3808594e+00  3.3769531e+00  3.3710938e+00
  3.3671875e+00  3.3554688e+00  3.3476562e+00  3.3359375e+00
  3.3339844e+00  3.3242188e+00  3.3222656e+00  3.3125000e+00
  3.3085938e+00  3.3066406e+00  3.2890625e+00  3.2832031e+00
  3.2734375e+00  3.2714844e+00  3.2480469e+00  3.2441406e+00
  3.2363281e+00  3.2343750e+00  3.2304688e+00  3.2207031e+00
  3.2148438e+00  3.2109375e+00  3.1542969e+00  3.1503906e+00
  3.1425781e+00  3.1386719e+00  3.1347656e+00  3.1210938e+00
  3.1191406e+00  3.1171875e+00  3.1152344e+00  3.1132812e+00
  3.1093750e+00  3.1074219e+00  3.1054688e+00  3.0996094e+00
  3.0820312e+00  3.0800781e+00  3.0644531e+00  3.0625000e+00
  3.0429688e+00  3.0410156e+00  3.0253906e+00  3.0156250e+00
  2.9687500e+00  2.9609375e+00  2.9375000e+00  2.9316406e+00
  2.9257812e+00  2.9238281e+00  2.9160156e+00  2.9121094e+00
  2.8984375e+00  2.8925781e+00  2.8300781e+00  2.8281250e+00
  2.6875000e+00  2.6855469e+00  2.6835938e+00  2.6777344e+00
  2.6679688e+00  2.6621094e+00  2.6484375e+00  2.6367188e+00
  2.6230469e+00  2.6171875e+00  2.6074219e+00  2.6054688e+00
  2.5898438e+00  2.5878906e+00  2.5820312e+00  2.5644531e+00
  2.5625000e+00  2.5546875e+00  2.5468750e+00  2.5449219e+00
  2.5371094e+00  2.5332031e+00  2.5312500e+00  2.5156250e+00
  2.5117188e+00  2.5000000e+00  2.4960938e+00  2.4902344e+00
  2.4843750e+00  2.4765625e+00  2.4511719e+00  2.4453125e+00
  2.4277344e+00  2.4238281e+00  2.4218750e+00  2.4140625e+00
  2.4082031e+00  2.4062500e+00  2.3828125e+00  2.3613281e+00
  2.3535156e+00  2.3339844e+00  2.3027344e+00  2.2949219e+00
  2.2792969e+00  2.2656250e+00  2.2187500e+00  2.2050781e+00
  2.1933594e+00  2.1796875e+00  2.1679688e+00  2.1562500e+00
  2.1464844e+00  2.1328125e+00  2.1308594e+00  2.1132812e+00
  2.1113281e+00  2.1074219e+00  2.1035156e+00  2.0742188e+00
  2.0683594e+00  2.0664062e+00  2.0507812e+00  2.0332031e+00
  2.0312500e+00  2.0292969e+00  2.0117188e+00  2.0039062e+00
  2.0000000e+00  1.9990234e+00  1.9970703e+00  1.9814453e+00
  1.9472656e+00  1.9277344e+00  1.9228516e+00  1.9130859e+00
  1.8984375e+00  1.8935547e+00  1.8408203e+00  1.8388672e+00
  1.8378906e+00  1.8359375e+00  1.8281250e+00  1.8261719e+00
  1.8222656e+00  1.8193359e+00  1.7988281e+00  1.7978516e+00
  1.7949219e+00  1.7675781e+00  1.7656250e+00  1.7597656e+00
  1.7539062e+00  1.6894531e+00  1.6113281e+00  1.5937500e+00
  1.5654297e+00  1.5429688e+00  1.5380859e+00  1.5332031e+00
  1.5234375e+00  1.5107422e+00  1.5097656e+00  1.5068359e+00
  1.5058594e+00  1.4892578e+00  1.4599609e+00  1.4482422e+00
  1.4433594e+00  1.4335938e+00  1.4306641e+00  1.4218750e+00
  1.4189453e+00  1.4150391e+00  1.3828125e+00  1.3359375e+00
  1.2861328e+00  1.2832031e+00  1.2792969e+00  1.2607422e+00
  1.2558594e+00  1.2285156e+00  1.1933594e+00  1.1806641e+00
  1.1054688e+00  1.0898438e+00  1.0097656e+00  1.0009766e+00
  9.7509766e-01  9.5849609e-01  9.5556641e-01  9.4287109e-01
  9.0917969e-01  9.0576172e-01  8.3300781e-01  8.2373047e-01
  8.2080078e-01  8.1005859e-01  7.6074219e-01  7.5097656e-01
  7.2851562e-01  7.2656250e-01  7.2265625e-01  7.0898438e-01
  6.6210938e-01  6.2792969e-01  6.2402344e-01  6.2011719e-01
  6.1914062e-01  5.9814453e-01  5.8789062e-01  5.6835938e-01
  4.6557617e-01  4.5190430e-01  3.4497070e-01  3.4204102e-01
  1.9921875e-01  1.9873047e-01  1.7456055e-02  5.2947998e-03
  1.5459061e-03 -2.0065308e-02 -1.9714355e-01 -2.2741699e-01
 -5.3564453e-01 -5.4052734e-01 -5.8642578e-01 -5.8984375e-01
 -7.2558594e-01 -7.2851562e-01 -8.3984375e-01 -8.4130859e-01
 -8.5107422e-01 -8.5302734e-01 -8.6181641e-01 -8.6279297e-01
 -9.1894531e-01 -9.3505859e-01 -9.5703125e-01 -9.7216797e-01
 -1.0205078e+00 -1.0419922e+00 -1.0986328e+00 -1.1015625e+00
 -1.2031250e+00 -1.2158203e+00 -1.5429688e+00 -1.5449219e+00
 -1.6171875e+00 -1.6240234e+00 -1.6777344e+00 -1.6865234e+00
 -1.7958984e+00 -1.8027344e+00 -2.1992188e+00 -2.2031250e+00
 -2.2128906e+00 -2.2246094e+00 -2.2265625e+00 -2.2402344e+00
 -2.2617188e+00 -2.2734375e+00 -2.3085938e+00 -2.3222656e+00
 -2.3300781e+00 -2.3378906e+00 -2.4648438e+00 -2.4667969e+00
 -2.4960938e+00 -2.4980469e+00 -2.5566406e+00 -2.5625000e+00
 -2.5644531e+00 -2.5683594e+00 -2.5859375e+00 -2.5878906e+00
 -2.6230469e+00 -2.6250000e+00 -2.6289062e+00 -2.6308594e+00
 -2.6718750e+00 -2.6757812e+00 -2.7343750e+00 -2.7402344e+00
 -2.7656250e+00 -2.7675781e+00 -2.8085938e+00 -2.8164062e+00
 -2.8535156e+00 -2.8632812e+00 -2.8789062e+00 -2.8808594e+00
 -2.8847656e+00 -2.8886719e+00 -2.9003906e+00 -2.9023438e+00
 -2.9218750e+00 -2.9238281e+00 -2.9335938e+00 -2.9355469e+00
 -2.9394531e+00 -2.9433594e+00 -2.9492188e+00 -2.9589844e+00
 -2.9648438e+00 -2.9785156e+00 -2.9804688e+00 -3.0097656e+00
 -3.0117188e+00 -3.0351562e+00 -3.0371094e+00 -3.0449219e+00
 -3.0488281e+00 -3.0683594e+00 -3.0722656e+00 -3.0820312e+00
 -3.0878906e+00 -3.0917969e+00 -3.0957031e+00 -3.1113281e+00
 -3.1191406e+00 -3.1210938e+00 -3.1347656e+00 -3.1406250e+00
 -3.1484375e+00 -3.1503906e+00 -3.1542969e+00 -3.1601562e+00
 -3.1621094e+00 -3.1777344e+00 -3.1796875e+00 -3.2050781e+00
 -3.2070312e+00 -3.2128906e+00 -3.2148438e+00 -3.2304688e+00
 -3.2460938e+00 -3.2656250e+00 -3.2734375e+00 -3.2792969e+00
 -3.2812500e+00 -3.2832031e+00 -3.2929688e+00 -3.2949219e+00
 -3.2968750e+00 -3.3085938e+00 -3.3105469e+00 -3.3125000e+00
 -3.3183594e+00 -3.3203125e+00 -3.3261719e+00 -3.3457031e+00
 -3.3476562e+00 -3.3593750e+00 -3.3632812e+00 -3.3847656e+00
 -3.3925781e+00 -3.3984375e+00 -3.4003906e+00 -3.4101562e+00
 -3.4121094e+00 -3.4277344e+00 -3.4296875e+00 -3.4394531e+00
 -3.4414062e+00 -3.4472656e+00 -3.4511719e+00 -3.4589844e+00
 -3.4863281e+00 -3.4921875e+00 -3.4941406e+00 -3.5097656e+00
 -3.5195312e+00 -3.5273438e+00 -3.5585938e+00 -3.5605469e+00
 -3.5644531e+00 -3.5761719e+00 -3.5820312e+00 -3.6015625e+00
 -3.6054688e+00 -3.6113281e+00 -3.6171875e+00 -3.6484375e+00
 -3.6582031e+00 -3.6640625e+00 -3.6835938e+00 -3.6855469e+00
 -3.6972656e+00 -3.7070312e+00 -3.7128906e+00 -3.7207031e+00
 -3.7265625e+00 -3.7421875e+00 -3.7519531e+00 -4.2421875e+00]
