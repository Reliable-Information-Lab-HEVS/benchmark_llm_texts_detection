log_loss_steps: 200
eval_steps: 504
check_degradation: 0
----------------Epoch 1/1----------------
Epoch 1/1, Loss after 192 samples: 0.6905
Epoch 1/1, Loss after 392 samples: 0.6854
Mean accuracy: 0.7806, std: 0.0095, lower bound: 0.7631, upper bound: 0.8002 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 496 samples: 0.7801 with eval loss: 0.6594
Best model with eval loss 0.6593523753215988 and eval accuracy 0.7801204819277109 with 496 samples seen is saved
Epoch 1/1, Loss after 592 samples: 0.6721
Epoch 1/1, Loss after 792 samples: 0.6024
Epoch 1/1, Loss after 992 samples: 0.4472
Mean accuracy: 0.8204, std: 0.0086, lower bound: 0.8032, upper bound: 0.8368 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 1000 samples: 0.8203 with eval loss: 0.3783
Best model with eval loss 0.3783084149341507 and eval accuracy 0.820281124497992 with 1000 samples seen is saved
Epoch 1/1, Loss after 1192 samples: 0.2740
Epoch 1/1, Loss after 1392 samples: 0.2458
Mean accuracy: 0.9105, std: 0.0065, lower bound: 0.8976, upper bound: 0.9232 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 1504 samples: 0.9101 with eval loss: 0.2113
Best model with eval loss 0.21126395152754573 and eval accuracy 0.910140562248996 with 1504 samples seen is saved
Epoch 1/1, Loss after 1592 samples: 0.2074
Epoch 1/1, Loss after 1792 samples: 0.3039
Epoch 1/1, Loss after 1992 samples: 0.1721
Mean accuracy: 0.9374, std: 0.0052, lower bound: 0.9267, upper bound: 0.9473 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 2008 samples: 0.9372 with eval loss: 0.1554
Best model with eval loss 0.15536476713586525 and eval accuracy 0.9372489959839357 with 2008 samples seen is saved
Epoch 1/1, Loss after 2192 samples: 0.1793
Epoch 1/1, Loss after 2392 samples: 0.1591
Mean accuracy: 0.9259, std: 0.0059, lower bound: 0.9147, upper bound: 0.9372 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 2512 samples: 0.9257 with eval loss: 0.1746
Epoch 1/1, Loss after 2592 samples: 0.1088
Epoch 1/1, Loss after 2792 samples: 0.1399
Epoch 1/1, Loss after 2992 samples: 0.1604
Mean accuracy: 0.9112, std: 0.0063, lower bound: 0.8991, upper bound: 0.9237 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 3016 samples: 0.9111 with eval loss: 0.2091
Epoch 1/1, Loss after 3192 samples: 0.1528
Epoch 1/1, Loss after 3392 samples: 0.1189
Mean accuracy: 0.9602, std: 0.0044, lower bound: 0.9513, upper bound: 0.9684 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 3520 samples: 0.9603 with eval loss: 0.1050
Best model with eval loss 0.10497268663352752 and eval accuracy 0.9603413654618473 with 3520 samples seen is saved
Epoch 1/1, Loss after 3592 samples: 0.1099
Epoch 1/1, Loss after 3792 samples: 0.1813
Epoch 1/1, Loss after 3992 samples: 0.1406
Mean accuracy: 0.9568, std: 0.0046, lower bound: 0.9473, upper bound: 0.9659 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 4024 samples: 0.9568 with eval loss: 0.1237
Epoch 1/1, Loss after 4192 samples: 0.1229
Epoch 1/1, Loss after 4392 samples: 0.1040
Mean accuracy: 0.9395, std: 0.0054, lower bound: 0.9292, upper bound: 0.9503 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 4528 samples: 0.9393 with eval loss: 0.1589
Epoch 1/1, Loss after 4592 samples: 0.1233
Epoch 1/1, Loss after 4792 samples: 0.1217
Epoch 1/1, Loss after 4992 samples: 0.1782
Mean accuracy: 0.9584, std: 0.0046, lower bound: 0.9493, upper bound: 0.9674 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 5032 samples: 0.9583 with eval loss: 0.1174
Epoch 1/1, Loss after 5192 samples: 0.1108
Epoch 1/1, Loss after 5392 samples: 0.0889
Mean accuracy: 0.8831, std: 0.0073, lower bound: 0.8685, upper bound: 0.8971 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 5536 samples: 0.8830 with eval loss: 0.3283
Epoch 1/1, Loss after 5592 samples: 0.1924
Epoch 1/1, Loss after 5792 samples: 0.1801
Epoch 1/1, Loss after 5992 samples: 0.1674
Mean accuracy: 0.9429, std: 0.0052, lower bound: 0.9332, upper bound: 0.9533 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 6040 samples: 0.9428 with eval loss: 0.1431
Epoch 1/1, Loss after 6192 samples: 0.1073
Epoch 1/1, Loss after 6392 samples: 0.0624
Mean accuracy: 0.9453, std: 0.0051, lower bound: 0.9342, upper bound: 0.9548 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 6544 samples: 0.9453 with eval loss: 0.1447
Epoch 1/1, Loss after 6592 samples: 0.1058
Epoch 1/1, Loss after 6792 samples: 0.1301
Epoch 1/1, Loss after 6992 samples: 0.0903
Mean accuracy: 0.9421, std: 0.0051, lower bound: 0.9312, upper bound: 0.9518 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 7048 samples: 0.9423 with eval loss: 0.1476
Epoch 1/1, Loss after 7192 samples: 0.1121
Epoch 1/1, Loss after 7392 samples: 0.1416
Mean accuracy: 0.9490, std: 0.0049, lower bound: 0.9393, upper bound: 0.9583 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 7552 samples: 0.9488 with eval loss: 0.1279
Epoch 1/1, Loss after 7592 samples: 0.1078
Epoch 1/1, Loss after 7792 samples: 0.1126
Epoch 1/1, Loss after 7992 samples: 0.1041
Mean accuracy: 0.9342, std: 0.0055, lower bound: 0.9232, upper bound: 0.9448 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 8056 samples: 0.9342 with eval loss: 0.1671
Epoch 1/1, Loss after 8192 samples: 0.1163
Epoch 1/1, Loss after 8392 samples: 0.1774
Mean accuracy: 0.9371, std: 0.0053, lower bound: 0.9267, upper bound: 0.9473 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 8560 samples: 0.9372 with eval loss: 0.1494
Epoch 1/1, Loss after 8592 samples: 0.1894
Epoch 1/1, Loss after 8792 samples: 0.0988
Epoch 1/1, Loss after 8992 samples: 0.0889
Mean accuracy: 0.9627, std: 0.0043, lower bound: 0.9538, upper bound: 0.9709 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 9064 samples: 0.9629 with eval loss: 0.0919
Best model with eval loss 0.09187437384482847 and eval accuracy 0.9628514056224899 with 9064 samples seen is saved
Epoch 1/1, Loss after 9192 samples: 0.0857
Epoch 1/1, Loss after 9392 samples: 0.0723
Mean accuracy: 0.9329, std: 0.0055, lower bound: 0.9222, upper bound: 0.9433 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 9568 samples: 0.9327 with eval loss: 0.1849
Epoch 1/1, Loss after 9592 samples: 0.0726
Epoch 1/1, Loss after 9792 samples: 0.0911
Epoch 1/1, Loss after 9992 samples: 0.1135
Mean accuracy: 0.9740, std: 0.0035, lower bound: 0.9664, upper bound: 0.9809 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 10072 samples: 0.9739 with eval loss: 0.0732
Best model with eval loss 0.07315097240559068 and eval accuracy 0.9738955823293173 with 10072 samples seen is saved
Epoch 1/1, Loss after 10192 samples: 0.0734
Epoch 1/1, Loss after 10392 samples: 0.0634
Mean accuracy: 0.9463, std: 0.0051, lower bound: 0.9357, upper bound: 0.9553 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 10576 samples: 0.9463 with eval loss: 0.1374
Epoch 1/1, Loss after 10592 samples: 0.0902
Epoch 1/1, Loss after 10792 samples: 0.0543
Epoch 1/1, Loss after 10992 samples: 0.0894
Mean accuracy: 0.9091, std: 0.0062, lower bound: 0.8971, upper bound: 0.9212 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 11080 samples: 0.9091 with eval loss: 0.2408
Epoch 1/1, Loss after 11192 samples: 0.0896
Epoch 1/1, Loss after 11392 samples: 0.0745
Mean accuracy: 0.9368, std: 0.0054, lower bound: 0.9262, upper bound: 0.9468 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 11584 samples: 0.9367 with eval loss: 0.1627
Epoch 1/1, Loss after 11592 samples: 0.0729
Epoch 1/1, Loss after 11792 samples: 0.0745
Epoch 1/1, Loss after 11992 samples: 0.0926
Mean accuracy: 0.9479, std: 0.0049, lower bound: 0.9382, upper bound: 0.9568 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 12088 samples: 0.9478 with eval loss: 0.1323
Epoch 1/1, Loss after 12192 samples: 0.0617
Epoch 1/1, Loss after 12392 samples: 0.0322
Epoch 1/1, Loss after 12592 samples: 0.0694
Mean accuracy: 0.9644, std: 0.0039, lower bound: 0.9568, upper bound: 0.9719 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 12592 samples: 0.9644 with eval loss: 0.0900
Epoch 1/1, Loss after 12792 samples: 0.1142
Epoch 1/1, Loss after 12992 samples: 0.1084
Mean accuracy: 0.9354, std: 0.0055, lower bound: 0.9247, upper bound: 0.9453 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 13096 samples: 0.9357 with eval loss: 0.1637
Epoch 1/1, Loss after 13192 samples: 0.1271
Epoch 1/1, Loss after 13392 samples: 0.0885
Epoch 1/1, Loss after 13592 samples: 0.0918
Mean accuracy: 0.9439, std: 0.0053, lower bound: 0.9337, upper bound: 0.9543 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 13600 samples: 0.9438 with eval loss: 0.1475
Epoch 1/1, Loss after 13792 samples: 0.0467
Epoch 1/1, Loss after 13992 samples: 0.0448
Mean accuracy: 0.9570, std: 0.0046, lower bound: 0.9478, upper bound: 0.9654 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 14104 samples: 0.9568 with eval loss: 0.1058
Epoch 1/1, Loss after 14192 samples: 0.0802
Epoch 1/1, Loss after 14392 samples: 0.0603
Epoch 1/1, Loss after 14592 samples: 0.0909
Mean accuracy: 0.9663, std: 0.0040, lower bound: 0.9588, upper bound: 0.9744 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 14608 samples: 0.9664 with eval loss: 0.0889
Epoch 1/1, Loss after 14792 samples: 0.1090
Epoch 1/1, Loss after 14992 samples: 0.1039
Mean accuracy: 0.9524, std: 0.0047, lower bound: 0.9433, upper bound: 0.9613 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 15112 samples: 0.9523 with eval loss: 0.1108
Epoch 1/1, Loss after 15192 samples: 0.0401
Epoch 1/1, Loss after 15392 samples: 0.0574
Epoch 1/1, Loss after 15592 samples: 0.1130
Mean accuracy: 0.9524, std: 0.0045, lower bound: 0.9433, upper bound: 0.9613 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 15616 samples: 0.9523 with eval loss: 0.1124
Epoch 1/1, Loss after 15792 samples: 0.0742
Training signal is False, stopping training
Training finished
Best model: {'eval_acc': 0.9738955823293173, 'nb_samples': 10072, 'eval_loss': 0.07315097240559068}
Training loss logs: [{'samples': 192, 'loss': 0.6905078125}, {'samples': 392, 'loss': 0.68537109375}, {'samples': 592, 'loss': 0.6721240234375}, {'samples': 792, 'loss': 0.602366943359375}, {'samples': 992, 'loss': 0.4472308349609375}, {'samples': 1192, 'loss': 0.2739658737182617}, {'samples': 1392, 'loss': 0.24584463119506836}, {'samples': 1592, 'loss': 0.2074107551574707}, {'samples': 1792, 'loss': 0.30391130447387693}, {'samples': 1992, 'loss': 0.17209256172180176}, {'samples': 2192, 'loss': 0.17926777124404908}, {'samples': 2392, 'loss': 0.1591373062133789}, {'samples': 2592, 'loss': 0.10882066249847412}, {'samples': 2792, 'loss': 0.1398968493938446}, {'samples': 2992, 'loss': 0.1603557860851288}, {'samples': 3192, 'loss': 0.15278947114944458}, {'samples': 3392, 'loss': 0.11891693115234375}, {'samples': 3592, 'loss': 0.10994482517242432}, {'samples': 3792, 'loss': 0.1813194513320923}, {'samples': 3992, 'loss': 0.14058744192123412}, {'samples': 4192, 'loss': 0.12287335157394409}, {'samples': 4392, 'loss': 0.10401628732681274}, {'samples': 4592, 'loss': 0.1233393156528473}, {'samples': 4792, 'loss': 0.12174732983112335}, {'samples': 4992, 'loss': 0.1782141673564911}, {'samples': 5192, 'loss': 0.11080469846725464}, {'samples': 5392, 'loss': 0.08891971707344055}, {'samples': 5592, 'loss': 0.19235147595405577}, {'samples': 5792, 'loss': 0.18011262655258178}, {'samples': 5992, 'loss': 0.1674450707435608}, {'samples': 6192, 'loss': 0.10728729248046875}, {'samples': 6392, 'loss': 0.062421157360076904}, {'samples': 6592, 'loss': 0.10584261894226074}, {'samples': 6792, 'loss': 0.13005518198013305}, {'samples': 6992, 'loss': 0.09027877330780029}, {'samples': 7192, 'loss': 0.11208420991897583}, {'samples': 7392, 'loss': 0.14157476186752319}, {'samples': 7592, 'loss': 0.10780911684036255}, {'samples': 7792, 'loss': 0.1125539368391037}, {'samples': 7992, 'loss': 0.1040552031993866}, {'samples': 8192, 'loss': 0.11631328403949738}, {'samples': 8392, 'loss': 0.17740703105926514}, {'samples': 8592, 'loss': 0.18936503291130066}, {'samples': 8792, 'loss': 0.09880327463150024}, {'samples': 8992, 'loss': 0.08887984633445739}, {'samples': 9192, 'loss': 0.0856950330734253}, {'samples': 9392, 'loss': 0.07229735851287841}, {'samples': 9592, 'loss': 0.07263287782669067}, {'samples': 9792, 'loss': 0.09112637996673584}, {'samples': 9992, 'loss': 0.1135342037677765}, {'samples': 10192, 'loss': 0.07343173265457154}, {'samples': 10392, 'loss': 0.06342621862888337}, {'samples': 10592, 'loss': 0.09017374098300934}, {'samples': 10792, 'loss': 0.054290514588356015}, {'samples': 10992, 'loss': 0.0893988287448883}, {'samples': 11192, 'loss': 0.0895568734407425}, {'samples': 11392, 'loss': 0.07450883507728577}, {'samples': 11592, 'loss': 0.07286557853221894}, {'samples': 11792, 'loss': 0.07445780813694}, {'samples': 11992, 'loss': 0.09264874458312988}, {'samples': 12192, 'loss': 0.061682666540145877}, {'samples': 12392, 'loss': 0.03220544397830963}, {'samples': 12592, 'loss': 0.06938913583755493}, {'samples': 12792, 'loss': 0.11421964466571807}, {'samples': 12992, 'loss': 0.10836594104766846}, {'samples': 13192, 'loss': 0.12712049543857573}, {'samples': 13392, 'loss': 0.08851927876472473}, {'samples': 13592, 'loss': 0.09176982760429382}, {'samples': 13792, 'loss': 0.04674839198589325}, {'samples': 13992, 'loss': 0.04478581845760345}, {'samples': 14192, 'loss': 0.08020306885242462}, {'samples': 14392, 'loss': 0.06029255867004395}, {'samples': 14592, 'loss': 0.09089748919010163}, {'samples': 14792, 'loss': 0.10897932708263397}, {'samples': 14992, 'loss': 0.10393157601356506}, {'samples': 15192, 'loss': 0.040124120712280276}, {'samples': 15392, 'loss': 0.05737469434738159}, {'samples': 15592, 'loss': 0.11297500491142273}, {'samples': 15792, 'loss': 0.07424099385738372}]
Evaluation accuracy logs: [{'samples': 496, 'accuracy': 0.7805763052208836, 'std': 0.009480877709604821, 'lower_bound': 0.7630522088353414, 'upper_bound': 0.8002008032128514}, {'samples': 1000, 'accuracy': 0.8204231927710843, 'std': 0.00861459402479378, 'lower_bound': 0.8032128514056225, 'upper_bound': 0.8368473895582329}, {'samples': 1504, 'accuracy': 0.9104764056224899, 'std': 0.006453985178529812, 'lower_bound': 0.8975903614457831, 'upper_bound': 0.9231927710843374}, {'samples': 2008, 'accuracy': 0.9373574297188755, 'std': 0.005210479284043423, 'lower_bound': 0.9267068273092369, 'upper_bound': 0.947289156626506}, {'samples': 2512, 'accuracy': 0.9258529116465863, 'std': 0.005880517867096036, 'lower_bound': 0.9146586345381527, 'upper_bound': 0.9372489959839357}, {'samples': 3016, 'accuracy': 0.9111777108433735, 'std': 0.006335127431643987, 'lower_bound': 0.8990963855421686, 'upper_bound': 0.9236947791164659}, {'samples': 3520, 'accuracy': 0.9602484939759035, 'std': 0.004386952086965256, 'lower_bound': 0.9513052208835341, 'upper_bound': 0.9683734939759037}, {'samples': 4024, 'accuracy': 0.9568308232931727, 'std': 0.004619700132960874, 'lower_bound': 0.947289156626506, 'upper_bound': 0.9658634538152611}, {'samples': 4528, 'accuracy': 0.939484437751004, 'std': 0.005408511411057331, 'lower_bound': 0.9292168674698795, 'upper_bound': 0.9503012048192772}, {'samples': 5032, 'accuracy': 0.9584191767068273, 'std': 0.004572732691288609, 'lower_bound': 0.9492971887550201, 'upper_bound': 0.9673694779116466}, {'samples': 5536, 'accuracy': 0.8831229919678715, 'std': 0.007265085476605721, 'lower_bound': 0.8684738955823293, 'upper_bound': 0.8970883534136547}, {'samples': 6040, 'accuracy': 0.9429352409638555, 'std': 0.00516146464062436, 'lower_bound': 0.9332329317269076, 'upper_bound': 0.9533132530120482}, {'samples': 6544, 'accuracy': 0.9453207831325301, 'std': 0.005097316461502851, 'lower_bound': 0.9342369477911646, 'upper_bound': 0.9548192771084337}, {'samples': 7048, 'accuracy': 0.9420868473895582, 'std': 0.005113715876846482, 'lower_bound': 0.9312248995983936, 'upper_bound': 0.9518072289156626}, {'samples': 7552, 'accuracy': 0.9489834337349398, 'std': 0.004873626120952403, 'lower_bound': 0.9392570281124498, 'upper_bound': 0.9583333333333334}, {'samples': 8056, 'accuracy': 0.9342429718875503, 'std': 0.005510298110627006, 'lower_bound': 0.9231927710843374, 'upper_bound': 0.9447791164658634}, {'samples': 8560, 'accuracy': 0.9370517068273093, 'std': 0.005324706953727908, 'lower_bound': 0.9267068273092369, 'upper_bound': 0.947289156626506}, {'samples': 9064, 'accuracy': 0.9626977911646586, 'std': 0.004274101338402198, 'lower_bound': 0.9538152610441767, 'upper_bound': 0.9708835341365462}, {'samples': 9568, 'accuracy': 0.9328900602409639, 'std': 0.005502257908280972, 'lower_bound': 0.9221887550200804, 'upper_bound': 0.9432730923694779}, {'samples': 10072, 'accuracy': 0.9739949799196789, 'std': 0.0035378852141070083, 'lower_bound': 0.9663654618473896, 'upper_bound': 0.9809236947791165}, {'samples': 10576, 'accuracy': 0.9462525100401605, 'std': 0.005117990352746126, 'lower_bound': 0.9357429718875502, 'upper_bound': 0.9553338353413654}, {'samples': 11080, 'accuracy': 0.9090943775100402, 'std': 0.006215843587531235, 'lower_bound': 0.8970758032128514, 'upper_bound': 0.9211847389558233}, {'samples': 11584, 'accuracy': 0.9368283132530121, 'std': 0.0053946072769738744, 'lower_bound': 0.9262048192771084, 'upper_bound': 0.9467871485943775}, {'samples': 12088, 'accuracy': 0.9479307228915662, 'std': 0.004873066102314683, 'lower_bound': 0.9382404618473895, 'upper_bound': 0.9568273092369478}, {'samples': 12592, 'accuracy': 0.9643654618473895, 'std': 0.003887427289868868, 'lower_bound': 0.9568273092369478, 'upper_bound': 0.9718875502008032}, {'samples': 13096, 'accuracy': 0.9354211847389559, 'std': 0.005461702483571225, 'lower_bound': 0.9246736947791164, 'upper_bound': 0.945281124497992}, {'samples': 13600, 'accuracy': 0.9438995983935743, 'std': 0.00530056053648437, 'lower_bound': 0.9337223895582328, 'upper_bound': 0.9543172690763052}, {'samples': 14104, 'accuracy': 0.9569578313253012, 'std': 0.004574238128605261, 'lower_bound': 0.9477911646586346, 'upper_bound': 0.9653614457831325}, {'samples': 14608, 'accuracy': 0.9663293172690762, 'std': 0.00402768199119819, 'lower_bound': 0.9588353413654619, 'upper_bound': 0.9743975903614458}, {'samples': 15112, 'accuracy': 0.9524342369477911, 'std': 0.004715322698064756, 'lower_bound': 0.9432605421686746, 'upper_bound': 0.9613453815261044}, {'samples': 15616, 'accuracy': 0.9524081325301205, 'std': 0.004528774445371365, 'lower_bound': 0.9432730923694779, 'upper_bound': 0.9613453815261044}]
Evaluating the best model on the test set of dataset ./fake_true_datasets/fake_true_dataset_phi_10k...
Test metrics:
accuracy: 0.9521611445783132
precision: 0.9149762789464929
recall: 0.997043104818876
f1_score: 0.9542285486235991
fp_rate: 0.09279167858989498
tp_rate: 0.997043104818876
std_accuracy: 0.004627413400699278
std_precision: 0.00820738329536835
std_recall: 0.001739576439778829
std_f1_score: 0.00453478842394674
std_fp_rate: 0.008921095925258582
std_tp_rate: 0.001739576439778829
TP: 993.81
TN: 902.895
FP: 92.347
FN: 2.948
roc_auc: 0.9974088119546459
fpr: [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00100402 0.00100402 0.00100402 0.00100402
 0.00100402 0.00100402 0.00100402 0.00100402 0.00100402 0.00100402
 0.00100402 0.00100402 0.00100402 0.00100402 0.00100402 0.00100402
 0.00100402 0.00100402 0.00100402 0.00100402 0.00100402 0.00100402
 0.00100402 0.00100402 0.00200803 0.00200803 0.00200803 0.00200803
 0.00301205 0.00301205 0.00301205 0.00401606 0.00401606 0.00401606
 0.00401606 0.00502008 0.0060241  0.0060241  0.00702811 0.00702811
 0.00803213 0.00803213 0.00803213 0.00803213 0.00903614 0.00903614
 0.01004016 0.01004016 0.01104418 0.01104418 0.01204819 0.01204819
 0.01305221 0.01305221 0.01606426 0.01606426 0.01706827 0.01907631
 0.02309237 0.02309237 0.02409639 0.02409639 0.02610442 0.02610442
 0.02911647 0.02911647 0.03012048 0.03012048 0.03012048 0.0311245
 0.0311245  0.03212851 0.03212851 0.03413655 0.03413655 0.03815261
 0.03815261 0.03915663 0.03915663 0.04417671 0.04417671 0.04618474
 0.04618474 0.05220884 0.05220884 0.05722892 0.05722892 0.06325301
 0.06325301 0.06726908 0.06726908 0.08534137 0.08534137 0.08935743
 0.08935743 0.13052209 0.13253012 0.16064257 0.1626506  0.16666667
 0.16666667 0.16967871 0.16967871 0.17369478 0.17570281 0.20080321
 0.20281124 0.22791165 0.22991968 0.23192771 0.23393574 0.25
 0.25200803 0.26104418 0.26305221 0.26807229 0.27008032 0.27610442
 0.27911647 0.28212851 0.28413655 0.29618474 0.29819277 0.30923695
 0.31124498 0.32429719 0.3253012  0.3313253  0.33534137 0.34939759
 0.35140562 0.36044177 0.3624498  0.3684739  0.37048193 0.37449799
 0.37650602 0.37951807 0.3815261  0.39156627 0.3935743  0.40763052
 0.40963855 0.41666667 0.41967871 0.42771084 0.43172691 0.43574297
 0.437751   0.43875502 0.44076305 0.4437751  0.44779116 0.44879518
 0.45080321 0.45883534 0.46084337 0.46787149 0.47188755 0.47289157
 0.4748996  0.47991968 0.48293173 0.48393574 0.48594378 0.48694779
 0.48895582 0.48995984 0.4939759  0.49598394 0.49899598 0.50100402
 0.50301205 0.5060241  0.50803213 0.51104418 0.51907631 0.52409639
 0.52710843 0.52911647 0.53313253 0.53614458 0.53815261 0.54216867
 0.54417671 0.54518072 0.54819277 0.55421687 0.55722892 0.56124498
 0.56325301 0.56526104 0.56726908 0.57228916 0.57429719 0.57630522
 0.58232932 0.58534137 0.5873494  0.59337349 0.59638554 0.60040161
 0.60140562 0.60341365 0.60441767 0.6064257  0.61445783 0.6184739
 0.62248996 0.62650602 0.62951807 0.63253012 0.63654618 0.63955823
 0.64257028 0.64457831 0.64658635 0.65060241 0.65261044 0.65562249
 0.65763052 0.66064257 0.6626506  0.66365462 0.66566265 0.66767068
 0.66967871 0.67068273 0.6746988  0.67971888 0.68373494 0.68674699
 0.68875502 0.69176707 0.6937751  0.69678715 0.69779116 0.70080321
 0.70281124 0.70381526 0.70783133 0.70883534 0.71084337 0.71586345
 0.71787149 0.72289157 0.7248996  0.72991968 0.73393574 0.73694779
 0.73995984 0.74598394 0.74899598 0.75702811 0.76305221 0.76506024
 0.76706827 0.76807229 0.77108434 0.77309237 0.7751004  0.77610442
 0.77911647 0.78012048 0.78313253 0.78413655 0.78815261 0.78915663
 0.79116466 0.79216867 0.79518072 0.79618474 0.79919679 0.8002008
 0.8062249  0.80823293 0.80923695 0.81526104 0.81827309 0.82028112
 0.82128514 0.82730924 0.83333333 0.83534137 0.83634538 0.83935743
 0.84236948 0.84638554 0.84839357 0.85040161 0.85240964 0.85441767
 0.86144578 0.86345382 0.86445783 0.87048193 0.87449799 0.87650602
 0.87851406 0.88052209 0.88253012 0.88453815 0.88554217 0.8875502
 0.88855422 0.89056225 0.89558233 0.89859438 0.90160643 0.90261044
 0.90461847 0.90662651 0.90863454 0.91164659 0.91365462 0.91465863
 0.91666667 0.92269076 0.92570281 0.93473896 0.93674699 0.94076305
 0.94477912 0.94578313 0.95180723 0.95381526 0.95783133 0.95983936
 0.96084337 0.96285141 0.96586345 0.96787149 0.96987952 0.97188755
 0.97389558 0.97590361 0.98393574 0.98795181 0.99297189 0.99497992
 1.        ]
tpr: [0.         0.00100402 0.00301205 0.00502008 0.00803213 0.01606426
 0.02108434 0.0311245  0.03212851 0.03815261 0.04116466 0.04417671
 0.04518072 0.04718876 0.04919679 0.05321285 0.0562249  0.05823293
 0.05923695 0.062249   0.06425703 0.06726908 0.06927711 0.07329317
 0.0753012  0.07630522 0.08433735 0.08534137 0.0873494  0.09036145
 0.09236948 0.09437751 0.09839357 0.10140562 0.10341365 0.10441767
 0.10742972 0.11044177 0.1124498  0.11345382 0.1184739  0.12048193
 0.12349398 0.12550201 0.12751004 0.13052209 0.13253012 0.13453815
 0.13554217 0.13855422 0.13955823 0.14156627 0.14257028 0.14859438
 0.15060241 0.15461847 0.15562249 0.15763052 0.15863454 0.17369478
 0.17570281 0.17771084 0.18273092 0.18674699 0.187751   0.19076305
 0.19277108 0.1937751  0.19578313 0.19779116 0.20783133 0.20883534
 0.21184739 0.21385542 0.22289157 0.22389558 0.22590361 0.22690763
 0.22991968 0.23092369 0.23393574 0.23995984 0.24297189 0.24497992
 0.24799197 0.24899598 0.25301205 0.26104418 0.26305221 0.26606426
 0.26907631 0.2751004  0.27610442 0.27811245 0.28514056 0.28815261
 0.29417671 0.29618474 0.29718876 0.3002008  0.30220884 0.30321285
 0.3062249  0.30823293 0.31024096 0.31425703 0.31626506 0.31827309
 0.31927711 0.32228916 0.32329317 0.3253012  0.32630522 0.33032129
 0.33232932 0.33634538 0.33835341 0.34236948 0.34538153 0.34738956
 0.34839357 0.35040161 0.35341365 0.35542169 0.36144578 0.3624498
 0.36445783 0.36746988 0.3684739  0.37048193 0.37148594 0.37349398
 0.37650602 0.3815261  0.38554217 0.3875502  0.39056225 0.39257028
 0.39558233 0.39859438 0.39959839 0.40361446 0.40461847 0.40863454
 0.41064257 0.41164659 0.41566265 0.4186747  0.42168675 0.43172691
 0.43273092 0.43674699 0.43975904 0.4437751  0.44578313 0.45180723
 0.45582329 0.45883534 0.46084337 0.46686747 0.47088353 0.47590361
 0.47791165 0.47891566 0.48192771 0.48393574 0.48694779 0.48795181
 0.49598394 0.50200803 0.50401606 0.51004016 0.51204819 0.51405622
 0.51606426 0.51907631 0.52108434 0.5251004  0.52610442 0.53012048
 0.5311245  0.53313253 0.53714859 0.53915663 0.54116466 0.54317269
 0.54718876 0.54819277 0.5502008  0.55321285 0.55823293 0.56024096
 0.56124498 0.56425703 0.56526104 0.56726908 0.57028112 0.57429719
 0.57931727 0.58232932 0.58433735 0.58634538 0.5873494  0.59136546
 0.59839357 0.60040161 0.60140562 0.60341365 0.60441767 0.60742972
 0.60943775 0.6124498  0.61445783 0.61546185 0.61947791 0.62751004
 0.63052209 0.63253012 0.63453815 0.6375502  0.64156627 0.65060241
 0.65261044 0.65662651 0.65863454 0.66164659 0.66365462 0.66566265
 0.66767068 0.6686747  0.67269076 0.6746988  0.67670683 0.67771084
 0.68072289 0.68172691 0.687751   0.68975904 0.69176707 0.69477912
 0.69678715 0.69879518 0.70080321 0.70381526 0.70582329 0.70983936
 0.71184739 0.71385542 0.71586345 0.71686747 0.71987952 0.72188755
 0.72389558 0.72791165 0.72891566 0.73293173 0.73594378 0.74196787
 0.74698795 0.74799197 0.75100402 0.75502008 0.7560241  0.76204819
 0.76506024 0.76907631 0.7751004  0.77811245 0.78714859 0.78915663
 0.79116466 0.79518072 0.79919679 0.80120482 0.80321285 0.80522088
 0.80823293 0.81024096 0.81024096 0.81726908 0.81927711 0.82228916
 0.82831325 0.83032129 0.83232932 0.83534137 0.8373494  0.83835341
 0.84036145 0.84337349 0.84738956 0.84939759 0.85140562 0.85441767
 0.8564257  0.85843373 0.86044177 0.86144578 0.86345382 0.86445783
 0.86646586 0.86947791 0.86947791 0.87550201 0.87751004 0.88253012
 0.88353414 0.88855422 0.89257028 0.89257028 0.90562249 0.90763052
 0.9186747  0.91967871 0.91967871 0.92971888 0.92971888 0.93273092
 0.93273092 0.93674699 0.93875502 0.94277108 0.94277108 0.9497992
 0.9497992  0.95281124 0.95281124 0.95381526 0.95381526 0.95983936
 0.95983936 0.96084337 0.96084337 0.96787149 0.96787149 0.96787149
 0.96787149 0.96987952 0.96987952 0.97088353 0.97088353 0.97289157
 0.97289157 0.97590361 0.97590361 0.97791165 0.97991968 0.97991968
 0.98092369 0.98092369 0.98293173 0.98293173 0.98393574 0.98393574
 0.98493976 0.98493976 0.98594378 0.98594378 0.98694779 0.98694779
 0.98795181 0.98795181 0.99096386 0.99096386 0.99196787 0.99196787
 0.99297189 0.99297189 0.9939759  0.9939759  0.99598394 0.99598394
 0.99698795 0.99698795 0.99698795 0.99698795 0.99698795 0.99698795
 0.99799197 0.99799197 0.99899598 0.99899598 0.99899598 0.99899598
 0.99899598 0.99899598 0.99899598 0.99899598 0.99899598 0.99899598
 0.99899598 0.99899598 0.99899598 0.99899598 0.99899598 0.99899598
 0.99899598 0.99899598 0.99899598 0.99899598 0.99899598 0.99899598
 0.99899598 0.99899598 1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.        ]
thresholds: [        inf  4.7226562   4.609375    4.6054688   4.5625      4.5273438
  4.5039062   4.4648438   4.4609375   4.4414062   4.4257812   4.421875
  4.4140625   4.4101562   4.4023438   4.3984375   4.3867188   4.3828125
  4.375       4.3710938   4.3671875   4.3515625   4.3476562   4.34375
  4.3359375   4.3320312   4.3164062   4.3125      4.3085938   4.3046875
  4.3007812   4.2890625   4.2734375   4.2695312   4.2617188   4.2578125
  4.2539062   4.2382812   4.234375    4.2304688   4.2265625   4.2226562
  4.21875     4.2148438   4.2070312   4.203125    4.1914062   4.1835938
  4.1796875   4.171875    4.1640625   4.1601562   4.15625     4.1484375
  4.1445312   4.1367188   4.1328125   4.1289062   4.125       4.0976562
  4.0820312   4.078125    4.0742188   4.0585938   4.0546875   4.0507812
  4.046875    4.0429688   4.0390625   4.03125     4.0078125   4.0039062
  4.          3.9941406   3.9863281   3.9746094   3.9726562   3.9707031
  3.9648438   3.9628906   3.9609375   3.9296875   3.9277344   3.9238281
  3.921875    3.9199219   3.9179688   3.8984375   3.8964844   3.8945312
  3.8886719   3.8808594   3.8789062   3.875       3.8535156   3.8515625
  3.8359375   3.8339844   3.8261719   3.8222656   3.8203125   3.8183594
  3.8164062   3.8144531   3.8085938   3.8046875   3.8007812   3.7988281
  3.796875    3.7949219   3.7929688   3.7910156   3.7890625   3.7871094
  3.7832031   3.78125     3.7714844   3.7675781   3.765625    3.7636719
  3.7617188   3.7597656   3.7578125   3.7539062   3.75        3.7480469
  3.7441406   3.7421875   3.7382812   3.734375    3.7324219   3.7285156
  3.7265625   3.7226562   3.7207031   3.71875     3.7167969   3.7148438
  3.7070312   3.703125    3.7011719   3.6894531   3.6875      3.6855469
  3.6816406   3.6796875   3.6738281   3.671875    3.6660156   3.6542969
  3.6523438   3.6484375   3.6445312   3.640625    3.6367188   3.6328125
  3.6289062   3.6230469   3.6210938   3.6152344   3.6113281   3.6015625
  3.5996094   3.5976562   3.5957031   3.5917969   3.5878906   3.5859375
  3.578125    3.5742188   3.5703125   3.5585938   3.5566406   3.5507812
  3.546875    3.5449219   3.5410156   3.5351562   3.5332031   3.53125
  3.5253906   3.5234375   3.5097656   3.5078125   3.5039062   3.5019531
  3.5         3.4980469   3.4960938   3.4902344   3.4785156   3.4765625
  3.4746094   3.4667969   3.4628906   3.4609375   3.4589844   3.453125
  3.4414062   3.4355469   3.4316406   3.4296875   3.4199219   3.4160156
  3.3964844   3.390625    3.3886719   3.3828125   3.3808594   3.3769531
  3.3730469   3.3691406   3.3652344   3.3574219   3.3535156   3.3261719
  3.3242188   3.3183594   3.3164062   3.3144531   3.3105469   3.2851562
  3.2832031   3.2695312   3.2675781   3.2617188   3.2597656   3.2519531
  3.25        3.2460938   3.2382812   3.234375    3.2285156   3.2265625
  3.2207031   3.21875     3.2109375   3.2070312   3.2050781   3.2011719
  3.1992188   3.1953125   3.1914062   3.1796875   3.1777344   3.1660156
  3.1601562   3.1542969   3.1484375   3.1464844   3.1445312   3.140625
  3.1367188   3.1347656   3.1328125   3.1308594   3.1171875   3.1074219
  3.1015625   3.0996094   3.0976562   3.0800781   3.078125    3.0664062
  3.0605469   3.0488281   3.0273438   3.0253906   3.0019531   3.
  2.984375    2.9804688   2.9628906   2.9609375   2.953125    2.9511719
  2.9394531   2.9375      2.9355469   2.8847656   2.8828125   2.8652344
  2.8574219   2.8476562   2.8457031   2.8359375   2.8339844   2.8320312
  2.8261719   2.8144531   2.8066406   2.796875    2.7949219   2.7773438
  2.7734375   2.7558594   2.7519531   2.75        2.7480469   2.7382812
  2.7363281   2.7265625   2.7148438   2.6816406   2.6757812   2.6523438
  2.6425781   2.609375    2.5957031   2.59375     2.5136719   2.5097656
  2.4179688   2.4140625   2.4101562   2.3222656   2.3203125   2.3085938
  2.2949219   2.2714844   2.2675781   2.2402344   2.2011719   2.1464844
  2.109375    2.0898438   2.0703125   2.0449219   2.0410156   1.9785156
  1.9746094   1.9707031   1.9199219   1.7402344   1.6992188   1.6953125
  1.65625     1.5800781   1.5371094   1.5361328   1.4912109   1.4765625
  1.4580078   1.4208984   1.40625     1.3720703   1.34375     1.3291016
  1.3251953   1.3037109   1.2919922   1.2460938   1.2382812   1.1650391
  1.1591797   1.1503906   1.1455078   1.0800781   1.0556641   1.0458984
  1.0068359   0.8925781   0.8623047   0.7973633   0.7841797   0.6142578
  0.60498047  0.5649414   0.5234375   0.22302246  0.21057129  0.14489746
  0.1262207  -0.4638672  -0.49267578 -0.8618164  -0.86279297 -0.8955078
 -0.9003906  -0.9350586  -0.9501953  -0.99365234 -1.0058594  -1.1796875
 -1.1972656  -1.4199219  -1.4238281  -1.4375     -1.4570312  -1.6386719
 -1.640625   -1.7041016  -1.7119141  -1.7412109  -1.7451172  -1.7998047
 -1.8037109  -1.8232422  -1.8271484  -1.9199219  -1.9267578  -1.9941406
 -1.9990234  -2.0820312  -2.0859375  -2.1308594  -2.1386719  -2.2070312
 -2.2148438  -2.2890625  -2.296875   -2.3417969  -2.34375    -2.3574219
 -2.3710938  -2.3925781  -2.3945312  -2.4570312  -2.4589844  -2.5175781
 -2.5273438  -2.5566406  -2.5664062  -2.6308594  -2.6367188  -2.6542969
 -2.6640625  -2.6757812  -2.6796875  -2.6914062  -2.703125   -2.7050781
 -2.7128906  -2.7519531  -2.7539062  -2.78125    -2.7890625  -2.7910156
 -2.7929688  -2.8203125  -2.8242188  -2.8300781  -2.8359375  -2.8398438
 -2.84375    -2.8476562  -2.8515625  -2.8554688  -2.859375   -2.8632812
 -2.8671875  -2.875      -2.8769531  -2.8886719  -2.9121094  -2.9375
 -2.9394531  -2.9472656  -2.9628906  -2.9785156  -2.9804688  -2.9902344
 -2.9941406  -3.         -3.0039062  -3.0234375  -3.0273438  -3.0527344
 -3.0546875  -3.0625     -3.0644531  -3.0820312  -3.0839844  -3.0898438
 -3.1054688  -3.1074219  -3.1113281  -3.1308594  -3.1425781  -3.1523438
 -3.1542969  -3.1621094  -3.1640625  -3.1660156  -3.1933594  -3.1953125
 -3.2050781  -3.2109375  -3.2285156  -3.2363281  -3.2480469  -3.2519531
 -3.2597656  -3.2617188  -3.2695312  -3.2753906  -3.2792969  -3.2890625
 -3.2910156  -3.296875   -3.3027344  -3.3066406  -3.3164062  -3.3203125
 -3.3261719  -3.3300781  -3.3339844  -3.3476562  -3.3535156  -3.3554688
 -3.3574219  -3.3671875  -3.3691406  -3.3710938  -3.3808594  -3.3886719
 -3.390625   -3.3925781  -3.3984375  -3.4042969  -3.40625    -3.4257812
 -3.4296875  -3.4394531  -3.4414062  -3.4433594  -3.4511719  -3.453125
 -3.4589844  -3.46875    -3.4707031  -3.4921875  -3.5019531  -3.5078125
 -3.5097656  -3.5117188  -3.5136719  -3.5234375  -3.5253906  -3.5273438
 -3.5332031  -3.5351562  -3.5371094  -3.5410156  -3.546875   -3.5507812
 -3.5546875  -3.5625     -3.5664062  -3.5703125  -3.578125   -3.5800781
 -3.5839844  -3.5878906  -3.5917969  -3.6035156  -3.6074219  -3.609375
 -3.6132812  -3.6328125  -3.640625   -3.6425781  -3.6445312  -3.6464844
 -3.6582031  -3.6660156  -3.6699219  -3.6757812  -3.6835938  -3.6875
 -3.7128906  -3.7148438  -3.7167969  -3.7246094  -3.75       -3.7519531
 -3.7597656  -3.7617188  -3.7675781  -3.7753906  -3.7773438  -3.7792969
 -3.7851562  -3.7929688  -3.796875   -3.8066406  -3.8105469  -3.8164062
 -3.8222656  -3.828125   -3.8300781  -3.8339844  -3.8378906  -3.8515625
 -3.8535156  -3.8847656  -3.8867188  -3.9160156  -3.9179688  -3.9335938
 -3.9394531  -3.9453125  -3.9785156  -3.9824219  -4.         -4.0117188
 -4.015625   -4.0195312  -4.0234375  -4.0273438  -4.0507812  -4.0546875
 -4.0625     -4.078125   -4.1601562  -4.1835938  -4.2539062  -4.3007812
 -4.59375   ]
