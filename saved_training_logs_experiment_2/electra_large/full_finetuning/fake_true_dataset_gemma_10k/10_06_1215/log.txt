log_loss_steps: 208
eval_steps: 208
check_degradation: 0
----------------Epoch 1/1----------------
Epoch 1/1, Loss after 192 samples: 0.6902
Mean accuracy: 0.5353, std: 0.0111, lower bound: 0.5126, upper bound: 0.5582 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 192 samples: 0.5359 with eval loss: 0.6910
Best model with eval loss 0.6909584782777294 and eval accuracy 0.5358948432760364 with 192 samples seen is saved
Epoch 1/1, Loss after 400 samples: 0.6836
Mean accuracy: 0.6710, std: 0.0108, lower bound: 0.6496, upper bound: 0.6916 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 400 samples: 0.6714 with eval loss: 0.6793
Best model with eval loss 0.6792632562498893 and eval accuracy 0.6713852376137512 with 400 samples seen is saved
Epoch 1/1, Loss after 608 samples: 0.6724
Mean accuracy: 0.7535, std: 0.0098, lower bound: 0.7341, upper bound: 0.7730 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 608 samples: 0.7533 with eval loss: 0.6577
Best model with eval loss 0.6577141299363105 and eval accuracy 0.7532861476238625 with 608 samples seen is saved
Epoch 1/1, Loss after 816 samples: 0.6494
Mean accuracy: 0.8530, std: 0.0079, lower bound: 0.8382, upper bound: 0.8686 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 816 samples: 0.8534 with eval loss: 0.6178
Best model with eval loss 0.6178312979398235 and eval accuracy 0.8533872598584429 with 816 samples seen is saved
Epoch 1/1, Loss after 1024 samples: 0.5808
Mean accuracy: 0.8578, std: 0.0077, lower bound: 0.8433, upper bound: 0.8726 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 1024 samples: 0.8579 with eval loss: 0.4794
Best model with eval loss 0.4793957125756048 and eval accuracy 0.8579373104145601 with 1024 samples seen is saved
Epoch 1/1, Loss after 1232 samples: 0.4195
Mean accuracy: 0.8533, std: 0.0078, lower bound: 0.8392, upper bound: 0.8691 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 1232 samples: 0.8534 with eval loss: 0.3672
Best model with eval loss 0.3671841222432352 and eval accuracy 0.8533872598584429 with 1232 samples seen is saved
Epoch 1/1, Loss after 1440 samples: 0.3456
Mean accuracy: 0.8886, std: 0.0070, lower bound: 0.8756, upper bound: 0.9019 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 1440 samples: 0.8883 with eval loss: 0.3113
Best model with eval loss 0.31132321203908614 and eval accuracy 0.8882709807886754 with 1440 samples seen is saved
Epoch 1/1, Loss after 1648 samples: 0.3891
Mean accuracy: 0.8656, std: 0.0077, lower bound: 0.8503, upper bound: 0.8802 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 1648 samples: 0.8655 with eval loss: 0.3343
Epoch 1/1, Loss after 1856 samples: 0.3037
Mean accuracy: 0.8960, std: 0.0069, lower bound: 0.8827, upper bound: 0.9095 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 1856 samples: 0.8959 with eval loss: 0.2600
Best model with eval loss 0.2600071876039428 and eval accuracy 0.8958543983822043 with 1856 samples seen is saved
Epoch 1/1, Loss after 2064 samples: 0.2578
Mean accuracy: 0.9200, std: 0.0062, lower bound: 0.9075, upper bound: 0.9312 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 2064 samples: 0.9201 with eval loss: 0.2242
Best model with eval loss 0.22418830627875944 and eval accuracy 0.9201213346814965 with 2064 samples seen is saved
Epoch 1/1, Loss after 2272 samples: 0.1986
Mean accuracy: 0.9162, std: 0.0061, lower bound: 0.9034, upper bound: 0.9282 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 2272 samples: 0.9161 with eval loss: 0.2169
Best model with eval loss 0.21690242521224484 and eval accuracy 0.916076845298281 with 2272 samples seen is saved
Epoch 1/1, Loss after 2480 samples: 0.2451
Mean accuracy: 0.8677, std: 0.0076, lower bound: 0.8534, upper bound: 0.8827 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 2480 samples: 0.8675 with eval loss: 0.3239
Epoch 1/1, Loss after 2688 samples: 0.2470
Mean accuracy: 0.9068, std: 0.0065, lower bound: 0.8938, upper bound: 0.9201 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 2688 samples: 0.9065 with eval loss: 0.2331
Epoch 1/1, Loss after 2896 samples: 0.2051
Mean accuracy: 0.9487, std: 0.0048, lower bound: 0.9398, upper bound: 0.9580 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 2896 samples: 0.9489 with eval loss: 0.1520
Best model with eval loss 0.15198105153056882 and eval accuracy 0.9489383215369059 with 2896 samples seen is saved
Epoch 1/1, Loss after 3104 samples: 0.1784
Mean accuracy: 0.9464, std: 0.0052, lower bound: 0.9363, upper bound: 0.9565 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 3104 samples: 0.9464 with eval loss: 0.1469
Best model with eval loss 0.1468813388698524 and eval accuracy 0.9464105156723963 with 3104 samples seen is saved
Epoch 1/1, Loss after 3312 samples: 0.2309
Mean accuracy: 0.9421, std: 0.0054, lower bound: 0.9312, upper bound: 0.9525 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 3312 samples: 0.9419 with eval loss: 0.1534
Epoch 1/1, Loss after 3520 samples: 0.2774
Mean accuracy: 0.9442, std: 0.0052, lower bound: 0.9343, upper bound: 0.9540 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 3520 samples: 0.9444 with eval loss: 0.1418
Best model with eval loss 0.1417576989699756 and eval accuracy 0.9443882709807887 with 3520 samples seen is saved
Epoch 1/1, Loss after 3728 samples: 0.1931
Mean accuracy: 0.9499, std: 0.0047, lower bound: 0.9403, upper bound: 0.9585 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 3728 samples: 0.9499 with eval loss: 0.1371
Best model with eval loss 0.1371433965621456 and eval accuracy 0.9499494438827099 with 3728 samples seen is saved
Epoch 1/1, Loss after 3936 samples: 0.1370
Mean accuracy: 0.9437, std: 0.0053, lower bound: 0.9332, upper bound: 0.9540 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 3936 samples: 0.9439 with eval loss: 0.1539
Epoch 1/1, Loss after 4144 samples: 0.1719
Mean accuracy: 0.9383, std: 0.0052, lower bound: 0.9277, upper bound: 0.9479 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 4144 samples: 0.9383 with eval loss: 0.1664
Epoch 1/1, Loss after 4352 samples: 0.1795
Mean accuracy: 0.9464, std: 0.0049, lower bound: 0.9368, upper bound: 0.9565 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 4352 samples: 0.9464 with eval loss: 0.1483
Epoch 1/1, Loss after 4560 samples: 0.1154
Mean accuracy: 0.9574, std: 0.0048, lower bound: 0.9474, upper bound: 0.9666 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 4560 samples: 0.9575 with eval loss: 0.1085
Best model with eval loss 0.10852197909186925 and eval accuracy 0.9575328614762386 with 4560 samples seen is saved
Epoch 1/1, Loss after 4768 samples: 0.0809
Mean accuracy: 0.9358, std: 0.0057, lower bound: 0.9252, upper bound: 0.9474 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 4768 samples: 0.9358 with eval loss: 0.1784
Epoch 1/1, Loss after 4976 samples: 0.1484
Mean accuracy: 0.9451, std: 0.0051, lower bound: 0.9343, upper bound: 0.9550 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 4976 samples: 0.9449 with eval loss: 0.1514
Epoch 1/1, Loss after 5184 samples: 0.0977
Mean accuracy: 0.9503, std: 0.0049, lower bound: 0.9408, upper bound: 0.9596 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 5184 samples: 0.9505 with eval loss: 0.1441
Epoch 1/1, Loss after 5392 samples: 0.0868
Mean accuracy: 0.9556, std: 0.0047, lower bound: 0.9464, upper bound: 0.9636 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 5392 samples: 0.9555 with eval loss: 0.1155
Epoch 1/1, Loss after 5600 samples: 0.1149
Mean accuracy: 0.9444, std: 0.0052, lower bound: 0.9338, upper bound: 0.9540 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 5600 samples: 0.9444 with eval loss: 0.1562
Epoch 1/1, Loss after 5808 samples: 0.1219
Mean accuracy: 0.9249, std: 0.0059, lower bound: 0.9135, upper bound: 0.9363 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 5808 samples: 0.9252 with eval loss: 0.2148
Epoch 1/1, Loss after 6016 samples: 0.1780
Mean accuracy: 0.9402, std: 0.0053, lower bound: 0.9302, upper bound: 0.9505 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 6016 samples: 0.9403 with eval loss: 0.1613
Epoch 1/1, Loss after 6224 samples: 0.2048
Mean accuracy: 0.9151, std: 0.0062, lower bound: 0.9029, upper bound: 0.9267 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 6224 samples: 0.9156 with eval loss: 0.2114
Epoch 1/1, Loss after 6432 samples: 0.1627
Mean accuracy: 0.9187, std: 0.0062, lower bound: 0.9065, upper bound: 0.9312 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 6432 samples: 0.9186 with eval loss: 0.2085
Epoch 1/1, Loss after 6640 samples: 0.1652
Mean accuracy: 0.9408, std: 0.0053, lower bound: 0.9307, upper bound: 0.9510 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 6640 samples: 0.9408 with eval loss: 0.1507
Epoch 1/1, Loss after 6848 samples: 0.1414
Mean accuracy: 0.9332, std: 0.0060, lower bound: 0.9216, upper bound: 0.9439 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 6848 samples: 0.9333 with eval loss: 0.1648
Epoch 1/1, Loss after 7056 samples: 0.1123
Mean accuracy: 0.9212, std: 0.0062, lower bound: 0.9100, upper bound: 0.9338 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 7056 samples: 0.9211 with eval loss: 0.1905
Epoch 1/1, Loss after 7264 samples: 0.1958
Mean accuracy: 0.9270, std: 0.0059, lower bound: 0.9151, upper bound: 0.9373 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 7264 samples: 0.9272 with eval loss: 0.1710
Epoch 1/1, Loss after 7472 samples: 0.1338
Mean accuracy: 0.9680, std: 0.0040, lower bound: 0.9601, upper bound: 0.9757 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 7472 samples: 0.9681 with eval loss: 0.0916
Best model with eval loss 0.09156197733095577 and eval accuracy 0.968149646107179 with 7472 samples seen is saved
Epoch 1/1, Loss after 7680 samples: 0.0924
Mean accuracy: 0.9656, std: 0.0040, lower bound: 0.9580, upper bound: 0.9732 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 7680 samples: 0.9656 with eval loss: 0.0915
Best model with eval loss 0.09146290613458521 and eval accuracy 0.9656218402426694 with 7680 samples seen is saved
Epoch 1/1, Loss after 7888 samples: 0.1096
Mean accuracy: 0.9443, std: 0.0051, lower bound: 0.9343, upper bound: 0.9545 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 7888 samples: 0.9444 with eval loss: 0.1520
Epoch 1/1, Loss after 8096 samples: 0.0880
Mean accuracy: 0.9622, std: 0.0042, lower bound: 0.9535, upper bound: 0.9702 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 8096 samples: 0.9621 with eval loss: 0.0952
Epoch 1/1, Loss after 8304 samples: 0.1103
Mean accuracy: 0.9435, std: 0.0051, lower bound: 0.9328, upper bound: 0.9525 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 8304 samples: 0.9434 with eval loss: 0.1446
Epoch 1/1, Loss after 8512 samples: 0.0686
Mean accuracy: 0.9555, std: 0.0046, lower bound: 0.9459, upper bound: 0.9646 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 8512 samples: 0.9555 with eval loss: 0.1190
Epoch 1/1, Loss after 8720 samples: 0.0891
Mean accuracy: 0.9496, std: 0.0051, lower bound: 0.9388, upper bound: 0.9590 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 8720 samples: 0.9494 with eval loss: 0.1297
Epoch 1/1, Loss after 8928 samples: 0.0689
Mean accuracy: 0.9704, std: 0.0038, lower bound: 0.9626, upper bound: 0.9783 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 8928 samples: 0.9702 with eval loss: 0.0843
Best model with eval loss 0.08430659425474944 and eval accuracy 0.9701718907987866 with 8928 samples seen is saved
Epoch 1/1, Loss after 9136 samples: 0.0759
Mean accuracy: 0.9354, std: 0.0056, lower bound: 0.9247, upper bound: 0.9464 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 9136 samples: 0.9353 with eval loss: 0.1804
Epoch 1/1, Loss after 9344 samples: 0.0717
Mean accuracy: 0.9399, std: 0.0053, lower bound: 0.9297, upper bound: 0.9500 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 9344 samples: 0.9398 with eval loss: 0.1663
Epoch 1/1, Loss after 9552 samples: 0.1055
Mean accuracy: 0.9247, std: 0.0057, lower bound: 0.9145, upper bound: 0.9363 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 9552 samples: 0.9247 with eval loss: 0.2073
Epoch 1/1, Loss after 9760 samples: 0.1396
Mean accuracy: 0.9572, std: 0.0045, lower bound: 0.9484, upper bound: 0.9656 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 9760 samples: 0.9570 with eval loss: 0.1138
Epoch 1/1, Loss after 9968 samples: 0.1586
Mean accuracy: 0.9133, std: 0.0063, lower bound: 0.9009, upper bound: 0.9257 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 9968 samples: 0.9130 with eval loss: 0.2504
Epoch 1/1, Loss after 10176 samples: 0.0691
Mean accuracy: 0.9644, std: 0.0043, lower bound: 0.9560, upper bound: 0.9722 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 10176 samples: 0.9646 with eval loss: 0.0996
Epoch 1/1, Loss after 10384 samples: 0.0917
Mean accuracy: 0.9611, std: 0.0042, lower bound: 0.9530, upper bound: 0.9692 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 10384 samples: 0.9611 with eval loss: 0.1143
Epoch 1/1, Loss after 10592 samples: 0.0827
Mean accuracy: 0.9418, std: 0.0051, lower bound: 0.9317, upper bound: 0.9515 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 10592 samples: 0.9419 with eval loss: 0.1610
Epoch 1/1, Loss after 10800 samples: 0.0614
Mean accuracy: 0.9515, std: 0.0048, lower bound: 0.9424, upper bound: 0.9606 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 10800 samples: 0.9515 with eval loss: 0.1394
Epoch 1/1, Loss after 11008 samples: 0.0701
Mean accuracy: 0.9712, std: 0.0038, lower bound: 0.9641, upper bound: 0.9788 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 11008 samples: 0.9712 with eval loss: 0.0839
Best model with eval loss 0.08391342989559616 and eval accuracy 0.9711830131445905 with 11008 samples seen is saved
Epoch 1/1, Loss after 11216 samples: 0.1159
Mean accuracy: 0.9682, std: 0.0041, lower bound: 0.9601, upper bound: 0.9752 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 11216 samples: 0.9681 with eval loss: 0.0888
Epoch 1/1, Loss after 11424 samples: 0.1033
Mean accuracy: 0.9527, std: 0.0047, lower bound: 0.9434, upper bound: 0.9621 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 11424 samples: 0.9525 with eval loss: 0.1367
Epoch 1/1, Loss after 11632 samples: 0.0631
Mean accuracy: 0.9518, std: 0.0049, lower bound: 0.9419, upper bound: 0.9611 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 11632 samples: 0.9520 with eval loss: 0.1381
Epoch 1/1, Loss after 11840 samples: 0.0718
Mean accuracy: 0.9678, std: 0.0039, lower bound: 0.9601, upper bound: 0.9752 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 11840 samples: 0.9676 with eval loss: 0.0784
Best model with eval loss 0.07842600422220365 and eval accuracy 0.967644084934277 with 11840 samples seen is saved
Epoch 1/1, Loss after 12048 samples: 0.1293
Mean accuracy: 0.9564, std: 0.0046, lower bound: 0.9474, upper bound: 0.9651 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 12048 samples: 0.9565 with eval loss: 0.1349
Epoch 1/1, Loss after 12256 samples: 0.0382
Mean accuracy: 0.9508, std: 0.0047, lower bound: 0.9414, upper bound: 0.9596 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 12256 samples: 0.9510 with eval loss: 0.1460
Epoch 1/1, Loss after 12464 samples: 0.0884
Mean accuracy: 0.9613, std: 0.0042, lower bound: 0.9525, upper bound: 0.9697 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 12464 samples: 0.9611 with eval loss: 0.1133
Epoch 1/1, Loss after 12672 samples: 0.0901
Mean accuracy: 0.9420, std: 0.0052, lower bound: 0.9317, upper bound: 0.9525 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 12672 samples: 0.9419 with eval loss: 0.1575
Epoch 1/1, Loss after 12880 samples: 0.0736
Mean accuracy: 0.9680, std: 0.0039, lower bound: 0.9601, upper bound: 0.9752 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 12880 samples: 0.9681 with eval loss: 0.0827
Epoch 1/1, Loss after 13088 samples: 0.0758
Mean accuracy: 0.9053, std: 0.0065, lower bound: 0.8933, upper bound: 0.9176 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 13088 samples: 0.9055 with eval loss: 0.2532
Epoch 1/1, Loss after 13296 samples: 0.1040
Mean accuracy: 0.9742, std: 0.0035, lower bound: 0.9676, upper bound: 0.9813 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 13296 samples: 0.9742 with eval loss: 0.0684
Best model with eval loss 0.06844920813736896 and eval accuracy 0.9742163801820021 with 13296 samples seen is saved
Epoch 1/1, Loss after 13504 samples: 0.0949
Mean accuracy: 0.9537, std: 0.0047, lower bound: 0.9444, upper bound: 0.9636 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 13504 samples: 0.9535 with eval loss: 0.1304
Epoch 1/1, Loss after 13712 samples: 0.0853
Mean accuracy: 0.9311, std: 0.0059, lower bound: 0.9191, upper bound: 0.9419 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 13712 samples: 0.9312 with eval loss: 0.1899
Epoch 1/1, Loss after 13920 samples: 0.0505
Mean accuracy: 0.9504, std: 0.0047, lower bound: 0.9408, upper bound: 0.9596 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 13920 samples: 0.9505 with eval loss: 0.1399
Epoch 1/1, Loss after 14128 samples: 0.0764
Mean accuracy: 0.9669, std: 0.0039, lower bound: 0.9590, upper bound: 0.9742 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 14128 samples: 0.9666 with eval loss: 0.0937
Epoch 1/1, Loss after 14336 samples: 0.0936
Mean accuracy: 0.9589, std: 0.0044, lower bound: 0.9499, upper bound: 0.9672 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 14336 samples: 0.9590 with eval loss: 0.1154
Epoch 1/1, Loss after 14544 samples: 0.0588
Mean accuracy: 0.9583, std: 0.0046, lower bound: 0.9494, upper bound: 0.9671 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 14544 samples: 0.9580 with eval loss: 0.1203
Epoch 1/1, Loss after 14752 samples: 0.0723
Mean accuracy: 0.9595, std: 0.0045, lower bound: 0.9505, upper bound: 0.9681 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 14752 samples: 0.9596 with eval loss: 0.1143
Epoch 1/1, Loss after 14960 samples: 0.0768
Mean accuracy: 0.9585, std: 0.0044, lower bound: 0.9494, upper bound: 0.9666 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 14960 samples: 0.9585 with eval loss: 0.1221
Epoch 1/1, Loss after 15168 samples: 0.0462
Mean accuracy: 0.9530, std: 0.0046, lower bound: 0.9429, upper bound: 0.9621 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 15168 samples: 0.9530 with eval loss: 0.1360
Epoch 1/1, Loss after 15376 samples: 0.0659
Mean accuracy: 0.9579, std: 0.0045, lower bound: 0.9494, upper bound: 0.9661 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 15376 samples: 0.9580 with eval loss: 0.1216
Epoch 1/1, Loss after 15584 samples: 0.0634
Mean accuracy: 0.9586, std: 0.0045, lower bound: 0.9499, upper bound: 0.9666 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 15584 samples: 0.9585 with eval loss: 0.1161
Epoch 1/1, Loss after 15792 samples: 0.0879
Mean accuracy: 0.9626, std: 0.0044, lower bound: 0.9540, upper bound: 0.9707 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 15792 samples: 0.9626 with eval loss: 0.1080
Training signal is False, stopping training
Training finished
Best model: {'eval_acc': 0.9742163801820021, 'nb_samples': 13296, 'eval_loss': 0.06844920813736896}
Training loss logs: [{'samples': 192, 'loss': 0.6901808518629807}, {'samples': 400, 'loss': 0.6835538423978366}, {'samples': 608, 'loss': 0.6724442702073318}, {'samples': 816, 'loss': 0.6494481013371394}, {'samples': 1024, 'loss': 0.5807723999023438}, {'samples': 1232, 'loss': 0.41947350135216344}, {'samples': 1440, 'loss': 0.3456312326284555}, {'samples': 1648, 'loss': 0.38913624103252703}, {'samples': 1856, 'loss': 0.30370829655573917}, {'samples': 2064, 'loss': 0.2578463921180138}, {'samples': 2272, 'loss': 0.1986387692964994}, {'samples': 2480, 'loss': 0.2451031758235051}, {'samples': 2688, 'loss': 0.24695631173940805}, {'samples': 2896, 'loss': 0.2050633797278771}, {'samples': 3104, 'loss': 0.17840429452749398}, {'samples': 3312, 'loss': 0.23090626643254206}, {'samples': 3520, 'loss': 0.27743372550377476}, {'samples': 3728, 'loss': 0.19309183267446664}, {'samples': 3936, 'loss': 0.13701805701622596}, {'samples': 4144, 'loss': 0.17192657177264875}, {'samples': 4352, 'loss': 0.17954309170062727}, {'samples': 4560, 'loss': 0.11536249747643104}, {'samples': 4768, 'loss': 0.08087284748370831}, {'samples': 4976, 'loss': 0.14835086235633263}, {'samples': 5184, 'loss': 0.09766244888305664}, {'samples': 5392, 'loss': 0.08684319716233474}, {'samples': 5600, 'loss': 0.11492052445044884}, {'samples': 5808, 'loss': 0.12187372721158542}, {'samples': 6016, 'loss': 0.17801293959984413}, {'samples': 6224, 'loss': 0.2048177719116211}, {'samples': 6432, 'loss': 0.16274287150456354}, {'samples': 6640, 'loss': 0.1652080279130202}, {'samples': 6848, 'loss': 0.1413871141580435}, {'samples': 7056, 'loss': 0.11225373928363507}, {'samples': 7264, 'loss': 0.1958021200620211}, {'samples': 7472, 'loss': 0.1338205704322228}, {'samples': 7680, 'loss': 0.09242125657888559}, {'samples': 7888, 'loss': 0.10957571176382211}, {'samples': 8096, 'loss': 0.08803129196166992}, {'samples': 8304, 'loss': 0.1103374591240516}, {'samples': 8512, 'loss': 0.0686484300173246}, {'samples': 8720, 'loss': 0.08905500632065994}, {'samples': 8928, 'loss': 0.06888237366309533}, {'samples': 9136, 'loss': 0.07585795109088604}, {'samples': 9344, 'loss': 0.07174351582160363}, {'samples': 9552, 'loss': 0.10552626389723557}, {'samples': 9760, 'loss': 0.1395653119454017}, {'samples': 9968, 'loss': 0.15855984504406267}, {'samples': 10176, 'loss': 0.06906857857337365}, {'samples': 10384, 'loss': 0.09166218684269832}, {'samples': 10592, 'loss': 0.08268584654881404}, {'samples': 10800, 'loss': 0.0614235492853018}, {'samples': 11008, 'loss': 0.07011770285092868}, {'samples': 11216, 'loss': 0.11586597332587609}, {'samples': 11424, 'loss': 0.10333174008589524}, {'samples': 11632, 'loss': 0.06312907659090482}, {'samples': 11840, 'loss': 0.07179857217348538}, {'samples': 12048, 'loss': 0.12932490385495699}, {'samples': 12256, 'loss': 0.03817341877863957}, {'samples': 12464, 'loss': 0.08843797903794509}, {'samples': 12672, 'loss': 0.09005462206326999}, {'samples': 12880, 'loss': 0.07364407869485709}, {'samples': 13088, 'loss': 0.075836181640625}, {'samples': 13296, 'loss': 0.10400047669043908}, {'samples': 13504, 'loss': 0.09487214455237755}, {'samples': 13712, 'loss': 0.08531514497903678}, {'samples': 13920, 'loss': 0.05054643520942101}, {'samples': 14128, 'loss': 0.07642896358783428}, {'samples': 14336, 'loss': 0.09364111606891339}, {'samples': 14544, 'loss': 0.05884701472062331}, {'samples': 14752, 'loss': 0.07233521571526161}, {'samples': 14960, 'loss': 0.07677727479201096}, {'samples': 15168, 'loss': 0.046163192162146933}, {'samples': 15376, 'loss': 0.06594187479752761}, {'samples': 15584, 'loss': 0.06341041051424466}, {'samples': 15792, 'loss': 0.08786272085629977}]
Evaluation accuracy logs: [{'samples': 192, 'accuracy': 0.5352922143579374, 'std': 0.01111479585648659, 'lower_bound': 0.512639029322548, 'upper_bound': 0.5581521739130435}, {'samples': 400, 'accuracy': 0.6710217391304347, 'std': 0.010776164645912117, 'lower_bound': 0.6496461071789686, 'upper_bound': 0.6916076845298281}, {'samples': 608, 'accuracy': 0.753515166835187, 'std': 0.009797689326801811, 'lower_bound': 0.7340748230535895, 'upper_bound': 0.77301567239636}, {'samples': 816, 'accuracy': 0.8529752275025277, 'std': 0.007886539997995944, 'lower_bound': 0.8382204246713852, 'upper_bound': 0.8685540950455005}, {'samples': 1024, 'accuracy': 0.8577709807886754, 'std': 0.007733941888206206, 'lower_bound': 0.8432633973710819, 'upper_bound': 0.8725985844287159}, {'samples': 1232, 'accuracy': 0.8533063700707786, 'std': 0.007786436475432097, 'lower_bound': 0.8392189079878665, 'upper_bound': 0.869072295247725}, {'samples': 1440, 'accuracy': 0.8886339737108191, 'std': 0.006983673454064241, 'lower_bound': 0.8756319514661274, 'upper_bound': 0.9019337714863499}, {'samples': 1648, 'accuracy': 0.8655894843276036, 'std': 0.00773071481137118, 'lower_bound': 0.8503412537917088, 'upper_bound': 0.8801820020222447}, {'samples': 1856, 'accuracy': 0.8959514661274014, 'std': 0.006932885103363755, 'lower_bound': 0.8827098078867543, 'upper_bound': 0.9095045500505561}, {'samples': 2064, 'accuracy': 0.9199625884732052, 'std': 0.0062146487145534265, 'lower_bound': 0.9074823053589485, 'upper_bound': 0.9312436804853387}, {'samples': 2272, 'accuracy': 0.916191102123357, 'std': 0.006122154263714656, 'lower_bound': 0.903437815975733, 'upper_bound': 0.9282103134479271}, {'samples': 2480, 'accuracy': 0.8676557128412538, 'std': 0.007584584861293997, 'lower_bound': 0.8533746208291203, 'upper_bound': 0.8827098078867543}, {'samples': 2688, 'accuracy': 0.9067866531850355, 'std': 0.006496699566387047, 'lower_bound': 0.8938321536905965, 'upper_bound': 0.9201213346814965}, {'samples': 2896, 'accuracy': 0.9487098078867543, 'std': 0.004828617276693597, 'lower_bound': 0.9398255813953488, 'upper_bound': 0.9580384226491405}, {'samples': 3104, 'accuracy': 0.9464135490394338, 'std': 0.005164003961337873, 'lower_bound': 0.9362866531850353, 'upper_bound': 0.9565217391304348}, {'samples': 3312, 'accuracy': 0.9420657229524773, 'std': 0.005363362262658109, 'lower_bound': 0.9312436804853387, 'upper_bound': 0.9524772497472194}, {'samples': 3520, 'accuracy': 0.9442239635995956, 'std': 0.005194733306436521, 'lower_bound': 0.9342770475227502, 'upper_bound': 0.9539939332659252}, {'samples': 3728, 'accuracy': 0.9498766430738119, 'std': 0.004722744122138423, 'lower_bound': 0.9403437815975733, 'upper_bound': 0.9585439838220424}, {'samples': 3936, 'accuracy': 0.9437406471183013, 'std': 0.0053388322435958735, 'lower_bound': 0.9332406471183013, 'upper_bound': 0.9539939332659252}, {'samples': 4144, 'accuracy': 0.9382618806875632, 'std': 0.005224530802274159, 'lower_bound': 0.9276921132457027, 'upper_bound': 0.9479398382204247}, {'samples': 4352, 'accuracy': 0.9463857431749242, 'std': 0.004862610227919259, 'lower_bound': 0.9368048533872598, 'upper_bound': 0.9565217391304348}, {'samples': 4560, 'accuracy': 0.9574277047522751, 'std': 0.004775954173091109, 'lower_bound': 0.9474216380182002, 'upper_bound': 0.9666329625884732}, {'samples': 4768, 'accuracy': 0.9358478260869565, 'std': 0.00567665623106628, 'lower_bound': 0.9251769464105156, 'upper_bound': 0.9474216380182002}, {'samples': 4976, 'accuracy': 0.9450945399393327, 'std': 0.00507784270441765, 'lower_bound': 0.9342770475227502, 'upper_bound': 0.955005055611729}, {'samples': 5184, 'accuracy': 0.9502881698685541, 'std': 0.004928355424171798, 'lower_bound': 0.9408493427704753, 'upper_bound': 0.9595551061678463}, {'samples': 5392, 'accuracy': 0.9555960566228514, 'std': 0.004682611156265386, 'lower_bound': 0.9464105156723963, 'upper_bound': 0.9635995955510617}, {'samples': 5600, 'accuracy': 0.9444004044489384, 'std': 0.0051845396632489715, 'lower_bound': 0.9337714863498483, 'upper_bound': 0.9539939332659252}, {'samples': 5808, 'accuracy': 0.9248928210313447, 'std': 0.005864077160583908, 'lower_bound': 0.9135490394337715, 'upper_bound': 0.9362992922143579}, {'samples': 6016, 'accuracy': 0.9402093023255813, 'std': 0.00533637106425201, 'lower_bound': 0.9302199191102123, 'upper_bound': 0.9504550050556118}, {'samples': 6224, 'accuracy': 0.9151319514661275, 'std': 0.006197978721101873, 'lower_bound': 0.9029322548028311, 'upper_bound': 0.9266936299292214}, {'samples': 6432, 'accuracy': 0.9187194135490395, 'std': 0.0061684569100535465, 'lower_bound': 0.9064711830131446, 'upper_bound': 0.9312436804853387}, {'samples': 6640, 'accuracy': 0.9408053589484328, 'std': 0.0052588431164247535, 'lower_bound': 0.9307381193124368, 'upper_bound': 0.9509732052578362}, {'samples': 6848, 'accuracy': 0.9331678463094035, 'std': 0.00595417927133094, 'lower_bound': 0.9216253791708796, 'upper_bound': 0.9438827098078868}, {'samples': 7056, 'accuracy': 0.9211895854398382, 'std': 0.00623908844451067, 'lower_bound': 0.9099974721941354, 'upper_bound': 0.9337841253791709}, {'samples': 7264, 'accuracy': 0.9269797775530837, 'std': 0.005862889058199154, 'lower_bound': 0.9150657229524772, 'upper_bound': 0.9373230535894843}, {'samples': 7472, 'accuracy': 0.9680303336703742, 'std': 0.003958976689544407, 'lower_bound': 0.9600606673407482, 'upper_bound': 0.9757330637007078}, {'samples': 7680, 'accuracy': 0.965649140546006, 'std': 0.0039902545421253936, 'lower_bound': 0.9580257836198179, 'upper_bound': 0.9732052578361982}, {'samples': 7888, 'accuracy': 0.944298786653185, 'std': 0.00511651063852068, 'lower_bound': 0.9342770475227502, 'upper_bound': 0.9544994944388271}, {'samples': 8096, 'accuracy': 0.962190091001011, 'std': 0.004247467793828414, 'lower_bound': 0.9534883720930233, 'upper_bound': 0.9701845298281092}, {'samples': 8304, 'accuracy': 0.9434509605662286, 'std': 0.0051145873582457806, 'lower_bound': 0.9327603640040445, 'upper_bound': 0.952489888776542}, {'samples': 8512, 'accuracy': 0.9554802831142568, 'std': 0.004629748037308688, 'lower_bound': 0.9458923154701718, 'upper_bound': 0.9646107178968655}, {'samples': 8720, 'accuracy': 0.9496001011122346, 'std': 0.005133463245391697, 'lower_bound': 0.9388270980788676, 'upper_bound': 0.9590495449949444}, {'samples': 8928, 'accuracy': 0.9703564206268959, 'std': 0.003806032097295659, 'lower_bound': 0.9625758341759353, 'upper_bound': 0.9782608695652174}, {'samples': 9136, 'accuracy': 0.9353781597573306, 'std': 0.005643618932912152, 'lower_bound': 0.9246713852376137, 'upper_bound': 0.9464105156723963}, {'samples': 9344, 'accuracy': 0.939933265925177, 'std': 0.0053310378352748895, 'lower_bound': 0.929726996966633, 'upper_bound': 0.9499620829120324}, {'samples': 9552, 'accuracy': 0.9247330637007077, 'std': 0.005698352108399565, 'lower_bound': 0.9145475227502528, 'upper_bound': 0.9362992922143579}, {'samples': 9760, 'accuracy': 0.957174924165824, 'std': 0.004491840207970876, 'lower_bound': 0.948432760364004, 'upper_bound': 0.9656218402426694}, {'samples': 9968, 'accuracy': 0.913298281092012, 'std': 0.0062750708973347835, 'lower_bound': 0.9009100101112234, 'upper_bound': 0.9256825075834176}, {'samples': 10176, 'accuracy': 0.9644024266936299, 'std': 0.0042542982011874115, 'lower_bound': 0.9560161779575329, 'upper_bound': 0.9721941354903944}, {'samples': 10384, 'accuracy': 0.9610652173913043, 'std': 0.004207375669867056, 'lower_bound': 0.9529701718907988, 'upper_bound': 0.9691607684529828}, {'samples': 10592, 'accuracy': 0.9417790697674419, 'std': 0.005102238950385523, 'lower_bound': 0.9317492416582407, 'upper_bound': 0.9514661274014156}, {'samples': 10800, 'accuracy': 0.9515171890798787, 'std': 0.004798273581298702, 'lower_bound': 0.942366026289181, 'upper_bound': 0.9605662285136501}, {'samples': 11008, 'accuracy': 0.9711552072800809, 'std': 0.003771024641226305, 'lower_bound': 0.9641051567239636, 'upper_bound': 0.9787664307381193}, {'samples': 11216, 'accuracy': 0.9681986855409505, 'std': 0.004080902533147216, 'lower_bound': 0.9600606673407482, 'upper_bound': 0.9752401415571285}, {'samples': 11424, 'accuracy': 0.9526835187057634, 'std': 0.004699204998916115, 'lower_bound': 0.9433771486349848, 'upper_bound': 0.962082912032356}, {'samples': 11632, 'accuracy': 0.9517917087967644, 'std': 0.004926544190109299, 'lower_bound': 0.9418604651162791, 'upper_bound': 0.961071789686552}, {'samples': 11840, 'accuracy': 0.9677997977755308, 'std': 0.003929909334271028, 'lower_bound': 0.9600606673407482, 'upper_bound': 0.9752275025278059}, {'samples': 12048, 'accuracy': 0.9563523761375126, 'std': 0.004580640208508479, 'lower_bound': 0.9474216380182002, 'upper_bound': 0.9651162790697675}, {'samples': 12256, 'accuracy': 0.9508144590495449, 'std': 0.004730914901956841, 'lower_bound': 0.9413549039433772, 'upper_bound': 0.9595551061678463}, {'samples': 12464, 'accuracy': 0.9612730030333672, 'std': 0.004242546826496769, 'lower_bound': 0.9524772497472194, 'upper_bound': 0.9696663296258847}, {'samples': 12672, 'accuracy': 0.9420096056622851, 'std': 0.005211155630256819, 'lower_bound': 0.9317492416582407, 'upper_bound': 0.9524772497472194}, {'samples': 12880, 'accuracy': 0.968026289180991, 'std': 0.003900772675368899, 'lower_bound': 0.9600606673407482, 'upper_bound': 0.9752275025278059}, {'samples': 13088, 'accuracy': 0.905345298281092, 'std': 0.006480132189591833, 'lower_bound': 0.8933265925176946, 'upper_bound': 0.9176061678463094}, {'samples': 13296, 'accuracy': 0.9742133468149646, 'std': 0.003470741484220023, 'lower_bound': 0.967644084934277, 'upper_bound': 0.9812942366026289}, {'samples': 13504, 'accuracy': 0.9536612740141557, 'std': 0.004696634344765526, 'lower_bound': 0.9443882709807887, 'upper_bound': 0.9635995955510617}, {'samples': 13712, 'accuracy': 0.9310798786653186, 'std': 0.005862003578783266, 'lower_bound': 0.9191102123356926, 'upper_bound': 0.9418604651162791}, {'samples': 13920, 'accuracy': 0.9503614762386249, 'std': 0.004674535041933144, 'lower_bound': 0.9408493427704753, 'upper_bound': 0.9595551061678463}, {'samples': 14128, 'accuracy': 0.9668670374115268, 'std': 0.0038899517678160377, 'lower_bound': 0.9590369059656219, 'upper_bound': 0.9742163801820021}, {'samples': 14336, 'accuracy': 0.958881193124368, 'std': 0.00440535316282909, 'lower_bound': 0.9499368048533873, 'upper_bound': 0.9671511627906977}, {'samples': 14544, 'accuracy': 0.9582922143579374, 'std': 0.004559702043236892, 'lower_bound': 0.9494438827098078, 'upper_bound': 0.9671385237613751}, {'samples': 14752, 'accuracy': 0.9595065722952477, 'std': 0.0045302564617633895, 'lower_bound': 0.9504550050556118, 'upper_bound': 0.968149646107179}, {'samples': 14960, 'accuracy': 0.9584762386248736, 'std': 0.004382257322150093, 'lower_bound': 0.9494438827098078, 'upper_bound': 0.9666329625884732}, {'samples': 15168, 'accuracy': 0.9529924165824064, 'std': 0.004621963976861295, 'lower_bound': 0.9428715874620829, 'upper_bound': 0.962082912032356}, {'samples': 15376, 'accuracy': 0.957938827098079, 'std': 0.004476491997836461, 'lower_bound': 0.9494312436804853, 'upper_bound': 0.9661400404448939}, {'samples': 15584, 'accuracy': 0.9585551061678463, 'std': 0.004492658944830865, 'lower_bound': 0.9499368048533873, 'upper_bound': 0.9666329625884732}, {'samples': 15792, 'accuracy': 0.9625596562184023, 'std': 0.00439296736178245, 'lower_bound': 0.9539939332659252, 'upper_bound': 0.9706774519716885}]
Evaluating the best model on the test set of dataset ./fake_true_datasets/fake_true_dataset_gemma_10k...
Test metrics:
accuracy: 0.960933265925177
precision: 0.9291882226411737
recall: 0.9979226983466465
f1_score: 0.9623131233530651
fp_rate: 0.07607032676364359
tp_rate: 0.9979226983466465
std_accuracy: 0.004222114583693089
std_precision: 0.00762356936136162
std_recall: 0.0014677601322060404
std_f1_score: 0.0041615896000727634
std_fp_rate: 0.008175153480379895
std_tp_rate: 0.0014677601322060404
TP: 987.082
TN: 913.644
FP: 75.22
FN: 2.054
roc_auc: 0.997569830317517
fpr: [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00101112 0.00101112 0.00101112
 0.00101112 0.00101112 0.00101112 0.00101112 0.00101112 0.00101112
 0.00101112 0.00101112 0.00101112 0.00101112 0.00101112 0.00101112
 0.00101112 0.00202224 0.00202224 0.00202224 0.00202224 0.00202224
 0.00202224 0.00202224 0.00202224 0.00202224 0.00202224 0.00202224
 0.00202224 0.00202224 0.00202224 0.00202224 0.00202224 0.00202224
 0.00202224 0.00202224 0.00202224 0.00202224 0.00202224 0.00202224
 0.00202224 0.00202224 0.00202224 0.00202224 0.00202224 0.00202224
 0.00202224 0.00202224 0.00202224 0.00202224 0.00202224 0.00202224
 0.00202224 0.00202224 0.00202224 0.00202224 0.00202224 0.00202224
 0.00202224 0.00202224 0.00202224 0.00202224 0.00202224 0.00202224
 0.00202224 0.00202224 0.00202224 0.00202224 0.00202224 0.00202224
 0.00202224 0.00202224 0.00202224 0.00202224 0.00202224 0.00202224
 0.00202224 0.00202224 0.00202224 0.00202224 0.00202224 0.00202224
 0.00202224 0.00303337 0.00303337 0.00303337 0.00303337 0.00303337
 0.00303337 0.00303337 0.00303337 0.00404449 0.00404449 0.00505561
 0.00505561 0.00505561 0.00505561 0.00505561 0.00505561 0.00505561
 0.00505561 0.00505561 0.00505561 0.00505561 0.00505561 0.00505561
 0.00606673 0.00606673 0.00606673 0.00606673 0.00707786 0.00707786
 0.00707786 0.00707786 0.00808898 0.00808898 0.0091001  0.0091001
 0.01011122 0.01011122 0.01112235 0.01112235 0.01213347 0.01213347
 0.01415571 0.01415571 0.01516684 0.01516684 0.01718908 0.01718908
 0.0182002  0.0182002  0.02022245 0.02022245 0.02123357 0.02224469
 0.02224469 0.02628918 0.02628918 0.02831143 0.03033367 0.03235592
 0.03235592 0.03943377 0.03943377 0.04954499 0.04954499 0.05257836
 0.05257836 0.06673407 0.06673407 0.07482305 0.07482305 0.08088979
 0.08088979 0.08998989 0.08998989 0.13852376 0.14054601 0.1627907
 0.16582406 0.1718908  0.17391304 0.1991911  0.20121335 0.24165824
 0.24368049 0.24671385 0.25682508 0.25884732 0.26188069 0.26390293
 0.26491405 0.26895854 0.26996967 0.27199191 0.29019211 0.2942366
 0.29625885 0.29929221 0.30738119 0.30940344 0.31041456 0.31344793
 0.31547017 0.31648129 0.31850354 0.32254803 0.32659252 0.32760364
 0.33063701 0.33670374 0.33771486 0.33973711 0.34782609 0.35187058
 0.37007078 0.37209302 0.37815976 0.38220425 0.38422649 0.38725986
 0.3892821  0.39130435 0.39332659 0.39737108 0.39939333 0.40343782
 0.40546006 0.40647118 0.40849343 0.40950455 0.41152679 0.41253792
 0.41456016 0.41658241 0.41961577 0.42366026 0.42669363 0.42871587
 0.43073812 0.43276036 0.43478261 0.43680485 0.4388271  0.44186047
 0.44590495 0.4479272  0.44994944 0.45096057 0.45298281 0.45601618
 0.45803842 0.46309403 0.46511628 0.46713852 0.46814965 0.47017189
 0.47320526 0.47724975 0.47927199 0.48230536 0.48533873 0.48736097
 0.49039434 0.49443883 0.49646107 0.50050556 0.50252781 0.50455005
 0.5065723  0.50859454 0.51466127 0.51668352 0.51870576 0.52173913
 0.52376138 0.5247725  0.52881699 0.53488372 0.53690597 0.53892821
 0.54095046 0.54196158 0.54398382 0.54701719 0.54903943 0.55308392
 0.55409505 0.55813953 0.55915066 0.56319515 0.57128413 0.57735086
 0.57836198 0.58240647 0.58442872 0.58847321 0.59049545 0.59453994
 0.59555106 0.59858443 0.60060667 0.60465116 0.60768453 0.61577351
 0.61678463 0.6289181  0.63498483 0.63599596 0.6380182  0.64408493
 0.65015167 0.65217391 0.65520728 0.66127401 0.6653185  0.66835187
 0.67239636 0.67846309 0.68250758 0.68351871 0.6875632  0.69261881
 0.6966633  0.69767442 0.69969666 0.70374115 0.70475228 0.70778564
 0.71284125 0.71587462 0.71991911 0.72194135 0.73003033 0.73104146
 0.7330637  0.73913043 0.74620829 0.75227503 0.75733064 0.76137513
 0.76744186 0.77047523 0.77553084 0.77957533 0.7826087  0.78766431
 0.78968655 0.80788675 0.81092012 0.8190091  0.82608696 0.83114257
 0.83923155 0.8463094  0.85035389 0.85844287 0.86754297 0.87158746
 0.87664307 0.88068756 0.88372093 0.89079879 0.8958544  0.90293225
 0.91304348 0.91708797 0.9231547  0.92719919 0.93326593 0.94337715
 0.94641052 0.95247725 0.95551062 0.96056623 0.96157735 0.9635996
 0.96764408 0.97067745 0.9726997  0.98078868 0.98281092 0.99191102
 0.99494439 0.99898888 1.        ]
tpr: [0.         0.00202224 0.00404449 0.00707786 0.0091001  0.01112235
 0.01415571 0.0182002  0.02123357 0.02325581 0.02628918 0.0273003
 0.03033367 0.03134479 0.03336704 0.03842265 0.04246714 0.04448938
 0.04853387 0.05257836 0.05460061 0.05965622 0.06167846 0.06875632
 0.07178969 0.07583418 0.0768453  0.08088979 0.08695652 0.08998989
 0.10212336 0.10313448 0.10819009 0.11122346 0.11830131 0.13043478
 0.13245703 0.13852376 0.14560162 0.15470172 0.15874621 0.16380182
 0.16784631 0.17087968 0.17492417 0.17997978 0.190091   0.20020222
 0.20728008 0.2173913  0.22042467 0.22851365 0.23862487 0.24165824
 0.24671385 0.25278059 0.2578362  0.26390293 0.27199191 0.27906977
 0.28715875 0.29120324 0.29625885 0.30030334 0.3033367  0.31547017
 0.32457027 0.33670374 0.33973711 0.34580384 0.34681496 0.35085945
 0.3528817  0.35692619 0.36501517 0.3710819  0.37411527 0.38220425
 0.38725986 0.39939333 0.40343782 0.40546006 0.40950455 0.41557128
 0.42163802 0.42871587 0.43276036 0.43983822 0.44590495 0.45096057
 0.46107179 0.46511628 0.46814965 0.47017189 0.47724975 0.48331648
 0.4843276  0.49039434 0.49443883 0.49949444 0.5065723  0.51061678
 0.51668352 0.52072801 0.52173913 0.5247725  0.52578362 0.53083923
 0.53286148 0.53690597 0.54095046 0.54701719 0.55106168 0.55611729
 0.55813953 0.5611729  0.56319515 0.56622851 0.57229525 0.57633974
 0.5793731  0.58442872 0.58746208 0.58948433 0.59150657 0.6016178
 0.60768453 0.60869565 0.61274014 0.61577351 0.61779575 0.62082912
 0.62487361 0.62689585 0.63195147 0.63397371 0.63599596 0.64206269
 0.64408493 0.6471183  0.65520728 0.65925177 0.66228514 0.66430738
 0.67037412 0.67239636 0.67644085 0.67846309 0.68351871 0.68452983
 0.69362993 0.69969666 0.70070779 0.70475228 0.70677452 0.70980789
 0.71081901 0.71284125 0.71587462 0.71789687 0.72093023 0.72295248
 0.7239636  0.72800809 0.73003033 0.73407482 0.73609707 0.73710819
 0.73913043 0.74115268 0.74317492 0.74418605 0.74823054 0.75025278
 0.75227503 0.75328615 0.75530839 0.75834176 0.760364   0.76238625
 0.77249747 0.77553084 0.77654196 0.77957533 0.78361982 0.78665319
 0.79271992 0.79373104 0.79575329 0.79878665 0.80182002 0.80384226
 0.80485339 0.80889788 0.81092012 0.81294237 0.81799798 0.82103134
 0.83013145 0.83013145 0.83518706 0.83822042 0.84833165 0.85237614
 0.85338726 0.8554095  0.85945399 0.86046512 0.86147624 0.86147624
 0.86349848 0.86450961 0.86653185 0.86956522 0.87158746 0.87462083
 0.87664307 0.8776542  0.87967644 0.89180991 0.8958544  0.89787664
 0.89888777 0.90091001 0.90293225 0.90697674 0.90697674 0.93124368
 0.93326593 0.93427705 0.93427705 0.93933266 0.93933266 0.94236603
 0.94236603 0.94843276 0.94843276 0.95753286 0.95753286 0.96157735
 0.96157735 0.96258847 0.96258847 0.96966633 0.96966633 0.97876643
 0.97876643 0.97977755 0.97977755 0.98281092 0.98281092 0.98382204
 0.98685541 0.98685541 0.98786653 0.98786653 0.98786653 0.98786653
 0.9908999  0.9908999  0.99292214 0.99292214 0.99393327 0.99393327
 0.99494439 0.99494439 0.99595551 0.99595551 0.99797776 0.99797776
 0.99898888 0.99898888 1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.        ]
thresholds: [        inf  3.1601562   3.1484375   3.1464844   3.1445312   3.140625
  3.1386719   3.1347656   3.1328125   3.1308594   3.1289062   3.1269531
  3.125       3.1230469   3.1210938   3.1191406   3.1171875   3.1152344
  3.1132812   3.109375    3.1054688   3.1035156   3.1015625   3.0996094
  3.0976562   3.0957031   3.09375     3.0917969   3.0898438   3.0878906
  3.0839844   3.0820312   3.0800781   3.078125    3.0761719   3.0742188
  3.0722656   3.0703125   3.0683594   3.0664062   3.0644531   3.0625
  3.0605469   3.0585938   3.0566406   3.0546875   3.0527344   3.0488281
  3.046875    3.0429688   3.0410156   3.0390625   3.0371094   3.0351562
  3.0332031   3.03125     3.0292969   3.0273438   3.0253906   3.0234375
  3.0214844   3.0195312   3.0175781   3.015625    3.0136719   3.0097656
  3.0078125   3.0039062   3.0019531   3.          2.9980469   2.9960938
  2.9941406   2.9921875   2.9902344   2.9882812   2.9863281   2.984375
  2.9824219   2.9785156   2.9765625   2.9746094   2.9726562   2.96875
  2.9667969   2.9648438   2.9609375   2.9589844   2.9550781   2.953125
  2.9511719   2.9492188   2.9472656   2.9453125   2.9433594   2.9414062
  2.9394531   2.9375      2.9355469   2.9335938   2.9316406   2.9296875
  2.9277344   2.9257812   2.9238281   2.921875    2.9199219   2.9179688
  2.9160156   2.9140625   2.9101562   2.90625     2.9023438   2.9003906
  2.8984375   2.8964844   2.8945312   2.8925781   2.890625    2.8886719
  2.8867188   2.8847656   2.8828125   2.8808594   2.8769531   2.8730469
  2.8671875   2.8632812   2.859375    2.8574219   2.8554688   2.8535156
  2.8515625   2.8496094   2.8457031   2.84375     2.8398438   2.8359375
  2.8339844   2.8320312   2.8222656   2.8183594   2.8125      2.8105469
  2.8085938   2.8066406   2.7890625   2.7871094   2.7851562   2.7832031
  2.7773438   2.7695312   2.7675781   2.7636719   2.7597656   2.7558594
  2.7519531   2.75        2.7363281   2.734375    2.7324219   2.7304688
  2.7265625   2.71875     2.7148438   2.7128906   2.7109375   2.7089844
  2.7070312   2.703125    2.6992188   2.6953125   2.6855469   2.6816406
  2.6796875   2.6777344   2.6757812   2.6621094   2.6601562   2.6542969
  2.6308594   2.6289062   2.6230469   2.6210938   2.6015625   2.5976562
  2.5898438   2.5878906   2.5839844   2.5722656   2.5683594   2.5664062
  2.5625      2.5585938   2.5527344   2.5507812   2.5371094   2.5292969
  2.4863281   2.484375    2.453125    2.4511719   2.40625     2.4023438
  2.3964844   2.3925781   2.3808594   2.3769531   2.3710938   2.3671875
  2.3613281   2.3496094   2.34375     2.3261719   2.3242188   2.2929688
  2.2910156   2.2890625   2.2851562   2.1816406   2.1699219   2.15625
  2.1542969   2.1484375   2.1367188   2.1113281   2.109375    1.9755859
  1.9716797   1.9697266   1.9677734   1.9462891   1.9335938   1.9238281
  1.9179688   1.8486328   1.84375     1.7792969   1.765625    1.7324219
  1.7128906   1.7070312   1.6972656   1.5820312   1.5458984   1.3837891
  1.3554688   1.3505859   1.3261719   1.3007812   1.2978516   1.2773438
  1.2275391   1.1796875   1.1660156   1.1376953   1.1328125   1.1103516
  1.0400391   0.9145508   0.89501953  0.59033203  0.5805664   0.48583984
  0.47631836  0.20385742  0.1977539   0.00374031 -0.04574585 -0.1697998
 -0.18847656 -0.3017578  -0.3232422  -1.078125   -1.1054688  -1.3017578
 -1.3222656  -1.3652344  -1.3808594  -1.5634766  -1.5761719  -1.8837891
 -1.8857422  -1.9042969  -1.9736328  -1.9785156  -1.9931641  -1.9960938
 -2.         -2.015625   -2.0195312  -2.0214844  -2.1054688  -2.1132812
 -2.1171875  -2.1191406  -2.1582031  -2.1679688  -2.171875   -2.1757812
 -2.1796875  -2.1816406  -2.1894531  -2.2109375  -2.21875    -2.2246094
 -2.2265625  -2.2324219  -2.234375   -2.2382812  -2.2617188  -2.2675781
 -2.3554688  -2.359375   -2.3886719  -2.3945312  -2.4003906  -2.4042969
 -2.40625    -2.4101562  -2.4140625  -2.4375     -2.4433594  -2.4570312
 -2.4589844  -2.4609375  -2.4648438  -2.4667969  -2.4726562  -2.4746094
 -2.4804688  -2.4882812  -2.4921875  -2.4960938  -2.4980469  -2.5019531
 -2.5039062  -2.5136719  -2.515625   -2.5214844  -2.5234375  -2.5410156
 -2.5488281  -2.5527344  -2.5546875  -2.5585938  -2.5625     -2.5683594
 -2.5703125  -2.5742188  -2.578125   -2.5800781  -2.5859375  -2.5878906
 -2.5898438  -2.5976562  -2.5996094  -2.6113281  -2.6132812  -2.6152344
 -2.6171875  -2.6191406  -2.6230469  -2.6328125  -2.6347656  -2.640625
 -2.6425781  -2.6464844  -2.6523438  -2.65625    -2.6582031  -2.6601562
 -2.6621094  -2.6640625  -2.6679688  -2.671875   -2.6738281  -2.6777344
 -2.6796875  -2.6816406  -2.6835938  -2.6855469  -2.6894531  -2.6914062
 -2.6933594  -2.6972656  -2.6992188  -2.7011719  -2.7109375  -2.7167969
 -2.71875    -2.7226562  -2.7246094  -2.7265625  -2.7285156  -2.7304688
 -2.7324219  -2.734375   -2.7363281  -2.7382812  -2.7421875  -2.7460938
 -2.7480469  -2.7558594  -2.7578125  -2.7597656  -2.7636719  -2.765625
 -2.7695312  -2.7734375  -2.7753906  -2.78125    -2.7832031  -2.7851562
 -2.7890625  -2.7929688  -2.7949219  -2.796875   -2.7988281  -2.8007812
 -2.8027344  -2.8046875  -2.8066406  -2.8085938  -2.8105469  -2.8144531
 -2.8164062  -2.8183594  -2.8203125  -2.8222656  -2.8261719  -2.828125
 -2.8300781  -2.8320312  -2.8339844  -2.8359375  -2.8378906  -2.8398438
 -2.8417969  -2.84375    -2.8457031  -2.8476562  -2.8496094  -2.8515625
 -2.8535156  -2.8574219  -2.859375   -2.8632812  -2.8652344  -2.8671875
 -2.8691406  -2.8710938  -2.8730469  -2.875      -2.8769531  -2.8789062
 -2.8808594  -2.8828125  -2.8847656  -2.8867188  -2.8886719  -2.890625
 -2.8945312  -2.8964844  -2.8984375  -2.9003906  -2.9023438  -2.90625
 -2.9082031  -2.9101562  -2.9121094  -2.9140625  -2.9160156  -2.9179688
 -2.9199219  -2.921875   -2.9238281  -2.9277344  -2.9316406  -2.9375
 -2.9453125  -2.9492188  -2.9511719 ]
