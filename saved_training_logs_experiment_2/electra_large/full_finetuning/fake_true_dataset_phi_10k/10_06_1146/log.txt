log_loss_steps: 208
eval_steps: 208
check_degradation: 0
----------------Epoch 1/1----------------
Epoch 1/1, Loss after 192 samples: 0.6967
Mean accuracy: 0.5360, std: 0.0113, lower bound: 0.5136, upper bound: 0.5577 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 192 samples: 0.5356 with eval loss: 0.6917
Best model with eval loss 0.6917490234375 and eval accuracy 0.5356425702811245 with 192 samples seen is saved
Epoch 1/1, Loss after 400 samples: 0.6935
Mean accuracy: 0.6997, std: 0.0105, lower bound: 0.6792, upper bound: 0.7204 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 400 samples: 0.6998 with eval loss: 0.6818
Best model with eval loss 0.681781982421875 and eval accuracy 0.6997991967871486 with 400 samples seen is saved
Epoch 1/1, Loss after 608 samples: 0.6773
Mean accuracy: 0.8517, std: 0.0079, lower bound: 0.8348, upper bound: 0.8660 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 608 samples: 0.8519 with eval loss: 0.6604
Best model with eval loss 0.6604423828125 and eval accuracy 0.8519076305220884 with 608 samples seen is saved
Epoch 1/1, Loss after 816 samples: 0.6438
Mean accuracy: 0.8603, std: 0.0079, lower bound: 0.8444, upper bound: 0.8760 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 816 samples: 0.8599 with eval loss: 0.5933
Best model with eval loss 0.5932569580078125 and eval accuracy 0.8599397590361446 with 816 samples seen is saved
Epoch 1/1, Loss after 1024 samples: 0.5366
Mean accuracy: 0.7380, std: 0.0100, lower bound: 0.7184, upper bound: 0.7565 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 1024 samples: 0.7380 with eval loss: 0.5240
Best model with eval loss 0.524001220703125 and eval accuracy 0.7379518072289156 with 1024 samples seen is saved
Epoch 1/1, Loss after 1232 samples: 0.3977
Mean accuracy: 0.8720, std: 0.0076, lower bound: 0.8564, upper bound: 0.8870 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 1232 samples: 0.8720 with eval loss: 0.3449
Best model with eval loss 0.34488214111328125 and eval accuracy 0.8719879518072289 with 1232 samples seen is saved
Epoch 1/1, Loss after 1440 samples: 0.2698
Mean accuracy: 0.8842, std: 0.0075, lower bound: 0.8695, upper bound: 0.8981 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 1440 samples: 0.8840 with eval loss: 0.2916
Best model with eval loss 0.2916487579345703 and eval accuracy 0.8840361445783133 with 1440 samples seen is saved
Epoch 1/1, Loss after 1648 samples: 0.2713
Mean accuracy: 0.9211, std: 0.0061, lower bound: 0.9086, upper bound: 0.9327 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 1648 samples: 0.9207 with eval loss: 0.2092
Best model with eval loss 0.20919956970214842 and eval accuracy 0.9206827309236948 with 1648 samples seen is saved
Epoch 1/1, Loss after 1856 samples: 0.2658
Mean accuracy: 0.8799, std: 0.0073, lower bound: 0.8654, upper bound: 0.8936 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 1856 samples: 0.8800 with eval loss: 0.2782
Epoch 1/1, Loss after 2064 samples: 0.2124
Mean accuracy: 0.8836, std: 0.0070, lower bound: 0.8695, upper bound: 0.8976 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 2064 samples: 0.8835 with eval loss: 0.2897
Epoch 1/1, Loss after 2272 samples: 0.2344
Mean accuracy: 0.9552, std: 0.0045, lower bound: 0.9463, upper bound: 0.9639 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 2272 samples: 0.9553 with eval loss: 0.1331
Best model with eval loss 0.13312605285644533 and eval accuracy 0.9553212851405622 with 2272 samples seen is saved
Epoch 1/1, Loss after 2480 samples: 0.1788
Mean accuracy: 0.9447, std: 0.0050, lower bound: 0.9342, upper bound: 0.9543 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 2480 samples: 0.9448 with eval loss: 0.1443
Epoch 1/1, Loss after 2688 samples: 0.2087
Mean accuracy: 0.9280, std: 0.0055, lower bound: 0.9172, upper bound: 0.9383 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 2688 samples: 0.9282 with eval loss: 0.1778
Epoch 1/1, Loss after 2896 samples: 0.1461
Mean accuracy: 0.9542, std: 0.0047, lower bound: 0.9448, upper bound: 0.9634 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 2896 samples: 0.9543 with eval loss: 0.1274
Best model with eval loss 0.12738930892944336 and eval accuracy 0.9543172690763052 with 2896 samples seen is saved
Epoch 1/1, Loss after 3104 samples: 0.1436
Mean accuracy: 0.9523, std: 0.0046, lower bound: 0.9428, upper bound: 0.9608 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 3104 samples: 0.9523 with eval loss: 0.1293
Epoch 1/1, Loss after 3312 samples: 0.1162
Mean accuracy: 0.9604, std: 0.0043, lower bound: 0.9518, upper bound: 0.9694 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 3312 samples: 0.9603 with eval loss: 0.1198
Best model with eval loss 0.11979061508178711 and eval accuracy 0.9603413654618473 with 3312 samples seen is saved
Epoch 1/1, Loss after 3520 samples: 0.1595
Mean accuracy: 0.9553, std: 0.0047, lower bound: 0.9458, upper bound: 0.9639 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 3520 samples: 0.9553 with eval loss: 0.1254
Epoch 1/1, Loss after 3728 samples: 0.1926
Mean accuracy: 0.9364, std: 0.0053, lower bound: 0.9262, upper bound: 0.9468 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 3728 samples: 0.9362 with eval loss: 0.1633
Epoch 1/1, Loss after 3936 samples: 0.1257
Mean accuracy: 0.9695, std: 0.0039, lower bound: 0.9618, upper bound: 0.9769 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 3936 samples: 0.9694 with eval loss: 0.0954
Best model with eval loss 0.0953902244567871 and eval accuracy 0.9693775100401606 with 3936 samples seen is saved
Epoch 1/1, Loss after 4144 samples: 0.1524
Mean accuracy: 0.9741, std: 0.0035, lower bound: 0.9669, upper bound: 0.9809 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 4144 samples: 0.9739 with eval loss: 0.0878
Best model with eval loss 0.08775182342529297 and eval accuracy 0.9738955823293173 with 4144 samples seen is saved
Epoch 1/1, Loss after 4352 samples: 0.1818
Mean accuracy: 0.9682, std: 0.0041, lower bound: 0.9598, upper bound: 0.9764 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 4352 samples: 0.9684 with eval loss: 0.0925
Epoch 1/1, Loss after 4560 samples: 0.1354
Mean accuracy: 0.9511, std: 0.0047, lower bound: 0.9423, upper bound: 0.9608 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 4560 samples: 0.9513 with eval loss: 0.1291
Epoch 1/1, Loss after 4768 samples: 0.0842
Mean accuracy: 0.9431, std: 0.0055, lower bound: 0.9322, upper bound: 0.9538 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 4768 samples: 0.9433 with eval loss: 0.1482
Epoch 1/1, Loss after 4976 samples: 0.1389
Mean accuracy: 0.9490, std: 0.0049, lower bound: 0.9393, upper bound: 0.9583 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 4976 samples: 0.9488 with eval loss: 0.1472
Epoch 1/1, Loss after 5184 samples: 0.1090
Mean accuracy: 0.9440, std: 0.0052, lower bound: 0.9337, upper bound: 0.9543 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 5184 samples: 0.9438 with eval loss: 0.1488
Epoch 1/1, Loss after 5392 samples: 0.0879
Mean accuracy: 0.9444, std: 0.0050, lower bound: 0.9347, upper bound: 0.9538 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 5392 samples: 0.9448 with eval loss: 0.1537
Epoch 1/1, Loss after 5600 samples: 0.1221
Mean accuracy: 0.9610, std: 0.0044, lower bound: 0.9518, upper bound: 0.9689 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 5600 samples: 0.9608 with eval loss: 0.1121
Epoch 1/1, Loss after 5808 samples: 0.0766
Mean accuracy: 0.9448, std: 0.0050, lower bound: 0.9347, upper bound: 0.9543 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 5808 samples: 0.9448 with eval loss: 0.1482
Epoch 1/1, Loss after 6016 samples: 0.1484
Mean accuracy: 0.8895, std: 0.0069, lower bound: 0.8760, upper bound: 0.9021 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 6016 samples: 0.8896 with eval loss: 0.3107
Epoch 1/1, Loss after 6224 samples: 0.1122
Mean accuracy: 0.9638, std: 0.0042, lower bound: 0.9558, upper bound: 0.9719 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 6224 samples: 0.9639 with eval loss: 0.0952
Epoch 1/1, Loss after 6432 samples: 0.0768
Mean accuracy: 0.9510, std: 0.0048, lower bound: 0.9413, upper bound: 0.9598 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 6432 samples: 0.9508 with eval loss: 0.1246
Epoch 1/1, Loss after 6640 samples: 0.1313
Mean accuracy: 0.9122, std: 0.0066, lower bound: 0.8996, upper bound: 0.9247 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 6640 samples: 0.9121 with eval loss: 0.2317
Epoch 1/1, Loss after 6848 samples: 0.0988
Mean accuracy: 0.9752, std: 0.0035, lower bound: 0.9679, upper bound: 0.9819 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 6848 samples: 0.9754 with eval loss: 0.0741
Best model with eval loss 0.0740844202041626 and eval accuracy 0.9754016064257028 with 6848 samples seen is saved
Epoch 1/1, Loss after 7056 samples: 0.0869
Mean accuracy: 0.9536, std: 0.0046, lower bound: 0.9448, upper bound: 0.9623 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 7056 samples: 0.9538 with eval loss: 0.1220
Epoch 1/1, Loss after 7264 samples: 0.1145
Mean accuracy: 0.9714, std: 0.0039, lower bound: 0.9634, upper bound: 0.9789 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 7264 samples: 0.9714 with eval loss: 0.0744
Epoch 1/1, Loss after 7472 samples: 0.1413
Mean accuracy: 0.9783, std: 0.0031, lower bound: 0.9724, upper bound: 0.9844 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 7472 samples: 0.9784 with eval loss: 0.0651
Best model with eval loss 0.06505369186401368 and eval accuracy 0.9784136546184738 with 7472 samples seen is saved
Epoch 1/1, Loss after 7680 samples: 0.1431
Mean accuracy: 0.9684, std: 0.0039, lower bound: 0.9603, upper bound: 0.9759 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 7680 samples: 0.9684 with eval loss: 0.0895
Epoch 1/1, Loss after 7888 samples: 0.0971
Mean accuracy: 0.9544, std: 0.0047, lower bound: 0.9448, upper bound: 0.9629 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 7888 samples: 0.9543 with eval loss: 0.1217
Epoch 1/1, Loss after 8096 samples: 0.0674
Mean accuracy: 0.9593, std: 0.0045, lower bound: 0.9503, upper bound: 0.9679 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 8096 samples: 0.9593 with eval loss: 0.1123
Epoch 1/1, Loss after 8304 samples: 0.1182
Mean accuracy: 0.9625, std: 0.0042, lower bound: 0.9543, upper bound: 0.9704 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 8304 samples: 0.9623 with eval loss: 0.0960
Epoch 1/1, Loss after 8512 samples: 0.0841
Mean accuracy: 0.9338, std: 0.0056, lower bound: 0.9227, upper bound: 0.9448 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 8512 samples: 0.9337 with eval loss: 0.1706
Epoch 1/1, Loss after 8720 samples: 0.0841
Mean accuracy: 0.9574, std: 0.0046, lower bound: 0.9478, upper bound: 0.9659 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 8720 samples: 0.9573 with eval loss: 0.1078
Epoch 1/1, Loss after 8928 samples: 0.0927
Mean accuracy: 0.9575, std: 0.0044, lower bound: 0.9483, upper bound: 0.9659 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 8928 samples: 0.9573 with eval loss: 0.1185
Epoch 1/1, Loss after 9136 samples: 0.0618
Mean accuracy: 0.9494, std: 0.0049, lower bound: 0.9398, upper bound: 0.9588 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 9136 samples: 0.9493 with eval loss: 0.1445
Epoch 1/1, Loss after 9344 samples: 0.0314
Mean accuracy: 0.9609, std: 0.0044, lower bound: 0.9518, upper bound: 0.9694 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 9344 samples: 0.9608 with eval loss: 0.1146
Epoch 1/1, Loss after 9552 samples: 0.0764
Mean accuracy: 0.9673, std: 0.0040, lower bound: 0.9593, upper bound: 0.9754 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 9552 samples: 0.9674 with eval loss: 0.0948
Epoch 1/1, Loss after 9760 samples: 0.0969
Mean accuracy: 0.9669, std: 0.0040, lower bound: 0.9588, upper bound: 0.9744 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 9760 samples: 0.9669 with eval loss: 0.0927
Epoch 1/1, Loss after 9968 samples: 0.1050
Mean accuracy: 0.9815, std: 0.0030, lower bound: 0.9759, upper bound: 0.9874 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 9968 samples: 0.9814 with eval loss: 0.0556
Best model with eval loss 0.0556458158493042 and eval accuracy 0.981425702811245 with 9968 samples seen is saved
Epoch 1/1, Loss after 10176 samples: 0.0716
Mean accuracy: 0.9637, std: 0.0043, lower bound: 0.9548, upper bound: 0.9719 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 10176 samples: 0.9639 with eval loss: 0.0994
Epoch 1/1, Loss after 10384 samples: 0.0191
Mean accuracy: 0.9452, std: 0.0050, lower bound: 0.9352, upper bound: 0.9548 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 10384 samples: 0.9453 with eval loss: 0.1625
Epoch 1/1, Loss after 10592 samples: 0.0513
Mean accuracy: 0.9773, std: 0.0034, lower bound: 0.9704, upper bound: 0.9834 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 10592 samples: 0.9774 with eval loss: 0.0675
Epoch 1/1, Loss after 10800 samples: 0.0303
Mean accuracy: 0.9598, std: 0.0044, lower bound: 0.9508, upper bound: 0.9684 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 10800 samples: 0.9598 with eval loss: 0.1206
Epoch 1/1, Loss after 11008 samples: 0.0683
Mean accuracy: 0.9713, std: 0.0036, lower bound: 0.9634, upper bound: 0.9779 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 11008 samples: 0.9714 with eval loss: 0.0799
Epoch 1/1, Loss after 11216 samples: 0.0363
Mean accuracy: 0.9589, std: 0.0045, lower bound: 0.9498, upper bound: 0.9674 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 11216 samples: 0.9588 with eval loss: 0.1103
Epoch 1/1, Loss after 11424 samples: 0.0735
Mean accuracy: 0.9828, std: 0.0029, lower bound: 0.9774, upper bound: 0.9885 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 11424 samples: 0.9829 with eval loss: 0.0516
Best model with eval loss 0.05163828659057617 and eval accuracy 0.9829317269076305 with 11424 samples seen is saved
Epoch 1/1, Loss after 11632 samples: 0.0559
Mean accuracy: 0.9713, std: 0.0037, lower bound: 0.9639, upper bound: 0.9784 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 11632 samples: 0.9714 with eval loss: 0.0819
Epoch 1/1, Loss after 11840 samples: 0.0656
Mean accuracy: 0.9495, std: 0.0048, lower bound: 0.9398, upper bound: 0.9583 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 11840 samples: 0.9493 with eval loss: 0.1507
Epoch 1/1, Loss after 12048 samples: 0.0600
Mean accuracy: 0.9779, std: 0.0033, lower bound: 0.9714, upper bound: 0.9839 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 12048 samples: 0.9779 with eval loss: 0.0604
Epoch 1/1, Loss after 12256 samples: 0.0689
Mean accuracy: 0.9614, std: 0.0041, lower bound: 0.9533, upper bound: 0.9694 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 12256 samples: 0.9613 with eval loss: 0.1191
Epoch 1/1, Loss after 12464 samples: 0.0517
Mean accuracy: 0.9720, std: 0.0037, lower bound: 0.9644, upper bound: 0.9789 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 12464 samples: 0.9719 with eval loss: 0.0736
Epoch 1/1, Loss after 12672 samples: 0.0725
Mean accuracy: 0.9663, std: 0.0040, lower bound: 0.9583, upper bound: 0.9739 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 12672 samples: 0.9664 with eval loss: 0.1073
Epoch 1/1, Loss after 12880 samples: 0.1079
Mean accuracy: 0.9488, std: 0.0049, lower bound: 0.9393, upper bound: 0.9588 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 12880 samples: 0.9488 with eval loss: 0.1609
Epoch 1/1, Loss after 13088 samples: 0.0890
Mean accuracy: 0.9704, std: 0.0037, lower bound: 0.9629, upper bound: 0.9774 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 13088 samples: 0.9704 with eval loss: 0.0888
Epoch 1/1, Loss after 13296 samples: 0.0268
Mean accuracy: 0.9528, std: 0.0048, lower bound: 0.9433, upper bound: 0.9618 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 13296 samples: 0.9528 with eval loss: 0.1440
Epoch 1/1, Loss after 13504 samples: 0.0772
Mean accuracy: 0.9740, std: 0.0035, lower bound: 0.9669, upper bound: 0.9804 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 13504 samples: 0.9739 with eval loss: 0.0656
Epoch 1/1, Loss after 13712 samples: 0.0469
Mean accuracy: 0.9628, std: 0.0041, lower bound: 0.9548, upper bound: 0.9709 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 13712 samples: 0.9629 with eval loss: 0.1109
Epoch 1/1, Loss after 13920 samples: 0.0292
Mean accuracy: 0.9709, std: 0.0036, lower bound: 0.9639, upper bound: 0.9779 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 13920 samples: 0.9709 with eval loss: 0.0806
Epoch 1/1, Loss after 14128 samples: 0.0352
Mean accuracy: 0.9776, std: 0.0033, lower bound: 0.9699, upper bound: 0.9839 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 14128 samples: 0.9774 with eval loss: 0.0622
Epoch 1/1, Loss after 14336 samples: 0.0860
Mean accuracy: 0.9785, std: 0.0033, lower bound: 0.9719, upper bound: 0.9849 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 14336 samples: 0.9784 with eval loss: 0.0573
Epoch 1/1, Loss after 14544 samples: 0.0501
Mean accuracy: 0.9759, std: 0.0032, lower bound: 0.9694, upper bound: 0.9824 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 14544 samples: 0.9759 with eval loss: 0.0659
Epoch 1/1, Loss after 14752 samples: 0.0513
Mean accuracy: 0.9724, std: 0.0037, lower bound: 0.9654, upper bound: 0.9794 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 14752 samples: 0.9724 with eval loss: 0.0756
Epoch 1/1, Loss after 14960 samples: 0.0556
Mean accuracy: 0.9635, std: 0.0040, lower bound: 0.9558, upper bound: 0.9714 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 14960 samples: 0.9634 with eval loss: 0.1133
Epoch 1/1, Loss after 15168 samples: 0.0568
Mean accuracy: 0.9602, std: 0.0044, lower bound: 0.9518, upper bound: 0.9684 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 15168 samples: 0.9603 with eval loss: 0.1282
Epoch 1/1, Loss after 15376 samples: 0.0426
Mean accuracy: 0.9666, std: 0.0041, lower bound: 0.9583, upper bound: 0.9744 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 15376 samples: 0.9664 with eval loss: 0.1007
Epoch 1/1, Loss after 15584 samples: 0.0808
Mean accuracy: 0.9683, std: 0.0040, lower bound: 0.9603, upper bound: 0.9759 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 15584 samples: 0.9684 with eval loss: 0.0851
Epoch 1/1, Loss after 15792 samples: 0.0264
Mean accuracy: 0.9683, std: 0.0038, lower bound: 0.9608, upper bound: 0.9754 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 15792 samples: 0.9684 with eval loss: 0.0858
Training signal is False, stopping training
Training finished
Best model: {'eval_acc': 0.9829317269076305, 'nb_samples': 11424, 'eval_loss': 0.05163828659057617}
Training loss logs: [{'samples': 192, 'loss': 0.6966951810396634}, {'samples': 400, 'loss': 0.6935307429387019}, {'samples': 608, 'loss': 0.6772789588341346}, {'samples': 816, 'loss': 0.6438211294320914}, {'samples': 1024, 'loss': 0.5365829467773438}, {'samples': 1232, 'loss': 0.39769568810096156}, {'samples': 1440, 'loss': 0.2698018000676082}, {'samples': 1648, 'loss': 0.2713031768798828}, {'samples': 1856, 'loss': 0.26583202068622297}, {'samples': 2064, 'loss': 0.21235223916860727}, {'samples': 2272, 'loss': 0.23441534775954026}, {'samples': 2480, 'loss': 0.1787890654343825}, {'samples': 2688, 'loss': 0.20866247323843148}, {'samples': 2896, 'loss': 0.1460574223445012}, {'samples': 3104, 'loss': 0.1436168597294734}, {'samples': 3312, 'loss': 0.11621647614699143}, {'samples': 3520, 'loss': 0.15954362429105318}, {'samples': 3728, 'loss': 0.19260047032282904}, {'samples': 3936, 'loss': 0.12574426944439226}, {'samples': 4144, 'loss': 0.1523621265704815}, {'samples': 4352, 'loss': 0.18178686728844276}, {'samples': 4560, 'loss': 0.13537018115703875}, {'samples': 4768, 'loss': 0.0841817855834961}, {'samples': 4976, 'loss': 0.1388580432304969}, {'samples': 5184, 'loss': 0.10899514418381911}, {'samples': 5392, 'loss': 0.08787201001093937}, {'samples': 5600, 'loss': 0.12213231967045711}, {'samples': 5808, 'loss': 0.0765829269702618}, {'samples': 6016, 'loss': 0.14835504385141227}, {'samples': 6224, 'loss': 0.11224464269784781}, {'samples': 6432, 'loss': 0.07675198408273551}, {'samples': 6640, 'loss': 0.13133991681612456}, {'samples': 6848, 'loss': 0.09879328654362605}, {'samples': 7056, 'loss': 0.08691102724808913}, {'samples': 7264, 'loss': 0.11446765752939078}, {'samples': 7472, 'loss': 0.14130029311546913}, {'samples': 7680, 'loss': 0.1431428102346567}, {'samples': 7888, 'loss': 0.0971216421860915}, {'samples': 8096, 'loss': 0.06739427493168758}, {'samples': 8304, 'loss': 0.1181946534376878}, {'samples': 8512, 'loss': 0.08407319509066068}, {'samples': 8720, 'loss': 0.08407118687262902}, {'samples': 8928, 'loss': 0.09266415009131798}, {'samples': 9136, 'loss': 0.06176575330587534}, {'samples': 9344, 'loss': 0.031441294229947604}, {'samples': 9552, 'loss': 0.07639607099386361}, {'samples': 9760, 'loss': 0.09694557923537034}, {'samples': 9968, 'loss': 0.10502601586855374}, {'samples': 10176, 'loss': 0.0716403447664701}, {'samples': 10384, 'loss': 0.019089680451613206}, {'samples': 10592, 'loss': 0.05132645827073317}, {'samples': 10800, 'loss': 0.030280424998356745}, {'samples': 11008, 'loss': 0.06829799138582669}, {'samples': 11216, 'loss': 0.036293075634883}, {'samples': 11424, 'loss': 0.07350454880641057}, {'samples': 11632, 'loss': 0.055881270995506875}, {'samples': 11840, 'loss': 0.06563034424415001}, {'samples': 12048, 'loss': 0.059996990057138294}, {'samples': 12256, 'loss': 0.06886125527895413}, {'samples': 12464, 'loss': 0.05172416797051063}, {'samples': 12672, 'loss': 0.0724560205753033}, {'samples': 12880, 'loss': 0.10791551149808444}, {'samples': 13088, 'loss': 0.0889972264950092}, {'samples': 13296, 'loss': 0.026765181468083307}, {'samples': 13504, 'loss': 0.07720051361964299}, {'samples': 13712, 'loss': 0.046938639420729414}, {'samples': 13920, 'loss': 0.02915189816401555}, {'samples': 14128, 'loss': 0.035225052099961504}, {'samples': 14336, 'loss': 0.08595926945026104}, {'samples': 14544, 'loss': 0.05009006536923922}, {'samples': 14752, 'loss': 0.05126661520737868}, {'samples': 14960, 'loss': 0.05562117466559777}, {'samples': 15168, 'loss': 0.056833432270930365}, {'samples': 15376, 'loss': 0.04264978262094351}, {'samples': 15584, 'loss': 0.08082625499138466}, {'samples': 15792, 'loss': 0.026363446162297174}]
Evaluation accuracy logs: [{'samples': 192, 'accuracy': 0.5360421686746988, 'std': 0.01125990009885907, 'lower_bound': 0.5135542168674698, 'upper_bound': 0.5577309236947792}, {'samples': 400, 'accuracy': 0.6997073293172691, 'std': 0.010511903283102267, 'lower_bound': 0.6792168674698795, 'upper_bound': 0.7203815261044176}, {'samples': 608, 'accuracy': 0.8517404618473895, 'std': 0.007891242300364615, 'lower_bound': 0.8348393574297188, 'upper_bound': 0.8659764056224899}, {'samples': 816, 'accuracy': 0.8602786144578313, 'std': 0.007917183539801257, 'lower_bound': 0.8443775100401606, 'upper_bound': 0.876004016064257}, {'samples': 1024, 'accuracy': 0.7379779116465863, 'std': 0.00997889561059416, 'lower_bound': 0.7183734939759037, 'upper_bound': 0.7565261044176707}, {'samples': 1232, 'accuracy': 0.871975903614458, 'std': 0.007642162174366055, 'lower_bound': 0.856425702811245, 'upper_bound': 0.8870481927710844}, {'samples': 1440, 'accuracy': 0.8842399598393574, 'std': 0.007474276934156016, 'lower_bound': 0.8694779116465864, 'upper_bound': 0.8980923694779116}, {'samples': 1648, 'accuracy': 0.9210753012048193, 'std': 0.006146687615360679, 'lower_bound': 0.9086345381526104, 'upper_bound': 0.9327309236947792}, {'samples': 1856, 'accuracy': 0.8798760040160643, 'std': 0.007270986856884416, 'lower_bound': 0.865449297188755, 'upper_bound': 0.893574297188755}, {'samples': 2064, 'accuracy': 0.8836049196787148, 'std': 0.007011618608377569, 'lower_bound': 0.8694779116465864, 'upper_bound': 0.8975903614457831}, {'samples': 2272, 'accuracy': 0.9552213855421687, 'std': 0.004548685859198967, 'lower_bound': 0.946285140562249, 'upper_bound': 0.963855421686747}, {'samples': 2480, 'accuracy': 0.944698795180723, 'std': 0.005045328076331229, 'lower_bound': 0.9342369477911646, 'upper_bound': 0.9543172690763052}, {'samples': 2688, 'accuracy': 0.9280331325301205, 'std': 0.005501601189044521, 'lower_bound': 0.9171686746987951, 'upper_bound': 0.9382530120481928}, {'samples': 2896, 'accuracy': 0.9542364457831325, 'std': 0.0046941664624335425, 'lower_bound': 0.9447791164658634, 'upper_bound': 0.9633534136546185}, {'samples': 3104, 'accuracy': 0.9522680722891567, 'std': 0.004635175732907466, 'lower_bound': 0.9427710843373494, 'upper_bound': 0.9608433734939759}, {'samples': 3312, 'accuracy': 0.9603729919678715, 'std': 0.004322374602096813, 'lower_bound': 0.9518072289156626, 'upper_bound': 0.9693775100401606}, {'samples': 3520, 'accuracy': 0.9552831325301205, 'std': 0.0046634418140890635, 'lower_bound': 0.9457705823293172, 'upper_bound': 0.963855421686747}, {'samples': 3728, 'accuracy': 0.936429718875502, 'std': 0.005306692121698373, 'lower_bound': 0.9261922690763051, 'upper_bound': 0.9467871485943775}, {'samples': 3936, 'accuracy': 0.9695251004016066, 'std': 0.0038585306605919407, 'lower_bound': 0.9618473895582329, 'upper_bound': 0.9769076305220884}, {'samples': 4144, 'accuracy': 0.9740712851405623, 'std': 0.003534703398999885, 'lower_bound': 0.9668674698795181, 'upper_bound': 0.9809236947791165}, {'samples': 4352, 'accuracy': 0.9682489959839358, 'std': 0.0041196020965046185, 'lower_bound': 0.9598393574297188, 'upper_bound': 0.9764056224899599}, {'samples': 4560, 'accuracy': 0.9511239959839357, 'std': 0.004713229313539396, 'lower_bound': 0.9422690763052208, 'upper_bound': 0.9608433734939759}, {'samples': 4768, 'accuracy': 0.9430642570281125, 'std': 0.005467163503699248, 'lower_bound': 0.9322289156626506, 'upper_bound': 0.9538152610441767}, {'samples': 4976, 'accuracy': 0.9490110441767067, 'std': 0.004865571550343072, 'lower_bound': 0.9392570281124498, 'upper_bound': 0.9583333333333334}, {'samples': 5184, 'accuracy': 0.943995983935743, 'std': 0.005248190865696908, 'lower_bound': 0.9337349397590361, 'upper_bound': 0.9543172690763052}, {'samples': 5392, 'accuracy': 0.9444171686746988, 'std': 0.005034546400936839, 'lower_bound': 0.9347264056224899, 'upper_bound': 0.9538152610441767}, {'samples': 5600, 'accuracy': 0.9609518072289156, 'std': 0.00438725564890361, 'lower_bound': 0.9518072289156626, 'upper_bound': 0.9688755020080321}, {'samples': 5808, 'accuracy': 0.9448373493975903, 'std': 0.004988973503158329, 'lower_bound': 0.9347389558232931, 'upper_bound': 0.9543172690763052}, {'samples': 6016, 'accuracy': 0.889527108433735, 'std': 0.006948961861928221, 'lower_bound': 0.876004016064257, 'upper_bound': 0.9021084337349398}, {'samples': 6224, 'accuracy': 0.9638137550200804, 'std': 0.0041667305058737504, 'lower_bound': 0.9558232931726908, 'upper_bound': 0.9718875502008032}, {'samples': 6432, 'accuracy': 0.9509949799196786, 'std': 0.004771561284525472, 'lower_bound': 0.9412525100401606, 'upper_bound': 0.9598393574297188}, {'samples': 6640, 'accuracy': 0.9122163654618474, 'std': 0.0065572852385543615, 'lower_bound': 0.8995983935742972, 'upper_bound': 0.9246987951807228}, {'samples': 6848, 'accuracy': 0.9752284136546184, 'std': 0.0035103241441161205, 'lower_bound': 0.9678714859437751, 'upper_bound': 0.9819277108433735}, {'samples': 7056, 'accuracy': 0.9535527108433735, 'std': 0.0046300674886219926, 'lower_bound': 0.9447665662650602, 'upper_bound': 0.9623493975903614}, {'samples': 7264, 'accuracy': 0.9714407630522089, 'std': 0.0039322933564823, 'lower_bound': 0.9633534136546185, 'upper_bound': 0.9789156626506024}, {'samples': 7472, 'accuracy': 0.9782720883534136, 'std': 0.003104442835925158, 'lower_bound': 0.9723770080321285, 'upper_bound': 0.9844377510040161}, {'samples': 7680, 'accuracy': 0.9684211847389558, 'std': 0.0039000556218968772, 'lower_bound': 0.9603413654618473, 'upper_bound': 0.9759036144578314}, {'samples': 7888, 'accuracy': 0.9543569277108434, 'std': 0.004655234979278162, 'lower_bound': 0.9447791164658634, 'upper_bound': 0.9628514056224899}, {'samples': 8096, 'accuracy': 0.9592766064257029, 'std': 0.00449504279638972, 'lower_bound': 0.9503012048192772, 'upper_bound': 0.9678840361445783}, {'samples': 8304, 'accuracy': 0.9624628514056225, 'std': 0.004198387864636236, 'lower_bound': 0.9543172690763052, 'upper_bound': 0.9703815261044176}, {'samples': 8512, 'accuracy': 0.9337776104417671, 'std': 0.005642350871657792, 'lower_bound': 0.9226907630522089, 'upper_bound': 0.9447916666666666}, {'samples': 8720, 'accuracy': 0.9573594377510041, 'std': 0.004566408110421017, 'lower_bound': 0.9477911646586346, 'upper_bound': 0.9658634538152611}, {'samples': 8928, 'accuracy': 0.957515562248996, 'std': 0.00442608724409791, 'lower_bound': 0.9482931726907631, 'upper_bound': 0.9658634538152611}, {'samples': 9136, 'accuracy': 0.9493604417670684, 'std': 0.004853930277920878, 'lower_bound': 0.9397590361445783, 'upper_bound': 0.9588353413654619}, {'samples': 9344, 'accuracy': 0.960863453815261, 'std': 0.004363324903793096, 'lower_bound': 0.9518072289156626, 'upper_bound': 0.9693775100401606}, {'samples': 9552, 'accuracy': 0.9673308232931727, 'std': 0.003951446489834629, 'lower_bound': 0.9593373493975904, 'upper_bound': 0.9754016064257028}, {'samples': 9760, 'accuracy': 0.9669176706827309, 'std': 0.003960840192199717, 'lower_bound': 0.9588353413654619, 'upper_bound': 0.9743975903614458}, {'samples': 9968, 'accuracy': 0.9815045180722891, 'std': 0.002995871511761155, 'lower_bound': 0.9758910642570281, 'upper_bound': 0.9874497991967871}, {'samples': 10176, 'accuracy': 0.9636882530120481, 'std': 0.004316279861220025, 'lower_bound': 0.9548192771084337, 'upper_bound': 0.9718875502008032}, {'samples': 10384, 'accuracy': 0.9451827309236949, 'std': 0.005013589780041903, 'lower_bound': 0.9352409638554217, 'upper_bound': 0.9548192771084337}, {'samples': 10592, 'accuracy': 0.9773187751004015, 'std': 0.0033851961272151647, 'lower_bound': 0.9703815261044176, 'upper_bound': 0.983433734939759}, {'samples': 10800, 'accuracy': 0.9597871485943774, 'std': 0.004368254095683319, 'lower_bound': 0.9508032128514057, 'upper_bound': 0.9683734939759037}, {'samples': 11008, 'accuracy': 0.971339859437751, 'std': 0.0036336754492673905, 'lower_bound': 0.9633534136546185, 'upper_bound': 0.9779116465863453}, {'samples': 11216, 'accuracy': 0.9588790160642571, 'std': 0.004505320889109873, 'lower_bound': 0.9497991967871486, 'upper_bound': 0.9673694779116466}, {'samples': 11424, 'accuracy': 0.982843875502008, 'std': 0.0028550258690492043, 'lower_bound': 0.9773970883534137, 'upper_bound': 0.9884538152610441}, {'samples': 11632, 'accuracy': 0.9712605421686746, 'std': 0.0037232280846226803, 'lower_bound': 0.963855421686747, 'upper_bound': 0.9784136546184738}, {'samples': 11840, 'accuracy': 0.9494723895582329, 'std': 0.004820092776313353, 'lower_bound': 0.9397590361445783, 'upper_bound': 0.9583458835341365}, {'samples': 12048, 'accuracy': 0.977933232931727, 'std': 0.003268189923483891, 'lower_bound': 0.9713855421686747, 'upper_bound': 0.9839357429718876}, {'samples': 12256, 'accuracy': 0.9613805220883533, 'std': 0.004145780987944249, 'lower_bound': 0.9533007028112449, 'upper_bound': 0.9693775100401606}, {'samples': 12464, 'accuracy': 0.9719588353413655, 'std': 0.0037215638783990863, 'lower_bound': 0.9643574297188755, 'upper_bound': 0.9789156626506024}, {'samples': 12672, 'accuracy': 0.966324297188755, 'std': 0.004028071783439772, 'lower_bound': 0.9583333333333334, 'upper_bound': 0.9738955823293173}, {'samples': 12880, 'accuracy': 0.9487751004016063, 'std': 0.004896840688139707, 'lower_bound': 0.9392570281124498, 'upper_bound': 0.9588353413654619}, {'samples': 13088, 'accuracy': 0.9703584337349398, 'std': 0.003693287933544902, 'lower_bound': 0.9628514056224899, 'upper_bound': 0.9774221887550201}, {'samples': 13296, 'accuracy': 0.9527901606425703, 'std': 0.004786436275371272, 'lower_bound': 0.9432730923694779, 'upper_bound': 0.9618473895582329}, {'samples': 13504, 'accuracy': 0.9740095381526104, 'std': 0.003526636653245525, 'lower_bound': 0.9668549196787148, 'upper_bound': 0.9804216867469879}, {'samples': 13712, 'accuracy': 0.9628328313253012, 'std': 0.004133012067656121, 'lower_bound': 0.9548067269076305, 'upper_bound': 0.9708835341365462}, {'samples': 13920, 'accuracy': 0.9708599397590362, 'std': 0.0035779049960584465, 'lower_bound': 0.963855421686747, 'upper_bound': 0.9779116465863453}, {'samples': 14128, 'accuracy': 0.9775592369477911, 'std': 0.0033269612668967263, 'lower_bound': 0.9698795180722891, 'upper_bound': 0.9839482931726907}, {'samples': 14336, 'accuracy': 0.9784929718875502, 'std': 0.003291543669721208, 'lower_bound': 0.9718875502008032, 'upper_bound': 0.9849397590361446}, {'samples': 14544, 'accuracy': 0.9759071285140563, 'std': 0.003215946324039108, 'lower_bound': 0.9693775100401606, 'upper_bound': 0.982429718875502}, {'samples': 14752, 'accuracy': 0.9723840361445784, 'std': 0.0036697054633431474, 'lower_bound': 0.9653614457831325, 'upper_bound': 0.9794176706827309}, {'samples': 14960, 'accuracy': 0.9635225903614457, 'std': 0.003987771544102477, 'lower_bound': 0.9558232931726908, 'upper_bound': 0.9713855421686747}, {'samples': 15168, 'accuracy': 0.9601646586345383, 'std': 0.00438520886313045, 'lower_bound': 0.9518072289156626, 'upper_bound': 0.9683734939759037}, {'samples': 15376, 'accuracy': 0.9666059236947792, 'std': 0.004054210123052248, 'lower_bound': 0.9583333333333334, 'upper_bound': 0.9743975903614458}, {'samples': 15584, 'accuracy': 0.9682916666666668, 'std': 0.003987992724717995, 'lower_bound': 0.9603288152610441, 'upper_bound': 0.9759036144578314}, {'samples': 15792, 'accuracy': 0.968308734939759, 'std': 0.003810183387927396, 'lower_bound': 0.9608433734939759, 'upper_bound': 0.9754016064257028}]
Evaluating the best model on the test set of dataset ./fake_true_datasets/fake_true_dataset_phi_10k...
Test metrics:
accuracy: 0.9744708835341365
precision: 0.9548584364983741
recall: 0.9960692392219109
f1_score: 0.9750165205155904
fp_rate: 0.047156727862868945
tp_rate: 0.9960692392219109
std_accuracy: 0.0035111392081108453
std_precision: 0.006438719226278653
std_recall: 0.001946164409980813
std_f1_score: 0.0034801589910938065
std_fp_rate: 0.00669391685438079
std_tp_rate: 0.001946164409980813
TP: 992.839
TN: 948.307
FP: 46.935
FN: 3.919
roc_auc: 0.9985408501475782
fpr: [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00100402
 0.00100402 0.00100402 0.00100402 0.00200803 0.00200803 0.00200803
 0.00200803 0.00200803 0.00200803 0.00200803 0.00200803 0.00200803
 0.00200803 0.00200803 0.00200803 0.00301205 0.00301205 0.00401606
 0.00401606 0.00401606 0.00401606 0.00401606 0.00401606 0.00401606
 0.00502008 0.00502008 0.0060241  0.0060241  0.00702811 0.00702811
 0.00803213 0.00803213 0.00903614 0.00903614 0.01004016 0.01004016
 0.01104418 0.01104418 0.01305221 0.01305221 0.01606426 0.01606426
 0.01706827 0.01706827 0.01807229 0.01807229 0.01907631 0.01907631
 0.02208835 0.02208835 0.02710843 0.02710843 0.03212851 0.03212851
 0.03514056 0.03514056 0.05220884 0.05220884 0.09738956 0.09738956
 0.10040161 0.10240964 0.1124498  0.11445783 0.12550201 0.12550201
 0.14457831 0.14658635 0.14859438 0.15060241 0.20381526 0.20582329
 0.20682731 0.20883534 0.21485944 0.21686747 0.21787149 0.21987952
 0.23092369 0.23293173 0.23393574 0.23594378 0.24196787 0.2439759
 0.24698795 0.24899598 0.25       0.25200803 0.25401606 0.2560241
 0.25702811 0.26305221 0.26506024 0.26706827 0.27811245 0.28012048
 0.28212851 0.28413655 0.29216867 0.29518072 0.30220884 0.30421687
 0.3062249  0.30823293 0.312249   0.31526104 0.31726908 0.31927711
 0.32128514 0.32329317 0.3253012  0.32931727 0.33232932 0.33634538
 0.33935743 0.34136546 0.34437751 0.34738956 0.34839357 0.35040161
 0.35441767 0.3564257  0.35943775 0.36345382 0.37048193 0.37349398
 0.37449799 0.37751004 0.38253012 0.38453815 0.3875502  0.39056225
 0.39056225 0.39257028 0.39658635 0.40060241 0.40963855 0.41164659
 0.4126506  0.4186747  0.42068273 0.42269076 0.4246988  0.42570281
 0.42871486 0.43273092 0.43473896 0.437751   0.43975904 0.44277108
 0.44678715 0.44879518 0.45180723 0.45481928 0.45783133 0.46586345
 0.46686747 0.47088353 0.47389558 0.47690763 0.47891566 0.48293173
 0.48594378 0.48795181 0.48995984 0.49497992 0.49698795 0.49799197
 0.5        0.50301205 0.50401606 0.5060241  0.50903614 0.51104418
 0.51405622 0.51706827 0.51907631 0.52008032 0.52309237 0.52710843
 0.53012048 0.53413655 0.53614458 0.53714859 0.53915663 0.54317269
 0.54518072 0.54819277 0.54919679 0.55321285 0.55421687 0.5562249
 0.562249   0.56425703 0.56827309 0.57128514 0.57228916 0.57630522
 0.57931727 0.58032129 0.58333333 0.5873494  0.58935743 0.59136546
 0.59538153 0.60040161 0.60441767 0.60742972 0.60843373 0.61746988
 0.62148594 0.62349398 0.62751004 0.62851406 0.63353414 0.63453815
 0.6375502  0.63955823 0.64558233 0.64959839 0.65060241 0.65261044
 0.65562249 0.65763052 0.66064257 0.6626506  0.66365462 0.66566265
 0.66967871 0.67068273 0.67269076 0.67670683 0.67871486 0.68172691
 0.68574297 0.687751   0.6997992  0.70180723 0.70883534 0.71084337
 0.71184739 0.7188755  0.72690763 0.73092369 0.73192771 0.73594378
 0.73895582 0.73995984 0.74497992 0.74799197 0.75       0.75100402
 0.76104418 0.76506024 0.77008032 0.77108434 0.77309237 0.77409639
 0.77911647 0.78012048 0.78212851 0.78614458 0.78915663 0.79919679
 0.80522088 0.80923695 0.81024096 0.812249   0.81526104 0.82028112
 0.82630522 0.83433735 0.84136546 0.85040161 0.85240964 0.85542169
 0.86044177 0.86445783 0.8684739  0.87048193 0.87349398 0.87751004
 0.87951807 0.8875502  0.89156627 0.89859438 0.90261044 0.91064257
 0.91365462 0.9186747  0.92068273 0.92369478 0.93172691 0.93473896
 0.93674699 0.94176707 0.94477912 0.94879518 0.95080321 0.95180723
 0.95682731 0.96084337 0.96787149 0.97791165 0.98293173 0.98493976
 0.98594378 0.98795181 0.98895582 0.99297189 0.99497992 0.99698795
 1.        ]
tpr: [0.         0.00100402 0.00200803 0.01204819 0.01305221 0.02008032
 0.02911647 0.03614458 0.03915663 0.04919679 0.05220884 0.06827309
 0.0753012  0.07831325 0.08534137 0.09136546 0.10943775 0.12449799
 0.13554217 0.14859438 0.15461847 0.16566265 0.17570281 0.18473896
 0.19578313 0.20080321 0.20983936 0.2248996  0.23694779 0.24297189
 0.25100402 0.26204819 0.27008032 0.2811245  0.29016064 0.29417671
 0.29919679 0.30823293 0.31526104 0.32329317 0.32931727 0.33433735
 0.34236948 0.34638554 0.35341365 0.36445783 0.37650602 0.38955823
 0.39959839 0.41164659 0.42771084 0.43975904 0.44879518 0.45883534
 0.46385542 0.46787149 0.47590361 0.48995984 0.49297189 0.49698795
 0.5060241  0.51706827 0.52208835 0.53012048 0.53714859 0.54216867
 0.54618474 0.54919679 0.55120482 0.55522088 0.55722892 0.562249
 0.56526104 0.57329317 0.57831325 0.58232932 0.58333333 0.58634538
 0.59136546 0.6064257  0.61445783 0.61947791 0.62248996 0.62851406
 0.63253012 0.63453815 0.64658635 0.64859438 0.65662651 0.6626506
 0.66666667 0.67068273 0.67670683 0.68172691 0.68473896 0.68875502
 0.69176707 0.69578313 0.69779116 0.6997992  0.70180723 0.70281124
 0.70481928 0.70783133 0.70983936 0.71084337 0.71787149 0.72389558
 0.72891566 0.73493976 0.73895582 0.74096386 0.74698795 0.75
 0.75200803 0.75401606 0.75803213 0.76706827 0.76807229 0.77208835
 0.77309237 0.77610442 0.78012048 0.7811245  0.78413655 0.78815261
 0.78915663 0.79116466 0.79417671 0.79618474 0.80220884 0.80421687
 0.8062249  0.80722892 0.80923695 0.81526104 0.81726908 0.81827309
 0.82028112 0.82329317 0.8253012  0.82630522 0.82931727 0.8313253
 0.83333333 0.83534137 0.8373494  0.83935743 0.84136546 0.84437751
 0.84638554 0.84738956 0.84939759 0.85040161 0.85341365 0.8564257
 0.86445783 0.86546185 0.86746988 0.8684739  0.87048193 0.87148594
 0.87349398 0.87650602 0.87851406 0.88052209 0.88253012 0.88253012
 0.88554217 0.88855422 0.8935743  0.8935743  0.89457831 0.89959839
 0.90060241 0.90361446 0.90562249 0.90863454 0.91064257 0.91365462
 0.91566265 0.91767068 0.91967871 0.91967871 0.92269076 0.92269076
 0.92570281 0.92771084 0.93072289 0.93373494 0.93574297 0.94578313
 0.94678715 0.9497992  0.9497992  0.95783133 0.95783133 0.96285141
 0.96285141 0.96686747 0.96686747 0.9688755  0.9688755  0.97188755
 0.97188755 0.97289157 0.97289157 0.97791165 0.97791165 0.98092369
 0.98092369 0.98493976 0.98493976 0.98795181 0.98795181 0.99196787
 0.99196787 0.99297189 0.99297189 0.9939759  0.9939759  0.99497992
 0.99497992 0.99598394 0.99598394 0.99698795 0.99698795 0.99799197
 0.99799197 0.99799197 0.99799197 0.99799197 0.99799197 0.99899598
 0.99899598 0.99899598 0.99899598 0.99899598 0.99899598 0.99899598
 0.99899598 0.99899598 0.99899598 0.99899598 0.99899598 0.99899598
 0.99899598 0.99899598 0.99899598 0.99899598 0.99899598 0.99899598
 0.99899598 0.99899598 0.99899598 0.99899598 0.99899598 0.99899598
 0.99899598 0.99899598 0.99899598 0.99899598 0.99899598 0.99899598
 0.99899598 0.99899598 0.99899598 0.99899598 0.99899598 0.99899598
 0.99899598 0.99899598 0.99899598 0.99899598 0.99899598 0.99899598
 0.99899598 0.99899598 0.99899598 0.99899598 0.99899598 0.99899598
 0.99899598 0.99899598 0.99899598 0.99899598 0.99899598 0.99899598
 0.99899598 0.99899598 0.99899598 0.99899598 0.99899598 0.99899598
 0.99899598 0.99899598 0.99899598 0.99899598 0.99899598 0.99899598
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.        ]
thresholds: [        inf  3.0234375   3.0214844   3.0117188   3.0097656   3.0078125
  3.0058594   3.0039062   3.0019531   3.          2.9980469   2.9941406
  2.9921875   2.9902344   2.9882812   2.9863281   2.9824219   2.9804688
  2.9785156   2.9765625   2.9746094   2.9726562   2.9707031   2.96875
  2.9667969   2.9648438   2.9628906   2.9609375   2.9589844   2.9570312
  2.9550781   2.953125    2.9511719   2.9492188   2.9472656   2.9453125
  2.9433594   2.9414062   2.9394531   2.9375      2.9355469   2.9335938
  2.9316406   2.9296875   2.9277344   2.9257812   2.9238281   2.921875
  2.9199219   2.9160156   2.9121094   2.9082031   2.90625     2.9042969
  2.9023438   2.9003906   2.8984375   2.8945312   2.8925781   2.890625
  2.8886719   2.8867188   2.8828125   2.8808594   2.8789062   2.8769531
  2.875       2.8730469   2.8710938   2.8691406   2.8671875   2.8652344
  2.8632812   2.8613281   2.859375    2.8574219   2.8554688   2.8535156
  2.8515625   2.8417969   2.8378906   2.8359375   2.8339844   2.8320312
  2.8300781   2.828125    2.8242188   2.8222656   2.8183594   2.8164062
  2.8144531   2.8085938   2.8066406   2.8046875   2.8027344   2.8007812
  2.7929688   2.7910156   2.7890625   2.7851562   2.7832031   2.78125
  2.7792969   2.7773438   2.7753906   2.7734375   2.7714844   2.765625
  2.7636719   2.7597656   2.7558594   2.7519531   2.75        2.7480469
  2.7441406   2.7421875   2.7402344   2.7324219   2.7304688   2.7265625
  2.7226562   2.7207031   2.7167969   2.7148438   2.7128906   2.7089844
  2.7070312   2.7050781   2.703125    2.6992188   2.6933594   2.6894531
  2.6875      2.6816406   2.6796875   2.6757812   2.6738281   2.6699219
  2.6679688   2.6621094   2.6601562   2.6582031   2.65625     2.6542969
  2.6464844   2.6425781   2.6367188   2.6328125   2.6289062   2.6269531
  2.625       2.6210938   2.6171875   2.6152344   2.6074219   2.5996094
  2.5878906   2.5859375   2.5839844   2.5800781   2.5742188   2.5722656
  2.5683594   2.5566406   2.5488281   2.5410156   2.5351562   2.5292969
  2.5117188   2.5097656   2.4941406   2.4902344   2.4882812   2.4863281
  2.4824219   2.4804688   2.4746094   2.4707031   2.4667969   2.4609375
  2.4511719   2.4492188   2.4414062   2.4355469   2.4179688   2.4023438
  2.3652344   2.359375    2.3515625   2.3359375   2.328125    2.2578125
  2.2519531   2.2167969   2.2070312   2.1230469   2.1171875   2.0390625
  2.0371094   1.9980469   1.9951172   1.9599609   1.9511719   1.8681641
  1.8613281   1.84375     1.8173828   1.7001953   1.5800781   1.5166016
  1.5087891   1.4160156   1.3974609   1.3300781   1.3291016   1.2021484
  1.1074219   1.0683594   0.9013672   0.8935547   0.66308594  0.6591797
  0.51220703  0.48657227 -0.0713501  -0.10736084 -0.8964844  -0.8989258
 -0.93115234 -0.9511719  -1.1113281  -1.1318359  -1.3095703  -1.3125
 -1.4892578  -1.4931641  -1.5097656  -1.5283203  -1.9160156  -1.9189453
 -1.9199219  -1.9345703  -1.9541016  -1.9560547  -1.9580078  -1.9648438
 -2.0175781  -2.0195312  -2.0214844  -2.0234375  -2.0839844  -2.0957031
 -2.1191406  -2.125      -2.1269531  -2.1289062  -2.1347656  -2.1445312
 -2.1464844  -2.1621094  -2.1738281  -2.1777344  -2.2246094  -2.2265625
 -2.2402344  -2.2460938  -2.2832031  -2.2871094  -2.3046875  -2.3066406
 -2.3105469  -2.3164062  -2.3320312  -2.3378906  -2.3417969  -2.3457031
 -2.3515625  -2.3554688  -2.3691406  -2.3828125  -2.4003906  -2.4101562
 -2.4160156  -2.4257812  -2.4335938  -2.4355469  -2.4375     -2.4453125
 -2.4570312  -2.4609375  -2.4667969  -2.46875    -2.4921875  -2.4960938
 -2.5        -2.5019531  -2.5136719  -2.515625   -2.5234375  -2.5273438
 -2.53125    -2.5332031  -2.5449219  -2.5507812  -2.5820312  -2.5878906
 -2.5898438  -2.6015625  -2.6035156  -2.609375   -2.6113281  -2.6152344
 -2.6171875  -2.6210938  -2.625      -2.6269531  -2.6308594  -2.6328125
 -2.6523438  -2.6542969  -2.6621094  -2.6640625  -2.6875     -2.6972656
 -2.6992188  -2.703125   -2.7089844  -2.7109375  -2.7167969  -2.7246094
 -2.7304688  -2.7402344  -2.7441406  -2.7597656  -2.7617188  -2.7636719
 -2.765625   -2.7675781  -2.7695312  -2.7773438  -2.7792969  -2.78125
 -2.7871094  -2.7890625  -2.7910156  -2.7929688  -2.7949219  -2.7988281
 -2.8007812  -2.8085938  -2.8105469  -2.8125     -2.8144531  -2.8222656
 -2.8242188  -2.8261719  -2.828125   -2.8320312  -2.8359375  -2.8378906
 -2.8398438  -2.8417969  -2.84375    -2.8457031  -2.8476562  -2.8515625
 -2.8535156  -2.8554688  -2.8574219  -2.859375   -2.8613281  -2.8652344
 -2.8691406  -2.8710938  -2.8789062  -2.8808594  -2.8828125  -2.890625
 -2.8925781  -2.8945312  -2.8984375  -2.9003906  -2.9023438  -2.9042969
 -2.90625    -2.9082031  -2.9101562  -2.9160156  -2.9179688  -2.9199219
 -2.921875   -2.9238281  -2.9257812  -2.9277344  -2.9296875  -2.9316406
 -2.9335938  -2.9355469  -2.9375     -2.9394531  -2.9414062  -2.9433594
 -2.9472656  -2.9511719  -2.9550781  -2.9570312  -2.9589844  -2.9609375
 -2.9628906  -2.9667969  -2.9707031  -2.9746094  -2.9765625  -2.9824219
 -2.984375   -2.9863281  -2.9882812  -2.9902344  -2.9921875  -2.9941406
 -2.9960938  -2.9980469  -3.0019531  -3.0039062  -3.0058594  -3.0078125
 -3.0097656  -3.0117188  -3.0136719  -3.015625   -3.0175781  -3.0214844
 -3.0234375  -3.0253906  -3.0273438  -3.0292969  -3.03125    -3.0332031
 -3.0371094  -3.0390625  -3.0410156  -3.0429688  -3.0449219  -3.046875
 -3.0488281  -3.0527344  -3.0546875  -3.0566406  -3.0585938  -3.0605469
 -3.0625     -3.0664062  -3.0703125  -3.0722656  -3.0761719  -3.0800781
 -3.0820312  -3.0839844  -3.0878906  -3.0898438  -3.09375    -3.0957031
 -3.0976562  -3.0996094  -3.1015625  -3.1035156  -3.1054688  -3.1074219
 -3.109375   -3.1132812  -3.1152344  -3.1269531  -3.140625   -3.1425781
 -3.1445312  -3.1484375  -3.1503906  -3.1542969  -3.1601562  -3.1621094
 -3.1796875 ]
