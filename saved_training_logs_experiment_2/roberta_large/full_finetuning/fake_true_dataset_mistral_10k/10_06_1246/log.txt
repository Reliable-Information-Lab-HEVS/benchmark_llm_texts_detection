log_loss_steps: 208
eval_steps: 208
check_degradation: 0
----------------Epoch 1/1----------------
Epoch 1/1, Loss after 192 samples: 0.7062
Mean accuracy: 0.5188, std: 0.0116, lower bound: 0.4965, upper bound: 0.5416 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 192 samples: 0.5188 with eval loss: 0.6897
Best model with eval loss 0.6896768385364164 and eval accuracy 0.518762677484787 with 192 samples seen is saved
Epoch 1/1, Loss after 400 samples: 0.7057
Mean accuracy: 0.6971, std: 0.0104, lower bound: 0.6775, upper bound: 0.7191 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 400 samples: 0.6973 with eval loss: 0.6790
Best model with eval loss 0.6789964245211694 and eval accuracy 0.697261663286004 with 400 samples seen is saved
Epoch 1/1, Loss after 608 samples: 0.6929
Mean accuracy: 0.7496, std: 0.0104, lower bound: 0.7292, upper bound: 0.7693 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 608 samples: 0.7490 with eval loss: 0.6446
Best model with eval loss 0.6446162808325983 and eval accuracy 0.7489858012170385 with 608 samples seen is saved
Epoch 1/1, Loss after 816 samples: 0.5940
Mean accuracy: 0.8115, std: 0.0089, lower bound: 0.7936, upper bound: 0.8281 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 816 samples: 0.8119 with eval loss: 0.4841
Best model with eval loss 0.4841433955777076 and eval accuracy 0.8118661257606491 with 816 samples seen is saved
Epoch 1/1, Loss after 1024 samples: 0.5232
Mean accuracy: 0.7890, std: 0.0090, lower bound: 0.7713, upper bound: 0.8053 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 1024 samples: 0.7890 with eval loss: 0.4529
Best model with eval loss 0.45286887691866967 and eval accuracy 0.7890466531440162 with 1024 samples seen is saved
Epoch 1/1, Loss after 1232 samples: 0.3979
Mean accuracy: 0.8346, std: 0.0086, lower bound: 0.8180, upper bound: 0.8519 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 1232 samples: 0.8342 with eval loss: 0.3550
Best model with eval loss 0.35495682301059844 and eval accuracy 0.8341784989858012 with 1232 samples seen is saved
Epoch 1/1, Loss after 1440 samples: 0.3376
Mean accuracy: 0.8541, std: 0.0080, lower bound: 0.8382, upper bound: 0.8692 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 1440 samples: 0.8545 with eval loss: 0.3448
Best model with eval loss 0.3448427040730753 and eval accuracy 0.8544624746450304 with 1440 samples seen is saved
Epoch 1/1, Loss after 1648 samples: 0.3191
Mean accuracy: 0.9045, std: 0.0065, lower bound: 0.8915, upper bound: 0.9178 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 1648 samples: 0.9047 with eval loss: 0.2346
Best model with eval loss 0.23463649807437773 and eval accuracy 0.9046653144016227 with 1648 samples seen is saved
Epoch 1/1, Loss after 1856 samples: 0.2680
Mean accuracy: 0.8363, std: 0.0085, lower bound: 0.8205, upper bound: 0.8529 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 1856 samples: 0.8367 with eval loss: 0.4061
Epoch 1/1, Loss after 2064 samples: 0.2918
Mean accuracy: 0.7978, std: 0.0084, lower bound: 0.7825, upper bound: 0.8144 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 2064 samples: 0.7977 with eval loss: 0.5136
Epoch 1/1, Loss after 2272 samples: 0.2311
Mean accuracy: 0.8560, std: 0.0081, lower bound: 0.8398, upper bound: 0.8712 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 2272 samples: 0.8565 with eval loss: 0.3488
Epoch 1/1, Loss after 2480 samples: 0.1988
Mean accuracy: 0.9147, std: 0.0063, lower bound: 0.9021, upper bound: 0.9265 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 2480 samples: 0.9143 with eval loss: 0.1917
Best model with eval loss 0.19168409368684214 and eval accuracy 0.9143002028397565 with 2480 samples seen is saved
Epoch 1/1, Loss after 2688 samples: 0.2220
Mean accuracy: 0.8788, std: 0.0075, lower bound: 0.8641, upper bound: 0.8930 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 2688 samples: 0.8788 with eval loss: 0.3182
Epoch 1/1, Loss after 2896 samples: 0.1824
Mean accuracy: 0.9235, std: 0.0059, lower bound: 0.9118, upper bound: 0.9356 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 2896 samples: 0.9234 with eval loss: 0.1733
Best model with eval loss 0.1732609125394975 and eval accuracy 0.9234279918864098 with 2896 samples seen is saved
Epoch 1/1, Loss after 3104 samples: 0.2785
Mean accuracy: 0.9115, std: 0.0065, lower bound: 0.8986, upper bound: 0.9234 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 3104 samples: 0.9113 with eval loss: 0.1966
Epoch 1/1, Loss after 3312 samples: 0.2139
Mean accuracy: 0.9106, std: 0.0063, lower bound: 0.8981, upper bound: 0.9224 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 3312 samples: 0.9108 with eval loss: 0.2079
Epoch 1/1, Loss after 3520 samples: 0.1999
Mean accuracy: 0.8873, std: 0.0074, lower bound: 0.8722, upper bound: 0.9026 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 3520 samples: 0.8874 with eval loss: 0.2663
Epoch 1/1, Loss after 3728 samples: 0.1978
Mean accuracy: 0.8349, std: 0.0087, lower bound: 0.8169, upper bound: 0.8504 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 3728 samples: 0.8352 with eval loss: 0.3800
Epoch 1/1, Loss after 3936 samples: 0.1648
Mean accuracy: 0.8936, std: 0.0068, lower bound: 0.8803, upper bound: 0.9067 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 3936 samples: 0.8935 with eval loss: 0.2558
Epoch 1/1, Loss after 4144 samples: 0.2385
Mean accuracy: 0.8142, std: 0.0088, lower bound: 0.7951, upper bound: 0.8322 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 4144 samples: 0.8144 with eval loss: 0.5050
Epoch 1/1, Loss after 4352 samples: 0.2964
Mean accuracy: 0.8613, std: 0.0075, lower bound: 0.8463, upper bound: 0.8758 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 4352 samples: 0.8616 with eval loss: 0.3079
Epoch 1/1, Loss after 4560 samples: 0.1640
Mean accuracy: 0.8712, std: 0.0072, lower bound: 0.8565, upper bound: 0.8849 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 4560 samples: 0.8712 with eval loss: 0.3257
Epoch 1/1, Loss after 4768 samples: 0.1901
Mean accuracy: 0.8604, std: 0.0077, lower bound: 0.8458, upper bound: 0.8753 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 4768 samples: 0.8605 with eval loss: 0.3478
Epoch 1/1, Loss after 4976 samples: 0.1926
Mean accuracy: 0.8944, std: 0.0070, lower bound: 0.8808, upper bound: 0.9077 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 4976 samples: 0.8945 with eval loss: 0.2586
Epoch 1/1, Loss after 5184 samples: 0.0843
Mean accuracy: 0.9375, std: 0.0056, lower bound: 0.9265, upper bound: 0.9483 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 5184 samples: 0.9376 with eval loss: 0.1642
Best model with eval loss 0.1642080878298129 and eval accuracy 0.9376267748478702 with 5184 samples seen is saved
Epoch 1/1, Loss after 5392 samples: 0.1632
Mean accuracy: 0.9345, std: 0.0057, lower bound: 0.9229, upper bound: 0.9447 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 5392 samples: 0.9346 with eval loss: 0.1788
Epoch 1/1, Loss after 5600 samples: 0.1700
Mean accuracy: 0.8838, std: 0.0071, lower bound: 0.8692, upper bound: 0.8976 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 5600 samples: 0.8839 with eval loss: 0.3046
Epoch 1/1, Loss after 5808 samples: 0.1935
Mean accuracy: 0.8589, std: 0.0078, lower bound: 0.8433, upper bound: 0.8737 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 5808 samples: 0.8585 with eval loss: 0.3381
Epoch 1/1, Loss after 6016 samples: 0.2143
Mean accuracy: 0.9162, std: 0.0063, lower bound: 0.9042, upper bound: 0.9280 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 6016 samples: 0.9163 with eval loss: 0.1773
Epoch 1/1, Loss after 6224 samples: 0.1965
Mean accuracy: 0.9310, std: 0.0056, lower bound: 0.9194, upper bound: 0.9412 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 6224 samples: 0.9310 with eval loss: 0.1563
Best model with eval loss 0.1562680188686617 and eval accuracy 0.9310344827586207 with 6224 samples seen is saved
Epoch 1/1, Loss after 6432 samples: 0.2098
Mean accuracy: 0.9099, std: 0.0064, lower bound: 0.8976, upper bound: 0.9224 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 6432 samples: 0.9097 with eval loss: 0.2057
Epoch 1/1, Loss after 6640 samples: 0.1364
Mean accuracy: 0.8385, std: 0.0084, lower bound: 0.8225, upper bound: 0.8540 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 6640 samples: 0.8382 with eval loss: 0.4503
Epoch 1/1, Loss after 6848 samples: 0.1377
Mean accuracy: 0.9274, std: 0.0061, lower bound: 0.9158, upper bound: 0.9391 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 6848 samples: 0.9275 with eval loss: 0.1943
Epoch 1/1, Loss after 7056 samples: 0.1280
Mean accuracy: 0.9177, std: 0.0062, lower bound: 0.9057, upper bound: 0.9295 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 7056 samples: 0.9178 with eval loss: 0.2273
Epoch 1/1, Loss after 7264 samples: 0.2158
Mean accuracy: 0.8491, std: 0.0078, lower bound: 0.8337, upper bound: 0.8641 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 7264 samples: 0.8489 with eval loss: 0.4221
Epoch 1/1, Loss after 7472 samples: 0.1592
Mean accuracy: 0.9244, std: 0.0061, lower bound: 0.9123, upper bound: 0.9361 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 7472 samples: 0.9244 with eval loss: 0.1803
Epoch 1/1, Loss after 7680 samples: 0.1249
Mean accuracy: 0.9463, std: 0.0052, lower bound: 0.9356, upper bound: 0.9559 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 7680 samples: 0.9462 with eval loss: 0.1314
Best model with eval loss 0.1313518494848282 and eval accuracy 0.9462474645030426 with 7680 samples seen is saved
Epoch 1/1, Loss after 7888 samples: 0.1610
Mean accuracy: 0.9165, std: 0.0063, lower bound: 0.9037, upper bound: 0.9285 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 7888 samples: 0.9163 with eval loss: 0.2066
Epoch 1/1, Loss after 8096 samples: 0.1008
Mean accuracy: 0.8974, std: 0.0067, lower bound: 0.8839, upper bound: 0.9113 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 8096 samples: 0.8976 with eval loss: 0.2690
Epoch 1/1, Loss after 8304 samples: 0.1103
Mean accuracy: 0.8861, std: 0.0070, lower bound: 0.8722, upper bound: 0.8996 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 8304 samples: 0.8864 with eval loss: 0.3389
Epoch 1/1, Loss after 8512 samples: 0.0759
Mean accuracy: 0.9271, std: 0.0059, lower bound: 0.9158, upper bound: 0.9386 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 8512 samples: 0.9270 with eval loss: 0.1935
Epoch 1/1, Loss after 8720 samples: 0.1497
Mean accuracy: 0.9198, std: 0.0061, lower bound: 0.9087, upper bound: 0.9315 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 8720 samples: 0.9199 with eval loss: 0.2334
Epoch 1/1, Loss after 8928 samples: 0.1136
Mean accuracy: 0.8703, std: 0.0075, lower bound: 0.8545, upper bound: 0.8849 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 8928 samples: 0.8702 with eval loss: 0.3868
Epoch 1/1, Loss after 9136 samples: 0.0865
Mean accuracy: 0.9345, std: 0.0055, lower bound: 0.9239, upper bound: 0.9452 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 9136 samples: 0.9346 with eval loss: 0.1635
Epoch 1/1, Loss after 9344 samples: 0.1069
Mean accuracy: 0.8753, std: 0.0075, lower bound: 0.8600, upper bound: 0.8895 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 9344 samples: 0.8758 with eval loss: 0.3856
Epoch 1/1, Loss after 9552 samples: 0.1773
Mean accuracy: 0.9087, std: 0.0063, lower bound: 0.8960, upper bound: 0.9204 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 9552 samples: 0.9087 with eval loss: 0.2262
Epoch 1/1, Loss after 9760 samples: 0.1609
Mean accuracy: 0.9456, std: 0.0050, lower bound: 0.9356, upper bound: 0.9559 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 9760 samples: 0.9452 with eval loss: 0.1261
Best model with eval loss 0.12608569487929344 and eval accuracy 0.9452332657200812 with 9760 samples seen is saved
Epoch 1/1, Loss after 9968 samples: 0.1597
Mean accuracy: 0.8713, std: 0.0076, lower bound: 0.8565, upper bound: 0.8859 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 9968 samples: 0.8712 with eval loss: 0.3100
Epoch 1/1, Loss after 10176 samples: 0.0971
Mean accuracy: 0.9002, std: 0.0067, lower bound: 0.8874, upper bound: 0.9128 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 10176 samples: 0.9001 with eval loss: 0.2806
Epoch 1/1, Loss after 10384 samples: 0.1024
Mean accuracy: 0.9047, std: 0.0067, lower bound: 0.8910, upper bound: 0.9168 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 10384 samples: 0.9047 with eval loss: 0.2781
Epoch 1/1, Loss after 10592 samples: 0.1108
Mean accuracy: 0.9021, std: 0.0067, lower bound: 0.8884, upper bound: 0.9148 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 10592 samples: 0.9021 with eval loss: 0.2863
Epoch 1/1, Loss after 10800 samples: 0.1245
Mean accuracy: 0.9387, std: 0.0053, lower bound: 0.9280, upper bound: 0.9488 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 10800 samples: 0.9386 with eval loss: 0.1535
Epoch 1/1, Loss after 11008 samples: 0.1127
Mean accuracy: 0.8884, std: 0.0067, lower bound: 0.8753, upper bound: 0.9016 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 11008 samples: 0.8884 with eval loss: 0.3351
Epoch 1/1, Loss after 11216 samples: 0.1079
Mean accuracy: 0.9348, std: 0.0055, lower bound: 0.9239, upper bound: 0.9447 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 11216 samples: 0.9346 with eval loss: 0.1604
Epoch 1/1, Loss after 11424 samples: 0.1532
Mean accuracy: 0.9148, std: 0.0061, lower bound: 0.9021, upper bound: 0.9270 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 11424 samples: 0.9148 with eval loss: 0.2212
Epoch 1/1, Loss after 11632 samples: 0.1111
Mean accuracy: 0.9314, std: 0.0055, lower bound: 0.9209, upper bound: 0.9417 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 11632 samples: 0.9315 with eval loss: 0.1714
Epoch 1/1, Loss after 11840 samples: 0.1065
Mean accuracy: 0.9390, std: 0.0054, lower bound: 0.9285, upper bound: 0.9493 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 11840 samples: 0.9391 with eval loss: 0.1489
Epoch 1/1, Loss after 12048 samples: 0.0867
Mean accuracy: 0.9271, std: 0.0060, lower bound: 0.9153, upper bound: 0.9386 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 12048 samples: 0.9270 with eval loss: 0.2007
Epoch 1/1, Loss after 12256 samples: 0.0887
Mean accuracy: 0.8616, std: 0.0075, lower bound: 0.8468, upper bound: 0.8758 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 12256 samples: 0.8611 with eval loss: 0.4556
Epoch 1/1, Loss after 12464 samples: 0.0924
Mean accuracy: 0.9488, std: 0.0047, lower bound: 0.9397, upper bound: 0.9579 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 12464 samples: 0.9488 with eval loss: 0.1386
Epoch 1/1, Loss after 12672 samples: 0.1261
Mean accuracy: 0.9048, std: 0.0068, lower bound: 0.8915, upper bound: 0.9173 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 12672 samples: 0.9052 with eval loss: 0.2820
Epoch 1/1, Loss after 12880 samples: 0.1018
Mean accuracy: 0.9371, std: 0.0055, lower bound: 0.9265, upper bound: 0.9478 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 12880 samples: 0.9371 with eval loss: 0.1691
Epoch 1/1, Loss after 13088 samples: 0.1056
Mean accuracy: 0.8786, std: 0.0075, lower bound: 0.8636, upper bound: 0.8930 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 13088 samples: 0.8788 with eval loss: 0.3645
Epoch 1/1, Loss after 13296 samples: 0.1383
Mean accuracy: 0.9280, std: 0.0057, lower bound: 0.9168, upper bound: 0.9391 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 13296 samples: 0.9280 with eval loss: 0.2019
Epoch 1/1, Loss after 13504 samples: 0.0884
Mean accuracy: 0.9019, std: 0.0067, lower bound: 0.8884, upper bound: 0.9148 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 13504 samples: 0.9016 with eval loss: 0.2857
Epoch 1/1, Loss after 13712 samples: 0.0912
Mean accuracy: 0.9133, std: 0.0065, lower bound: 0.9006, upper bound: 0.9250 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 13712 samples: 0.9133 with eval loss: 0.2437
Epoch 1/1, Loss after 13920 samples: 0.0608
Mean accuracy: 0.8834, std: 0.0073, lower bound: 0.8692, upper bound: 0.8971 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 13920 samples: 0.8834 with eval loss: 0.3580
Epoch 1/1, Loss after 14128 samples: 0.1443
Mean accuracy: 0.9312, std: 0.0057, lower bound: 0.9209, upper bound: 0.9422 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 14128 samples: 0.9310 with eval loss: 0.1838
Epoch 1/1, Loss after 14336 samples: 0.1705
Mean accuracy: 0.9164, std: 0.0063, lower bound: 0.9047, upper bound: 0.9290 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 14336 samples: 0.9163 with eval loss: 0.2350
Epoch 1/1, Loss after 14544 samples: 0.1330
Mean accuracy: 0.9282, std: 0.0060, lower bound: 0.9163, upper bound: 0.9402 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 14544 samples: 0.9280 with eval loss: 0.1843
Epoch 1/1, Loss after 14752 samples: 0.1135
Mean accuracy: 0.9189, std: 0.0060, lower bound: 0.9072, upper bound: 0.9305 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 14752 samples: 0.9189 with eval loss: 0.2155
Epoch 1/1, Loss after 14960 samples: 0.0847
Mean accuracy: 0.9039, std: 0.0065, lower bound: 0.8915, upper bound: 0.9168 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 14960 samples: 0.9042 with eval loss: 0.2594
Epoch 1/1, Loss after 15168 samples: 0.1172
Mean accuracy: 0.9099, std: 0.0064, lower bound: 0.8966, upper bound: 0.9219 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 15168 samples: 0.9097 with eval loss: 0.2405
Epoch 1/1, Loss after 15376 samples: 0.1412
Mean accuracy: 0.9105, std: 0.0064, lower bound: 0.8986, upper bound: 0.9229 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 15376 samples: 0.9102 with eval loss: 0.2404
Epoch 1/1, Loss after 15584 samples: 0.1206
Mean accuracy: 0.9140, std: 0.0063, lower bound: 0.9016, upper bound: 0.9255 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 15584 samples: 0.9143 with eval loss: 0.2235
Training signal is False, stopping training
Training finished
Best model: {'eval_acc': 0.9452332657200812, 'nb_samples': 9760, 'eval_loss': 0.12608569487929344}
Training loss logs: [{'samples': 192, 'loss': 0.7061955378605769}, {'samples': 400, 'loss': 0.7057096041165866}, {'samples': 608, 'loss': 0.6928523137019231}, {'samples': 816, 'loss': 0.5939988356370193}, {'samples': 1024, 'loss': 0.5231607877291166}, {'samples': 1232, 'loss': 0.3979152532724234}, {'samples': 1440, 'loss': 0.3376365624941312}, {'samples': 1648, 'loss': 0.31909504303565395}, {'samples': 1856, 'loss': 0.2680247196784386}, {'samples': 2064, 'loss': 0.291827706190256}, {'samples': 2272, 'loss': 0.23105037212371826}, {'samples': 2480, 'loss': 0.19875641969534066}, {'samples': 2688, 'loss': 0.22195535898208618}, {'samples': 2896, 'loss': 0.18242944662387556}, {'samples': 3104, 'loss': 0.27852728733649623}, {'samples': 3312, 'loss': 0.21392491230597863}, {'samples': 3520, 'loss': 0.1999406860424922}, {'samples': 3728, 'loss': 0.19782477158766526}, {'samples': 3936, 'loss': 0.16483494410148034}, {'samples': 4144, 'loss': 0.23853359314111564}, {'samples': 4352, 'loss': 0.2963701578286978}, {'samples': 4560, 'loss': 0.1640324959388146}, {'samples': 4768, 'loss': 0.1900811745570256}, {'samples': 4976, 'loss': 0.1926151170180394}, {'samples': 5184, 'loss': 0.08429142374258775}, {'samples': 5392, 'loss': 0.16324912699369284}, {'samples': 5600, 'loss': 0.17001731808368975}, {'samples': 5808, 'loss': 0.19349066340006316}, {'samples': 6016, 'loss': 0.21433135408621568}, {'samples': 6224, 'loss': 0.1964927315711975}, {'samples': 6432, 'loss': 0.20984700780648452}, {'samples': 6640, 'loss': 0.13636847184254572}, {'samples': 6848, 'loss': 0.13773010327265814}, {'samples': 7056, 'loss': 0.12798790977551386}, {'samples': 7264, 'loss': 0.21576536618746245}, {'samples': 7472, 'loss': 0.1591592408143557}, {'samples': 7680, 'loss': 0.12489251448557927}, {'samples': 7888, 'loss': 0.1609723040690789}, {'samples': 8096, 'loss': 0.10075219319416927}, {'samples': 8304, 'loss': 0.1103419317648961}, {'samples': 8512, 'loss': 0.07585804737531222}, {'samples': 8720, 'loss': 0.14970447237675005}, {'samples': 8928, 'loss': 0.11359869172939888}, {'samples': 9136, 'loss': 0.08649720824681796}, {'samples': 9344, 'loss': 0.10693119351680462}, {'samples': 9552, 'loss': 0.17729231944450966}, {'samples': 9760, 'loss': 0.16093665590653053}, {'samples': 9968, 'loss': 0.15966844787964454}, {'samples': 10176, 'loss': 0.09707985703761761}, {'samples': 10384, 'loss': 0.10244651253406818}, {'samples': 10592, 'loss': 0.11079231821573697}, {'samples': 10800, 'loss': 0.12454848679212424}, {'samples': 11008, 'loss': 0.11266986108743228}, {'samples': 11216, 'loss': 0.10788756322402221}, {'samples': 11424, 'loss': 0.15317007440787095}, {'samples': 11632, 'loss': 0.11110061980210818}, {'samples': 11840, 'loss': 0.10654782905028416}, {'samples': 12048, 'loss': 0.0867494659928175}, {'samples': 12256, 'loss': 0.08868327507605919}, {'samples': 12464, 'loss': 0.09237258824018332}, {'samples': 12672, 'loss': 0.12605813031013197}, {'samples': 12880, 'loss': 0.1018221011528602}, {'samples': 13088, 'loss': 0.10557352464932662}, {'samples': 13296, 'loss': 0.1382690782730396}, {'samples': 13504, 'loss': 0.08839061397772568}, {'samples': 13712, 'loss': 0.09115495761999717}, {'samples': 13920, 'loss': 0.06078637849826079}, {'samples': 14128, 'loss': 0.1443041110268006}, {'samples': 14336, 'loss': 0.1705008246577703}, {'samples': 14544, 'loss': 0.13304790281332457}, {'samples': 14752, 'loss': 0.11349385747542748}, {'samples': 14960, 'loss': 0.08473685727669643}, {'samples': 15168, 'loss': 0.1171924093594918}, {'samples': 15376, 'loss': 0.14115599829417008}, {'samples': 15584, 'loss': 0.12064396991179539}]
Evaluation accuracy logs: [{'samples': 192, 'accuracy': 0.5187763691683569, 'std': 0.011623999790824832, 'lower_bound': 0.4964503042596349, 'upper_bound': 0.5415948275862069}, {'samples': 400, 'accuracy': 0.6971475659229209, 'std': 0.010425405750920349, 'lower_bound': 0.6774721095334685, 'upper_bound': 0.7190669371196755}, {'samples': 608, 'accuracy': 0.7495618661257606, 'std': 0.010434418754401823, 'lower_bound': 0.72920892494929, 'upper_bound': 0.7692824543610548}, {'samples': 816, 'accuracy': 0.8115461460446247, 'std': 0.008893789520013346, 'lower_bound': 0.7935978701825558, 'upper_bound': 0.8280933062880325}, {'samples': 1024, 'accuracy': 0.7890451318458418, 'std': 0.009012013549020424, 'lower_bound': 0.7712981744421906, 'upper_bound': 0.8052738336713996}, {'samples': 1232, 'accuracy': 0.8346237322515213, 'std': 0.008577404695471729, 'lower_bound': 0.8179513184584178, 'upper_bound': 0.8519269776876268}, {'samples': 1440, 'accuracy': 0.8541237322515213, 'std': 0.007970334197615012, 'lower_bound': 0.8382352941176471, 'upper_bound': 0.8691683569979716}, {'samples': 1648, 'accuracy': 0.9045344827586207, 'std': 0.006547622021026956, 'lower_bound': 0.8914807302231237, 'upper_bound': 0.9178498985801217}, {'samples': 1856, 'accuracy': 0.8363463488843813, 'std': 0.008546864643426851, 'lower_bound': 0.8204741379310344, 'upper_bound': 0.8529411764705882}, {'samples': 2064, 'accuracy': 0.7977971602434077, 'std': 0.008413976478761267, 'lower_bound': 0.7824543610547667, 'upper_bound': 0.8144016227180527}, {'samples': 2272, 'accuracy': 0.8560055780933062, 'std': 0.008091265416371235, 'lower_bound': 0.8397565922920892, 'upper_bound': 0.8712094320486816}, {'samples': 2480, 'accuracy': 0.9146810344827586, 'std': 0.006252085116431107, 'lower_bound': 0.902129817444219, 'upper_bound': 0.9264705882352942}, {'samples': 2688, 'accuracy': 0.8787941176470588, 'std': 0.007501912539165325, 'lower_bound': 0.8640973630831643, 'upper_bound': 0.8930020283975659}, {'samples': 2896, 'accuracy': 0.9235157200811359, 'std': 0.005944214459693805, 'lower_bound': 0.9117520283975659, 'upper_bound': 0.9355983772819473}, {'samples': 3104, 'accuracy': 0.9114781947261663, 'std': 0.006473770236793612, 'lower_bound': 0.8985801217038539, 'upper_bound': 0.9234279918864098}, {'samples': 3312, 'accuracy': 0.9105512170385396, 'std': 0.006329781719172497, 'lower_bound': 0.8980730223123732, 'upper_bound': 0.9224137931034483}, {'samples': 3520, 'accuracy': 0.8872540567951318, 'std': 0.007419347177859949, 'lower_bound': 0.8722109533468559, 'upper_bound': 0.9026369168356998}, {'samples': 3728, 'accuracy': 0.8349477687626775, 'std': 0.00872269490405612, 'lower_bound': 0.8169371196754563, 'upper_bound': 0.8504183569979716}, {'samples': 3936, 'accuracy': 0.8935588235294117, 'std': 0.006840550806393459, 'lower_bound': 0.8803245436105477, 'upper_bound': 0.9067063894523327}, {'samples': 4144, 'accuracy': 0.814185598377282, 'std': 0.00875698432505995, 'lower_bound': 0.7951191683569979, 'upper_bound': 0.8321501014198783}, {'samples': 4352, 'accuracy': 0.8613265720081136, 'std': 0.007511727216611205, 'lower_bound': 0.8463488843813387, 'upper_bound': 0.8757733265720081}, {'samples': 4560, 'accuracy': 0.8711612576064909, 'std': 0.007184349308326864, 'lower_bound': 0.8564908722109533, 'upper_bound': 0.8848884381338742}, {'samples': 4768, 'accuracy': 0.8604467545638945, 'std': 0.007711169493024654, 'lower_bound': 0.845841784989858, 'upper_bound': 0.8752535496957403}, {'samples': 4976, 'accuracy': 0.8943635902636917, 'std': 0.006997048749303404, 'lower_bound': 0.8808316430020284, 'upper_bound': 0.907707910750507}, {'samples': 5184, 'accuracy': 0.9375496957403651, 'std': 0.0055553402815785614, 'lower_bound': 0.9264579107505071, 'upper_bound': 0.9482758620689655}, {'samples': 5392, 'accuracy': 0.9344543610547668, 'std': 0.005692638773997229, 'lower_bound': 0.922920892494929, 'upper_bound': 0.9447388438133875}, {'samples': 5600, 'accuracy': 0.883787018255578, 'std': 0.007122254117140767, 'lower_bound': 0.8691683569979716, 'upper_bound': 0.8975659229208925}, {'samples': 5808, 'accuracy': 0.8588519269776876, 'std': 0.007778046185283886, 'lower_bound': 0.8433062880324543, 'upper_bound': 0.8737322515212982}, {'samples': 6016, 'accuracy': 0.9161790060851926, 'std': 0.0062512413959147275, 'lower_bound': 0.904158215010142, 'upper_bound': 0.9280045638945233}, {'samples': 6224, 'accuracy': 0.9309650101419878, 'std': 0.0056438187374439125, 'lower_bound': 0.9193711967545639, 'upper_bound': 0.9411764705882353}, {'samples': 6432, 'accuracy': 0.9098661257606491, 'std': 0.006398832099083856, 'lower_bound': 0.8975659229208925, 'upper_bound': 0.9224137931034483}, {'samples': 6640, 'accuracy': 0.8384695740365111, 'std': 0.008371296465905157, 'lower_bound': 0.8225152129817445, 'upper_bound': 0.8539553752535497}, {'samples': 6848, 'accuracy': 0.9274036511156187, 'std': 0.006084186867514761, 'lower_bound': 0.9158088235294117, 'upper_bound': 0.9391480730223124}, {'samples': 7056, 'accuracy': 0.9176866125760648, 'std': 0.006207698495834393, 'lower_bound': 0.9056668356997971, 'upper_bound': 0.9295131845841785}, {'samples': 7264, 'accuracy': 0.8491333671399596, 'std': 0.007763357457799068, 'lower_bound': 0.8336587221095334, 'upper_bound': 0.8640973630831643}, {'samples': 7472, 'accuracy': 0.9244016227180527, 'std': 0.00605366239703826, 'lower_bound': 0.9122718052738337, 'upper_bound': 0.936105476673428}, {'samples': 7680, 'accuracy': 0.946251014198783, 'std': 0.005163431104559417, 'lower_bound': 0.9355983772819473, 'upper_bound': 0.9558823529411765}, {'samples': 7888, 'accuracy': 0.9164756592292089, 'std': 0.006322614370715289, 'lower_bound': 0.9036511156186613, 'upper_bound': 0.928498985801217}, {'samples': 8096, 'accuracy': 0.8973640973630832, 'std': 0.0066513502968408465, 'lower_bound': 0.8838742393509128, 'upper_bound': 0.9112576064908722}, {'samples': 8304, 'accuracy': 0.8860502028397566, 'std': 0.00704281131879382, 'lower_bound': 0.8722109533468559, 'upper_bound': 0.8995943204868154}, {'samples': 8512, 'accuracy': 0.9270775862068965, 'std': 0.005875577316689609, 'lower_bound': 0.9158215010141988, 'upper_bound': 0.9386409736308317}, {'samples': 8720, 'accuracy': 0.9198453346855984, 'std': 0.006072094909379038, 'lower_bound': 0.9087221095334685, 'upper_bound': 0.9315415821501014}, {'samples': 8928, 'accuracy': 0.8703453346855984, 'std': 0.007530997222579857, 'lower_bound': 0.8544624746450304, 'upper_bound': 0.8848884381338742}, {'samples': 9136, 'accuracy': 0.9345177484787018, 'std': 0.005523377202568219, 'lower_bound': 0.9239350912778904, 'upper_bound': 0.9452332657200812}, {'samples': 9344, 'accuracy': 0.8752505070993915, 'std': 0.00754736935980882, 'lower_bound': 0.8600405679513184, 'upper_bound': 0.8894523326572008}, {'samples': 9552, 'accuracy': 0.9086861054766735, 'std': 0.00631793662521713, 'lower_bound': 0.8960446247464503, 'upper_bound': 0.9203980730223124}, {'samples': 9760, 'accuracy': 0.9455983772819473, 'std': 0.005006424617250323, 'lower_bound': 0.9355983772819473, 'upper_bound': 0.9558823529411765}, {'samples': 9968, 'accuracy': 0.87126369168357, 'std': 0.007571260128816191, 'lower_bound': 0.8564781947261663, 'upper_bound': 0.8859026369168357}, {'samples': 10176, 'accuracy': 0.9001921906693713, 'std': 0.006726114008921859, 'lower_bound': 0.8874239350912779, 'upper_bound': 0.9127915821501015}, {'samples': 10384, 'accuracy': 0.9046876267748478, 'std': 0.006686909999049266, 'lower_bound': 0.890973630831643, 'upper_bound': 0.9168483772819472}, {'samples': 10592, 'accuracy': 0.9020867139959432, 'std': 0.006731441683447564, 'lower_bound': 0.8884381338742393, 'upper_bound': 0.9148073022312373}, {'samples': 10800, 'accuracy': 0.9387053752535496, 'std': 0.005336094661741968, 'lower_bound': 0.9279792089249492, 'upper_bound': 0.9487956389452333}, {'samples': 11008, 'accuracy': 0.8883833671399595, 'std': 0.006708645828006105, 'lower_bound': 0.8752535496957403, 'upper_bound': 0.9016227180527383}, {'samples': 11216, 'accuracy': 0.9347844827586207, 'std': 0.005465980447493873, 'lower_bound': 0.9239350912778904, 'upper_bound': 0.9447261663286004}, {'samples': 11424, 'accuracy': 0.9148443204868154, 'std': 0.006129899813756245, 'lower_bound': 0.902129817444219, 'upper_bound': 0.9269776876267748}, {'samples': 11632, 'accuracy': 0.9314426977687627, 'std': 0.005479231217565759, 'lower_bound': 0.920892494929006, 'upper_bound': 0.941683569979716}, {'samples': 11840, 'accuracy': 0.9389812373225151, 'std': 0.005444669254904523, 'lower_bound': 0.928498985801217, 'upper_bound': 0.949290060851927}, {'samples': 12048, 'accuracy': 0.9271485801217039, 'std': 0.006005552016788524, 'lower_bound': 0.915314401622718, 'upper_bound': 0.9386409736308317}, {'samples': 12256, 'accuracy': 0.8615791075050709, 'std': 0.0074506490599138215, 'lower_bound': 0.8468433062880324, 'upper_bound': 0.8757606490872211}, {'samples': 12464, 'accuracy': 0.948817444219067, 'std': 0.00472874943556127, 'lower_bound': 0.9396551724137931, 'upper_bound': 0.9579107505070994}, {'samples': 12672, 'accuracy': 0.9047555780933062, 'std': 0.006797370207459858, 'lower_bound': 0.8914807302231237, 'upper_bound': 0.9173427991886409}, {'samples': 12880, 'accuracy': 0.9370796146044624, 'std': 0.005464375583917794, 'lower_bound': 0.9264579107505071, 'upper_bound': 0.9477687626774848}, {'samples': 13088, 'accuracy': 0.8786430020283976, 'std': 0.007546827263623623, 'lower_bound': 0.8635902636916836, 'upper_bound': 0.893014705882353}, {'samples': 13296, 'accuracy': 0.92802738336714, 'std': 0.005700597905971324, 'lower_bound': 0.9168356997971603, 'upper_bound': 0.9391480730223124}, {'samples': 13504, 'accuracy': 0.901882860040568, 'std': 0.00666417801862555, 'lower_bound': 0.8884381338742393, 'upper_bound': 0.9148073022312373}, {'samples': 13712, 'accuracy': 0.9132545638945232, 'std': 0.006462486980298892, 'lower_bound': 0.9006085192697769, 'upper_bound': 0.924961967545639}, {'samples': 13920, 'accuracy': 0.883432555780933, 'std': 0.007263999374229916, 'lower_bound': 0.8691683569979716, 'upper_bound': 0.8970588235294118}, {'samples': 14128, 'accuracy': 0.9312129817444219, 'std': 0.005721063241973221, 'lower_bound': 0.920892494929006, 'upper_bound': 0.9421906693711968}, {'samples': 14336, 'accuracy': 0.916392494929006, 'std': 0.00631358341786248, 'lower_bound': 0.9046653144016227, 'upper_bound': 0.9290060851926978}, {'samples': 14544, 'accuracy': 0.9282261663286003, 'std': 0.0060100703636784564, 'lower_bound': 0.9163159229208925, 'upper_bound': 0.9401622718052738}, {'samples': 14752, 'accuracy': 0.9188722109533469, 'std': 0.005989265536915139, 'lower_bound': 0.9072008113590264, 'upper_bound': 0.9305273833671399}, {'samples': 14960, 'accuracy': 0.9039406693711968, 'std': 0.006483994180878739, 'lower_bound': 0.8914807302231237, 'upper_bound': 0.9168356997971603}, {'samples': 15168, 'accuracy': 0.909867139959432, 'std': 0.006370376156459571, 'lower_bound': 0.896551724137931, 'upper_bound': 0.9219066937119675}, {'samples': 15376, 'accuracy': 0.9104903651115619, 'std': 0.006377807230759014, 'lower_bound': 0.8985674442190669, 'upper_bound': 0.9229335699797161}, {'samples': 15584, 'accuracy': 0.9140360040567952, 'std': 0.006345302062241905, 'lower_bound': 0.9016227180527383, 'upper_bound': 0.9254563894523327}]
Evaluating the best model on the test set of dataset ./fake_true_datasets/fake_true_dataset_mistral_10k...
Test metrics:
accuracy: 0.9039477687626775
precision: 0.840376609132141
recall: 0.996985038119539
f1_score: 0.9119716489270204
fp_rate: 0.18882940475888205
tp_rate: 0.996985038119539
std_accuracy: 0.006564553417806723
std_precision: 0.010458876283951759
std_recall: 0.0017805925396436892
std_f1_score: 0.006275105754753583
std_fp_rate: 0.012325772778000262
std_tp_rate: 0.0017805925396436892
TP: 981.591
TN: 800.994
FP: 186.447
FN: 2.968
roc_auc: 0.9950570666820272
fpr: [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.0010142
 0.0010142  0.0010142  0.0010142  0.0010142  0.0010142  0.0010142
 0.0010142  0.0010142  0.0010142  0.0010142  0.0010142  0.0010142
 0.0010142  0.0010142  0.0010142  0.0010142  0.0010142  0.0010142
 0.0010142  0.0010142  0.0010142  0.0010142  0.0010142  0.0010142
 0.0010142  0.0010142  0.0010142  0.0010142  0.0010142  0.0010142
 0.0010142  0.0010142  0.0020284  0.0020284  0.0020284  0.0020284
 0.0020284  0.0020284  0.0020284  0.0020284  0.0020284  0.0020284
 0.0020284  0.0030426  0.0030426  0.0030426  0.0030426  0.0030426
 0.0030426  0.0030426  0.0030426  0.0040568  0.0040568  0.0040568
 0.00608519 0.00608519 0.00709939 0.00709939 0.00709939 0.00709939
 0.00811359 0.00912779 0.00912779 0.00912779 0.00912779 0.01014199
 0.01014199 0.01014199 0.01115619 0.01115619 0.01217039 0.01217039
 0.01217039 0.01217039 0.01318458 0.01419878 0.01419878 0.01419878
 0.01419878 0.01521298 0.01521298 0.01521298 0.01521298 0.01521298
 0.01521298 0.01622718 0.01622718 0.01724138 0.01724138 0.01825558
 0.01825558 0.01825558 0.01926978 0.01926978 0.02028398 0.02028398
 0.02129817 0.02129817 0.02231237 0.02231237 0.02332657 0.02332657
 0.02636917 0.02636917 0.02738337 0.02738337 0.02941176 0.02941176
 0.03042596 0.03042596 0.03042596 0.03042596 0.03144016 0.03144016
 0.03245436 0.03245436 0.03651116 0.03651116 0.03752535 0.03752535
 0.03853955 0.03853955 0.03853955 0.03955375 0.03955375 0.04259635
 0.04259635 0.04563895 0.04563895 0.04665314 0.04665314 0.04766734
 0.04766734 0.05172414 0.05172414 0.05679513 0.05679513 0.05983773
 0.05983773 0.06490872 0.06490872 0.06896552 0.06896552 0.07200811
 0.07200811 0.07707911 0.07707911 0.07910751 0.07910751 0.0851927
 0.0851927  0.0872211  0.0872211  0.09229209 0.09229209 0.09330629
 0.09330629 0.09533469 0.10547667 0.10547667 0.10750507 0.10750507
 0.12778905 0.12981744 0.15010142 0.15010142 0.17241379 0.17444219
 0.21703854 0.21906694 0.22109533 0.22312373 0.23833671 0.23833671
 0.29716024 0.29918864 0.29918864 0.31643002 0.31643002 0.3296146
 0.331643   0.3346856  0.336714   0.34178499 0.34381339 0.35496957
 0.35902637 0.36916836 0.37119675 0.38945233 0.39148073 0.40162272
 0.40365112 0.4168357  0.4188641  0.43204868 0.43407708 0.45233266
 0.45436105 0.45638945 0.45943205 0.46653144 0.47261663 0.47667343
 0.47870183 0.48884381 0.49087221 0.5        0.5020284  0.52231237
 0.52434077 0.52941176 0.53144016 0.54259635 0.54462475 0.55172414
 0.55375254 0.55983773 0.56186613 0.56997972 0.57403651 0.5821501
 0.5841785  0.58823529 0.59026369 0.59127789 0.59533469 0.60040568
 0.60243408 0.60750507 0.60953347 0.61359026 0.61561866 0.61663286
 0.61866126 0.62271805 0.62474645 0.63184584 0.63387424 0.63488844
 0.63691684 0.63995943 0.64198783 0.64401623 0.64503043 0.64705882
 0.65111562 0.65415822 0.65720081 0.65922921 0.6643002  0.6663286
 0.6693712  0.67139959 0.67545639 0.67748479 0.68052738 0.68255578
 0.68458418 0.68864097 0.69776876 0.70081136 0.70283976 0.71095335
 0.71703854 0.72109533 0.72413793 0.73022312 0.73123732 0.73427992
 0.73529412 0.73935091 0.74239351 0.74340771 0.74543611 0.7464503
 0.7484787  0.7505071  0.7525355  0.75456389 0.75862069 0.76470588
 0.76673428 0.77079108 0.77383367 0.77586207 0.77991886 0.78296146
 0.79006085 0.79208925 0.80020284 0.80223124 0.80425963 0.80831643
 0.81034483 0.81237323 0.81541582 0.81643002 0.81947262 0.82048682
 0.82352941 0.82454361 0.82758621 0.8296146  0.8306288  0.8336714
 0.83772819 0.84077079 0.84279919 0.84584178 0.84989858 0.85294118
 0.85496957 0.85699797 0.85801217 0.86105477 0.86308316 0.86511156
 0.87018256 0.87221095 0.87525355 0.87626775 0.88235294 0.88843813
 0.88945233 0.89249493 0.89452333 0.89858012 0.90060852 0.90162272
 0.90466531 0.90567951 0.90973631 0.91176471 0.9137931  0.9158215
 0.92089249 0.92292089 0.92596349 0.93103448 0.93509128 0.93610548
 0.94016227 0.94117647 0.94320487 0.94421907 0.94624746 0.94726166
 0.95131846 0.95740365 0.95841785 0.96146045 0.96247465 0.96450304
 0.96653144 0.96957404 0.97464503 0.97768763 0.98174442 0.98275862
 0.98580122 0.98782961 1.        ]
tpr: [0.         0.0010142  0.01217039 0.01419878 0.01521298 0.01724138
 0.02028398 0.02231237 0.02535497 0.03144016 0.03245436 0.03549696
 0.03752535 0.03853955 0.04259635 0.04462475 0.04665314 0.04766734
 0.05273834 0.05882353 0.06085193 0.06186613 0.06592292 0.06693712
 0.07099391 0.07403651 0.07505071 0.07910751 0.0811359  0.0821501
 0.0841785  0.09026369 0.09432049 0.09533469 0.09736308 0.10141988
 0.10344828 0.10446247 0.10953347 0.11561866 0.11764706 0.12170385
 0.12677485 0.13488844 0.13691684 0.13995943 0.14097363 0.14807302
 0.15010142 0.15314402 0.15922921 0.16125761 0.1643002  0.1653144
 0.17139959 0.17342799 0.18154158 0.18356998 0.18762677 0.18864097
 0.19371197 0.19776876 0.20385396 0.20791075 0.20892495 0.21196755
 0.22210953 0.23123732 0.23529412 0.23630832 0.24442191 0.2505071
 0.26064909 0.26470588 0.27079108 0.27687627 0.27991886 0.28194726
 0.28498986 0.28904665 0.29006085 0.29208925 0.29310345 0.29716024
 0.30527383 0.30933063 0.31237323 0.31338742 0.31541582 0.31845842
 0.32657201 0.32758621 0.3296146  0.3336714  0.33772819 0.34381339
 0.34787018 0.35091278 0.35294118 0.35496957 0.35699797 0.36409736
 0.36713996 0.36916836 0.37119675 0.37423935 0.37525355 0.37728195
 0.37931034 0.38133874 0.38235294 0.38438134 0.38742394 0.38945233
 0.39148073 0.39350913 0.39553753 0.39756592 0.40060852 0.40669371
 0.40872211 0.41075051 0.4137931  0.4178499  0.42190669 0.42393509
 0.42494929 0.42900609 0.43002028 0.43306288 0.43711968 0.43914807
 0.44117647 0.44219067 0.44421907 0.44726166 0.45131846 0.45334686
 0.45638945 0.45841785 0.46247465 0.46348884 0.46754564 0.47160243
 0.47363083 0.47971602 0.48174442 0.48275862 0.48681542 0.48782961
 0.48985801 0.49087221 0.49290061 0.4979716  0.5        0.5020284
 0.5040568  0.50507099 0.50912779 0.52129817 0.52332657 0.52434077
 0.52839757 0.53752535 0.53955375 0.54158215 0.54361055 0.54665314
 0.54868154 0.55070994 0.55375254 0.55780933 0.55882353 0.56085193
 0.56795132 0.56997972 0.57099391 0.57302231 0.57403651 0.57606491
 0.57707911 0.57910751 0.5811359  0.5831643  0.5851927  0.58924949
 0.59229209 0.59432049 0.59533469 0.60344828 0.60547667 0.61054767
 0.61663286 0.61866126 0.62170385 0.62778905 0.63286004 0.63590264
 0.63691684 0.64300203 0.64604462 0.64908722 0.65212982 0.65415822
 0.65720081 0.65922921 0.66024341 0.663286   0.6653144  0.6653144
 0.6663286  0.6703854  0.67342799 0.67647059 0.67849899 0.68356998
 0.68762677 0.69269777 0.69472617 0.69878296 0.70081136 0.70283976
 0.70892495 0.71501014 0.71805274 0.71906694 0.72109533 0.72718053
 0.73022312 0.73225152 0.73427992 0.73833671 0.74442191 0.7484787
 0.7505071  0.75456389 0.75557809 0.76166329 0.77079108 0.77484787
 0.77890467 0.78093306 0.78093306 0.78296146 0.78397566 0.78803245
 0.79006085 0.79208925 0.79716024 0.80527383 0.81135903 0.81338742
 0.81541582 0.81541582 0.81947262 0.82150101 0.8306288  0.8326572
 0.83874239 0.84077079 0.84381339 0.84482759 0.84888438 0.85192698
 0.85192698 0.85294118 0.85395538 0.85801217 0.86004057 0.86612576
 0.86713996 0.86713996 0.87525355 0.87728195 0.88742394 0.88742394
 0.88843813 0.89046653 0.89046653 0.89148073 0.89148073 0.89249493
 0.89655172 0.90162272 0.90162272 0.90263692 0.90466531 0.90669371
 0.91075051 0.91075051 0.9137931  0.9158215  0.9198783  0.92190669
 0.92494929 0.92494929 0.92799189 0.92799189 0.93407708 0.93407708
 0.93610548 0.93914807 0.94016227 0.94117647 0.94117647 0.94320487
 0.94320487 0.94523327 0.94523327 0.94624746 0.94624746 0.94726166
 0.94726166 0.94827586 0.94827586 0.94929006 0.94929006 0.95030426
 0.95030426 0.95537525 0.95740365 0.96247465 0.96247465 0.96450304
 0.96450304 0.96551724 0.96551724 0.96653144 0.96653144 0.96754564
 0.96754564 0.96957404 0.97160243 0.97160243 0.97261663 0.97261663
 0.97363083 0.97363083 0.97565923 0.97565923 0.97870183 0.97870183
 0.97971602 0.97971602 0.98073022 0.98073022 0.98174442 0.98174442
 0.98377282 0.98377282 0.98478702 0.98478702 0.98580122 0.98580122
 0.98782961 0.98782961 0.98884381 0.98884381 0.98985801 0.98985801
 0.99087221 0.99087221 0.99188641 0.99188641 0.99290061 0.99290061
 0.99391481 0.99391481 0.99391481 0.99492901 0.99492901 0.9959432
 0.9959432  0.9959432  0.9959432  0.9969574  0.9969574  0.9969574
 0.9969574  0.9969574  0.9969574  0.9969574  0.9969574  0.9979716
 0.9979716  0.9979716  0.9989858  0.9989858  1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.        ]
thresholds: [        inf  4.6523438   4.4804688   4.4765625   4.4726562   4.46875
  4.4570312   4.453125    4.4492188   4.4375      4.4296875   4.4257812
  4.421875    4.4179688   4.4101562   4.3984375   4.3945312   4.3867188
  4.3828125   4.3789062   4.375       4.3710938   4.3671875   4.3632812
  4.359375    4.3554688   4.3515625   4.3476562   4.34375     4.3398438
  4.3359375   4.3320312   4.328125    4.3203125   4.3164062   4.3125
  4.3085938   4.3046875   4.3007812   4.296875    4.2929688   4.2890625
  4.2851562   4.2773438   4.2734375   4.2695312   4.265625    4.2617188
  4.2578125   4.2539062   4.25        4.2460938   4.2421875   4.2382812
  4.234375    4.2265625   4.2226562   4.21875     4.2148438   4.2070312
  4.203125    4.1992188   4.1953125   4.1875      4.1835938   4.1796875
  4.171875    4.1601562   4.15625     4.1523438   4.1445312   4.1328125
  4.125       4.1210938   4.1132812   4.1015625   4.0976562   4.09375
  4.0898438   4.0820312   4.078125    4.0742188   4.0703125   4.0625
  4.0546875   4.046875    4.0429688   4.0390625   4.0351562   4.03125
  4.0234375   4.0195312   4.015625    4.0117188   4.0039062   4.
  3.9960938   3.9882812   3.984375    3.9746094   3.9726562   3.9492188
  3.9472656   3.9453125   3.9414062   3.9375      3.9355469   3.9335938
  3.9238281   3.9199219   3.9179688   3.9160156   3.9140625   3.9121094
  3.9042969   3.9003906   3.8886719   3.8867188   3.8847656   3.8652344
  3.8613281   3.859375    3.8496094   3.8339844   3.8183594   3.8125
  3.8105469   3.8046875   3.7988281   3.7929688   3.7851562   3.78125
  3.7792969   3.7773438   3.7753906   3.7734375   3.7714844   3.7617188
  3.75        3.7421875   3.7382812   3.734375    3.7304688   3.7148438
  3.7128906   3.703125    3.6992188   3.6953125   3.6914062   3.6894531
  3.6835938   3.6796875   3.6777344   3.6660156   3.6640625   3.6582031
  3.6542969   3.6523438   3.6484375   3.6035156   3.5996094   3.5976562
  3.5957031   3.5683594   3.5644531   3.5488281   3.5449219   3.5332031
  3.53125     3.5253906   3.5234375   3.5195312   3.5175781   3.5117188
  3.4902344   3.4882812   3.4863281   3.4804688   3.4785156   3.4765625
  3.4746094   3.4726562   3.4667969   3.4648438   3.4609375   3.4511719
  3.4335938   3.4316406   3.4277344   3.40625     3.4003906   3.3964844
  3.3730469   3.3710938   3.3652344   3.3417969   3.328125    3.3261719
  3.3242188   3.3183594   3.3125      3.3105469   3.2988281   3.2929688
  3.2910156   3.2890625   3.2871094   3.2851562   3.2792969   3.2773438
  3.2734375   3.2695312   3.2558594   3.25        3.2460938   3.2265625
  3.2226562   3.203125    3.1992188   3.1875      3.1835938   3.1738281
  3.1601562   3.1308594   3.1269531   3.1230469   3.1191406   3.1152344
  3.0996094   3.09375     3.0859375   3.0722656   3.0371094   3.03125
  3.0214844   3.0175781   3.015625    3.0097656   2.9472656   2.9394531
  2.9121094   2.9101562   2.9082031   2.8964844   2.8945312   2.8769531
  2.8710938   2.8613281   2.8300781   2.8105469   2.7773438   2.7753906
  2.7695312   2.7675781   2.7460938   2.7441406   2.6738281   2.6582031
  2.6308594   2.6289062   2.6210938   2.6191406   2.6152344   2.6035156
  2.5976562   2.5898438   2.5839844   2.5605469   2.546875    2.5253906
  2.5214844   2.5195312   2.4707031   2.453125    2.3925781   2.375
  2.3710938   2.3671875   2.3476562   2.34375     2.3417969   2.3359375
  2.3261719   2.2890625   2.2792969   2.2773438   2.2714844   2.2578125
  2.2324219   2.2285156   2.2148438   2.2109375   2.1953125   2.1933594
  2.1386719   2.1367188   2.1054688   2.0996094   2.0136719   1.9765625
  1.9716797   1.9326172   1.9287109   1.9179688   1.8974609   1.8886719
  1.8818359   1.8544922   1.8525391   1.8476562   1.8427734   1.8339844
  1.8144531   1.8037109   1.7919922   1.7861328   1.7626953   1.7255859
  1.6884766   1.6142578   1.59375     1.5351562   1.5302734   1.4970703
  1.4873047   1.4667969   1.4472656   1.4365234   1.4267578   1.4013672
  1.3896484   1.34375     1.3212891   1.3164062   1.3115234   1.2441406
  1.2294922   1.2070312   1.1953125   1.1943359   1.1875      1.1582031
  1.1572266   1.0664062   1.0556641   0.97021484  0.9638672   0.9291992
  0.92285156  0.86816406  0.8520508   0.81591797  0.81347656  0.7480469
  0.7294922   0.7104492   0.7080078   0.67871094  0.640625    0.6064453
  0.5957031   0.5908203   0.59033203  0.55859375  0.54345703  0.5258789
  0.50439453  0.48266602  0.3540039   0.34838867  0.33374023  0.3190918
  0.16027832  0.1550293  -0.02861023 -0.02906799 -0.17382812 -0.18261719
 -0.5991211  -0.60498047 -0.61816406 -0.6357422  -0.75146484 -0.75683594
 -1.3652344  -1.375      -1.3798828  -1.5820312  -1.5830078  -1.6865234
 -1.6992188  -1.7265625  -1.7314453  -1.7724609  -1.7832031  -1.8378906
 -1.8642578  -1.9228516  -1.9257812  -2.0625     -2.0664062  -2.1464844
 -2.1523438  -2.3203125  -2.328125   -2.4589844  -2.4902344  -2.6503906
 -2.6777344  -2.6914062  -2.6953125  -2.7734375  -2.7871094  -2.8007812
 -2.8027344  -2.8652344  -2.8671875  -2.9628906  -2.9765625  -3.0820312
 -3.0839844  -3.1035156  -3.1152344  -3.1660156  -3.1679688  -3.203125
 -3.2050781  -3.2441406  -3.2519531  -3.3046875  -3.3183594  -3.3515625
 -3.3535156  -3.3789062  -3.3828125  -3.3867188  -3.3984375  -3.4160156
 -3.4199219  -3.4667969  -3.4765625  -3.5        -3.5039062  -3.5058594
 -3.5078125  -3.53125    -3.5351562  -3.5703125  -3.5722656  -3.5820312
 -3.5878906  -3.5917969  -3.6054688  -3.6074219  -3.6113281  -3.6230469
 -3.6269531  -3.6289062  -3.6464844  -3.6523438  -3.6660156  -3.6679688
 -3.6738281  -3.6816406  -3.6953125  -3.6972656  -3.7128906  -3.7207031
 -3.7246094  -3.7265625  -3.7617188  -3.7675781  -3.7714844  -3.8027344
 -3.8085938  -3.8222656  -3.8242188  -3.8457031  -3.8476562  -3.8496094
 -3.8535156  -3.8574219  -3.859375   -3.8613281  -3.8652344  -3.8671875
 -3.8710938  -3.8789062  -3.8828125  -3.8867188  -3.890625   -3.9140625
 -3.9179688  -3.9316406  -3.9355469  -3.9394531  -3.9453125  -3.9472656
 -3.9628906  -3.9667969  -3.9863281  -3.9882812  -3.9980469  -4.
 -4.0039062  -4.0117188  -4.015625   -4.0195312  -4.0234375  -4.0273438
 -4.03125    -4.0351562  -4.0390625  -4.0429688  -4.046875   -4.0507812
 -4.0546875  -4.0664062  -4.0703125  -4.0820312  -4.0859375  -4.0898438
 -4.0976562  -4.1015625  -4.1054688  -4.109375   -4.1132812  -4.1210938
 -4.1328125  -4.1367188  -4.140625   -4.1484375  -4.15625    -4.1640625
 -4.1679688  -4.171875   -4.1757812  -4.1796875  -4.1914062  -4.1953125
 -4.1992188  -4.203125   -4.2070312  -4.2109375  -4.21875    -4.2226562
 -4.2265625  -4.2304688  -4.234375   -4.2382812  -4.2421875  -4.2460938
 -4.2539062  -4.2578125  -4.2617188  -4.265625   -4.2695312  -4.2734375
 -4.28125    -4.2929688  -4.296875   -4.3007812  -4.3046875  -4.3125
 -4.3359375  -4.3398438  -4.359375   -4.3632812  -4.375      -4.3789062
 -4.3945312  -4.4101562  -4.5820312 ]
