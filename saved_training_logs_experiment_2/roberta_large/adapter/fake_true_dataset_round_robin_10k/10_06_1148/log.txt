log_loss_steps: 208
eval_steps: 512
check_degradation: 0
----------------Epoch 1/1----------------
Epoch 1/1, Loss after 192 samples: 0.7300
Epoch 1/1, Loss after 400 samples: 0.7164
Mean accuracy: 0.4995, std: 0.0180, lower bound: 0.4640, upper bound: 0.5347 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 496 samples: 0.5000 with eval loss: 0.6662
Best model with eval loss 0.666205919803457 and eval accuracy 0.5 with 496 samples seen is saved
Epoch 1/1, Loss after 608 samples: 0.6656
Epoch 1/1, Loss after 816 samples: 0.6477
Mean accuracy: 0.5076, std: 0.0182, lower bound: 0.4720, upper bound: 0.5413 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 1008 samples: 0.5080 with eval loss: 0.6372
Best model with eval loss 0.6371684201220249 and eval accuracy 0.508 with 1008 samples seen is saved
Epoch 1/1, Loss after 1024 samples: 0.6718
Epoch 1/1, Loss after 1232 samples: 0.5692
Epoch 1/1, Loss after 1440 samples: 0.5295
Mean accuracy: 0.8191, std: 0.0141, lower bound: 0.7907, upper bound: 0.8454 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 1520 samples: 0.8187 with eval loss: 0.4678
Best model with eval loss 0.4678054096850943 and eval accuracy 0.8186666666666667 with 1520 samples seen is saved
Epoch 1/1, Loss after 1648 samples: 0.6136
Epoch 1/1, Loss after 1856 samples: 0.5784
Mean accuracy: 0.7763, std: 0.0151, lower bound: 0.7467, upper bound: 0.8053 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 2032 samples: 0.7760 with eval loss: 0.4582
Best model with eval loss 0.4581571031124034 and eval accuracy 0.776 with 2032 samples seen is saved
Epoch 1/1, Loss after 2064 samples: 0.4886
Epoch 1/1, Loss after 2272 samples: 0.4918
Epoch 1/1, Loss after 2480 samples: 0.4036
Mean accuracy: 0.8500, std: 0.0129, lower bound: 0.8240, upper bound: 0.8747 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 2544 samples: 0.8507 with eval loss: 0.3776
Best model with eval loss 0.37761687788557496 and eval accuracy 0.8506666666666667 with 2544 samples seen is saved
Epoch 1/1, Loss after 2688 samples: 0.3730
Epoch 1/1, Loss after 2896 samples: 0.3530
Mean accuracy: 0.8615, std: 0.0127, lower bound: 0.8360, upper bound: 0.8867 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 3056 samples: 0.8613 with eval loss: 0.3201
Best model with eval loss 0.32010416971876265 and eval accuracy 0.8613333333333333 with 3056 samples seen is saved
Epoch 1/1, Loss after 3104 samples: 0.4336
Epoch 1/1, Loss after 3312 samples: 0.3033
Epoch 1/1, Loss after 3520 samples: 0.3369
Mean accuracy: 0.8559, std: 0.0129, lower bound: 0.8293, upper bound: 0.8800 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 3568 samples: 0.8560 with eval loss: 0.3830
Epoch 1/1, Loss after 3728 samples: 0.3222
Epoch 1/1, Loss after 3936 samples: 0.3322
Mean accuracy: 0.8319, std: 0.0136, lower bound: 0.8040, upper bound: 0.8574 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 4080 samples: 0.8320 with eval loss: 0.3958
Epoch 1/1, Loss after 4144 samples: 0.2957
Epoch 1/1, Loss after 4352 samples: 0.2382
Epoch 1/1, Loss after 4560 samples: 0.2400
Mean accuracy: 0.8986, std: 0.0106, lower bound: 0.8773, upper bound: 0.9187 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 4592 samples: 0.8987 with eval loss: 0.2205
Best model with eval loss 0.22046660045359998 and eval accuracy 0.8986666666666666 with 4592 samples seen is saved
Epoch 1/1, Loss after 4768 samples: 0.2609
Epoch 1/1, Loss after 4976 samples: 0.3060
Mean accuracy: 0.9147, std: 0.0101, lower bound: 0.8933, upper bound: 0.9347 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 5104 samples: 0.9147 with eval loss: 0.2051
Best model with eval loss 0.2051088423170942 and eval accuracy 0.9146666666666666 with 5104 samples seen is saved
Epoch 1/1, Loss after 5184 samples: 0.3490
Epoch 1/1, Loss after 5392 samples: 0.2906
Epoch 1/1, Loss after 5600 samples: 0.2348
Mean accuracy: 0.8522, std: 0.0134, lower bound: 0.8266, upper bound: 0.8787 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 5616 samples: 0.8520 with eval loss: 0.3519
Epoch 1/1, Loss after 5808 samples: 0.2698
Epoch 1/1, Loss after 6016 samples: 0.1991
Mean accuracy: 0.8926, std: 0.0109, lower bound: 0.8707, upper bound: 0.9120 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 6128 samples: 0.8933 with eval loss: 0.2668
Epoch 1/1, Loss after 6224 samples: 0.2511
Epoch 1/1, Loss after 6432 samples: 0.2396
Epoch 1/1, Loss after 6640 samples: 0.2089
Mean accuracy: 0.9024, std: 0.0112, lower bound: 0.8813, upper bound: 0.9240 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 6640 samples: 0.9027 with eval loss: 0.2407
Epoch 1/1, Loss after 6848 samples: 0.2767
Epoch 1/1, Loss after 7056 samples: 0.2296
Mean accuracy: 0.8812, std: 0.0115, lower bound: 0.8600, upper bound: 0.9040 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 7152 samples: 0.8813 with eval loss: 0.3091
Epoch 1/1, Loss after 7264 samples: 0.2870
Epoch 1/1, Loss after 7472 samples: 0.2581
Training signal is False, stopping training
Training finished
Best model: {'eval_acc': 0.9146666666666666, 'nb_samples': 5104, 'eval_loss': 0.2051088423170942}
Training loss logs: [{'samples': 192, 'loss': 0.7300332876352164}, {'samples': 400, 'loss': 0.7163561307466947}, {'samples': 608, 'loss': 0.6656388502854568}, {'samples': 816, 'loss': 0.6476767613337591}, {'samples': 1024, 'loss': 0.6718338452852689}, {'samples': 1232, 'loss': 0.5691875311044546}, {'samples': 1440, 'loss': 0.5295224556556115}, {'samples': 1648, 'loss': 0.6135557156342727}, {'samples': 1856, 'loss': 0.5783566328195425}, {'samples': 2064, 'loss': 0.4885605298555814}, {'samples': 2272, 'loss': 0.49179731882535493}, {'samples': 2480, 'loss': 0.4035700261592865}, {'samples': 2688, 'loss': 0.37303245526093703}, {'samples': 2896, 'loss': 0.35298455907748294}, {'samples': 3104, 'loss': 0.4335910907158485}, {'samples': 3312, 'loss': 0.30330544939407933}, {'samples': 3520, 'loss': 0.3369493003074939}, {'samples': 3728, 'loss': 0.32216855654349696}, {'samples': 3936, 'loss': 0.33222703291819644}, {'samples': 4144, 'loss': 0.29570995844327486}, {'samples': 4352, 'loss': 0.23823808362850776}, {'samples': 4560, 'loss': 0.2400086080798736}, {'samples': 4768, 'loss': 0.2608694296616774}, {'samples': 4976, 'loss': 0.306021864597614}, {'samples': 5184, 'loss': 0.3489962907937857}, {'samples': 5392, 'loss': 0.2905721412255214}, {'samples': 5600, 'loss': 0.2347557355578129}, {'samples': 5808, 'loss': 0.26979930584247297}, {'samples': 6016, 'loss': 0.1991282798922979}, {'samples': 6224, 'loss': 0.25113807962490964}, {'samples': 6432, 'loss': 0.23958230649049467}, {'samples': 6640, 'loss': 0.2088526452963169}, {'samples': 6848, 'loss': 0.27672941180375904}, {'samples': 7056, 'loss': 0.22957622832976854}, {'samples': 7264, 'loss': 0.2869633906162702}, {'samples': 7472, 'loss': 0.2581112545270186}]
Evaluation accuracy logs: [{'samples': 496, 'accuracy': 0.49945066666666665, 'std': 0.01797609553453326, 'lower_bound': 0.464, 'upper_bound': 0.5346666666666666}, {'samples': 1008, 'accuracy': 0.5075533333333333, 'std': 0.01823816633083247, 'lower_bound': 0.47196666666666665, 'upper_bound': 0.5413333333333333}, {'samples': 1520, 'accuracy': 0.8190826666666667, 'std': 0.014106556773358975, 'lower_bound': 0.7906666666666666, 'upper_bound': 0.8453666666666667}, {'samples': 2032, 'accuracy': 0.7762680000000001, 'std': 0.015078850471954269, 'lower_bound': 0.7466666666666667, 'upper_bound': 0.8053333333333333}, {'samples': 2544, 'accuracy': 0.8499933333333334, 'std': 0.012943087387138782, 'lower_bound': 0.824, 'upper_bound': 0.8746666666666667}, {'samples': 3056, 'accuracy': 0.86152, 'std': 0.012730673369639347, 'lower_bound': 0.836, 'upper_bound': 0.8866666666666667}, {'samples': 3568, 'accuracy': 0.8559053333333334, 'std': 0.012928793636255813, 'lower_bound': 0.8293333333333334, 'upper_bound': 0.88}, {'samples': 4080, 'accuracy': 0.8319493333333332, 'std': 0.013586303953287339, 'lower_bound': 0.804, 'upper_bound': 0.8573666666666666}, {'samples': 4592, 'accuracy': 0.8985519999999999, 'std': 0.010640184125390976, 'lower_bound': 0.8773333333333333, 'upper_bound': 0.9186666666666666}, {'samples': 5104, 'accuracy': 0.9146946666666667, 'std': 0.010056423839737691, 'lower_bound': 0.8933333333333333, 'upper_bound': 0.9346666666666666}, {'samples': 5616, 'accuracy': 0.8522053333333334, 'std': 0.013437875758053757, 'lower_bound': 0.8266333333333333, 'upper_bound': 0.8786666666666667}, {'samples': 6128, 'accuracy': 0.8926000000000001, 'std': 0.010946293132075955, 'lower_bound': 0.8706666666666667, 'upper_bound': 0.9120333333333334}, {'samples': 6640, 'accuracy': 0.9024266666666667, 'std': 0.011188851594332633, 'lower_bound': 0.8813, 'upper_bound': 0.924}, {'samples': 7152, 'accuracy': 0.881152, 'std': 0.01147236323615245, 'lower_bound': 0.86, 'upper_bound': 0.904}]
Evaluating the best model on the test set of dataset ./fake_true_datasets/fake_true_dataset_round_robin_10k...
Test metrics:
accuracy: 0.9059466666666667
precision: 0.8695692711052705
recall: 0.9457425908879057
f1_score: 0.9048987027915218
fp_rate: 0.1310560745823917
tp_rate: 0.9457425908879057
std_accuracy: 0.03354886055359324
std_precision: 0.05428322445843759
std_recall: 0.03934185190976528
std_f1_score: 0.03655162081382273
std_fp_rate: 0.05403219290175137
std_tp_rate: 0.03934185190976528
TP: 34.111
TN: 33.835
FP: 5.111
FN: 1.943
roc_auc: 0.9572649572649572
fpr: [0.         0.         0.         0.         0.         0.
 0.         0.02564103 0.1025641  0.1025641  0.12820513 0.12820513
 0.15384615 0.20512821 0.23076923 0.28205128 0.35897436 0.43589744
 0.46153846 0.51282051 0.58974359 0.64102564 0.64102564 0.71794872
 0.74358974 0.82051282 0.92307692 1.        ]
tpr: [0.         0.02777778 0.13888889 0.19444444 0.55555556 0.61111111
 0.75       0.75       0.75       0.88888889 0.88888889 0.97222222
 0.97222222 0.97222222 0.97222222 0.97222222 0.97222222 0.97222222
 0.97222222 0.97222222 0.97222222 0.97222222 1.         1.
 1.         1.         1.         1.        ]
thresholds: [        inf  3.828125    3.3710938   3.2910156   2.4003906   2.3808594
  2.1660156   1.7216797   1.7050781   1.4238281   1.3613281   0.18664551
  0.07775879  0.07281494 -0.17297363 -0.1763916  -0.8354492  -0.94970703
 -0.9946289  -0.99609375 -1.03125    -1.0830078  -1.1210938  -1.2021484
 -1.4052734  -1.4960938  -1.6425781  -1.7041016 ]
