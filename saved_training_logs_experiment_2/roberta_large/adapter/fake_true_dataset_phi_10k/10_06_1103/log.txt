log_loss_steps: 208
eval_steps: 512
check_degradation: 0
----------------Epoch 1/1----------------
Epoch 1/1, Loss after 192 samples: 0.7043
Epoch 1/1, Loss after 400 samples: 0.7113
Mean accuracy: 0.4999, std: 0.0110, lower bound: 0.4779, upper bound: 0.5216 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 496 samples: 0.5000 with eval loss: 0.6873
Best model with eval loss 0.68726220703125 and eval accuracy 0.5 with 496 samples seen is saved
Epoch 1/1, Loss after 608 samples: 0.6931
Epoch 1/1, Loss after 816 samples: 0.6661
Mean accuracy: 0.7875, std: 0.0093, lower bound: 0.7686, upper bound: 0.8062 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 1008 samples: 0.7877 with eval loss: 0.5004
Best model with eval loss 0.5004061279296875 and eval accuracy 0.7876506024096386 with 1008 samples seen is saved
Epoch 1/1, Loss after 1024 samples: 0.6083
Epoch 1/1, Loss after 1232 samples: 0.4946
Epoch 1/1, Loss after 1440 samples: 0.3935
Mean accuracy: 0.8221, std: 0.0086, lower bound: 0.8057, upper bound: 0.8384 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 1520 samples: 0.8213 with eval loss: 0.3883
Best model with eval loss 0.38829598689079287 and eval accuracy 0.821285140562249 with 1520 samples seen is saved
Epoch 1/1, Loss after 1648 samples: 0.4283
Epoch 1/1, Loss after 1856 samples: 0.3809
Mean accuracy: 0.8680, std: 0.0075, lower bound: 0.8529, upper bound: 0.8825 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 2032 samples: 0.8680 with eval loss: 0.2917
Best model with eval loss 0.29168819785118105 and eval accuracy 0.8679718875502008 with 2032 samples seen is saved
Epoch 1/1, Loss after 2064 samples: 0.3702
Epoch 1/1, Loss after 2272 samples: 0.3060
Epoch 1/1, Loss after 2480 samples: 0.3018
Mean accuracy: 0.6627, std: 0.0108, lower bound: 0.6416, upper bound: 0.6832 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 2544 samples: 0.6627 with eval loss: 0.6979
Epoch 1/1, Loss after 2688 samples: 0.4061
Epoch 1/1, Loss after 2896 samples: 0.3066
Mean accuracy: 0.7050, std: 0.0104, lower bound: 0.6847, upper bound: 0.7259 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 3056 samples: 0.7048 with eval loss: 1.0118
Epoch 1/1, Loss after 3104 samples: 0.2409
Epoch 1/1, Loss after 3312 samples: 0.1779
Epoch 1/1, Loss after 3520 samples: 0.2948
Mean accuracy: 0.9081, std: 0.0065, lower bound: 0.8951, upper bound: 0.9207 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 3568 samples: 0.9081 with eval loss: 0.2367
Best model with eval loss 0.2367147564291954 and eval accuracy 0.9081325301204819 with 3568 samples seen is saved
Epoch 1/1, Loss after 3728 samples: 0.2270
Epoch 1/1, Loss after 3936 samples: 0.2242
Mean accuracy: 0.8903, std: 0.0069, lower bound: 0.8760, upper bound: 0.9036 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 4080 samples: 0.8906 with eval loss: 0.3038
Epoch 1/1, Loss after 4144 samples: 0.2717
Epoch 1/1, Loss after 4352 samples: 0.2039
Epoch 1/1, Loss after 4560 samples: 0.2123
Mean accuracy: 0.9088, std: 0.0062, lower bound: 0.8961, upper bound: 0.9212 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 4592 samples: 0.9086 with eval loss: 0.2232
Best model with eval loss 0.22322058379650117 and eval accuracy 0.9086345381526104 with 4592 samples seen is saved
Epoch 1/1, Loss after 4768 samples: 0.2383
Epoch 1/1, Loss after 4976 samples: 0.2108
Mean accuracy: 0.9003, std: 0.0069, lower bound: 0.8870, upper bound: 0.9142 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 5104 samples: 0.9001 with eval loss: 0.2340
Epoch 1/1, Loss after 5184 samples: 0.2555
Epoch 1/1, Loss after 5392 samples: 0.2254
Epoch 1/1, Loss after 5600 samples: 0.2065
Mean accuracy: 0.9321, std: 0.0058, lower bound: 0.9202, upper bound: 0.9433 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 5616 samples: 0.9322 with eval loss: 0.1802
Best model with eval loss 0.18019112879037857 and eval accuracy 0.9322289156626506 with 5616 samples seen is saved
Epoch 1/1, Loss after 5808 samples: 0.2505
Epoch 1/1, Loss after 6016 samples: 0.2285
Mean accuracy: 0.8644, std: 0.0077, lower bound: 0.8489, upper bound: 0.8795 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 6128 samples: 0.8645 with eval loss: 0.3176
Epoch 1/1, Loss after 6224 samples: 0.1441
Epoch 1/1, Loss after 6432 samples: 0.1433
Epoch 1/1, Loss after 6640 samples: 0.1375
Mean accuracy: 0.9127, std: 0.0063, lower bound: 0.9006, upper bound: 0.9257 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 6640 samples: 0.9127 with eval loss: 0.2608
Epoch 1/1, Loss after 6848 samples: 0.1898
Epoch 1/1, Loss after 7056 samples: 0.1597
Mean accuracy: 0.8566, std: 0.0079, lower bound: 0.8409, upper bound: 0.8720 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 7152 samples: 0.8564 with eval loss: 0.4260
Epoch 1/1, Loss after 7264 samples: 0.2385
Epoch 1/1, Loss after 7472 samples: 0.1898
Mean accuracy: 0.9033, std: 0.0067, lower bound: 0.8896, upper bound: 0.9157 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 7664 samples: 0.9036 with eval loss: 0.2396
Epoch 1/1, Loss after 7680 samples: 0.2311
Epoch 1/1, Loss after 7888 samples: 0.1409
Epoch 1/1, Loss after 8096 samples: 0.1374
Mean accuracy: 0.8693, std: 0.0074, lower bound: 0.8544, upper bound: 0.8825 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 8176 samples: 0.8695 with eval loss: 0.3740
Epoch 1/1, Loss after 8304 samples: 0.1421
Epoch 1/1, Loss after 8512 samples: 0.2037
Mean accuracy: 0.9435, std: 0.0051, lower bound: 0.9332, upper bound: 0.9538 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 8688 samples: 0.9438 with eval loss: 0.1529
Best model with eval loss 0.15292709481716155 and eval accuracy 0.9437751004016064 with 8688 samples seen is saved
Epoch 1/1, Loss after 8720 samples: 0.1819
Epoch 1/1, Loss after 8928 samples: 0.1488
Epoch 1/1, Loss after 9136 samples: 0.1560
Mean accuracy: 0.8348, std: 0.0086, lower bound: 0.8178, upper bound: 0.8509 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 9200 samples: 0.8348 with eval loss: 0.5058
Epoch 1/1, Loss after 9344 samples: 0.1369
Epoch 1/1, Loss after 9552 samples: 0.1177
Mean accuracy: 0.8628, std: 0.0079, lower bound: 0.8469, upper bound: 0.8780 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 9712 samples: 0.8630 with eval loss: 0.3958
Epoch 1/1, Loss after 9760 samples: 0.1558
Epoch 1/1, Loss after 9968 samples: 0.2395
Epoch 1/1, Loss after 10176 samples: 0.1285
Mean accuracy: 0.8437, std: 0.0082, lower bound: 0.8278, upper bound: 0.8599 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 10224 samples: 0.8439 with eval loss: 0.4326
Epoch 1/1, Loss after 10384 samples: 0.1043
Epoch 1/1, Loss after 10592 samples: 0.0712
Mean accuracy: 0.9008, std: 0.0069, lower bound: 0.8875, upper bound: 0.9142 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 10736 samples: 0.9006 with eval loss: 0.3254
Epoch 1/1, Loss after 10800 samples: 0.0963
Epoch 1/1, Loss after 11008 samples: 0.1382
Epoch 1/1, Loss after 11216 samples: 0.0800
Mean accuracy: 0.9073, std: 0.0067, lower bound: 0.8941, upper bound: 0.9202 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 11248 samples: 0.9071 with eval loss: 0.2819
Epoch 1/1, Loss after 11424 samples: 0.1325
Epoch 1/1, Loss after 11632 samples: 0.0820
Mean accuracy: 0.9113, std: 0.0063, lower bound: 0.8991, upper bound: 0.9237 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 11760 samples: 0.9111 with eval loss: 0.2566
Epoch 1/1, Loss after 11840 samples: 0.1165
Epoch 1/1, Loss after 12048 samples: 0.1395
Epoch 1/1, Loss after 12256 samples: 0.1084
Mean accuracy: 0.8707, std: 0.0076, lower bound: 0.8554, upper bound: 0.8850 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 12272 samples: 0.8710 with eval loss: 0.3945
Epoch 1/1, Loss after 12464 samples: 0.0707
Epoch 1/1, Loss after 12672 samples: 0.0793
Mean accuracy: 0.8562, std: 0.0080, lower bound: 0.8404, upper bound: 0.8715 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 12784 samples: 0.8559 with eval loss: 0.4777
Epoch 1/1, Loss after 12880 samples: 0.1734
Epoch 1/1, Loss after 13088 samples: 0.1446
Epoch 1/1, Loss after 13296 samples: 0.1259
Mean accuracy: 0.8909, std: 0.0070, lower bound: 0.8770, upper bound: 0.9051 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 13296 samples: 0.8906 with eval loss: 0.3282
Epoch 1/1, Loss after 13504 samples: 0.1622
Epoch 1/1, Loss after 13712 samples: 0.0994
Mean accuracy: 0.8707, std: 0.0077, lower bound: 0.8564, upper bound: 0.8860 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 13808 samples: 0.8710 with eval loss: 0.3762
Epoch 1/1, Loss after 13920 samples: 0.0761
Epoch 1/1, Loss after 14128 samples: 0.0595
Mean accuracy: 0.9279, std: 0.0057, lower bound: 0.9162, upper bound: 0.9388 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 14320 samples: 0.9277 with eval loss: 0.2113
Epoch 1/1, Loss after 14336 samples: 0.1032
Epoch 1/1, Loss after 14544 samples: 0.0956
Epoch 1/1, Loss after 14752 samples: 0.1526
Mean accuracy: 0.9147, std: 0.0061, lower bound: 0.9026, upper bound: 0.9262 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 14832 samples: 0.9147 with eval loss: 0.2542
Epoch 1/1, Loss after 14960 samples: 0.1160
Epoch 1/1, Loss after 15168 samples: 0.0814
Mean accuracy: 0.9022, std: 0.0068, lower bound: 0.8881, upper bound: 0.9147 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 15344 samples: 0.9021 with eval loss: 0.2819
Epoch 1/1, Loss after 15376 samples: 0.0584
Epoch 1/1, Loss after 15584 samples: 0.0945
Epoch 1/1, Loss after 15792 samples: 0.1446
Mean accuracy: 0.8900, std: 0.0071, lower bound: 0.8765, upper bound: 0.9026 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 15856 samples: 0.8901 with eval loss: 0.3296
Training signal is False, stopping training
Training finished
Best model: {'eval_acc': 0.9437751004016064, 'nb_samples': 8688, 'eval_loss': 0.15292709481716155}
Training loss logs: [{'samples': 192, 'loss': 0.7042987530048077}, {'samples': 400, 'loss': 0.7112943209134616}, {'samples': 608, 'loss': 0.6930976280799279}, {'samples': 816, 'loss': 0.6661224365234375}, {'samples': 1024, 'loss': 0.6083379892202524}, {'samples': 1232, 'loss': 0.4946242112379808}, {'samples': 1440, 'loss': 0.39351035081423247}, {'samples': 1648, 'loss': 0.42831249420459455}, {'samples': 1856, 'loss': 0.3808531348521893}, {'samples': 2064, 'loss': 0.3702079516190749}, {'samples': 2272, 'loss': 0.30598526161450607}, {'samples': 2480, 'loss': 0.3018136746608294}, {'samples': 2688, 'loss': 0.4061056490127857}, {'samples': 2896, 'loss': 0.30655962228775024}, {'samples': 3104, 'loss': 0.24086348311259195}, {'samples': 3312, 'loss': 0.1778843763929147}, {'samples': 3520, 'loss': 0.2948024318768428}, {'samples': 3728, 'loss': 0.2269740505860402}, {'samples': 3936, 'loss': 0.22422869503498077}, {'samples': 4144, 'loss': 0.27168996746723467}, {'samples': 4352, 'loss': 0.20386171914063966}, {'samples': 4560, 'loss': 0.2122984270636852}, {'samples': 4768, 'loss': 0.23831354540127975}, {'samples': 4976, 'loss': 0.21078393837580314}, {'samples': 5184, 'loss': 0.25551295509705174}, {'samples': 5392, 'loss': 0.225362849350159}, {'samples': 5600, 'loss': 0.20651507377624512}, {'samples': 5808, 'loss': 0.25054976458732897}, {'samples': 6016, 'loss': 0.22854504046531823}, {'samples': 6224, 'loss': 0.14406461440599883}, {'samples': 6432, 'loss': 0.14332371262403634}, {'samples': 6640, 'loss': 0.13748476482354677}, {'samples': 6848, 'loss': 0.18980706139252737}, {'samples': 7056, 'loss': 0.1596678913785861}, {'samples': 7264, 'loss': 0.23850294603751257}, {'samples': 7472, 'loss': 0.18979169371036383}, {'samples': 7680, 'loss': 0.23111075564072683}, {'samples': 7888, 'loss': 0.1409159554884984}, {'samples': 8096, 'loss': 0.13739544095901343}, {'samples': 8304, 'loss': 0.142088060768751}, {'samples': 8512, 'loss': 0.20373987291867918}, {'samples': 8720, 'loss': 0.18193174038942045}, {'samples': 8928, 'loss': 0.14880399864453536}, {'samples': 9136, 'loss': 0.1559638839501601}, {'samples': 9344, 'loss': 0.1368747055530548}, {'samples': 9552, 'loss': 0.11771883700902645}, {'samples': 9760, 'loss': 0.15578038417376006}, {'samples': 9968, 'loss': 0.23949185071083215}, {'samples': 10176, 'loss': 0.12847546430734488}, {'samples': 10384, 'loss': 0.10431669423213372}, {'samples': 10592, 'loss': 0.07122133099115811}, {'samples': 10800, 'loss': 0.09626174259644288}, {'samples': 11008, 'loss': 0.13821535672132784}, {'samples': 11216, 'loss': 0.08000337160550632}, {'samples': 11424, 'loss': 0.13254886521742895}, {'samples': 11632, 'loss': 0.08202997480447476}, {'samples': 11840, 'loss': 0.11646547798927014}, {'samples': 12048, 'loss': 0.1395415130716104}, {'samples': 12256, 'loss': 0.10835859695306191}, {'samples': 12464, 'loss': 0.07072407924211942}, {'samples': 12672, 'loss': 0.07933029360496081}, {'samples': 12880, 'loss': 0.1733743130014493}, {'samples': 13088, 'loss': 0.14460607847342125}, {'samples': 13296, 'loss': 0.1259045394567343}, {'samples': 13504, 'loss': 0.16220854623959616}, {'samples': 13712, 'loss': 0.09936962162072842}, {'samples': 13920, 'loss': 0.07613671857577103}, {'samples': 14128, 'loss': 0.059493062587884754}, {'samples': 14336, 'loss': 0.10321784076782373}, {'samples': 14544, 'loss': 0.09558482754688996}, {'samples': 14752, 'loss': 0.1526133859386811}, {'samples': 14960, 'loss': 0.11600266855496627}, {'samples': 15168, 'loss': 0.08144511970189902}, {'samples': 15376, 'loss': 0.05842762841628148}, {'samples': 15584, 'loss': 0.09448546515061305}, {'samples': 15792, 'loss': 0.14464055116360003}]
Evaluation accuracy logs: [{'samples': 496, 'accuracy': 0.49985391566265064, 'std': 0.011044043434299936, 'lower_bound': 0.4779116465863454, 'upper_bound': 0.5215988955823293}, {'samples': 1008, 'accuracy': 0.7874558232931727, 'std': 0.009299100309181355, 'lower_bound': 0.768574297188755, 'upper_bound': 0.8062248995983936}, {'samples': 1520, 'accuracy': 0.8220873493975903, 'std': 0.008564768966841056, 'lower_bound': 0.8057228915662651, 'upper_bound': 0.8383534136546185}, {'samples': 2032, 'accuracy': 0.8679769076305222, 'std': 0.007545397822376941, 'lower_bound': 0.8528990963855421, 'upper_bound': 0.8825301204819277}, {'samples': 2544, 'accuracy': 0.6627279116465863, 'std': 0.010756759862326683, 'lower_bound': 0.641566265060241, 'upper_bound': 0.6832329317269076}, {'samples': 3056, 'accuracy': 0.7049678714859438, 'std': 0.010419471292247393, 'lower_bound': 0.6847264056224899, 'upper_bound': 0.7259161646586345}, {'samples': 3568, 'accuracy': 0.9080572289156625, 'std': 0.00648044538907374, 'lower_bound': 0.8950803212851406, 'upper_bound': 0.9206827309236948}, {'samples': 4080, 'accuracy': 0.8902916666666666, 'std': 0.006932701461104293, 'lower_bound': 0.876004016064257, 'upper_bound': 0.9036144578313253}, {'samples': 4592, 'accuracy': 0.9087715863453815, 'std': 0.006242010566650683, 'lower_bound': 0.8960843373493976, 'upper_bound': 0.9211847389558233}, {'samples': 5104, 'accuracy': 0.9002530120481927, 'std': 0.006917765109141563, 'lower_bound': 0.8870481927710844, 'upper_bound': 0.9141566265060241}, {'samples': 5616, 'accuracy': 0.9321104417670684, 'std': 0.00577868386457141, 'lower_bound': 0.9201807228915663, 'upper_bound': 0.9432730923694779}, {'samples': 6128, 'accuracy': 0.8644362449799197, 'std': 0.007658496580127507, 'lower_bound': 0.8488955823293173, 'upper_bound': 0.8795306224899597}, {'samples': 6640, 'accuracy': 0.912742469879518, 'std': 0.006344863431869998, 'lower_bound': 0.9006024096385542, 'upper_bound': 0.925715361445783}, {'samples': 7152, 'accuracy': 0.8566059236947791, 'std': 0.0079182775601926, 'lower_bound': 0.8408634538152611, 'upper_bound': 0.8719879518072289}, {'samples': 7664, 'accuracy': 0.9032786144578314, 'std': 0.006650866424700356, 'lower_bound': 0.8895582329317269, 'upper_bound': 0.9156626506024096}, {'samples': 8176, 'accuracy': 0.8693388554216868, 'std': 0.0073852235784703955, 'lower_bound': 0.8544176706827309, 'upper_bound': 0.8825301204819277}, {'samples': 8688, 'accuracy': 0.9435210843373494, 'std': 0.005137579828707952, 'lower_bound': 0.9332329317269076, 'upper_bound': 0.9538152610441767}, {'samples': 9200, 'accuracy': 0.8348363453815262, 'std': 0.008575995324351193, 'lower_bound': 0.8177710843373494, 'upper_bound': 0.8509161646586345}, {'samples': 9712, 'accuracy': 0.862789156626506, 'std': 0.00788771123024919, 'lower_bound': 0.8468875502008032, 'upper_bound': 0.8780120481927711}, {'samples': 10224, 'accuracy': 0.8437289156626506, 'std': 0.008221793327369371, 'lower_bound': 0.8278112449799196, 'upper_bound': 0.8599397590361446}, {'samples': 10736, 'accuracy': 0.9008052208835342, 'std': 0.006870045196920155, 'lower_bound': 0.8875376506024096, 'upper_bound': 0.9141566265060241}, {'samples': 11248, 'accuracy': 0.9073428714859437, 'std': 0.006748464511855262, 'lower_bound': 0.8940763052208835, 'upper_bound': 0.9201932730923694}, {'samples': 11760, 'accuracy': 0.9112520080321285, 'std': 0.00631633448395544, 'lower_bound': 0.8990963855421686, 'upper_bound': 0.9237073293172691}, {'samples': 12272, 'accuracy': 0.8707469879518073, 'std': 0.007555499667967534, 'lower_bound': 0.8554216867469879, 'upper_bound': 0.8850401606425703}, {'samples': 12784, 'accuracy': 0.8562008032128515, 'std': 0.008034156621462119, 'lower_bound': 0.8403614457831325, 'upper_bound': 0.8714859437751004}, {'samples': 13296, 'accuracy': 0.8908739959839358, 'std': 0.006957211194598616, 'lower_bound': 0.8769954819277108, 'upper_bound': 0.9051204819277109}, {'samples': 13808, 'accuracy': 0.8706817269076305, 'std': 0.007702766422703784, 'lower_bound': 0.8564131526104417, 'upper_bound': 0.8860441767068273}, {'samples': 14320, 'accuracy': 0.9279116465863454, 'std': 0.005709444852171847, 'lower_bound': 0.9161646586345381, 'upper_bound': 0.9387550200803213}, {'samples': 14832, 'accuracy': 0.9147073293172692, 'std': 0.006090075242342729, 'lower_bound': 0.9026104417670683, 'upper_bound': 0.9262048192771084}, {'samples': 15344, 'accuracy': 0.902245983935743, 'std': 0.006768120695039136, 'lower_bound': 0.8880522088353414, 'upper_bound': 0.9146586345381527}, {'samples': 15856, 'accuracy': 0.890027108433735, 'std': 0.007064015509318, 'lower_bound': 0.8764934738955823, 'upper_bound': 0.9026229919678714}]
Evaluating the best model on the test set of dataset ./fake_true_datasets/fake_true_dataset_phi_10k...
Test metrics:
accuracy: 0.8948945783132528
precision: 0.8279823481300206
recall: 0.9970915621500183
f1_score: 0.9046653645166722
fp_rate: 0.20746980585700725
tp_rate: 0.9970915621500183
std_accuracy: 0.006612516799322009
std_precision: 0.010553899035325609
std_recall: 0.0016306289739537589
std_f1_score: 0.006353567384366144
std_fp_rate: 0.012476712116971999
std_tp_rate: 0.0016306289739537589
TP: 993.861
TN: 788.769
FP: 206.473
FN: 2.897
roc_auc: 0.9936618965823132
fpr: [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00100402 0.00100402 0.00100402 0.00100402 0.00100402
 0.00100402 0.00100402 0.00100402 0.00100402 0.00100402 0.00100402
 0.00100402 0.00100402 0.00100402 0.00100402 0.00100402 0.00100402
 0.00100402 0.00100402 0.00100402 0.00100402 0.00100402 0.00100402
 0.00100402 0.00100402 0.00100402 0.00100402 0.00100402 0.00100402
 0.00100402 0.00100402 0.00100402 0.00100402 0.00100402 0.00100402
 0.00100402 0.00100402 0.00100402 0.00100402 0.00100402 0.00100402
 0.00100402 0.00100402 0.00100402 0.00100402 0.00100402 0.00100402
 0.00100402 0.00100402 0.00100402 0.00100402 0.00100402 0.00100402
 0.00200803 0.00200803 0.00200803 0.00200803 0.00200803 0.00200803
 0.00200803 0.00200803 0.00200803 0.00200803 0.00200803 0.00200803
 0.00200803 0.00200803 0.00200803 0.00200803 0.00200803 0.00200803
 0.00301205 0.00301205 0.00301205 0.00301205 0.00301205 0.00301205
 0.00301205 0.00301205 0.00301205 0.00301205 0.00301205 0.00301205
 0.00301205 0.00301205 0.00301205 0.00301205 0.00301205 0.00301205
 0.00401606 0.00401606 0.00401606 0.00401606 0.00401606 0.00502008
 0.00502008 0.0060241  0.0060241  0.0060241  0.0060241  0.0060241
 0.0060241  0.00702811 0.00702811 0.00702811 0.00803213 0.00803213
 0.00903614 0.00903614 0.00903614 0.00903614 0.00903614 0.00903614
 0.00903614 0.00903614 0.01004016 0.01004016 0.01104418 0.01104418
 0.01104418 0.01104418 0.01104418 0.01104418 0.01204819 0.01204819
 0.01204819 0.01204819 0.01305221 0.01305221 0.01405622 0.01405622
 0.01506024 0.01506024 0.01606426 0.01606426 0.01807229 0.01807229
 0.01907631 0.01907631 0.01907631 0.02008032 0.02008032 0.02108434
 0.02108434 0.02309237 0.02309237 0.02409639 0.02409639 0.0251004
 0.0251004  0.02610442 0.02710843 0.02710843 0.02811245 0.02811245
 0.02911647 0.02911647 0.03012048 0.03012048 0.0311245  0.0311245
 0.03614458 0.03614458 0.03815261 0.03815261 0.03915663 0.03915663
 0.04016064 0.04016064 0.04016064 0.04216867 0.04216867 0.04417671
 0.04417671 0.04618474 0.04618474 0.04718876 0.04718876 0.04819277
 0.04819277 0.04919679 0.04919679 0.0502008  0.0502008  0.0502008
 0.0502008  0.05120482 0.05120482 0.05220884 0.05220884 0.05823293
 0.05823293 0.06124498 0.06124498 0.062249   0.062249   0.06827309
 0.06827309 0.07630522 0.07630522 0.07730924 0.07730924 0.07931727
 0.07931727 0.09337349 0.09337349 0.10040161 0.10040161 0.10240964
 0.10441767 0.10943775 0.10943775 0.13052209 0.13253012 0.13453815
 0.13453815 0.1375502  0.1375502  0.14156627 0.14156627 0.15963855
 0.15963855 0.18674699 0.18875502 0.19477912 0.19477912 0.22991968
 0.23192771 0.26807229 0.27008032 0.27309237 0.2751004  0.29618474
 0.29618474 0.30823293 0.30823293 0.31827309 0.32028112 0.32630522
 0.32831325 0.34236948 0.34437751 0.36947791 0.37148594 0.37550201
 0.37751004 0.3815261  0.38353414 0.39457831 0.39658635 0.41566265
 0.41767068 0.4186747  0.42068273 0.4246988  0.42670683 0.46787149
 0.46987952 0.47891566 0.48092369 0.48493976 0.48694779 0.50200803
 0.50401606 0.51506024 0.51907631 0.52309237 0.5251004  0.53915663
 0.54116466 0.55823293 0.56024096 0.562249   0.56425703 0.56626506
 0.56827309 0.57630522 0.57831325 0.5813253  0.58333333 0.59136546
 0.59337349 0.59638554 0.60240964 0.60441767 0.60843373 0.61044177
 0.61646586 0.61947791 0.62148594 0.62751004 0.6315261  0.6375502
 0.64056225 0.64257028 0.64457831 0.65261044 0.67068273 0.6746988
 0.68072289 0.68473896 0.69578313 0.69779116 0.70381526 0.70582329
 0.70883534 0.71084337 0.71485944 0.71686747 0.72088353 0.72289157
 0.72991968 0.73393574 0.74196787 0.74799197 0.75100402 0.75401606
 0.75502008 0.75702811 0.76204819 0.76405622 0.76807229 0.77008032
 0.77108434 0.7751004  0.77811245 0.78012048 0.78815261 0.79116466
 0.79216867 0.79618474 0.79919679 0.80120482 0.80722892 0.80923695
 0.81124498 0.81325301 0.81827309 0.82028112 0.8253012  0.82730924
 0.82831325 0.83032129 0.83232932 0.83534137 0.84337349 0.84538153
 0.85742972 0.85943775 0.87349398 0.87751004 0.87851406 0.88052209
 0.88654618 0.88855422 0.89056225 0.89257028 0.89658635 0.89859438
 0.90863454 0.91164659 0.91767068 0.91967871 0.92971888 0.93172691
 0.93574297 0.937751   0.93975904 0.94277108 0.94477912 0.94678715
 0.94779116 0.95080321 0.95381526 0.95682731 0.95783133 0.95983936
 0.97991968 0.98192771 1.        ]
tpr: [0.         0.00100402 0.00903614 0.01204819 0.01305221 0.01506024
 0.0251004  0.02710843 0.03012048 0.03614458 0.03815261 0.04417671
 0.0502008  0.05220884 0.05321285 0.05522088 0.0562249  0.06124498
 0.06425703 0.06526104 0.06726908 0.07028112 0.07329317 0.07730924
 0.07931727 0.08032129 0.08232932 0.08534137 0.08634538 0.09036145
 0.09136546 0.09437751 0.09538153 0.09738956 0.09839357 0.10140562
 0.10240964 0.10441767 0.10843373 0.11044177 0.11445783 0.11546185
 0.11746988 0.1184739  0.12248996 0.12449799 0.12650602 0.12951807
 0.1315261  0.13554217 0.13955823 0.1435743  0.14658635 0.14959839
 0.15060241 0.15461847 0.15662651 0.16064257 0.16164659 0.16365462
 0.16767068 0.1686747  0.17068273 0.17168675 0.1746988  0.17771084
 0.17971888 0.18072289 0.18273092 0.18373494 0.18975904 0.19277108
 0.19678715 0.20281124 0.20682731 0.20783133 0.21084337 0.21184739
 0.21385542 0.2188755  0.21987952 0.22289157 0.2248996  0.22791165
 0.22991968 0.23192771 0.23795181 0.24196787 0.24598394 0.24698795
 0.24899598 0.25401606 0.2560241  0.25803213 0.26104418 0.26305221
 0.26606426 0.26807229 0.27108434 0.27309237 0.27610442 0.28012048
 0.28212851 0.28614458 0.28915663 0.29116466 0.29216867 0.29819277
 0.30120482 0.30522088 0.30722892 0.31024096 0.31626506 0.31827309
 0.32028112 0.32128514 0.3253012  0.32831325 0.33032129 0.33232932
 0.33433735 0.33534137 0.34437751 0.34839357 0.35040161 0.35441767
 0.3564257  0.35843373 0.36044177 0.36144578 0.36345382 0.36947791
 0.37048193 0.37449799 0.37550201 0.37751004 0.38052209 0.38253012
 0.38855422 0.38955823 0.39257028 0.40461847 0.40763052 0.40963855
 0.41465863 0.41666667 0.4246988  0.42570281 0.43072289 0.43473896
 0.43674699 0.437751   0.43975904 0.44076305 0.4437751  0.44578313
 0.44879518 0.45080321 0.45281124 0.45883534 0.46084337 0.46485944
 0.46586345 0.46787149 0.4688755  0.4748996  0.47891566 0.48293173
 0.48594378 0.48795181 0.48995984 0.49297189 0.49598394 0.50200803
 0.50301205 0.50702811 0.50903614 0.51104418 0.51305221 0.51606426
 0.51706827 0.51907631 0.52108434 0.52409639 0.52610442 0.52710843
 0.52911647 0.53212851 0.53413655 0.53815261 0.54116466 0.54317269
 0.54618474 0.54919679 0.55120482 0.55421687 0.55522088 0.55823293
 0.55923695 0.56124498 0.56425703 0.56526104 0.56927711 0.57028112
 0.57630522 0.57931727 0.58032129 0.58433735 0.58634538 0.58935743
 0.59136546 0.59236948 0.59538153 0.59738956 0.6064257  0.61044177
 0.6124498  0.61646586 0.62148594 0.62550201 0.6315261  0.63253012
 0.63453815 0.6375502  0.64156627 0.64457831 0.64558233 0.64759036
 0.65461847 0.65662651 0.65963855 0.66465863 0.66666667 0.6686747
 0.66967871 0.67369478 0.67670683 0.67971888 0.68172691 0.68574297
 0.69176707 0.69277108 0.69477912 0.69879518 0.6997992  0.70582329
 0.70783133 0.71987952 0.72289157 0.72590361 0.72791165 0.73393574
 0.73493976 0.73594378 0.73795181 0.73995984 0.74297189 0.74497992
 0.74899598 0.75200803 0.75301205 0.75502008 0.76305221 0.76506024
 0.77309237 0.7751004  0.77911647 0.78212851 0.78413655 0.78915663
 0.78915663 0.79618474 0.79819277 0.80421687 0.8062249  0.8062249
 0.80722892 0.80722892 0.812249   0.81425703 0.81626506 0.81827309
 0.81927711 0.81927711 0.82028112 0.82329317 0.82329317 0.82429719
 0.82429719 0.8253012  0.82730924 0.83835341 0.84036145 0.84136546
 0.84538153 0.8624498  0.8624498  0.86345382 0.86345382 0.86947791
 0.87349398 0.87751004 0.87951807 0.8815261  0.8815261  0.88253012
 0.88453815 0.89056225 0.89056225 0.89257028 0.89257028 0.89859438
 0.89859438 0.90160643 0.90160643 0.9126506  0.9126506  0.91365462
 0.91365462 0.91465863 0.91666667 0.91767068 0.9186747  0.9186747
 0.92670683 0.92670683 0.92771084 0.92771084 0.93172691 0.93172691
 0.93373494 0.93373494 0.93473896 0.93574297 0.93574297 0.937751
 0.937751   0.94277108 0.94277108 0.9437751  0.94578313 0.94879518
 0.94879518 0.9497992  0.9497992  0.95180723 0.95381526 0.95481928
 0.95481928 0.95682731 0.95783133 0.95783133 0.95983936 0.95983936
 0.96184739 0.96184739 0.96285141 0.96285141 0.96586345 0.96586345
 0.96686747 0.96787149 0.9688755  0.9688755  0.97088353 0.97289157
 0.97590361 0.97590361 0.97791165 0.97791165 0.97891566 0.97891566
 0.97991968 0.97991968 0.98092369 0.98092369 0.98393574 0.98393574
 0.98493976 0.98493976 0.98694779 0.98694779 0.98895582 0.98895582
 0.98995984 0.98995984 0.99096386 0.99096386 0.99196787 0.99196787
 0.99196787 0.99196787 0.99297189 0.99297189 0.99297189 0.99297189
 0.9939759  0.9939759  0.99497992 0.99497992 0.99598394 0.99598394
 0.99698795 0.99698795 0.99698795 0.99698795 0.99799197 0.99799197
 0.99799197 0.99799197 0.99799197 0.99799197 0.99799197 0.99799197
 0.99899598 0.99899598 1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.        ]
thresholds: [        inf  7.0078125   6.46875     6.4414062   6.4375      6.3984375
  6.2617188   6.25        6.2421875   6.2109375   6.203125    6.1640625
  6.1484375   6.1445312   6.140625    6.1367188   6.1328125   6.109375
  6.1054688   6.1015625   6.09375     6.0859375   6.0703125   6.0664062
  6.0507812   6.0429688   6.0390625   6.0351562   6.0234375   6.0117188
  5.9882812   5.984375    5.9804688   5.9726562   5.9609375   5.9570312
  5.953125    5.9414062   5.9140625   5.9101562   5.90625     5.9023438
  5.8984375   5.8945312   5.8867188   5.875       5.8632812   5.859375
  5.8554688   5.8359375   5.8242188   5.8203125   5.8085938   5.8046875
  5.796875    5.78125     5.7734375   5.7695312   5.765625    5.7617188
  5.7578125   5.7539062   5.75        5.7421875   5.7382812   5.7265625
  5.7226562   5.71875     5.7148438   5.7109375   5.6953125   5.6875
  5.6679688   5.6523438   5.6484375   5.640625    5.6367188   5.6328125
  5.6289062   5.625       5.6210938   5.6171875   5.6132812   5.6015625
  5.5976562   5.5820312   5.5742188   5.5703125   5.5625      5.5585938
  5.546875    5.5429688   5.5351562   5.5273438   5.5234375   5.5195312
  5.5117188   5.5039062   5.5         5.4882812   5.484375    5.46875
  5.4648438   5.4609375   5.4570312   5.453125    5.4492188   5.4414062
  5.4296875   5.421875    5.4140625   5.40625     5.3945312   5.3867188
  5.3828125   5.375       5.3671875   5.3632812   5.359375    5.3515625
  5.3476562   5.34375     5.3320312   5.328125    5.3242188   5.3203125
  5.3125      5.3046875   5.3007812   5.296875    5.2929688   5.2851562
  5.28125     5.2773438   5.2734375   5.2695312   5.265625    5.2578125
  5.25        5.2421875   5.2382812   5.2070312   5.1953125   5.1914062
  5.1875      5.171875    5.1640625   5.1601562   5.15625     5.140625
  5.1367188   5.1328125   5.1289062   5.1171875   5.109375    5.0976562
  5.09375     5.0859375   5.0742188   5.0625      5.0585938   5.0546875
  5.0507812   5.046875    5.0429688   5.0234375   5.0078125   5.0039062
  5.          4.9921875   4.9882812   4.984375    4.9726562   4.9570312
  4.953125    4.9375      4.9179688   4.9140625   4.9023438   4.8984375
  4.8945312   4.8867188   4.8828125   4.8710938   4.8671875   4.8632812
  4.859375    4.8398438   4.8359375   4.8125      4.8007812   4.796875
  4.7773438   4.7734375   4.7617188   4.7578125   4.7539062   4.7460938
  4.7421875   4.734375    4.7304688   4.7265625   4.71875     4.7070312
  4.6953125   4.6914062   4.6875      4.6835938   4.6757812   4.671875
  4.6679688   4.6523438   4.6445312   4.640625    4.625       4.6054688
  4.6015625   4.5976562   4.578125    4.5742188   4.5664062   4.5546875
  4.546875    4.5429688   4.5273438   4.5234375   4.5195312   4.515625
  4.4804688   4.4765625   4.4726562   4.4453125   4.4414062   4.421875
  4.4179688   4.40625     4.390625    4.3867188   4.3828125   4.3554688
  4.3359375   4.3320312   4.3203125   4.3164062   4.3125      4.3046875
  4.296875    4.2226562   4.21875     4.1914062   4.1835938   4.140625
  4.1367188   4.1328125   4.1210938   4.1015625   4.0976562   4.0859375
  4.0546875   4.0507812   4.0390625   4.0234375   3.9746094   3.9726562
  3.9160156   3.9101562   3.9042969   3.8828125   3.8769531   3.8535156
  3.84375     3.8203125   3.8105469   3.7832031   3.7792969   3.7734375
  3.7539062   3.7460938   3.7246094   3.7148438   3.7011719   3.6855469
  3.6816406   3.6796875   3.6738281   3.6679688   3.6621094   3.65625
  3.6484375   3.6445312   3.640625    3.5566406   3.5410156   3.5371094
  3.5214844   3.3632812   3.3613281   3.3574219   3.3554688   3.2871094
  3.25        3.2246094   3.2226562   3.203125    3.1835938   3.1816406
  3.1777344   3.1035156   3.0976562   3.0488281   3.0429688   2.9960938
  2.9882812   2.9550781   2.9375      2.8496094   2.8242188   2.8144531
  2.8125      2.8046875   2.796875    2.7949219   2.7890625   2.7832031
  2.6679688   2.6582031   2.6445312   2.6386719   2.609375    2.5957031
  2.5566406   2.5390625   2.5371094   2.5351562   2.5292969   2.5214844
  2.5195312   2.484375    2.4804688   2.4765625   2.4472656   2.4238281
  2.3730469   2.3632812   2.3339844   2.3164062   2.3085938   2.2832031
  2.25        2.2460938   2.2382812   2.2363281   2.2265625   2.2089844
  2.1933594   2.1835938   2.1699219   2.1503906   2.1132812   2.1054688
  2.0839844   2.0664062   2.0566406   2.0253906   1.9990234   1.9951172
  1.9541016   1.9335938   1.9208984   1.8652344   1.8388672   1.7705078
  1.7587891   1.7207031   1.6992188   1.6679688   1.6367188   1.5097656
  1.5058594   1.3105469   1.2861328   1.2734375   1.2382812   1.2236328
  1.2001953   0.9921875   0.96972656  0.8876953   0.86621094  0.8310547
  0.828125    0.8051758   0.76904297  0.578125    0.57666016  0.56640625
  0.55908203  0.54052734  0.5395508   0.45996094  0.45703125  0.2553711
  0.2409668   0.03720093  0.03274536 -0.01015472 -0.01651001 -0.28051758
 -0.28393555 -0.54541016 -0.56103516 -0.5834961  -0.58740234 -0.7324219
 -0.73876953 -0.7949219  -0.8120117  -0.87890625 -0.8876953  -0.94140625
 -0.94189453 -1.0546875  -1.0556641  -1.2070312  -1.2167969  -1.2255859
 -1.2314453  -1.2578125  -1.2607422  -1.3027344  -1.3085938  -1.4277344
 -1.4394531  -1.4433594  -1.4501953  -1.4707031  -1.4785156  -1.7236328
 -1.7324219  -1.8066406  -1.8095703  -1.8378906  -1.8408203  -1.9443359
 -1.9472656  -1.984375   -2.0039062  -2.0351562  -2.0371094  -2.1503906
 -2.1699219  -2.2636719  -2.265625   -2.2714844  -2.2792969  -2.2910156
 -2.2929688  -2.3632812  -2.3710938  -2.390625   -2.4003906  -2.4355469
 -2.4375     -2.4414062  -2.4746094  -2.4765625  -2.5        -2.5019531
 -2.5292969  -2.5351562  -2.5390625  -2.5625     -2.5722656  -2.59375
 -2.5996094  -2.609375   -2.6171875  -2.6386719  -2.7480469  -2.7578125
 -2.7851562  -2.7910156  -2.8476562  -2.8496094  -2.8789062  -2.8828125
 -2.9023438  -2.90625    -2.9179688  -2.921875   -2.9355469  -2.9414062
 -3.0058594  -3.0136719  -3.0546875  -3.0625     -3.0898438  -3.09375
 -3.1113281  -3.1171875  -3.1621094  -3.1640625  -3.2089844  -3.2109375
 -3.2207031  -3.2285156  -3.2460938  -3.265625   -3.2949219  -3.3085938
 -3.3125     -3.3359375  -3.3554688  -3.3574219  -3.3925781  -3.3984375
 -3.4082031  -3.4101562  -3.4316406  -3.4335938  -3.4550781  -3.4589844
 -3.4628906  -3.4648438  -3.4941406  -3.5        -3.5332031  -3.5371094
 -3.6074219  -3.609375   -3.7050781  -3.7109375  -3.7167969  -3.71875
 -3.7714844  -3.7753906  -3.8027344  -3.8085938  -3.8476562  -3.8515625
 -3.9023438  -3.9082031  -3.9492188  -3.9570312  -4.0390625  -4.0429688
 -4.0859375  -4.109375   -4.1171875  -4.1328125  -4.203125   -4.2148438
 -4.21875    -4.2226562  -4.2539062  -4.2578125  -4.2695312  -4.2734375
 -4.640625   -4.671875   -5.53125   ]
