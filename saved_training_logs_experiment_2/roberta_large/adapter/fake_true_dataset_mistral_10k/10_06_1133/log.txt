log_loss_steps: 208
eval_steps: 512
check_degradation: 0
----------------Epoch 1/1----------------
Epoch 1/1, Loss after 192 samples: 0.7107
Epoch 1/1, Loss after 400 samples: 0.6881
Mean accuracy: 0.6070, std: 0.0109, lower bound: 0.5847, upper bound: 0.6283 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 496 samples: 0.6070 with eval loss: 0.6730
Best model with eval loss 0.6730120258946573 and eval accuracy 0.6069979716024341 with 496 samples seen is saved
Epoch 1/1, Loss after 608 samples: 0.6774
Epoch 1/1, Loss after 816 samples: 0.6348
Mean accuracy: 0.7783, std: 0.0096, lower bound: 0.7601, upper bound: 0.7977 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 1008 samples: 0.7789 with eval loss: 0.5280
Best model with eval loss 0.5280390093403478 and eval accuracy 0.7789046653144016 with 1008 samples seen is saved
Epoch 1/1, Loss after 1024 samples: 0.6132
Epoch 1/1, Loss after 1232 samples: 0.5239
Epoch 1/1, Loss after 1440 samples: 0.5196
Mean accuracy: 0.7910, std: 0.0089, lower bound: 0.7738, upper bound: 0.8083 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 1520 samples: 0.7906 with eval loss: 0.4209
Best model with eval loss 0.4208723754892426 and eval accuracy 0.7905679513184585 with 1520 samples seen is saved
Epoch 1/1, Loss after 1648 samples: 0.4512
Epoch 1/1, Loss after 1856 samples: 0.4553
Mean accuracy: 0.7359, std: 0.0102, lower bound: 0.7155, upper bound: 0.7546 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 2032 samples: 0.7358 with eval loss: 0.4882
Epoch 1/1, Loss after 2064 samples: 0.3984
Epoch 1/1, Loss after 2272 samples: 0.3930
Epoch 1/1, Loss after 2480 samples: 0.4575
Mean accuracy: 0.8133, std: 0.0083, lower bound: 0.7971, upper bound: 0.8296 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 2544 samples: 0.8134 with eval loss: 0.3846
Best model with eval loss 0.384620696186058 and eval accuracy 0.8133874239350912 with 2544 samples seen is saved
Epoch 1/1, Loss after 2688 samples: 0.3886
Epoch 1/1, Loss after 2896 samples: 0.2808
Mean accuracy: 0.8706, std: 0.0075, lower bound: 0.8555, upper bound: 0.8849 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 3056 samples: 0.8707 with eval loss: 0.2901
Best model with eval loss 0.29010652726696384 and eval accuracy 0.8706896551724138 with 3056 samples seen is saved
Epoch 1/1, Loss after 3104 samples: 0.3390
Epoch 1/1, Loss after 3312 samples: 0.2831
Epoch 1/1, Loss after 3520 samples: 0.3002
Mean accuracy: 0.8876, std: 0.0071, lower bound: 0.8742, upper bound: 0.9011 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 3568 samples: 0.8874 with eval loss: 0.2398
Best model with eval loss 0.23977145672805847 and eval accuracy 0.8874239350912779 with 3568 samples seen is saved
Epoch 1/1, Loss after 3728 samples: 0.3082
Epoch 1/1, Loss after 3936 samples: 0.2986
Mean accuracy: 0.9057, std: 0.0064, lower bound: 0.8935, upper bound: 0.9178 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 4080 samples: 0.9057 with eval loss: 0.2196
Best model with eval loss 0.21963611106959083 and eval accuracy 0.9056795131845842 with 4080 samples seen is saved
Epoch 1/1, Loss after 4144 samples: 0.3518
Epoch 1/1, Loss after 4352 samples: 0.4257
Epoch 1/1, Loss after 4560 samples: 0.2963
Mean accuracy: 0.7864, std: 0.0097, lower bound: 0.7677, upper bound: 0.8058 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 4592 samples: 0.7860 with eval loss: 0.4183
Epoch 1/1, Loss after 4768 samples: 0.2423
Epoch 1/1, Loss after 4976 samples: 0.3067
Mean accuracy: 0.8831, std: 0.0073, lower bound: 0.8682, upper bound: 0.8966 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 5104 samples: 0.8829 with eval loss: 0.2599
Epoch 1/1, Loss after 5184 samples: 0.1808
Epoch 1/1, Loss after 5392 samples: 0.2449
Epoch 1/1, Loss after 5600 samples: 0.2235
Mean accuracy: 0.8837, std: 0.0071, lower bound: 0.8692, upper bound: 0.8981 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 5616 samples: 0.8839 with eval loss: 0.2910
Epoch 1/1, Loss after 5808 samples: 0.2992
Epoch 1/1, Loss after 6016 samples: 0.1881
Mean accuracy: 0.8604, std: 0.0078, lower bound: 0.8458, upper bound: 0.8753 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 6128 samples: 0.8605 with eval loss: 0.3398
Epoch 1/1, Loss after 6224 samples: 0.2280
Epoch 1/1, Loss after 6432 samples: 0.2128
Epoch 1/1, Loss after 6640 samples: 0.1711
Mean accuracy: 0.8467, std: 0.0080, lower bound: 0.8301, upper bound: 0.8621 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 6640 samples: 0.8469 with eval loss: 0.3826
Epoch 1/1, Loss after 6848 samples: 0.1563
Epoch 1/1, Loss after 7056 samples: 0.1573
Mean accuracy: 0.9054, std: 0.0064, lower bound: 0.8925, upper bound: 0.9174 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 7152 samples: 0.9057 with eval loss: 0.2415
Epoch 1/1, Loss after 7264 samples: 0.2174
Epoch 1/1, Loss after 7472 samples: 0.1962
Mean accuracy: 0.9235, std: 0.0061, lower bound: 0.9113, upper bound: 0.9351 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 7664 samples: 0.9234 with eval loss: 0.1877
Best model with eval loss 0.18766561138533777 and eval accuracy 0.9234279918864098 with 7664 samples seen is saved
Epoch 1/1, Loss after 7680 samples: 0.1683
Epoch 1/1, Loss after 7888 samples: 0.2354
Epoch 1/1, Loss after 8096 samples: 0.2042
Mean accuracy: 0.8952, std: 0.0069, lower bound: 0.8813, upper bound: 0.9087 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 8176 samples: 0.8950 with eval loss: 0.2717
Epoch 1/1, Loss after 8304 samples: 0.1646
Epoch 1/1, Loss after 8512 samples: 0.1368
Mean accuracy: 0.9046, std: 0.0066, lower bound: 0.8915, upper bound: 0.9173 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 8688 samples: 0.9047 with eval loss: 0.2513
Epoch 1/1, Loss after 8720 samples: 0.1483
Epoch 1/1, Loss after 8928 samples: 0.1737
Epoch 1/1, Loss after 9136 samples: 0.1370
Mean accuracy: 0.8817, std: 0.0071, lower bound: 0.8676, upper bound: 0.8950 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 9200 samples: 0.8818 with eval loss: 0.2985
Epoch 1/1, Loss after 9344 samples: 0.1504
Epoch 1/1, Loss after 9552 samples: 0.2026
Mean accuracy: 0.9383, std: 0.0053, lower bound: 0.9280, upper bound: 0.9488 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 9712 samples: 0.9381 with eval loss: 0.1684
Best model with eval loss 0.16839409589527116 and eval accuracy 0.9381338742393509 with 9712 samples seen is saved
Epoch 1/1, Loss after 9760 samples: 0.2892
Epoch 1/1, Loss after 9968 samples: 0.2256
Epoch 1/1, Loss after 10176 samples: 0.1836
Mean accuracy: 0.8229, std: 0.0084, lower bound: 0.8063, upper bound: 0.8392 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 10224 samples: 0.8230 with eval loss: 0.4128
Epoch 1/1, Loss after 10384 samples: 0.1705
Epoch 1/1, Loss after 10592 samples: 0.1322
Mean accuracy: 0.8537, std: 0.0081, lower bound: 0.8387, upper bound: 0.8692 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 10736 samples: 0.8540 with eval loss: 0.3931
Epoch 1/1, Loss after 10800 samples: 0.1383
Epoch 1/1, Loss after 11008 samples: 0.1592
Epoch 1/1, Loss after 11216 samples: 0.1498
Mean accuracy: 0.8933, std: 0.0069, lower bound: 0.8798, upper bound: 0.9067 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 11248 samples: 0.8935 with eval loss: 0.2965
Epoch 1/1, Loss after 11424 samples: 0.2917
Epoch 1/1, Loss after 11632 samples: 0.1392
Mean accuracy: 0.9088, std: 0.0067, lower bound: 0.8966, upper bound: 0.9224 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 11760 samples: 0.9092 with eval loss: 0.2438
Epoch 1/1, Loss after 11840 samples: 0.1425
Epoch 1/1, Loss after 12048 samples: 0.1753
Epoch 1/1, Loss after 12256 samples: 0.1885
Mean accuracy: 0.8198, std: 0.0088, lower bound: 0.8027, upper bound: 0.8367 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 12272 samples: 0.8200 with eval loss: 0.4864
Epoch 1/1, Loss after 12464 samples: 0.1863
Epoch 1/1, Loss after 12672 samples: 0.1170
Mean accuracy: 0.9102, std: 0.0064, lower bound: 0.8976, upper bound: 0.9229 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 12784 samples: 0.9097 with eval loss: 0.2294
Epoch 1/1, Loss after 12880 samples: 0.1725
Epoch 1/1, Loss after 13088 samples: 0.1522
Epoch 1/1, Loss after 13296 samples: 0.1812
Mean accuracy: 0.8947, std: 0.0070, lower bound: 0.8808, upper bound: 0.9077 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 13296 samples: 0.8945 with eval loss: 0.2637
Epoch 1/1, Loss after 13504 samples: 0.1312
Epoch 1/1, Loss after 13712 samples: 0.1263
Mean accuracy: 0.8564, std: 0.0079, lower bound: 0.8408, upper bound: 0.8722 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 13808 samples: 0.8565 with eval loss: 0.3703
Epoch 1/1, Loss after 13920 samples: 0.0919
Epoch 1/1, Loss after 14128 samples: 0.2145
Mean accuracy: 0.8798, std: 0.0074, lower bound: 0.8656, upper bound: 0.8940 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 14320 samples: 0.8798 with eval loss: 0.3064
Epoch 1/1, Loss after 14336 samples: 0.2014
Epoch 1/1, Loss after 14544 samples: 0.1471
Epoch 1/1, Loss after 14752 samples: 0.1336
Mean accuracy: 0.8608, std: 0.0078, lower bound: 0.8463, upper bound: 0.8763 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 14832 samples: 0.8611 with eval loss: 0.3445
Epoch 1/1, Loss after 14960 samples: 0.1221
Epoch 1/1, Loss after 15168 samples: 0.1431
Mean accuracy: 0.8687, std: 0.0073, lower bound: 0.8540, upper bound: 0.8818 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 15344 samples: 0.8687 with eval loss: 0.3317
Epoch 1/1, Loss after 15376 samples: 0.1418
Epoch 1/1, Loss after 15584 samples: 0.1719
Training signal is False, stopping training
Training finished
Best model: {'eval_acc': 0.9381338742393509, 'nb_samples': 9712, 'eval_loss': 0.16839409589527116}
Training loss logs: [{'samples': 192, 'loss': 0.7107215294471154}, {'samples': 400, 'loss': 0.6881479116586539}, {'samples': 608, 'loss': 0.6774221567007211}, {'samples': 816, 'loss': 0.6348360501802884}, {'samples': 1024, 'loss': 0.6131735581618089}, {'samples': 1232, 'loss': 0.5239335573636569}, {'samples': 1440, 'loss': 0.519586430146144}, {'samples': 1648, 'loss': 0.45123621133657604}, {'samples': 1856, 'loss': 0.4552575441507193}, {'samples': 2064, 'loss': 0.39836356273064244}, {'samples': 2272, 'loss': 0.39296976648844206}, {'samples': 2480, 'loss': 0.45745980510344875}, {'samples': 2688, 'loss': 0.3885578421445993}, {'samples': 2896, 'loss': 0.28081375589737523}, {'samples': 3104, 'loss': 0.3390107527375221}, {'samples': 3312, 'loss': 0.2831191007907574}, {'samples': 3520, 'loss': 0.30023356355153596}, {'samples': 3728, 'loss': 0.30815644218371463}, {'samples': 3936, 'loss': 0.2986302954646257}, {'samples': 4144, 'loss': 0.3517641700231112}, {'samples': 4352, 'loss': 0.42569796626384443}, {'samples': 4560, 'loss': 0.29634025463691127}, {'samples': 4768, 'loss': 0.24230279601537263}, {'samples': 4976, 'loss': 0.306721809391792}, {'samples': 5184, 'loss': 0.18083789486151475}, {'samples': 5392, 'loss': 0.24493963099442995}, {'samples': 5600, 'loss': 0.2235206474478428}, {'samples': 5808, 'loss': 0.2992289668092361}, {'samples': 6016, 'loss': 0.18810815994556135}, {'samples': 6224, 'loss': 0.22802039579703257}, {'samples': 6432, 'loss': 0.21283951516334826}, {'samples': 6640, 'loss': 0.17106095816080386}, {'samples': 6848, 'loss': 0.15629575630793205}, {'samples': 7056, 'loss': 0.15732153677023375}, {'samples': 7264, 'loss': 0.21741149574518204}, {'samples': 7472, 'loss': 0.19621437501448852}, {'samples': 7680, 'loss': 0.1683079101718389}, {'samples': 7888, 'loss': 0.2353936920945461}, {'samples': 8096, 'loss': 0.2042275329048817}, {'samples': 8304, 'loss': 0.1646002559707715}, {'samples': 8512, 'loss': 0.13677940918849066}, {'samples': 8720, 'loss': 0.14834785346801466}, {'samples': 8928, 'loss': 0.1736716954753949}, {'samples': 9136, 'loss': 0.13696051904788384}, {'samples': 9344, 'loss': 0.15040287375450134}, {'samples': 9552, 'loss': 0.202603640464636}, {'samples': 9760, 'loss': 0.28916787986571973}, {'samples': 9968, 'loss': 0.2256370519216244}, {'samples': 10176, 'loss': 0.1836229356435629}, {'samples': 10384, 'loss': 0.17046071359744439}, {'samples': 10592, 'loss': 0.13223351079684037}, {'samples': 10800, 'loss': 0.1383263090482125}, {'samples': 11008, 'loss': 0.15917502114405999}, {'samples': 11216, 'loss': 0.14979200810194016}, {'samples': 11424, 'loss': 0.2917458821947758}, {'samples': 11632, 'loss': 0.13923847274138376}, {'samples': 11840, 'loss': 0.1424925929078689}, {'samples': 12048, 'loss': 0.17526166886091232}, {'samples': 12256, 'loss': 0.18851484988744444}, {'samples': 12464, 'loss': 0.1862893087359575}, {'samples': 12672, 'loss': 0.11700182981215991}, {'samples': 12880, 'loss': 0.17245498872720277}, {'samples': 13088, 'loss': 0.1522087168235045}, {'samples': 13296, 'loss': 0.1812400072813034}, {'samples': 13504, 'loss': 0.13118234964517447}, {'samples': 13712, 'loss': 0.12625782363689864}, {'samples': 13920, 'loss': 0.09187154873059346}, {'samples': 14128, 'loss': 0.2145464913203166}, {'samples': 14336, 'loss': 0.2013507789144149}, {'samples': 14544, 'loss': 0.1471084081209623}, {'samples': 14752, 'loss': 0.13362207607581064}, {'samples': 14960, 'loss': 0.12205065557589898}, {'samples': 15168, 'loss': 0.1431411418777246}, {'samples': 15376, 'loss': 0.14183840728723085}, {'samples': 15584, 'loss': 0.17192892672923896}]
Evaluation accuracy logs: [{'samples': 496, 'accuracy': 0.6070273833671399, 'std': 0.010852337341296082, 'lower_bound': 0.584685598377282, 'upper_bound': 0.6283088235294118}, {'samples': 1008, 'accuracy': 0.7783331643002029, 'std': 0.009627637805061548, 'lower_bound': 0.7601419878296146, 'upper_bound': 0.7976800202839757}, {'samples': 1520, 'accuracy': 0.7909528397565924, 'std': 0.00886770143112547, 'lower_bound': 0.7738336713995944, 'upper_bound': 0.808316430020284}, {'samples': 2032, 'accuracy': 0.7359361054766734, 'std': 0.010217533596302831, 'lower_bound': 0.7155172413793104, 'upper_bound': 0.7545765720081136}, {'samples': 2544, 'accuracy': 0.8132712981744421, 'std': 0.008298361348542099, 'lower_bound': 0.7971475659229208, 'upper_bound': 0.8296146044624746}, {'samples': 3056, 'accuracy': 0.8706206896551724, 'std': 0.007451557751375592, 'lower_bound': 0.8554766734279919, 'upper_bound': 0.8848884381338742}, {'samples': 3568, 'accuracy': 0.887642494929006, 'std': 0.007075649212378862, 'lower_bound': 0.8742393509127789, 'upper_bound': 0.9011282961460446}, {'samples': 4080, 'accuracy': 0.9056820486815416, 'std': 0.006385526640219263, 'lower_bound': 0.8935091277890467, 'upper_bound': 0.9178498985801217}, {'samples': 4592, 'accuracy': 0.7863762677484788, 'std': 0.009703015837797814, 'lower_bound': 0.7677484787018256, 'upper_bound': 0.8057936105476674}, {'samples': 5104, 'accuracy': 0.8830892494929007, 'std': 0.007346218091688338, 'lower_bound': 0.8681541582150102, 'upper_bound': 0.896551724137931}, {'samples': 5616, 'accuracy': 0.8837261663286003, 'std': 0.007140397656199511, 'lower_bound': 0.8691683569979716, 'upper_bound': 0.8980730223123732}, {'samples': 6128, 'accuracy': 0.8603737322515214, 'std': 0.007804346144444851, 'lower_bound': 0.845841784989858, 'upper_bound': 0.8752535496957403}, {'samples': 6640, 'accuracy': 0.8467297160243409, 'std': 0.008045240105159966, 'lower_bound': 0.8301217038539553, 'upper_bound': 0.8620816430020284}, {'samples': 7152, 'accuracy': 0.9053843813387423, 'std': 0.0063727137552844195, 'lower_bound': 0.8924822515212981, 'upper_bound': 0.917355476673428}, {'samples': 7664, 'accuracy': 0.9234518255578094, 'std': 0.006060904685959342, 'lower_bound': 0.9112576064908722, 'upper_bound': 0.9350912778904665}, {'samples': 8176, 'accuracy': 0.8952418864097363, 'std': 0.006873789975687703, 'lower_bound': 0.8813387423935092, 'upper_bound': 0.9087347870182556}, {'samples': 8688, 'accuracy': 0.9046120689655172, 'std': 0.0066019194601948635, 'lower_bound': 0.8914807302231237, 'upper_bound': 0.9173427991886409}, {'samples': 9200, 'accuracy': 0.881713995943205, 'std': 0.007062101428193197, 'lower_bound': 0.8676470588235294, 'upper_bound': 0.8950304259634888}, {'samples': 9712, 'accuracy': 0.9382809330628804, 'std': 0.00532623125963786, 'lower_bound': 0.9279792089249492, 'upper_bound': 0.9487956389452333}, {'samples': 10224, 'accuracy': 0.8229102434077079, 'std': 0.00843512485783924, 'lower_bound': 0.8062880324543611, 'upper_bound': 0.8392494929006086}, {'samples': 10736, 'accuracy': 0.8537292089249493, 'std': 0.008095871308824805, 'lower_bound': 0.8387423935091278, 'upper_bound': 0.8691683569979716}, {'samples': 11248, 'accuracy': 0.8932814401622718, 'std': 0.00688680179363555, 'lower_bound': 0.8798047667342799, 'upper_bound': 0.9066937119675457}, {'samples': 11760, 'accuracy': 0.9088260649087221, 'std': 0.006713141307604504, 'lower_bound': 0.896551724137931, 'upper_bound': 0.9224137931034483}, {'samples': 12272, 'accuracy': 0.8197920892494929, 'std': 0.008753997031635721, 'lower_bound': 0.8027256592292089, 'upper_bound': 0.8367139959432048}, {'samples': 12784, 'accuracy': 0.9102378296146044, 'std': 0.006404263376337422, 'lower_bound': 0.8975659229208925, 'upper_bound': 0.922920892494929}, {'samples': 13296, 'accuracy': 0.8946861054766735, 'std': 0.0069699507904022965, 'lower_bound': 0.8808316430020284, 'upper_bound': 0.907707910750507}, {'samples': 13808, 'accuracy': 0.8563975659229208, 'std': 0.007888487161587965, 'lower_bound': 0.8407707910750507, 'upper_bound': 0.8722109533468559}, {'samples': 14320, 'accuracy': 0.8798433062880324, 'std': 0.007381395580087375, 'lower_bound': 0.8656186612576064, 'upper_bound': 0.8940162271805274}, {'samples': 14832, 'accuracy': 0.8608174442190669, 'std': 0.007780712294372017, 'lower_bound': 0.8463488843813387, 'upper_bound': 0.8762677484787018}, {'samples': 15344, 'accuracy': 0.8686648073022314, 'std': 0.007258201844082212, 'lower_bound': 0.8539553752535497, 'upper_bound': 0.8818458417849898}]
Evaluating the best model on the test set of dataset ./fake_true_datasets/fake_true_dataset_mistral_10k...
Test metrics:
accuracy: 0.8800273833671399
precision: 0.8072274632632982
recall: 0.9980333964124664
f1_score: 0.892506843475686
fp_rate: 0.23764458620806225
tp_rate: 0.9980333964124664
std_accuracy: 0.006966124656172557
std_precision: 0.010823912671098087
std_recall: 0.0014445277235453182
std_f1_score: 0.006656394068827643
std_fp_rate: 0.012978682219234964
std_tp_rate: 0.0014445277235453182
TP: 982.623
TN: 752.791
FP: 234.65
FN: 1.936
roc_auc: 0.9864996358758933
fpr: [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.0010142  0.0010142  0.0010142
 0.0010142  0.0010142  0.0010142  0.0010142  0.0010142  0.0010142
 0.0010142  0.0010142  0.0010142  0.0010142  0.0010142  0.0010142
 0.0010142  0.0010142  0.0010142  0.0010142  0.0010142  0.0010142
 0.0010142  0.0010142  0.0010142  0.0010142  0.0010142  0.0010142
 0.0010142  0.0010142  0.0010142  0.0010142  0.0010142  0.0010142
 0.0010142  0.0010142  0.0010142  0.0010142  0.0010142  0.0020284
 0.0020284  0.0020284  0.0020284  0.0020284  0.0030426  0.0030426
 0.0030426  0.0030426  0.0030426  0.0030426  0.0030426  0.0030426
 0.0030426  0.0040568  0.0040568  0.0040568  0.0040568  0.0040568
 0.0040568  0.0040568  0.0040568  0.0040568  0.0040568  0.0040568
 0.0040568  0.0040568  0.0040568  0.0040568  0.0040568  0.0040568
 0.0040568  0.0040568  0.0040568  0.00507099 0.00608519 0.00608519
 0.00608519 0.00608519 0.00608519 0.00608519 0.00709939 0.00709939
 0.00709939 0.00709939 0.00709939 0.00709939 0.00709939 0.00811359
 0.00811359 0.00811359 0.00811359 0.00811359 0.00811359 0.00811359
 0.00811359 0.00912779 0.00912779 0.00912779 0.00912779 0.00912779
 0.00912779 0.00912779 0.00912779 0.00912779 0.00912779 0.00912779
 0.00912779 0.00912779 0.00912779 0.00912779 0.00912779 0.00912779
 0.00912779 0.01014199 0.01014199 0.01014199 0.01014199 0.01115619
 0.01115619 0.01115619 0.01115619 0.01115619 0.01115619 0.01115619
 0.01115619 0.01115619 0.01115619 0.01115619 0.01115619 0.01115619
 0.01115619 0.01115619 0.01217039 0.01217039 0.01217039 0.01217039
 0.01217039 0.01318458 0.01318458 0.01318458 0.01318458 0.01318458
 0.01318458 0.01419878 0.01419878 0.01419878 0.01419878 0.01419878
 0.01419878 0.01521298 0.01521298 0.01622718 0.01622718 0.01622718
 0.01724138 0.01724138 0.01724138 0.01724138 0.01724138 0.01724138
 0.01825558 0.02028398 0.02028398 0.02129817 0.02129817 0.02129817
 0.02231237 0.02231237 0.02231237 0.02231237 0.02231237 0.02231237
 0.02231237 0.02231237 0.02332657 0.02332657 0.02332657 0.02535497
 0.02535497 0.02535497 0.02535497 0.02636917 0.02636917 0.02738337
 0.02738337 0.02738337 0.02839757 0.02941176 0.03042596 0.03042596
 0.03144016 0.03144016 0.03346856 0.03346856 0.03346856 0.03346856
 0.03549696 0.03549696 0.03752535 0.03752535 0.03853955 0.03853955
 0.04056795 0.04056795 0.04158215 0.04158215 0.04259635 0.04462475
 0.04462475 0.04563895 0.04563895 0.04665314 0.04766734 0.04766734
 0.04969574 0.04969574 0.05172414 0.05273834 0.05273834 0.05476673
 0.05476673 0.05578093 0.05578093 0.05780933 0.06085193 0.06085193
 0.06186613 0.06186613 0.06389452 0.06592292 0.06592292 0.06693712
 0.06693712 0.06896552 0.06997972 0.06997972 0.07099391 0.07099391
 0.07200811 0.07200811 0.07505071 0.07505071 0.07606491 0.07606491
 0.07809331 0.07809331 0.07910751 0.07910751 0.0841785  0.0841785
 0.0851927  0.0872211  0.0872211  0.09330629 0.09330629 0.09533469
 0.09634888 0.09634888 0.09736308 0.09736308 0.09939148 0.09939148
 0.10141988 0.10141988 0.10243408 0.10243408 0.10446247 0.10446247
 0.10953347 0.10953347 0.11257606 0.11257606 0.11663286 0.11663286
 0.11764706 0.11764706 0.11967546 0.12271805 0.12271805 0.12981744
 0.12981744 0.13184584 0.13387424 0.13387424 0.13793103 0.13793103
 0.13894523 0.13894523 0.14097363 0.14097363 0.15010142 0.15010142
 0.15922921 0.15922921 0.18154158 0.18154158 0.19574037 0.19574037
 0.20791075 0.20791075 0.21399594 0.21399594 0.21501014 0.21501014
 0.21805274 0.21805274 0.22920892 0.22920892 0.23022312 0.23022312
 0.23732252 0.23732252 0.2484787  0.2505071  0.27079108 0.27281947
 0.28296146 0.28296146 0.28701826 0.28904665 0.30628803 0.30831643
 0.31034483 0.31237323 0.31845842 0.32048682 0.35294118 0.35496957
 0.37322515 0.37525355 0.37626775 0.38032454 0.38640974 0.38843813
 0.38843813 0.40872211 0.41075051 0.41176471 0.4137931  0.45334686
 0.45537525 0.46044625 0.46247465 0.46348884 0.46551724 0.47464503
 0.47667343 0.47870183 0.48073022 0.5010142  0.5030426  0.51217039
 0.51419878 0.52231237 0.52434077 0.53245436 0.53448276 0.53752535
 0.53955375 0.56085193 0.56288032 0.59026369 0.59229209 0.59330629
 0.59533469 0.59736308 0.60040568 0.60243408 0.61054767 0.61156187
 0.61764706 0.61866126 0.62271805 0.62880325 0.63083164 0.63286004
 0.63488844 0.63590264 0.63793103 0.63995943 0.64198783 0.65314402
 0.65517241 0.65618661 0.65922921 0.66125761 0.6643002  0.6673428
 0.67139959 0.67342799 0.67647059 0.67849899 0.68154158 0.68559838
 0.68762677 0.68965517 0.69472617 0.69675456 0.69979716 0.70283976
 0.70486815 0.70588235 0.70892495 0.71095335 0.71906694 0.72210953
 0.72413793 0.72616633 0.72819473 0.72920892 0.73123732 0.73935091
 0.74543611 0.7464503  0.7484787  0.75456389 0.75659229 0.76064909
 0.76470588 0.76774848 0.76977688 0.77586207 0.77789047 0.77890467
 0.78093306 0.78194726 0.78803245 0.79006085 0.79208925 0.79716024
 0.79918864 0.80730223 0.80933063 0.81338742 0.81541582 0.81744422
 0.82555781 0.8296146  0.831643   0.8336714  0.8356998  0.836714
 0.83874239 0.84077079 0.84279919 0.84584178 0.84787018 0.84989858
 0.85192698 0.85496957 0.85699797 0.86206897 0.86409736 0.87018256
 0.87221095 0.87931034 0.88235294 0.88640974 0.88843813 0.89046653
 0.89452333 0.89959432 0.90365112 0.93610548 0.94016227 0.94219067
 0.94421907 0.94624746 0.94827586 0.95233266 0.95436105 0.95537525
 0.95740365 0.96146045 0.96348884 0.98985801 0.99188641 1.        ]
tpr: [0.         0.0010142  0.00507099 0.00709939 0.00912779 0.01115619
 0.02434077 0.03245436 0.03651116 0.03955375 0.04563895 0.04665314
 0.05070994 0.05172414 0.05375254 0.06186613 0.06389452 0.06490872
 0.06896552 0.07302231 0.07707911 0.07809331 0.0841785  0.0851927
 0.09330629 0.09432049 0.09837728 0.09939148 0.10141988 0.10243408
 0.10446247 0.10750507 0.10953347 0.11257606 0.11460446 0.11866126
 0.11967546 0.12170385 0.12677485 0.12880325 0.13083164 0.13286004
 0.13387424 0.13590264 0.13691684 0.13995943 0.14097363 0.14705882
 0.14908722 0.15111562 0.15517241 0.15720081 0.15821501 0.16024341
 0.16125761 0.1643002  0.1703854  0.17241379 0.17545639 0.17647059
 0.17951318 0.18154158 0.18458418 0.18661258 0.19066937 0.19675456
 0.19878296 0.20588235 0.20791075 0.21095335 0.21501014 0.21805274
 0.21906694 0.22210953 0.22413793 0.22616633 0.22718053 0.22920892
 0.23225152 0.23427992 0.23529412 0.23935091 0.24137931 0.24340771
 0.2535497  0.25862069 0.26267748 0.26673428 0.26977688 0.27383367
 0.27586207 0.27789047 0.27991886 0.28194726 0.28498986 0.28701826
 0.29918864 0.30121704 0.31237323 0.31643002 0.32555781 0.32758621
 0.3296146  0.3326572  0.34482759 0.34685598 0.36511156 0.36713996
 0.37221095 0.37423935 0.37728195 0.37728195 0.38336714 0.38539554
 0.38640974 0.38843813 0.38945233 0.39148073 0.39452333 0.39655172
 0.39756592 0.39959432 0.40466531 0.40669371 0.40872211 0.41075051
 0.42799189 0.43002028 0.43306288 0.43509128 0.44929006 0.45334686
 0.45537525 0.45943205 0.47160243 0.47363083 0.47768763 0.47971602
 0.48073022 0.48275862 0.48681542 0.48985801 0.4959432  0.4979716
 0.50709939 0.50912779 0.51115619 0.51318458 0.51419878 0.51419878
 0.51724138 0.51825558 0.52231237 0.52332657 0.52434077 0.52941176
 0.53245436 0.53448276 0.53651116 0.53955375 0.54563895 0.54766734
 0.54969574 0.54969574 0.55679513 0.55882353 0.55983773 0.56186613
 0.56288032 0.56490872 0.56693712 0.56896552 0.56997972 0.57200811
 0.57505071 0.57910751 0.5821501  0.5851927  0.5872211  0.59026369
 0.59229209 0.59634888 0.60243408 0.60446247 0.60547667 0.60953347
 0.61156187 0.61359026 0.61561866 0.61663286 0.61663286 0.62170385
 0.62474645 0.62677485 0.62778905 0.62981744 0.63083164 0.63083164
 0.63184584 0.63387424 0.63488844 0.63691684 0.63793103 0.63995943
 0.64198783 0.64198783 0.64908722 0.65111562 0.65517241 0.65821501
 0.65922921 0.66125761 0.66227181 0.6663286  0.6673428  0.6693712
 0.6703854  0.67241379 0.67647059 0.67849899 0.68255578 0.68458418
 0.68661258 0.68864097 0.69269777 0.69472617 0.70385396 0.70385396
 0.70791075 0.71196755 0.71501014 0.71703854 0.72413793 0.72920892
 0.73123732 0.73427992 0.73630832 0.73833671 0.73935091 0.74137931
 0.74340771 0.74543611 0.74543611 0.7484787  0.7515213  0.75659229
 0.75963489 0.76064909 0.76267748 0.76572008 0.77079108 0.77281947
 0.77383367 0.77383367 0.77687627 0.77890467 0.78701826 0.79107505
 0.79310345 0.79310345 0.79918864 0.79918864 0.80730223 0.81135903
 0.81135903 0.81237323 0.81440162 0.81643002 0.82048682 0.82150101
 0.82555781 0.82555781 0.82657201 0.82860041 0.8296146  0.831643
 0.831643   0.84077079 0.84482759 0.84584178 0.84787018 0.85496957
 0.85699797 0.86105477 0.86105477 0.86206897 0.86409736 0.86409736
 0.86916836 0.87119675 0.87626775 0.87626775 0.87931034 0.87931034
 0.88133874 0.89452333 0.89655172 0.89655172 0.89756592 0.89959432
 0.90060852 0.90162272 0.90162272 0.90669371 0.90872211 0.9137931
 0.9137931  0.9148073  0.9148073  0.9188641  0.9188641  0.92089249
 0.92089249 0.92190669 0.92190669 0.92393509 0.92494929 0.92494929
 0.92900609 0.92900609 0.93407708 0.93509128 0.93509128 0.93711968
 0.93711968 0.93914807 0.93914807 0.93914807 0.94117647 0.94117647
 0.94219067 0.94219067 0.94421907 0.94421907 0.94421907 0.94624746
 0.94624746 0.94929006 0.94929006 0.94929006 0.95030426 0.95030426
 0.95131846 0.95131846 0.95233266 0.95334686 0.95334686 0.95740365
 0.95740365 0.95841785 0.95841785 0.95943205 0.95943205 0.96044625
 0.96044625 0.96146045 0.96146045 0.96247465 0.96247465 0.96348884
 0.96348884 0.96348884 0.96450304 0.96450304 0.96551724 0.96551724
 0.96551724 0.96653144 0.96653144 0.96754564 0.96754564 0.96957404
 0.96957404 0.97160243 0.97160243 0.97261663 0.97261663 0.97363083
 0.97363083 0.97667343 0.97667343 0.97768763 0.97768763 0.97971602
 0.97971602 0.98073022 0.98073022 0.98073022 0.98174442 0.98174442
 0.98275862 0.98275862 0.98275862 0.98377282 0.98377282 0.98478702
 0.98478702 0.98580122 0.98580122 0.98681542 0.98681542 0.98782961
 0.98782961 0.98884381 0.98884381 0.98985801 0.98985801 0.99087221
 0.99087221 0.99188641 0.99188641 0.99290061 0.99290061 0.99391481
 0.99391481 0.99492901 0.99492901 0.9959432  0.9959432  0.9969574
 0.9969574  0.9979716  0.9979716  0.9979716  0.9979716  0.9979716
 0.9979716  0.9989858  0.9989858  0.9989858  0.9989858  0.9989858
 0.9989858  0.9989858  0.9989858  0.9989858  0.9989858  0.9989858
 0.9989858  0.9989858  0.9989858  0.9989858  0.9989858  0.9989858
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.        ]
thresholds: [        inf  7.1054688   6.796875    6.78125     6.7617188   6.7382812
  6.6015625   6.5507812   6.5234375   6.515625    6.5         6.4960938
  6.4921875   6.484375    6.4609375   6.4101562   6.40625     6.4023438
  6.3945312   6.3710938   6.359375    6.3554688   6.34375     6.3320312
  6.3125      6.3085938   6.296875    6.2734375   6.2695312   6.2539062
  6.25        6.2382812   6.234375    6.2109375   6.2070312   6.1953125
  6.1875      6.171875    6.1445312   6.140625    6.1328125   6.1289062
  6.125       6.1210938   6.1132812   6.109375    6.1054688   6.0859375
  6.0742188   6.0664062   6.0429688   6.0351562   6.03125     6.0273438
  6.0234375   6.0195312   6.0039062   5.984375    5.9804688   5.9726562
  5.96875     5.9648438   5.9609375   5.9335938   5.9179688   5.8867188
  5.8828125   5.8085938   5.8046875   5.7734375   5.7539062   5.75
  5.7460938   5.7382812   5.7226562   5.6992188   5.6914062   5.6640625
  5.6328125   5.6289062   5.6171875   5.6132812   5.6015625   5.59375
  5.5         5.484375    5.4140625   5.3984375   5.3789062   5.3398438
  5.2851562   5.2773438   5.2617188   5.2539062   5.2070312   5.1992188
  5.0078125   5.          4.7929688   4.7773438   4.5273438   4.5117188
  4.40625     4.3945312   4.0234375   4.0078125   3.7910156   3.78125
  3.7246094   3.7226562   3.6972656   3.6953125   3.6542969   3.6523438
  3.6445312   3.6289062   3.6054688   3.6035156   3.578125    3.5761719
  3.5722656   3.5664062   3.53125     3.5292969   3.5253906   3.5195312
  3.4160156   3.3945312   3.3789062   3.3652344   3.2851562   3.28125
  3.2753906   3.2695312   3.2148438   3.2109375   3.1953125   3.1933594
  3.1875      3.1855469   3.1640625   3.1601562   3.1308594   3.1269531
  3.0859375   3.0761719   3.0664062   3.0605469   3.0585938   3.0546875
  3.0527344   3.0507812   3.0449219   3.0429688   3.0410156   3.0253906
  3.0234375   3.0136719   3.0117188   3.0039062   3.          2.9980469
  2.9824219   2.9804688   2.9394531   2.9375      2.9355469   2.9277344
  2.9257812   2.9238281   2.9160156   2.9140625   2.9101562   2.9082031
  2.90625     2.9003906   2.890625    2.875       2.8730469   2.8710938
  2.8652344   2.8613281   2.8398438   2.8320312   2.8300781   2.8144531
  2.8125      2.8027344   2.8007812   2.7949219   2.7910156   2.7695312
  2.7675781   2.7597656   2.7558594   2.7539062   2.7519531   2.7460938
  2.7421875   2.7324219   2.7304688   2.7265625   2.7226562   2.7207031
  2.7050781   2.703125    2.6757812   2.6738281   2.6601562   2.6582031
  2.65625     2.6542969   2.6523438   2.6484375   2.6386719   2.6367188
  2.6308594   2.6289062   2.6132812   2.6113281   2.6015625   2.5996094
  2.5898438   2.5878906   2.5742188   2.5722656   2.546875    2.5449219
  2.5234375   2.5214844   2.5078125   2.4980469   2.4804688   2.4785156
  2.4667969   2.4648438   2.4609375   2.4589844   2.4550781   2.453125
  2.4414062   2.4355469   2.4277344   2.4140625   2.4101562   2.390625
  2.3886719   2.3769531   2.3730469   2.3691406   2.3398438   2.3378906
  2.3320312   2.328125    2.3183594   2.3164062   2.28125     2.2773438
  2.2675781   2.2597656   2.2304688   2.2285156   2.1835938   2.1796875
  2.1738281   2.171875    2.1699219   2.1621094   2.1542969   2.1523438
  2.1503906   2.1386719   2.1367188   2.125       2.1171875   2.109375
  2.1074219   2.0585938   2.0546875   2.0507812   2.0488281   2.0253906
  2.0195312   1.9980469   1.9951172   1.9931641   1.9833984   1.9716797
  1.9560547   1.9423828   1.9277344   1.9238281   1.9072266   1.90625
  1.9023438   1.8486328   1.8427734   1.8271484   1.8242188   1.8212891
  1.8203125   1.8154297   1.7978516   1.7792969   1.7724609   1.7236328
  1.7177734   1.7148438   1.7021484   1.6660156   1.6601562   1.6523438
  1.6386719   1.6376953   1.6337891   1.6025391   1.5869141   1.5820312
  1.5615234   1.5498047   1.5039062   1.4980469   1.4941406   1.4824219
  1.46875     1.4511719   1.4501953   1.4492188   1.4345703   1.4208984
  1.4140625   1.40625     1.3847656   1.3671875   1.3232422   1.3076172
  1.2929688   1.2617188   1.2607422   1.2373047   1.2304688   1.2294922
  1.2099609   1.1894531   1.1796875   1.1728516   1.1640625   1.1474609
  1.1425781   1.1210938   1.1025391   1.1005859   1.0888672   1.0839844
  1.0722656   1.0546875   1.0449219   1.0439453   0.98583984  0.97753906
  0.96777344  0.9604492   0.9560547   0.9248047   0.90234375  0.90185547
  0.8979492   0.89697266  0.8930664   0.8925781   0.8808594   0.87109375
  0.85302734  0.8417969   0.83740234  0.82958984  0.81396484  0.79833984
  0.76416016  0.7416992   0.7036133   0.69873047  0.67822266  0.6591797
  0.6586914   0.65234375  0.65185547  0.5991211   0.59472656  0.5600586
  0.5488281   0.53759766  0.5288086   0.52734375  0.48217773  0.47973633
  0.46948242  0.46777344  0.44750977  0.44384766  0.38134766  0.38012695
  0.33325195  0.31860352  0.1529541   0.15283203  0.07611084  0.07421875
 -0.01178741 -0.02084351 -0.05584717 -0.05673218 -0.06002808 -0.06134033
 -0.06451416 -0.07110596 -0.11810303 -0.12072754 -0.13049316 -0.15112305
 -0.19836426 -0.19909668 -0.23864746 -0.25341797 -0.37426758 -0.4050293
 -0.47583008 -0.4951172  -0.5214844  -0.5307617  -0.62890625 -0.64404297
 -0.6582031  -0.6586914  -0.6826172  -0.6948242  -0.90283203 -0.9086914
 -1.0029297  -1.015625   -1.0185547  -1.0332031  -1.0625     -1.0644531
 -1.0654297  -1.1396484  -1.1621094  -1.1699219  -1.1845703  -1.3828125
 -1.3886719  -1.4082031  -1.4101562  -1.4150391  -1.4179688  -1.4658203
 -1.4677734  -1.4863281  -1.4951172  -1.5957031  -1.6005859  -1.6318359
 -1.6396484  -1.6708984  -1.6806641  -1.7373047  -1.7470703  -1.7763672
 -1.7783203  -1.921875   -1.9257812  -2.03125    -2.0332031  -2.0390625
 -2.0429688  -2.0546875  -2.0566406  -2.0664062  -2.0761719  -2.078125
 -2.0917969  -2.09375    -2.1152344  -2.140625   -2.1464844  -2.1542969
 -2.1582031  -2.1640625  -2.1679688  -2.1914062  -2.1953125  -2.2558594
 -2.2578125  -2.2636719  -2.2695312  -2.2714844  -2.2890625  -2.2910156
 -2.3398438  -2.3457031  -2.3632812  -2.3652344  -2.3808594  -2.3867188
 -2.3945312  -2.3984375  -2.4140625  -2.4160156  -2.4355469  -2.4375
 -2.4394531  -2.4433594  -2.4453125  -2.453125   -2.5058594  -2.5078125
 -2.5097656  -2.5136719  -2.5175781  -2.5253906  -2.5410156  -2.5625
 -2.5761719  -2.5839844  -2.5878906  -2.6171875  -2.6191406  -2.6367188
 -2.65625    -2.6816406  -2.6894531  -2.7128906  -2.7148438  -2.7246094
 -2.7304688  -2.7324219  -2.7460938  -2.75       -2.7519531  -2.7734375
 -2.7773438  -2.8144531  -2.8164062  -2.8300781  -2.84375    -2.859375
 -2.8710938  -2.8925781  -2.9003906  -2.9101562  -2.9160156  -2.9296875
 -2.9335938  -2.9375     -2.9433594  -2.953125   -2.9550781  -2.9746094
 -2.9824219  -2.9921875  -2.9941406  -3.0214844  -3.0253906  -3.0585938
 -3.0605469  -3.0957031  -3.0976562  -3.109375   -3.1152344  -3.1191406
 -3.1269531  -3.1445312  -3.1601562  -3.3222656  -3.3261719  -3.3359375
 -3.3457031  -3.3632812  -3.3671875  -3.3964844  -3.4023438  -3.4082031
 -3.4101562  -3.4511719  -3.4589844  -3.71875    -3.7285156  -4.15625   ]
