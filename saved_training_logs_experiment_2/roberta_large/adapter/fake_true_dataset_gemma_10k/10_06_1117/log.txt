log_loss_steps: 208
eval_steps: 512
check_degradation: 0
----------------Epoch 1/1----------------
Epoch 1/1, Loss after 192 samples: 0.7202
Epoch 1/1, Loss after 400 samples: 0.7044
Mean accuracy: 0.6265, std: 0.0111, lower bound: 0.6047, upper bound: 0.6476 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 496 samples: 0.6264 with eval loss: 0.6766
Best model with eval loss 0.676632837903115 and eval accuracy 0.6263902932254802 with 496 samples seen is saved
Epoch 1/1, Loss after 608 samples: 0.6872
Epoch 1/1, Loss after 816 samples: 0.6624
Mean accuracy: 0.7930, std: 0.0091, lower bound: 0.7745, upper bound: 0.8099 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 1008 samples: 0.7932 with eval loss: 0.5280
Best model with eval loss 0.5280492046187001 and eval accuracy 0.7932254802831142 with 1008 samples seen is saved
Epoch 1/1, Loss after 1024 samples: 0.6185
Epoch 1/1, Loss after 1232 samples: 0.5539
Epoch 1/1, Loss after 1440 samples: 0.4949
Mean accuracy: 0.8439, std: 0.0082, lower bound: 0.8281, upper bound: 0.8600 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 1520 samples: 0.8438 with eval loss: 0.3856
Best model with eval loss 0.38555691775775724 and eval accuracy 0.8437815975733064 with 1520 samples seen is saved
Epoch 1/1, Loss after 1648 samples: 0.4794
Epoch 1/1, Loss after 1856 samples: 0.4406
Mean accuracy: 0.8077, std: 0.0090, lower bound: 0.7907, upper bound: 0.8246 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 2032 samples: 0.8079 with eval loss: 0.3866
Epoch 1/1, Loss after 2064 samples: 0.3456
Epoch 1/1, Loss after 2272 samples: 0.3574
Epoch 1/1, Loss after 2480 samples: 0.3445
Mean accuracy: 0.7867, std: 0.0089, lower bound: 0.7695, upper bound: 0.8043 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 2544 samples: 0.7867 with eval loss: 0.4706
Epoch 1/1, Loss after 2688 samples: 0.2848
Epoch 1/1, Loss after 2896 samples: 0.2327
Mean accuracy: 0.8267, std: 0.0082, lower bound: 0.8104, upper bound: 0.8418 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 3056 samples: 0.8266 with eval loss: 0.4034
Epoch 1/1, Loss after 3104 samples: 0.2585
Epoch 1/1, Loss after 3312 samples: 0.2694
Epoch 1/1, Loss after 3520 samples: 0.2681
Mean accuracy: 0.8812, std: 0.0071, lower bound: 0.8665, upper bound: 0.8943 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 3568 samples: 0.8812 with eval loss: 0.3158
Best model with eval loss 0.3157715768583359 and eval accuracy 0.8811931243680485 with 3568 samples seen is saved
Epoch 1/1, Loss after 3728 samples: 0.2266
Epoch 1/1, Loss after 3936 samples: 0.2338
Mean accuracy: 0.8648, std: 0.0075, lower bound: 0.8498, upper bound: 0.8797 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 4080 samples: 0.8650 with eval loss: 0.2964
Best model with eval loss 0.2964024889853693 and eval accuracy 0.8650151668351871 with 4080 samples seen is saved
Epoch 1/1, Loss after 4144 samples: 0.3797
Epoch 1/1, Loss after 4352 samples: 0.2533
Epoch 1/1, Loss after 4560 samples: 0.2044
Mean accuracy: 0.8420, std: 0.0079, lower bound: 0.8266, upper bound: 0.8569 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 4592 samples: 0.8423 with eval loss: 0.4284
Epoch 1/1, Loss after 4768 samples: 0.2213
Epoch 1/1, Loss after 4976 samples: 0.2744
Mean accuracy: 0.8874, std: 0.0072, lower bound: 0.8736, upper bound: 0.9014 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 5104 samples: 0.8873 with eval loss: 0.3278
Epoch 1/1, Loss after 5184 samples: 0.1894
Epoch 1/1, Loss after 5392 samples: 0.2209
Epoch 1/1, Loss after 5600 samples: 0.2310
Mean accuracy: 0.9024, std: 0.0067, lower bound: 0.8893, upper bound: 0.9151 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 5616 samples: 0.9024 with eval loss: 0.2711
Best model with eval loss 0.27109611118512766 and eval accuracy 0.9024266936299292 with 5616 samples seen is saved
Epoch 1/1, Loss after 5808 samples: 0.2199
Epoch 1/1, Loss after 6016 samples: 0.1447
Mean accuracy: 0.9287, std: 0.0060, lower bound: 0.9166, upper bound: 0.9399 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 6128 samples: 0.9287 with eval loss: 0.1884
Best model with eval loss 0.18839387319261028 and eval accuracy 0.9287158746208292 with 6128 samples seen is saved
Epoch 1/1, Loss after 6224 samples: 0.2851
Epoch 1/1, Loss after 6432 samples: 0.2640
Epoch 1/1, Loss after 6640 samples: 0.1826
Mean accuracy: 0.8888, std: 0.0068, lower bound: 0.8756, upper bound: 0.9024 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 6640 samples: 0.8883 with eval loss: 0.3107
Epoch 1/1, Loss after 6848 samples: 0.1679
Epoch 1/1, Loss after 7056 samples: 0.1594
Mean accuracy: 0.9347, std: 0.0058, lower bound: 0.9236, upper bound: 0.9464 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 7152 samples: 0.9348 with eval loss: 0.1925
Epoch 1/1, Loss after 7264 samples: 0.2294
Epoch 1/1, Loss after 7472 samples: 0.2365
Mean accuracy: 0.9325, std: 0.0056, lower bound: 0.9221, upper bound: 0.9429 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 7664 samples: 0.9323 with eval loss: 0.1836
Best model with eval loss 0.18363190165931179 and eval accuracy 0.9322548028311426 with 7664 samples seen is saved
Epoch 1/1, Loss after 7680 samples: 0.1547
Epoch 1/1, Loss after 7888 samples: 0.1916
Epoch 1/1, Loss after 8096 samples: 0.1101
Mean accuracy: 0.9204, std: 0.0061, lower bound: 0.9080, upper bound: 0.9312 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 8176 samples: 0.9201 with eval loss: 0.2406
Epoch 1/1, Loss after 8304 samples: 0.0829
Epoch 1/1, Loss after 8512 samples: 0.1100
Mean accuracy: 0.8940, std: 0.0071, lower bound: 0.8797, upper bound: 0.9070 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 8688 samples: 0.8943 with eval loss: 0.3799
Epoch 1/1, Loss after 8720 samples: 0.1166
Epoch 1/1, Loss after 8928 samples: 0.1690
Epoch 1/1, Loss after 9136 samples: 0.1108
Mean accuracy: 0.9183, std: 0.0060, lower bound: 0.9065, upper bound: 0.9297 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 9200 samples: 0.9181 with eval loss: 0.2461
Epoch 1/1, Loss after 9344 samples: 0.1313
Epoch 1/1, Loss after 9552 samples: 0.1394
Mean accuracy: 0.9283, std: 0.0059, lower bound: 0.9166, upper bound: 0.9398 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 9712 samples: 0.9282 with eval loss: 0.1985
Epoch 1/1, Loss after 9760 samples: 0.1491
Epoch 1/1, Loss after 9968 samples: 0.1628
Epoch 1/1, Loss after 10176 samples: 0.1104
Mean accuracy: 0.8958, std: 0.0069, lower bound: 0.8827, upper bound: 0.9085 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 10224 samples: 0.8959 with eval loss: 0.3087
Epoch 1/1, Loss after 10384 samples: 0.1014
Epoch 1/1, Loss after 10592 samples: 0.1333
Mean accuracy: 0.9011, std: 0.0066, lower bound: 0.8873, upper bound: 0.9135 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 10736 samples: 0.9009 with eval loss: 0.2951
Epoch 1/1, Loss after 10800 samples: 0.1311
Epoch 1/1, Loss after 11008 samples: 0.0998
Epoch 1/1, Loss after 11216 samples: 0.2264
Mean accuracy: 0.9252, std: 0.0060, lower bound: 0.9135, upper bound: 0.9368 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 11248 samples: 0.9257 with eval loss: 0.2191
Epoch 1/1, Loss after 11424 samples: 0.1369
Epoch 1/1, Loss after 11632 samples: 0.1248
Mean accuracy: 0.9343, std: 0.0056, lower bound: 0.9226, upper bound: 0.9444 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 11760 samples: 0.9343 with eval loss: 0.1831
Best model with eval loss 0.1831342922223191 and eval accuracy 0.9342770475227502 with 11760 samples seen is saved
Epoch 1/1, Loss after 11840 samples: 0.1141
Epoch 1/1, Loss after 12048 samples: 0.2313
Epoch 1/1, Loss after 12256 samples: 0.0966
Mean accuracy: 0.8808, std: 0.0072, lower bound: 0.8665, upper bound: 0.8943 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 12272 samples: 0.8812 with eval loss: 0.3675
Epoch 1/1, Loss after 12464 samples: 0.0799
Epoch 1/1, Loss after 12672 samples: 0.1281
Mean accuracy: 0.9198, std: 0.0063, lower bound: 0.9080, upper bound: 0.9317 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 12784 samples: 0.9201 with eval loss: 0.2490
Epoch 1/1, Loss after 12880 samples: 0.1262
Epoch 1/1, Loss after 13088 samples: 0.1005
Epoch 1/1, Loss after 13296 samples: 0.1701
Mean accuracy: 0.9278, std: 0.0059, lower bound: 0.9161, upper bound: 0.9388 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 13296 samples: 0.9277 with eval loss: 0.2160
Epoch 1/1, Loss after 13504 samples: 0.0953
Epoch 1/1, Loss after 13712 samples: 0.1312
Mean accuracy: 0.9078, std: 0.0064, lower bound: 0.8953, upper bound: 0.9201 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 13808 samples: 0.9080 with eval loss: 0.3075
Epoch 1/1, Loss after 13920 samples: 0.0950
Epoch 1/1, Loss after 14128 samples: 0.1244
Mean accuracy: 0.9125, std: 0.0065, lower bound: 0.8994, upper bound: 0.9252 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 14320 samples: 0.9125 with eval loss: 0.2892
Epoch 1/1, Loss after 14336 samples: 0.1291
Epoch 1/1, Loss after 14544 samples: 0.1052
Epoch 1/1, Loss after 14752 samples: 0.1603
Mean accuracy: 0.8991, std: 0.0069, lower bound: 0.8852, upper bound: 0.9120 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 14832 samples: 0.8994 with eval loss: 0.3332
Epoch 1/1, Loss after 14960 samples: 0.1242
Epoch 1/1, Loss after 15168 samples: 0.1128
Mean accuracy: 0.9055, std: 0.0064, lower bound: 0.8928, upper bound: 0.9186 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 15344 samples: 0.9055 with eval loss: 0.3048
Epoch 1/1, Loss after 15376 samples: 0.1178
Epoch 1/1, Loss after 15584 samples: 0.1707
Epoch 1/1, Loss after 15792 samples: 0.1500
Training signal is False, stopping training
Training finished
Best model: {'eval_acc': 0.9342770475227502, 'nb_samples': 11760, 'eval_loss': 0.1831342922223191}
Training loss logs: [{'samples': 192, 'loss': 0.7202465350811298}, {'samples': 400, 'loss': 0.704376220703125}, {'samples': 608, 'loss': 0.6871560903695914}, {'samples': 816, 'loss': 0.6623541025015024}, {'samples': 1024, 'loss': 0.6184739332932693}, {'samples': 1232, 'loss': 0.5539032862736628}, {'samples': 1440, 'loss': 0.49491411447525024}, {'samples': 1648, 'loss': 0.4793922992853018}, {'samples': 1856, 'loss': 0.440577683540491}, {'samples': 2064, 'loss': 0.3456113659418546}, {'samples': 2272, 'loss': 0.3574251096982222}, {'samples': 2480, 'loss': 0.34448546171188354}, {'samples': 2688, 'loss': 0.2847813138594994}, {'samples': 2896, 'loss': 0.23272849390139946}, {'samples': 3104, 'loss': 0.2585190614828697}, {'samples': 3312, 'loss': 0.26937924554714787}, {'samples': 3520, 'loss': 0.2681007648889835}, {'samples': 3728, 'loss': 0.22657992748113778}, {'samples': 3936, 'loss': 0.23379558955247587}, {'samples': 4144, 'loss': 0.3796536188859206}, {'samples': 4352, 'loss': 0.25332166197208256}, {'samples': 4560, 'loss': 0.20442242920398712}, {'samples': 4768, 'loss': 0.22126623758902916}, {'samples': 4976, 'loss': 0.27443385926576763}, {'samples': 5184, 'loss': 0.1894262094910328}, {'samples': 5392, 'loss': 0.22090275299090606}, {'samples': 5600, 'loss': 0.23104843210715514}, {'samples': 5808, 'loss': 0.2198566095187114}, {'samples': 6016, 'loss': 0.14465602544637826}, {'samples': 6224, 'loss': 0.2851025863335683}, {'samples': 6432, 'loss': 0.2640399531676219}, {'samples': 6640, 'loss': 0.182572463957163}, {'samples': 6848, 'loss': 0.1678638458251953}, {'samples': 7056, 'loss': 0.1594457528912104}, {'samples': 7264, 'loss': 0.22935441136360168}, {'samples': 7472, 'loss': 0.2365017430140422}, {'samples': 7680, 'loss': 0.15468592655200225}, {'samples': 7888, 'loss': 0.19161461809506783}, {'samples': 8096, 'loss': 0.11011759421000114}, {'samples': 8304, 'loss': 0.08290564612700389}, {'samples': 8512, 'loss': 0.11002837178798822}, {'samples': 8720, 'loss': 0.1165537157884011}, {'samples': 8928, 'loss': 0.16900247106185326}, {'samples': 9136, 'loss': 0.11077895416663243}, {'samples': 9344, 'loss': 0.1312572228220793}, {'samples': 9552, 'loss': 0.13936480650534996}, {'samples': 9760, 'loss': 0.1490732844059284}, {'samples': 9968, 'loss': 0.16277686678446257}, {'samples': 10176, 'loss': 0.11038068452706704}, {'samples': 10384, 'loss': 0.10137199610471725}, {'samples': 10592, 'loss': 0.1332936264001406}, {'samples': 10800, 'loss': 0.131076285472283}, {'samples': 11008, 'loss': 0.09979790563766773}, {'samples': 11216, 'loss': 0.2264267888206702}, {'samples': 11424, 'loss': 0.13690406886430886}, {'samples': 11632, 'loss': 0.12476107879326893}, {'samples': 11840, 'loss': 0.11408101881925876}, {'samples': 12048, 'loss': 0.2312894990810981}, {'samples': 12256, 'loss': 0.09661805228545116}, {'samples': 12464, 'loss': 0.07989859351745018}, {'samples': 12672, 'loss': 0.12811564596799704}, {'samples': 12880, 'loss': 0.1261714037794333}, {'samples': 13088, 'loss': 0.10051310979402982}, {'samples': 13296, 'loss': 0.17006499091019997}, {'samples': 13504, 'loss': 0.0953085244848178}, {'samples': 13712, 'loss': 0.13122322066472128}, {'samples': 13920, 'loss': 0.0949621876844993}, {'samples': 14128, 'loss': 0.12443687652166073}, {'samples': 14336, 'loss': 0.12913449796346518}, {'samples': 14544, 'loss': 0.10523694925583325}, {'samples': 14752, 'loss': 0.16026557638094976}, {'samples': 14960, 'loss': 0.12415700348523948}, {'samples': 15168, 'loss': 0.11277454575667015}, {'samples': 15376, 'loss': 0.11775540216610982}, {'samples': 15584, 'loss': 0.17074482486798212}, {'samples': 15792, 'loss': 0.1499546353633587}]
Evaluation accuracy logs: [{'samples': 496, 'accuracy': 0.6264732052578362, 'std': 0.011090416344104759, 'lower_bound': 0.6046511627906976, 'upper_bound': 0.6476365015166835}, {'samples': 1008, 'accuracy': 0.7929524772497473, 'std': 0.009061574439421438, 'lower_bound': 0.7745197168857432, 'upper_bound': 0.8099089989888777}, {'samples': 1520, 'accuracy': 0.843927199191102, 'std': 0.008187641721422124, 'lower_bound': 0.8280965621840243, 'upper_bound': 0.8599595551061678}, {'samples': 2032, 'accuracy': 0.8077042467138524, 'std': 0.008973371573929034, 'lower_bound': 0.7906976744186046, 'upper_bound': 0.8245702730030333}, {'samples': 2544, 'accuracy': 0.7866764408493427, 'std': 0.00889908134826493, 'lower_bound': 0.7694641051567239, 'upper_bound': 0.8043478260869565}, {'samples': 3056, 'accuracy': 0.826721941354904, 'std': 0.00819817698681496, 'lower_bound': 0.810401921132457, 'upper_bound': 0.8417719919110213}, {'samples': 3568, 'accuracy': 0.8811764408493428, 'std': 0.007050900405975754, 'lower_bound': 0.8665318503538928, 'upper_bound': 0.8943377148634984}, {'samples': 4080, 'accuracy': 0.8648119312436805, 'std': 0.007508398671576064, 'lower_bound': 0.8498483316481295, 'upper_bound': 0.8796764408493428}, {'samples': 4592, 'accuracy': 0.8419681496461071, 'std': 0.00787842808982443, 'lower_bound': 0.8265798786653185, 'upper_bound': 0.8569388270980789}, {'samples': 5104, 'accuracy': 0.8873584428715875, 'std': 0.007227597284540805, 'lower_bound': 0.8736097067745198, 'upper_bound': 0.901428210313448}, {'samples': 5616, 'accuracy': 0.9024256825075835, 'std': 0.006669308037061854, 'lower_bound': 0.8892821031344793, 'upper_bound': 0.9150657229524772}, {'samples': 6128, 'accuracy': 0.9287355915065723, 'std': 0.006027881127906371, 'lower_bound': 0.916582406471183, 'upper_bound': 0.9398508594539939}, {'samples': 6640, 'accuracy': 0.8887730030333671, 'std': 0.006787315515434102, 'lower_bound': 0.8756319514661274, 'upper_bound': 0.9024266936299292}, {'samples': 7152, 'accuracy': 0.9346850353892822, 'std': 0.005846739031583503, 'lower_bound': 0.9236476238624873, 'upper_bound': 0.9464105156723963}, {'samples': 7664, 'accuracy': 0.9324959555106167, 'std': 0.005624864408176913, 'lower_bound': 0.9221435793731041, 'upper_bound': 0.9428715874620829}, {'samples': 8176, 'accuracy': 0.9204357937310415, 'std': 0.00608446843390776, 'lower_bound': 0.9079878665318504, 'upper_bound': 0.9312436804853387}, {'samples': 8688, 'accuracy': 0.8940045500505561, 'std': 0.0070676786385336565, 'lower_bound': 0.8796764408493428, 'upper_bound': 0.906989383215369}, {'samples': 9200, 'accuracy': 0.9183104145601617, 'std': 0.006016035124184942, 'lower_bound': 0.9064711830131446, 'upper_bound': 0.929726996966633}, {'samples': 9712, 'accuracy': 0.9283008088978766, 'std': 0.00593802977540427, 'lower_bound': 0.916582406471183, 'upper_bound': 0.9398382204246714}, {'samples': 10224, 'accuracy': 0.8957856420626896, 'std': 0.006869486339275323, 'lower_bound': 0.8827098078867543, 'upper_bound': 0.9084934277047523}, {'samples': 10736, 'accuracy': 0.9011102123356927, 'std': 0.0066056009179434215, 'lower_bound': 0.8872598584428716, 'upper_bound': 0.9135490394337715}, {'samples': 11248, 'accuracy': 0.9251946410515672, 'std': 0.0059526640090326105, 'lower_bound': 0.9135490394337715, 'upper_bound': 0.9368174924165824}, {'samples': 11760, 'accuracy': 0.9342714863498484, 'std': 0.005575743367626379, 'lower_bound': 0.9226491405460061, 'upper_bound': 0.9444009100101113}, {'samples': 12272, 'accuracy': 0.8807598584428715, 'std': 0.007175524156212114, 'lower_bound': 0.8665318503538928, 'upper_bound': 0.8943377148634984}, {'samples': 12784, 'accuracy': 0.9198073811931244, 'std': 0.006277745867638783, 'lower_bound': 0.9079752275025278, 'upper_bound': 0.9317492416582407}, {'samples': 13296, 'accuracy': 0.9277724974721941, 'std': 0.005949706366617568, 'lower_bound': 0.9160642062689585, 'upper_bound': 0.9388397371081901}, {'samples': 13808, 'accuracy': 0.9077952477249749, 'std': 0.006438631456111575, 'lower_bound': 0.8953361981799798, 'upper_bound': 0.9201213346814965}, {'samples': 14320, 'accuracy': 0.9125065722952477, 'std': 0.006523701431531517, 'lower_bound': 0.8993933265925177, 'upper_bound': 0.9251769464105156}, {'samples': 14832, 'accuracy': 0.8991258847320526, 'std': 0.006933843993358784, 'lower_bound': 0.8852249747219413, 'upper_bound': 0.9120323559150657}, {'samples': 15344, 'accuracy': 0.9055313447927199, 'std': 0.00643694968637754, 'lower_bound': 0.8928210313447927, 'upper_bound': 0.9186046511627907}]
Evaluating the best model on the test set of dataset ./fake_true_datasets/fake_true_dataset_gemma_10k...
Test metrics:
accuracy: 0.9047052578361982
precision: 0.8428546595926987
recall: 0.9949434673431814
f1_score: 0.9125708363475917
fp_rate: 0.18557559320104186
tp_rate: 0.9949434673431814
std_accuracy: 0.00647295916521627
std_precision: 0.010339684827414602
std_recall: 0.0022981499992966005
std_f1_score: 0.006160161493584
std_fp_rate: 0.012338082274400159
std_tp_rate: 0.0022981499992966005
TP: 984.133
TN: 805.374
FP: 183.49
FN: 5.003
roc_auc: 0.9900528666698701
fpr: [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00101112 0.00101112 0.00101112 0.00101112
 0.00101112 0.00101112 0.00101112 0.00101112 0.00202224 0.00202224
 0.00202224 0.00202224 0.00202224 0.00202224 0.00202224 0.00202224
 0.00202224 0.00202224 0.00202224 0.00202224 0.00202224 0.00202224
 0.00202224 0.00202224 0.00202224 0.00202224 0.00202224 0.00202224
 0.00202224 0.00202224 0.00202224 0.00202224 0.00202224 0.00202224
 0.00202224 0.00202224 0.00202224 0.00202224 0.00202224 0.00202224
 0.00202224 0.00202224 0.00202224 0.00202224 0.00202224 0.00202224
 0.00202224 0.00202224 0.00202224 0.00202224 0.00202224 0.00202224
 0.00202224 0.00303337 0.00303337 0.00303337 0.00303337 0.00303337
 0.00303337 0.00303337 0.00303337 0.00303337 0.00303337 0.00303337
 0.00303337 0.00303337 0.00404449 0.00404449 0.00404449 0.00404449
 0.00505561 0.00505561 0.00505561 0.00505561 0.00505561 0.00505561
 0.00606673 0.00606673 0.00707786 0.00707786 0.00707786 0.00707786
 0.00808898 0.00808898 0.0091001  0.0091001  0.0091001  0.0091001
 0.01011122 0.01011122 0.01011122 0.01112235 0.01112235 0.01112235
 0.01112235 0.01112235 0.01112235 0.01213347 0.01213347 0.01314459
 0.01314459 0.01415571 0.01516684 0.01516684 0.01718908 0.01718908
 0.0182002  0.0182002  0.01921132 0.01921132 0.02022245 0.02022245
 0.02022245 0.02022245 0.02022245 0.02022245 0.02123357 0.02123357
 0.02224469 0.02224469 0.02224469 0.02224469 0.02325581 0.02325581
 0.02426694 0.02426694 0.02527806 0.02527806 0.02527806 0.02628918
 0.02628918 0.02628918 0.02628918 0.0273003  0.0273003  0.02831143
 0.02831143 0.02831143 0.02831143 0.02831143 0.02932255 0.02932255
 0.03033367 0.03033367 0.03033367 0.03033367 0.03134479 0.03235592
 0.03437816 0.03437816 0.03538928 0.03538928 0.0364004  0.0364004
 0.03943377 0.03943377 0.04044489 0.04044489 0.04145602 0.04145602
 0.04246714 0.04246714 0.04347826 0.04347826 0.04448938 0.04448938
 0.04651163 0.04651163 0.04853387 0.05257836 0.05257836 0.05358948
 0.05561173 0.05561173 0.05763397 0.05763397 0.05965622 0.05965622
 0.06370071 0.06572295 0.06875632 0.06875632 0.07381193 0.07381193
 0.07482305 0.07482305 0.07583418 0.07583418 0.0768453  0.0768453
 0.07785642 0.07886754 0.07987867 0.07987867 0.08088979 0.08088979
 0.08190091 0.08190091 0.0859454  0.0859454  0.08796764 0.09100101
 0.09100101 0.09201213 0.09201213 0.0950455  0.0950455  0.09706775
 0.09706775 0.10111223 0.10111223 0.10819009 0.10819009 0.1132457
 0.1132457  0.12537917 0.12537917 0.12740142 0.12740142 0.12841254
 0.12841254 0.13245703 0.13447927 0.13751264 0.13751264 0.14256825
 0.14256825 0.17290192 0.17290192 0.18604651 0.18604651 0.19615774
 0.19615774 0.22548028 0.22548028 0.25176946 0.25176946 0.26794742
 0.27199191 0.30232558 0.30434783 0.30535895 0.30738119 0.32254803
 0.32457027 0.32861476 0.33063701 0.33670374 0.34074823 0.35085945
 0.3528817  0.35389282 0.35389282 0.35490394 0.35793731 0.3710819
 0.37310415 0.39332659 0.39534884 0.40546006 0.40546006 0.43983822
 0.44186047 0.44388271 0.44590495 0.45096057 0.45298281 0.45601618
 0.45904954 0.46107179 0.46309403 0.46511628 0.47017189 0.47623862
 0.48230536 0.4843276  0.48533873 0.48736097 0.48938322 0.49140546
 0.49646107 0.50050556 0.50252781 0.50556117 0.50758342 0.50960566
 0.51061678 0.51263903 0.5156724  0.51769464 0.52275025 0.5247725
 0.53589484 0.53791709 0.54095046 0.54499494 0.54802831 0.5520728
 0.55510617 0.55813953 0.56016178 0.56723964 0.56926188 0.58240647
 0.58442872 0.58645096 0.58847321 0.59049545 0.59251769 0.59555106
 0.59757331 0.6016178  0.60364004 0.60465116 0.60667341 0.60768453
 0.60970677 0.62588473 0.62790698 0.63498483 0.63700708 0.64004044
 0.64307381 0.64509606 0.64610718 0.64812942 0.65318504 0.65520728
 0.65722952 0.65925177 0.6653185  0.66734075 0.66835187 0.67138524
 0.67745197 0.68149646 0.6966633  0.69868554 0.70273003 0.70879676
 0.71284125 0.7148635  0.73104146 0.7330637  0.7421638  0.74620829
 0.74823054 0.75025278 0.75328615 0.75935288 0.76137513 0.76339737
 0.76541962 0.76744186 0.76946411 0.77350859 0.77553084 0.77856421
 0.78058645 0.78463094 0.78867543 0.78968655 0.7917088  0.79271992
 0.79474216 0.79676441 0.79878665 0.80687563 0.80889788 0.81294237
 0.81698686 0.82406471 0.82608696 0.8372093  0.84024267 0.84327604
 0.84529828 0.84833165 0.85035389 0.8554095  0.85743175 0.86552073
 0.86754297 0.87057634 0.87259858 0.87462083 0.87664307 0.87967644
 0.88169869 0.90091001 0.90293225 0.91304348 0.91506572 0.92517695
 0.92719919 0.9322548  0.93528817 0.93731041 0.9413549  0.94337715
 0.95753286 0.95955511 0.96258847 0.96461072 0.96764408 0.96966633
 0.97472194 0.97674419 0.9817998  0.98382204 0.99797776 1.        ]
tpr: [0.         0.00101112 0.01516684 0.01718908 0.02628918 0.02932255
 0.03336704 0.03538928 0.03741153 0.03943377 0.04246714 0.04853387
 0.05055612 0.05257836 0.05763397 0.05965622 0.06066734 0.06268959
 0.06471183 0.06875632 0.06976744 0.07178969 0.07583418 0.0768453
 0.08088979 0.08190091 0.08392315 0.08998989 0.09403438 0.09807887
 0.10212336 0.10313448 0.10515672 0.11122346 0.1132457  0.11425683
 0.11729019 0.11931244 0.12436805 0.12639029 0.12942366 0.13346815
 0.13549039 0.13852376 0.13953488 0.14155713 0.1445905  0.15065723
 0.15267947 0.15672396 0.15975733 0.16177958 0.16481294 0.16683519
 0.16986855 0.17087968 0.17391304 0.17795753 0.1809909  0.18200202
 0.18402427 0.18604651 0.18806876 0.18907988 0.19110212 0.19312437
 0.19514661 0.1991911  0.20121335 0.2082912  0.21132457 0.21334681
 0.21941355 0.22143579 0.22244692 0.22548028 0.22952477 0.23053589
 0.23255814 0.23660263 0.23761375 0.239636   0.24266936 0.24469161
 0.24671385 0.2487361  0.24974722 0.25682508 0.25884732 0.26188069
 0.26289181 0.2669363  0.28210313 0.28311426 0.28715875 0.29322548
 0.2942366  0.29625885 0.30434783 0.30637007 0.31142568 0.3124368
 0.31648129 0.31850354 0.32052578 0.32153691 0.32355915 0.3255814
 0.32962588 0.33265925 0.3346815  0.33569262 0.33872599 0.34277048
 0.3437816  0.34580384 0.34782609 0.34984833 0.35085945 0.3528817
 0.35591507 0.35692619 0.35995956 0.36097068 0.36501517 0.36703741
 0.37007078 0.3710819  0.37310415 0.37613751 0.37815976 0.38220425
 0.38321537 0.38725986 0.3892821  0.39029323 0.39433771 0.39635996
 0.39939333 0.40343782 0.40546006 0.40748231 0.41051567 0.41253792
 0.41557128 0.41961577 0.4206269  0.42264914 0.42669363 0.42770475
 0.43073812 0.4388271  0.43983822 0.44186047 0.44691608 0.44893832
 0.45197169 0.45399393 0.4570273  0.46006067 0.46107179 0.46916077
 0.47320526 0.47724975 0.47927199 0.48129424 0.4843276  0.48634985
 0.48938322 0.4934277  0.49747219 0.50252781 0.50455005 0.50758342
 0.50960566 0.51061678 0.51365015 0.5156724  0.51870576 0.52072801
 0.52275025 0.52679474 0.53083923 0.5338726  0.53589484 0.53690597
 0.53993933 0.54398382 0.54499494 0.54701719 0.54903943 0.55308392
 0.55510617 0.55712841 0.56016178 0.56218402 0.56420627 0.56622851
 0.56825076 0.570273   0.57229525 0.57330637 0.57735086 0.5793731
 0.58240647 0.58442872 0.58543984 0.58746208 0.58847321 0.59049545
 0.59555106 0.59757331 0.59858443 0.6016178  0.60364004 0.60566229
 0.60667341 0.6107179  0.61172902 0.61779575 0.62082912 0.62386249
 0.62487361 0.62790698 0.62992922 0.63296259 0.63397371 0.63599596
 0.63700708 0.64105157 0.64509606 0.64610718 0.64812942 0.65015167
 0.65217391 0.65722952 0.65925177 0.66835187 0.67037412 0.67138524
 0.6744186  0.67644085 0.68149646 0.68250758 0.68452983 0.6875632
 0.69160768 0.69261881 0.69464105 0.69565217 0.69969666 0.7057634
 0.70778564 0.70778564 0.70980789 0.71284125 0.71688574 0.71789687
 0.71991911 0.72295248 0.72497472 0.72800809 0.73003033 0.73104146
 0.7330637  0.73811931 0.73811931 0.74014156 0.74317492 0.74721941
 0.74721941 0.7512639  0.75328615 0.75429727 0.75631951 0.76238625
 0.76238625 0.76744186 0.76744186 0.76845298 0.77047523 0.77249747
 0.77249747 0.78564206 0.78564206 0.79474216 0.79676441 0.79777553
 0.79777553 0.79979778 0.8008089  0.80283114 0.81092012 0.81496461
 0.82305359 0.82507583 0.82608696 0.82709808 0.83013145 0.83013145
 0.83215369 0.83215369 0.83316481 0.83923155 0.83923155 0.84024267
 0.84024267 0.84327604 0.84327604 0.84428716 0.84428716 0.8463094
 0.84833165 0.84934277 0.85136502 0.8554095  0.8554095  0.86046512
 0.86046512 0.86248736 0.86450961 0.86552073 0.86653185 0.87057634
 0.87057634 0.87259858 0.87259858 0.87360971 0.87563195 0.87563195
 0.88169869 0.88372093 0.88473205 0.88473205 0.89484328 0.89484328
 0.89686552 0.90596562 0.90798787 0.91001011 0.91001011 0.91304348
 0.91304348 0.91607685 0.91809909 0.91911021 0.91911021 0.92012133
 0.92012133 0.92416582 0.92416582 0.92922144 0.92922144 0.93023256
 0.93023256 0.93124368 0.93124368 0.93427705 0.93427705 0.93933266
 0.93933266 0.94742164 0.94742164 0.95146613 0.95146613 0.95247725
 0.95247725 0.95348837 0.95348837 0.95348837 0.95854398 0.95955511
 0.95955511 0.96056623 0.96056623 0.96157735 0.96157735 0.9635996
 0.9635996  0.9635996  0.9635996  0.96461072 0.96461072 0.96562184
 0.96562184 0.96663296 0.96663296 0.96764408 0.96764408 0.96865521
 0.96865521 0.96966633 0.96966633 0.97168857 0.97168857 0.9726997
 0.9726997  0.97371082 0.97371082 0.97472194 0.97472194 0.97472194
 0.97573306 0.97573306 0.97674419 0.97674419 0.97775531 0.97775531
 0.97876643 0.97876643 0.97977755 0.97977755 0.98078868 0.98078868
 0.98483316 0.98483316 0.98584429 0.98584429 0.98786653 0.98786653
 0.9908999  0.9908999  0.9908999  0.9908999  0.99191102 0.99191102
 0.99292214 0.99292214 0.99393327 0.99393327 0.99494439 0.99494439
 0.99595551 0.99595551 0.99696663 0.99696663 0.99797776 0.99797776
 0.99797776 0.99797776 0.99797776 0.99797776 0.99797776 0.99797776
 0.99797776 0.99797776 0.99797776 0.99797776 0.99797776 0.99797776
 0.99797776 0.99797776 0.99898888 0.99898888 0.99898888 0.99898888
 0.99898888 0.99898888 0.99898888 0.99898888 1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.        ]
thresholds: [        inf  6.8632812   6.4492188   6.4335938   6.296875    6.2929688
  6.2539062   6.25        6.2382812   6.234375    6.2148438   6.1992188
  6.1835938   6.1796875   6.140625    6.1367188   6.1289062   6.125
  6.1132812   6.109375    6.1054688   6.0976562   6.09375     6.0898438
  6.0742188   6.0703125   6.0664062   6.03125     6.0234375   5.9960938
  5.9765625   5.9726562   5.96875     5.9453125   5.9414062   5.9375
  5.9335938   5.9296875   5.90625     5.9023438   5.8867188   5.8789062
  5.8671875   5.8632812   5.859375    5.8554688   5.84375     5.8242188
  5.8164062   5.8007812   5.7851562   5.78125     5.7773438   5.765625
  5.7617188   5.7578125   5.75        5.7421875   5.7382812   5.7304688
  5.7265625   5.71875     5.7148438   5.7109375   5.7070312   5.6992188
  5.6953125   5.6640625   5.6601562   5.6328125   5.6289062   5.6132812
  5.6015625   5.59375     5.5898438   5.5859375   5.5820312   5.578125
  5.5742188   5.5703125   5.5664062   5.5625      5.546875    5.5429688
  5.53125     5.5273438   5.5234375   5.515625    5.5117188   5.5078125
  5.5039062   5.5         5.4765625   5.4726562   5.453125    5.4375
  5.4335938   5.4257812   5.4101562   5.3984375   5.3945312   5.3867188
  5.3789062   5.3710938   5.3671875   5.3632812   5.359375    5.34375
  5.3359375   5.3320312   5.3242188   5.3164062   5.3125      5.2929688
  5.2890625   5.2851562   5.2773438   5.2695312   5.2617188   5.2578125
  5.25        5.2382812   5.234375    5.2304688   5.2265625   5.2226562
  5.21875     5.2148438   5.2109375   5.2070312   5.1914062   5.1757812
  5.1679688   5.1640625   5.15625     5.1523438   5.1484375   5.1367188
  5.1328125   5.125       5.1171875   5.109375    5.1054688   5.0976562
  5.078125    5.0703125   5.0664062   5.0625      5.0585938   5.0546875
  5.0507812   5.0117188   5.0078125   5.0039062   4.9765625   4.9726562
  4.96875     4.9648438   4.9414062   4.9375      4.921875    4.90625
  4.875       4.8710938   4.8632812   4.8554688   4.8515625   4.8476562
  4.8320312   4.8242188   4.7890625   4.7773438   4.7734375   4.765625
  4.7578125   4.75        4.7460938   4.7382812   4.7304688   4.71875
  4.7148438   4.7109375   4.6953125   4.6679688   4.6640625   4.6601562
  4.6445312   4.6367188   4.6328125   4.625       4.6132812   4.6054688
  4.5976562   4.59375     4.578125    4.5703125   4.5625      4.5546875
  4.5390625   4.5351562   4.5273438   4.5195312   4.515625    4.5078125
  4.5039062   4.4960938   4.4882812   4.484375    4.4804688   4.4765625
  4.4414062   4.4335938   4.4257812   4.421875    4.4101562   4.40625
  4.4023438   4.3945312   4.3867188   4.375       4.359375    4.3554688
  4.3515625   4.3476562   4.3398438   4.3359375   4.3320312   4.328125
  4.3242188   4.3164062   4.296875    4.2890625   4.28125     4.2734375
  4.2695312   4.234375    4.2304688   4.1523438   4.140625    4.1289062
  4.1210938   4.09375     4.0898438   4.0859375   4.0820312   4.0664062
  4.0585938   4.0546875   4.0507812   4.046875    4.0273438   3.984375
  3.9804688   3.9746094   3.9550781   3.9433594   3.9355469   3.9335938
  3.9296875   3.921875    3.9199219   3.8828125   3.8789062   3.8730469
  3.8613281   3.828125    3.8144531   3.8007812   3.796875    3.7792969
  3.7714844   3.7402344   3.7363281   3.7324219   3.7304688   3.7050781
  3.6894531   3.65625     3.6523438   3.6503906   3.6425781   3.6347656
  3.6328125   3.5292969   3.5273438   3.4824219   3.4804688   3.4765625
  3.4707031   3.46875     3.4589844   3.4394531   3.4121094   3.3886719
  3.3457031   3.3378906   3.3359375   3.3339844   3.3125      3.3105469
  3.296875    3.2949219   3.2832031   3.25        3.2382812   3.2324219
  3.2285156   3.2148438   3.1972656   3.1953125   3.171875    3.1582031
  3.1542969   3.1523438   3.1113281   3.0917969   3.0859375   3.0332031
  3.0253906   3.          2.9804688   2.9609375   2.9550781   2.9042969
  2.8984375   2.8847656   2.875       2.8691406   2.8671875   2.8515625
  2.7675781   2.7636719   2.75        2.7460938   2.6660156   2.6640625
  2.6601562   2.6210938   2.5917969   2.5625      2.5585938   2.5351562
  2.5332031   2.5019531   2.4804688   2.4667969   2.4628906   2.4570312
  2.4394531   2.4121094   2.4101562   2.3476562   2.3359375   2.3320312
  2.2949219   2.2832031   2.2773438   2.1953125   2.1757812   2.125
  2.1152344   2.0332031   2.0253906   1.9589844   1.9580078   1.9521484
  1.9482422   1.9423828   1.9404297   1.859375    1.8125      1.7998047
  1.7919922   1.7900391   1.7822266   1.7783203   1.7568359   1.7138672
  1.6289062   1.6015625   1.5722656   1.5703125   1.5068359   1.5058594
  1.4726562   1.4667969   1.453125    1.4492188   1.4414062   1.4316406
  1.3994141   1.3554688   1.3535156   1.3300781   1.3271484   1.3261719
  1.3203125   1.3183594   1.2734375   1.2353516   1.2265625   1.1474609
  1.1298828   1.1289062   1.1230469   1.0683594   1.0673828   1.0214844
  1.0039062   0.97558594  0.9741211   0.9399414   0.9223633   0.85595703
  0.81884766  0.7426758   0.7402344   0.7246094   0.7109375   0.7006836
  0.67822266  0.5917969   0.5751953   0.56103516  0.5307617   0.4855957
  0.47070312  0.15710449  0.15600586  0.05764771  0.05517578 -0.06634521
 -0.06689453 -0.35351562 -0.35375977 -0.6098633  -0.6196289  -0.7675781
 -0.79345703 -1.0175781  -1.0234375  -1.0322266  -1.0332031  -1.1503906
 -1.1660156  -1.1845703  -1.1875     -1.234375   -1.2558594  -1.3798828
 -1.3808594  -1.3876953  -1.4121094  -1.4160156  -1.4189453  -1.4785156
 -1.5029297  -1.6347656  -1.6396484  -1.7373047  -1.7392578  -2.0175781
 -2.0234375  -2.0410156  -2.0429688  -2.0566406  -2.0585938  -2.0605469
 -2.1035156  -2.1171875  -2.125      -2.1269531  -2.1523438  -2.1601562
 -2.2011719  -2.2050781  -2.2070312  -2.2148438  -2.2226562  -2.2363281
 -2.2558594  -2.2636719  -2.2675781  -2.2695312  -2.2871094  -2.2890625
 -2.2910156  -2.2949219  -2.3027344  -2.3066406  -2.3320312  -2.3378906
 -2.3945312  -2.3964844  -2.4140625  -2.4277344  -2.4394531  -2.4921875
 -2.4941406  -2.515625   -2.5195312  -2.5488281  -2.5566406  -2.6191406
 -2.6230469  -2.6542969  -2.65625    -2.6601562  -2.6660156  -2.6933594
 -2.6953125  -2.7148438  -2.71875    -2.7207031  -2.7324219  -2.734375
 -2.7363281  -2.8007812  -2.8027344  -2.8359375  -2.8378906  -2.8496094
 -2.8515625  -2.8535156  -2.8554688  -2.8574219  -2.8847656  -2.8925781
 -2.9101562  -2.9160156  -2.9394531  -2.9433594  -2.9453125  -2.9492188
 -2.9902344  -2.9960938  -3.0585938  -3.0625     -3.1054688  -3.1152344
 -3.1328125  -3.140625   -3.2363281  -3.2421875  -3.2890625  -3.3007812
 -3.3300781  -3.3378906  -3.3398438  -3.3613281  -3.3632812  -3.3691406
 -3.3730469  -3.3789062  -3.3808594  -3.4082031  -3.4121094  -3.4335938
 -3.4394531  -3.4511719  -3.4570312  -3.4589844  -3.46875    -3.4863281
 -3.4882812  -3.5019531  -3.5078125  -3.5429688  -3.546875   -3.5664062
 -3.5703125  -3.5996094  -3.6054688  -3.6679688  -3.6757812  -3.6894531
 -3.6933594  -3.7148438  -3.7167969  -3.7382812  -3.7421875  -3.7773438
 -3.7792969  -3.8027344  -3.8183594  -3.828125   -3.8378906  -3.8457031
 -3.8476562  -3.9316406  -3.9355469  -4.0507812  -4.0585938  -4.1367188
 -4.1484375  -4.1875     -4.1914062  -4.1992188  -4.234375   -4.2382812
 -4.4335938  -4.4375     -4.484375   -4.4960938  -4.5273438  -4.53125
 -4.5898438  -4.5976562  -4.625      -4.65625    -5.1015625  -5.1054688 ]
