log_loss_steps: 256
eval_steps: 512
check_degradation: 0
----------------Epoch 1/1----------------
Epoch 1/1, Loss after 192 samples: 0.7549
Epoch 1/1, Loss after 448 samples: 0.7608
Mean accuracy: 0.5001, std: 0.0109, lower bound: 0.4787, upper bound: 0.5218 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 448 samples: 0.5000 with eval loss: 0.7740
Best model with eval loss 0.773963043766637 and eval accuracy 0.5 with 448 samples seen is saved
Epoch 1/1, Loss after 704 samples: 0.7011
Epoch 1/1, Loss after 960 samples: 0.6811
Mean accuracy: 0.5452, std: 0.0113, lower bound: 0.5233, upper bound: 0.5659 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 960 samples: 0.5456 with eval loss: 0.6533
Best model with eval loss 0.6532630035954137 and eval accuracy 0.5456389452332657 with 960 samples seen is saved
Epoch 1/1, Loss after 1216 samples: 0.6381
Epoch 1/1, Loss after 1472 samples: 0.6064
Mean accuracy: 0.7047, std: 0.0104, lower bound: 0.6851, upper bound: 0.7262 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 1472 samples: 0.7044 with eval loss: 0.5711
Best model with eval loss 0.5710838417853078 and eval accuracy 0.7043610547667343 with 1472 samples seen is saved
Epoch 1/1, Loss after 1728 samples: 0.6235
Epoch 1/1, Loss after 1984 samples: 0.5626
Mean accuracy: 0.7144, std: 0.0102, lower bound: 0.6942, upper bound: 0.7348 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 1984 samples: 0.7145 with eval loss: 0.5537
Best model with eval loss 0.5536643247450551 and eval accuracy 0.7145030425963489 with 1984 samples seen is saved
Epoch 1/1, Loss after 2240 samples: 0.5603
Epoch 1/1, Loss after 2496 samples: 0.5618
Mean accuracy: 0.7499, std: 0.0097, lower bound: 0.7307, upper bound: 0.7693 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 2496 samples: 0.7500 with eval loss: 0.5058
Best model with eval loss 0.505795867212357 and eval accuracy 0.75 with 2496 samples seen is saved
Epoch 1/1, Loss after 2752 samples: 0.5323
Epoch 1/1, Loss after 3008 samples: 0.5325
Mean accuracy: 0.6895, std: 0.0104, lower bound: 0.6689, upper bound: 0.7094 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 3008 samples: 0.6897 with eval loss: 0.5349
Epoch 1/1, Loss after 3264 samples: 0.5576
Epoch 1/1, Loss after 3520 samples: 0.5235
Mean accuracy: 0.7762, std: 0.0092, lower bound: 0.7576, upper bound: 0.7946 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 3520 samples: 0.7759 with eval loss: 0.4815
Best model with eval loss 0.48153306015076175 and eval accuracy 0.7758620689655172 with 3520 samples seen is saved
Epoch 1/1, Loss after 3776 samples: 0.5566
Epoch 1/1, Loss after 4032 samples: 0.5206
Mean accuracy: 0.7689, std: 0.0096, lower bound: 0.7500, upper bound: 0.7865 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 4032 samples: 0.7693 with eval loss: 0.4767
Best model with eval loss 0.47672995444267025 and eval accuracy 0.7692697768762677 with 4032 samples seen is saved
Epoch 1/1, Loss after 4288 samples: 0.5162
Epoch 1/1, Loss after 4544 samples: 0.5546
Mean accuracy: 0.7304, std: 0.0102, lower bound: 0.7104, upper bound: 0.7495 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 4544 samples: 0.7307 with eval loss: 0.5054
Epoch 1/1, Loss after 4800 samples: 0.4659
Epoch 1/1, Loss after 5056 samples: 0.4928
Mean accuracy: 0.7867, std: 0.0091, lower bound: 0.7683, upper bound: 0.8038 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 5056 samples: 0.7865 with eval loss: 0.4503
Best model with eval loss 0.4503409535654129 and eval accuracy 0.7865111561866126 with 5056 samples seen is saved
Epoch 1/1, Loss after 5312 samples: 0.4618
Epoch 1/1, Loss after 5568 samples: 0.4957
Mean accuracy: 0.7886, std: 0.0097, lower bound: 0.7703, upper bound: 0.8073 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 5568 samples: 0.7885 with eval loss: 0.4412
Best model with eval loss 0.4411548010764583 and eval accuracy 0.7885395537525355 with 5568 samples seen is saved
Epoch 1/1, Loss after 5824 samples: 0.4910
Epoch 1/1, Loss after 6080 samples: 0.4969
Mean accuracy: 0.7854, std: 0.0093, lower bound: 0.7672, upper bound: 0.8043 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 6080 samples: 0.7855 with eval loss: 0.4480
Epoch 1/1, Loss after 6336 samples: 0.4803
Epoch 1/1, Loss after 6592 samples: 0.4834
Mean accuracy: 0.8073, std: 0.0093, lower bound: 0.7896, upper bound: 0.8251 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 6592 samples: 0.8073 with eval loss: 0.4312
Best model with eval loss 0.43115107763198113 and eval accuracy 0.8073022312373225 with 6592 samples seen is saved
Epoch 1/1, Loss after 6848 samples: 0.5160
Epoch 1/1, Loss after 7104 samples: 0.4634
Mean accuracy: 0.7973, std: 0.0085, lower bound: 0.7804, upper bound: 0.8144 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 7104 samples: 0.7972 with eval loss: 0.4358
Epoch 1/1, Loss after 7360 samples: 0.4967
Epoch 1/1, Loss after 7616 samples: 0.4774
Mean accuracy: 0.8125, std: 0.0093, lower bound: 0.7931, upper bound: 0.8306 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 7616 samples: 0.8129 with eval loss: 0.4174
Best model with eval loss 0.4174125829050618 and eval accuracy 0.8128803245436106 with 7616 samples seen is saved
Epoch 1/1, Loss after 7872 samples: 0.4708
Epoch 1/1, Loss after 8128 samples: 0.4270
Mean accuracy: 0.7640, std: 0.0092, lower bound: 0.7469, upper bound: 0.7819 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 8128 samples: 0.7637 with eval loss: 0.4618
Epoch 1/1, Loss after 8384 samples: 0.5620
Epoch 1/1, Loss after 8640 samples: 0.5166
Mean accuracy: 0.7708, std: 0.0095, lower bound: 0.7515, upper bound: 0.7885 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 8640 samples: 0.7713 with eval loss: 0.4514
Epoch 1/1, Loss after 8896 samples: 0.4906
Epoch 1/1, Loss after 9152 samples: 0.4864
Mean accuracy: 0.7957, std: 0.0091, lower bound: 0.7769, upper bound: 0.8119 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 9152 samples: 0.7956 with eval loss: 0.4278
Epoch 1/1, Loss after 9408 samples: 0.4624
Epoch 1/1, Loss after 9664 samples: 0.4826
Mean accuracy: 0.8110, std: 0.0087, lower bound: 0.7936, upper bound: 0.8276 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 9664 samples: 0.8109 with eval loss: 0.4111
Best model with eval loss 0.41114749639264997 and eval accuracy 0.8108519269776876 with 9664 samples seen is saved
Epoch 1/1, Loss after 9920 samples: 0.4667
Epoch 1/1, Loss after 10176 samples: 0.4650
Mean accuracy: 0.7918, std: 0.0087, lower bound: 0.7748, upper bound: 0.8088 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 10176 samples: 0.7911 with eval loss: 0.4453
Epoch 1/1, Loss after 10432 samples: 0.4568
Epoch 1/1, Loss after 10688 samples: 0.4429
Mean accuracy: 0.7990, std: 0.0089, lower bound: 0.7819, upper bound: 0.8169 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 10688 samples: 0.7992 with eval loss: 0.4319
Epoch 1/1, Loss after 10944 samples: 0.4616
Epoch 1/1, Loss after 11200 samples: 0.4456
Mean accuracy: 0.8165, std: 0.0088, lower bound: 0.7997, upper bound: 0.8342 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 11200 samples: 0.8164 with eval loss: 0.4195
Epoch 1/1, Loss after 11456 samples: 0.4920
Epoch 1/1, Loss after 11712 samples: 0.4698
Mean accuracy: 0.8192, std: 0.0088, lower bound: 0.8022, upper bound: 0.8357 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 11712 samples: 0.8195 with eval loss: 0.4133
Epoch 1/1, Loss after 11968 samples: 0.4616
Epoch 1/1, Loss after 12224 samples: 0.4725
Mean accuracy: 0.8235, std: 0.0086, lower bound: 0.8063, upper bound: 0.8403 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 12224 samples: 0.8235 with eval loss: 0.4029
Best model with eval loss 0.4028868877118634 and eval accuracy 0.8235294117647058 with 12224 samples seen is saved
Epoch 1/1, Loss after 12480 samples: 0.4131
Epoch 1/1, Loss after 12736 samples: 0.4941
Mean accuracy: 0.8017, std: 0.0092, lower bound: 0.7845, upper bound: 0.8200 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 12736 samples: 0.8017 with eval loss: 0.4246
Epoch 1/1, Loss after 12992 samples: 0.4668
Epoch 1/1, Loss after 13248 samples: 0.4621
Mean accuracy: 0.8271, std: 0.0087, lower bound: 0.8103, upper bound: 0.8448 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 13248 samples: 0.8271 with eval loss: 0.4036
Epoch 1/1, Loss after 13504 samples: 0.4634
Epoch 1/1, Loss after 13760 samples: 0.4857
Mean accuracy: 0.8301, std: 0.0085, lower bound: 0.8129, upper bound: 0.8479 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 13760 samples: 0.8301 with eval loss: 0.4050
Epoch 1/1, Loss after 14016 samples: 0.4345
Epoch 1/1, Loss after 14272 samples: 0.4702
Mean accuracy: 0.8296, std: 0.0089, lower bound: 0.8114, upper bound: 0.8469 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 14272 samples: 0.8301 with eval loss: 0.4031
Epoch 1/1, Loss after 14528 samples: 0.4289
Epoch 1/1, Loss after 14784 samples: 0.4676
Mean accuracy: 0.8204, std: 0.0085, lower bound: 0.8027, upper bound: 0.8367 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 14784 samples: 0.8205 with eval loss: 0.4068
Epoch 1/1, Loss after 15040 samples: 0.4895
Epoch 1/1, Loss after 15296 samples: 0.5440
Mean accuracy: 0.8222, std: 0.0083, lower bound: 0.8063, upper bound: 0.8377 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 15296 samples: 0.8220 with eval loss: 0.4069
Epoch 1/1, Loss after 15552 samples: 0.4312
Training signal is False, stopping training
Training finished
Best model: {'eval_acc': 0.8235294117647058, 'nb_samples': 12224, 'eval_loss': 0.4028868877118634}
Training loss logs: [{'samples': 192, 'loss': 0.7549400329589844}, {'samples': 448, 'loss': 0.7608475685119629}, {'samples': 704, 'loss': 0.7011299133300781}, {'samples': 960, 'loss': 0.6810942888259888}, {'samples': 1216, 'loss': 0.6380638480186462}, {'samples': 1472, 'loss': 0.6064006686210632}, {'samples': 1728, 'loss': 0.6235218197107315}, {'samples': 1984, 'loss': 0.5625879764556885}, {'samples': 2240, 'loss': 0.5603102445602417}, {'samples': 2496, 'loss': 0.5618380308151245}, {'samples': 2752, 'loss': 0.5322911441326141}, {'samples': 3008, 'loss': 0.5325386226177216}, {'samples': 3264, 'loss': 0.5576272457838058}, {'samples': 3520, 'loss': 0.5234644860029221}, {'samples': 3776, 'loss': 0.5566389560699463}, {'samples': 4032, 'loss': 0.5205883234739304}, {'samples': 4288, 'loss': 0.5162070393562317}, {'samples': 4544, 'loss': 0.5546145886182785}, {'samples': 4800, 'loss': 0.46592824161052704}, {'samples': 5056, 'loss': 0.4928259551525116}, {'samples': 5312, 'loss': 0.46176912635564804}, {'samples': 5568, 'loss': 0.49573393166065216}, {'samples': 5824, 'loss': 0.49099089205265045}, {'samples': 6080, 'loss': 0.49686646461486816}, {'samples': 6336, 'loss': 0.4802713021636009}, {'samples': 6592, 'loss': 0.4833652749657631}, {'samples': 6848, 'loss': 0.5160216987133026}, {'samples': 7104, 'loss': 0.4634155258536339}, {'samples': 7360, 'loss': 0.4967285096645355}, {'samples': 7616, 'loss': 0.47735031694173813}, {'samples': 7872, 'loss': 0.47078177332878113}, {'samples': 8128, 'loss': 0.4270319566130638}, {'samples': 8384, 'loss': 0.5619906783103943}, {'samples': 8640, 'loss': 0.5166095495223999}, {'samples': 8896, 'loss': 0.4906450882554054}, {'samples': 9152, 'loss': 0.48636142909526825}, {'samples': 9408, 'loss': 0.46240925043821335}, {'samples': 9664, 'loss': 0.4826318845152855}, {'samples': 9920, 'loss': 0.46671725809574127}, {'samples': 10176, 'loss': 0.46495305746793747}, {'samples': 10432, 'loss': 0.4567839354276657}, {'samples': 10688, 'loss': 0.4428917467594147}, {'samples': 10944, 'loss': 0.4616246744990349}, {'samples': 11200, 'loss': 0.4456230700016022}, {'samples': 11456, 'loss': 0.492016963660717}, {'samples': 11712, 'loss': 0.46976032108068466}, {'samples': 11968, 'loss': 0.46163301914930344}, {'samples': 12224, 'loss': 0.4725402966141701}, {'samples': 12480, 'loss': 0.41314025968313217}, {'samples': 12736, 'loss': 0.4940715581178665}, {'samples': 12992, 'loss': 0.4668227583169937}, {'samples': 13248, 'loss': 0.46208811551332474}, {'samples': 13504, 'loss': 0.463360920548439}, {'samples': 13760, 'loss': 0.4856666252017021}, {'samples': 14016, 'loss': 0.4345490038394928}, {'samples': 14272, 'loss': 0.470210462808609}, {'samples': 14528, 'loss': 0.42887450009584427}, {'samples': 14784, 'loss': 0.467618852853775}, {'samples': 15040, 'loss': 0.4894788786768913}, {'samples': 15296, 'loss': 0.5440045148134232}, {'samples': 15552, 'loss': 0.4312063604593277}]
Evaluation accuracy logs: [{'samples': 448, 'accuracy': 0.5001277890466531, 'std': 0.010948257024919274, 'lower_bound': 0.4787018255578093, 'upper_bound': 0.5218179513184584}, {'samples': 960, 'accuracy': 0.5451764705882354, 'std': 0.011343919431226595, 'lower_bound': 0.5233265720081136, 'upper_bound': 0.565935598377282}, {'samples': 1472, 'accuracy': 0.7046541582150101, 'std': 0.01042914860267086, 'lower_bound': 0.6850786004056795, 'upper_bound': 0.7261663286004056}, {'samples': 1984, 'accuracy': 0.7143504056795131, 'std': 0.01022325954896662, 'lower_bound': 0.6942190669371197, 'upper_bound': 0.7347870182555781}, {'samples': 2496, 'accuracy': 0.7499249492900608, 'std': 0.009690112050020499, 'lower_bound': 0.7307302231237323, 'upper_bound': 0.7692697768762677}, {'samples': 3008, 'accuracy': 0.6895131845841784, 'std': 0.010446882550260956, 'lower_bound': 0.6688514198782961, 'upper_bound': 0.7094320486815415}, {'samples': 3520, 'accuracy': 0.7762058823529412, 'std': 0.009213959885213908, 'lower_bound': 0.757606490872211, 'upper_bound': 0.7946247464503042}, {'samples': 4032, 'accuracy': 0.7689056795131847, 'std': 0.00962982246447503, 'lower_bound': 0.75, 'upper_bound': 0.7865238336713997}, {'samples': 4544, 'accuracy': 0.7303742393509128, 'std': 0.010165631618381745, 'lower_bound': 0.710446247464503, 'upper_bound': 0.7495055780933063}, {'samples': 5056, 'accuracy': 0.78673630831643, 'std': 0.009118499667946896, 'lower_bound': 0.7682555780933062, 'upper_bound': 0.8037652129817444}, {'samples': 5568, 'accuracy': 0.7885846855983772, 'std': 0.009689090082993203, 'lower_bound': 0.7702839756592292, 'upper_bound': 0.8073022312373225}, {'samples': 6080, 'accuracy': 0.7854036511156187, 'std': 0.009252363571824216, 'lower_bound': 0.7672287018255578, 'upper_bound': 0.8042596348884381}, {'samples': 6592, 'accuracy': 0.80727738336714, 'std': 0.009315687710590163, 'lower_bound': 0.789553752535497, 'upper_bound': 0.8250507099391481}, {'samples': 7104, 'accuracy': 0.7973296146044625, 'std': 0.008530648430776343, 'lower_bound': 0.7804259634888439, 'upper_bound': 0.8144016227180527}, {'samples': 7616, 'accuracy': 0.8125354969574037, 'std': 0.009323703223935782, 'lower_bound': 0.793090770791075, 'upper_bound': 0.8306288032454361}, {'samples': 8128, 'accuracy': 0.7640400608519269, 'std': 0.009162137184945067, 'lower_bound': 0.7469447261663286, 'upper_bound': 0.781947261663286}, {'samples': 8640, 'accuracy': 0.7707611561866127, 'std': 0.009519560641423466, 'lower_bound': 0.7515212981744422, 'upper_bound': 0.7885395537525355}, {'samples': 9152, 'accuracy': 0.7956673427991886, 'std': 0.009079300780842606, 'lower_bound': 0.7768762677484787, 'upper_bound': 0.8118661257606491}, {'samples': 9664, 'accuracy': 0.8110035496957404, 'std': 0.008685942027430508, 'lower_bound': 0.7936105476673428, 'upper_bound': 0.8275988843813388}, {'samples': 10176, 'accuracy': 0.7918078093306288, 'std': 0.008741119417307769, 'lower_bound': 0.7748478701825557, 'upper_bound': 0.8088362068965518}, {'samples': 10688, 'accuracy': 0.7990481744421907, 'std': 0.008928381217692688, 'lower_bound': 0.781947261663286, 'upper_bound': 0.8169371196754563}, {'samples': 11200, 'accuracy': 0.8165471602434078, 'std': 0.008776184428349234, 'lower_bound': 0.7996830628803245, 'upper_bound': 0.8341784989858012}, {'samples': 11712, 'accuracy': 0.8192327586206897, 'std': 0.008790436985014007, 'lower_bound': 0.8022312373225152, 'upper_bound': 0.8356997971602435}, {'samples': 12224, 'accuracy': 0.8234695740365112, 'std': 0.008593592959290597, 'lower_bound': 0.8062880324543611, 'upper_bound': 0.840276369168357}, {'samples': 12736, 'accuracy': 0.801698275862069, 'std': 0.009235091618154608, 'lower_bound': 0.7844827586206896, 'upper_bound': 0.8199797160243407}, {'samples': 13248, 'accuracy': 0.8271262677484787, 'std': 0.008675733336350773, 'lower_bound': 0.8103321501014198, 'upper_bound': 0.8448275862068966}, {'samples': 13760, 'accuracy': 0.8301470588235292, 'std': 0.008517964197748, 'lower_bound': 0.8128803245436106, 'upper_bound': 0.847870182555781}, {'samples': 14272, 'accuracy': 0.8296120689655172, 'std': 0.008885460523686724, 'lower_bound': 0.8113590263691683, 'upper_bound': 0.8468559837728195}, {'samples': 14784, 'accuracy': 0.8204442190669372, 'std': 0.008475767662954999, 'lower_bound': 0.802738336713996, 'upper_bound': 0.8367139959432048}, {'samples': 15296, 'accuracy': 0.8221501014198782, 'std': 0.00831348584901206, 'lower_bound': 0.806275354969574, 'upper_bound': 0.8377281947261663}]
Evaluating the best model on the test set of dataset ./fake_true_datasets/fake_true_dataset_mistral_10k...
Test metrics:
accuracy: 0.8363042596348884
precision: 0.8336224622161492
recall: 0.8397213642392417
f1_score: 0.8365937545587394
fp_rate: 0.16711862777998976
tp_rate: 0.8397213642392417
std_accuracy: 0.008036415674512376
std_precision: 0.011590784452084694
std_recall: 0.0113621096443507
std_f1_score: 0.008693654540540429
std_fp_rate: 0.011799959355429289
std_tp_rate: 0.0113621096443507
TP: 826.763
TN: 822.429
FP: 165.012
FN: 157.796
roc_auc: 0.917865327567692
fpr: [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.0010142  0.0010142  0.0020284  0.0020284  0.0020284  0.0020284
 0.0020284  0.0020284  0.0020284  0.0020284  0.0020284  0.0020284
 0.0020284  0.0020284  0.0030426  0.0030426  0.0040568  0.0040568
 0.0040568  0.0040568  0.0040568  0.0040568  0.0040568  0.0040568
 0.0040568  0.0040568  0.00507099 0.00507099 0.00507099 0.00507099
 0.00507099 0.00507099 0.00507099 0.00507099 0.00507099 0.00507099
 0.00507099 0.00507099 0.00507099 0.00608519 0.00608519 0.00608519
 0.00608519 0.00709939 0.00709939 0.00709939 0.00811359 0.00811359
 0.00811359 0.00811359 0.00912779 0.00912779 0.00912779 0.00912779
 0.00912779 0.00912779 0.00912779 0.00912779 0.00912779 0.01014199
 0.01014199 0.01115619 0.01115619 0.01115619 0.01217039 0.01217039
 0.01217039 0.01217039 0.01318458 0.01318458 0.01419878 0.01419878
 0.01521298 0.01521298 0.01724138 0.01724138 0.01926978 0.01926978
 0.02129817 0.02129817 0.02231237 0.02231237 0.02332657 0.02332657
 0.02434077 0.02434077 0.02636917 0.02636917 0.02738337 0.02738337
 0.02738337 0.02738337 0.02839757 0.02839757 0.02839757 0.02839757
 0.02941176 0.02941176 0.02941176 0.03042596 0.03042596 0.03042596
 0.03042596 0.03245436 0.03346856 0.03346856 0.03448276 0.03448276
 0.03651116 0.03651116 0.03853955 0.03853955 0.03955375 0.03955375
 0.03955375 0.03955375 0.04056795 0.04056795 0.04158215 0.04158215
 0.04259635 0.04259635 0.04462475 0.04462475 0.04563895 0.04665314
 0.04665314 0.04766734 0.04766734 0.04868154 0.04868154 0.04969574
 0.04969574 0.04969574 0.05070994 0.05070994 0.05070994 0.05070994
 0.05172414 0.05172414 0.05476673 0.05476673 0.05476673 0.05578093
 0.05578093 0.05679513 0.05679513 0.05679513 0.05679513 0.05780933
 0.05780933 0.05882353 0.06085193 0.06085193 0.06186613 0.06186613
 0.06389452 0.06490872 0.06490872 0.06592292 0.06592292 0.06592292
 0.06795132 0.06795132 0.06896552 0.06896552 0.07099391 0.07302231
 0.07302231 0.07403651 0.07403651 0.07403651 0.07403651 0.07403651
 0.07505071 0.07505071 0.07505071 0.07606491 0.07606491 0.07707911
 0.07707911 0.0801217  0.0801217  0.0821501  0.0821501  0.0831643
 0.0831643  0.0851927  0.0851927  0.0862069  0.08823529 0.08823529
 0.08924949 0.08924949 0.09127789 0.09127789 0.09432049 0.09432049
 0.09533469 0.09533469 0.09533469 0.09837728 0.09837728 0.09837728
 0.09837728 0.10040568 0.10141988 0.10141988 0.10141988 0.10243408
 0.10243408 0.10446247 0.10446247 0.10649087 0.10649087 0.10750507
 0.10750507 0.11054767 0.11257606 0.11257606 0.11359026 0.11359026
 0.11460446 0.11460446 0.11561866 0.11561866 0.11663286 0.11764706
 0.11764706 0.11967546 0.11967546 0.11967546 0.12170385 0.12170385
 0.12271805 0.12271805 0.12474645 0.12474645 0.12576065 0.12576065
 0.13286004 0.13286004 0.13387424 0.13387424 0.13488844 0.13488844
 0.13590264 0.13691684 0.13691684 0.14097363 0.14097363 0.14198783
 0.14198783 0.14401623 0.14401623 0.14503043 0.14503043 0.14807302
 0.14807302 0.14908722 0.14908722 0.15010142 0.15010142 0.15111562
 0.15111562 0.15212982 0.15212982 0.15415822 0.15415822 0.15821501
 0.15821501 0.15922921 0.15922921 0.16227181 0.16227181 0.1663286
 0.1663286  0.1673428  0.1673428  0.1703854  0.1703854  0.17139959
 0.17241379 0.17241379 0.17342799 0.17444219 0.17545639 0.17545639
 0.17647059 0.17647059 0.17748479 0.17748479 0.17849899 0.17849899
 0.18052738 0.18052738 0.18154158 0.18154158 0.18356998 0.18356998
 0.18559838 0.18559838 0.18661258 0.18661258 0.18661258 0.18762677
 0.18762677 0.19269777 0.19269777 0.19472617 0.19472617 0.19574037
 0.19574037 0.19675456 0.19675456 0.19878296 0.19878296 0.19979716
 0.19979716 0.20182556 0.20182556 0.20283976 0.20283976 0.21095335
 0.21298174 0.21602434 0.21602434 0.21703854 0.21703854 0.21906694
 0.21906694 0.22312373 0.22312373 0.22413793 0.22413793 0.22515213
 0.22515213 0.23123732 0.23123732 0.23225152 0.23225152 0.23529412
 0.23630832 0.24036511 0.24036511 0.24137931 0.24137931 0.24239351
 0.24239351 0.24340771 0.24340771 0.24543611 0.24543611 0.2525355
 0.25456389 0.25659229 0.25659229 0.25862069 0.25963489 0.25963489
 0.26369168 0.26369168 0.26572008 0.26572008 0.26977688 0.26977688
 0.27180527 0.27180527 0.27281947 0.27281947 0.27383367 0.27789047
 0.27789047 0.27991886 0.27991886 0.28194726 0.28194726 0.28498986
 0.28498986 0.29208925 0.29208925 0.29310345 0.29310345 0.30020284
 0.30020284 0.30324544 0.30324544 0.30527383 0.30628803 0.30730223
 0.30730223 0.31135903 0.31135903 0.31541582 0.31541582 0.32251521
 0.32251521 0.32454361 0.32454361 0.32555781 0.32555781 0.32758621
 0.32758621 0.3296146  0.3296146  0.3336714  0.3336714  0.3346856
 0.3346856  0.33772819 0.33772819 0.34077079 0.34077079 0.34787018
 0.34888438 0.35192698 0.35192698 0.35294118 0.35294118 0.35598377
 0.35801217 0.37525355 0.37525355 0.37626775 0.37626775 0.38640974
 0.38640974 0.38945233 0.38945233 0.39046653 0.39046653 0.39249493
 0.39249493 0.39655172 0.39655172 0.40770791 0.40770791 0.40973631
 0.40973631 0.41176471 0.41176471 0.4198783  0.4198783  0.43002028
 0.43002028 0.43103448 0.43103448 0.43407708 0.43407708 0.43813387
 0.43813387 0.44219067 0.44219067 0.44320487 0.44523327 0.44624746
 0.44624746 0.45030426 0.45030426 0.45943205 0.45943205 0.47768763
 0.47870183 0.48884381 0.48884381 0.49188641 0.49188641 0.49492901
 0.49492901 0.4979716  0.4989858  0.5030426  0.5030426  0.50811359
 0.50811359 0.51115619 0.51115619 0.51318458 0.51318458 0.52332657
 0.52332657 0.52636917 0.52839757 0.53549696 0.53752535 0.53853955
 0.53955375 0.54462475 0.54462475 0.54868154 0.54868154 0.55273834
 0.55273834 0.55578093 0.55780933 0.56288032 0.56693712 0.5841785
 0.5872211  0.59229209 0.59229209 0.60446247 0.60649087 0.60851927
 0.60851927 0.61359026 0.61359026 0.62677485 0.62778905 0.63286004
 0.63286004 0.63590264 0.63590264 0.64198783 0.64401623 0.65212982
 0.65314402 0.65415822 0.65415822 0.67139959 0.67241379 0.67342799
 0.67545639 0.68255578 0.68458418 0.68661258 0.68762677 0.69472617
 0.69675456 0.70283976 0.70283976 0.71095335 0.71095335 0.71298174
 0.71501014 0.72413793 0.72616633 0.72616633 0.73225152 0.73427992
 0.73529412 0.73732252 0.73935091 0.74137931 0.74137931 0.74442191
 0.7464503  0.7505071  0.7505071  0.75760649 0.75963489 0.76166329
 0.76166329 0.76470588 0.76673428 0.76876268 0.77281947 0.77991886
 0.77991886 0.78498986 0.78701826 0.79107505 0.79310345 0.80121704
 0.80121704 0.82150101 0.82251521 0.82454361 0.82657201 0.82758621
 0.82758621 0.8346856  0.8346856  0.84279919 0.84482759 0.85294118
 0.85496957 0.85902637 0.86004057 0.86206897 0.86409736 0.86511156
 0.86713996 0.86815416 0.86815416 0.87221095 0.87423935 0.87728195
 0.87728195 0.88032454 0.88438134 0.90669371 0.90872211 0.90973631
 0.91176471 0.9158215  0.9168357  0.92089249 0.93204868 0.93407708
 0.94624746 0.94827586 0.97261663 0.97464503 0.98884381 0.98884381
 0.9979716  0.9979716  1.        ]
tpr: [0.         0.0010142  0.05273834 0.05476673 0.07099391 0.07707911
 0.0841785  0.0862069  0.09127789 0.09330629 0.09634888 0.09837728
 0.11054767 0.11460446 0.11764706 0.11967546 0.13590264 0.14401623
 0.14604462 0.14807302 0.14908722 0.15111562 0.15314402 0.15720081
 0.16024341 0.16227181 0.1663286  0.1693712  0.17444219 0.17647059
 0.17849899 0.18052738 0.19371197 0.19574037 0.19776876 0.20081136
 0.20283976 0.20993915 0.21298174 0.21602434 0.21805274 0.22008114
 0.22210953 0.22616633 0.22819473 0.23225152 0.23427992 0.24036511
 0.24239351 0.26369168 0.26572008 0.28296146 0.28498986 0.32555781
 0.32555781 0.3306288  0.3306288  0.34077079 0.34482759 0.34787018
 0.34989858 0.35496957 0.35699797 0.37221095 0.37626775 0.37931034
 0.38133874 0.38235294 0.38235294 0.39148073 0.39148073 0.40263692
 0.40466531 0.42393509 0.42596349 0.42799189 0.43002028 0.43711968
 0.43914807 0.44117647 0.44117647 0.44219067 0.44523327 0.44929006
 0.45233266 0.45436105 0.46146045 0.46450304 0.46551724 0.47160243
 0.47261663 0.47565923 0.47971602 0.48073022 0.48377282 0.48580122
 0.48681542 0.48681542 0.48782961 0.48985801 0.48985801 0.49087221
 0.49290061 0.4969574  0.4979716  0.5010142  0.5030426  0.50811359
 0.51014199 0.51115619 0.51318458 0.51724138 0.51926978 0.52028398
 0.52231237 0.52231237 0.52434077 0.53346856 0.53346856 0.54766734
 0.54969574 0.55476673 0.55476673 0.55882353 0.55882353 0.56186613
 0.56288032 0.57302231 0.57302231 0.5862069  0.5862069  0.5872211
 0.5872211  0.58823529 0.58823529 0.59127789 0.59127789 0.59229209
 0.59229209 0.59432049 0.59432049 0.59634888 0.59736308 0.59837728
 0.60040568 0.60243408 0.60243408 0.60446247 0.60649087 0.61460446
 0.61561866 0.61764706 0.62170385 0.62170385 0.63083164 0.63286004
 0.63387424 0.63387424 0.63488844 0.63793103 0.63793103 0.63995943
 0.63995943 0.64097363 0.64097363 0.64198783 0.64300203 0.64705882
 0.64908722 0.65415822 0.65415822 0.65821501 0.65922921 0.66227181
 0.663286   0.6653144  0.6653144  0.6673428  0.6693712  0.6693712
 0.67951318 0.67951318 0.68154158 0.68154158 0.68356998 0.68356998
 0.68762677 0.68864097 0.68864097 0.69066937 0.69269777 0.69371197
 0.69371197 0.69472617 0.69472617 0.69878296 0.69979716 0.70081136
 0.70486815 0.70588235 0.71095335 0.71298174 0.71399594 0.71399594
 0.71501014 0.71602434 0.71602434 0.71805274 0.71805274 0.72109533
 0.72109533 0.72210953 0.72312373 0.72312373 0.72515213 0.72718053
 0.72718053 0.72819473 0.72819473 0.72920892 0.72920892 0.73123732
 0.73326572 0.73326572 0.73529412 0.73630832 0.74036511 0.74137931
 0.74239351 0.74442191 0.74543611 0.74543611 0.7494929  0.7494929
 0.7515213  0.7515213  0.7535497  0.7535497  0.75456389 0.75456389
 0.75557809 0.75557809 0.76166329 0.76166329 0.76166329 0.76267748
 0.76267748 0.76369168 0.76369168 0.76572008 0.76572008 0.76673428
 0.76673428 0.76774848 0.76977688 0.76977688 0.77079108 0.77281947
 0.77586207 0.77586207 0.77586207 0.77687627 0.77890467 0.77890467
 0.77991886 0.77991886 0.78093306 0.78093306 0.78296146 0.78296146
 0.78498986 0.78498986 0.78498986 0.78803245 0.78803245 0.78904665
 0.78904665 0.79006085 0.79006085 0.79208925 0.79310345 0.79310345
 0.79411765 0.79411765 0.79614604 0.79817444 0.79817444 0.79918864
 0.80020284 0.80121704 0.80121704 0.80223124 0.80324544 0.80628803
 0.80628803 0.80730223 0.80730223 0.80933063 0.80933063 0.81034483
 0.81135903 0.81135903 0.81237323 0.81237323 0.81338742 0.81338742
 0.81440162 0.81440162 0.81744422 0.81744422 0.82048682 0.82048682
 0.82251521 0.82251521 0.82352941 0.82352941 0.82657201 0.82657201
 0.82860041 0.82860041 0.8296146  0.8296146  0.8306288  0.8306288
 0.8346856  0.8346856  0.8356998  0.8356998  0.83874239 0.83874239
 0.83975659 0.83975659 0.84077079 0.84077079 0.84178499 0.84279919
 0.84279919 0.84381339 0.84381339 0.84482759 0.84482759 0.84584178
 0.84584178 0.84685598 0.84685598 0.84787018 0.84787018 0.84888438
 0.84888438 0.84989858 0.84989858 0.85192698 0.85192698 0.85395538
 0.85395538 0.85496957 0.85496957 0.85699797 0.85801217 0.85801217
 0.86004057 0.86004057 0.86105477 0.86105477 0.86206897 0.86206897
 0.86409736 0.86409736 0.86713996 0.86713996 0.86916836 0.86916836
 0.87221095 0.87221095 0.87423935 0.87423935 0.87626775 0.87626775
 0.87626775 0.87626775 0.87829615 0.87931034 0.88032454 0.88032454
 0.88133874 0.88133874 0.88235294 0.88235294 0.88539554 0.88539554
 0.88640974 0.88640974 0.88742394 0.88742394 0.88843813 0.88843813
 0.88945233 0.88945233 0.89148073 0.89148073 0.89249493 0.89249493
 0.89350913 0.89350913 0.89452333 0.89452333 0.89553753 0.89553753
 0.89553753 0.89553753 0.89655172 0.89655172 0.89655172 0.89858012
 0.89858012 0.89959432 0.89959432 0.90060852 0.90060852 0.90263692
 0.90263692 0.90365112 0.90365112 0.90466531 0.90567951 0.90567951
 0.90669371 0.90669371 0.90770791 0.90770791 0.90973631 0.90973631
 0.91075051 0.91075051 0.9127789  0.9127789  0.9137931  0.9137931
 0.9148073  0.9148073  0.9158215  0.9158215  0.9168357  0.9168357
 0.9178499  0.9178499  0.9188641  0.9188641  0.9198783  0.9198783
 0.92089249 0.92089249 0.92190669 0.92190669 0.92292089 0.92292089
 0.92393509 0.92393509 0.92494929 0.92494929 0.92596349 0.92596349
 0.92697769 0.92697769 0.92799189 0.92799189 0.92900609 0.92900609
 0.93002028 0.93002028 0.93103448 0.93103448 0.93204868 0.93204868
 0.93204868 0.93204868 0.93306288 0.93306288 0.93509128 0.93509128
 0.93610548 0.93610548 0.93711968 0.93711968 0.93813387 0.93813387
 0.93914807 0.93914807 0.94016227 0.94016227 0.94117647 0.94117647
 0.94219067 0.94219067 0.94320487 0.94320487 0.94523327 0.94523327
 0.94624746 0.94624746 0.94726166 0.94726166 0.94827586 0.94827586
 0.94929006 0.94929006 0.95030426 0.95030426 0.95030426 0.95030426
 0.95131846 0.95131846 0.95233266 0.95233266 0.95334686 0.95334686
 0.95436105 0.95436105 0.95537525 0.95537525 0.95638945 0.95638945
 0.95740365 0.95740365 0.95841785 0.95841785 0.95943205 0.95943205
 0.96044625 0.96044625 0.96146045 0.96146045 0.96450304 0.96450304
 0.96551724 0.96551724 0.96551724 0.96551724 0.96551724 0.96551724
 0.96653144 0.96653144 0.96754564 0.96754564 0.96957404 0.96957404
 0.97160243 0.97160243 0.97160243 0.97160243 0.97160243 0.97160243
 0.97160243 0.97160243 0.97261663 0.97261663 0.97261663 0.97261663
 0.97363083 0.97363083 0.97464503 0.97464503 0.97565923 0.97565923
 0.97667343 0.97667343 0.97768763 0.97768763 0.97768763 0.97768763
 0.97870183 0.97870183 0.97971602 0.97971602 0.98073022 0.98073022
 0.98073022 0.98073022 0.98073022 0.98073022 0.98174442 0.98174442
 0.98174442 0.98174442 0.98275862 0.98275862 0.98377282 0.98377282
 0.98377282 0.98377282 0.98377282 0.98478702 0.98478702 0.98478702
 0.98478702 0.98478702 0.98478702 0.98478702 0.98580122 0.98580122
 0.98580122 0.98580122 0.98681542 0.98681542 0.98681542 0.98681542
 0.98782961 0.98782961 0.98782961 0.98782961 0.98782961 0.98782961
 0.98884381 0.98884381 0.98884381 0.98884381 0.98884381 0.98884381
 0.98985801 0.98985801 0.99087221 0.99087221 0.99087221 0.99087221
 0.99188641 0.99188641 0.99290061 0.99290061 0.99290061 0.99290061
 0.99290061 0.99290061 0.99391481 0.99391481 0.99492901 0.99492901
 0.99492901 0.99492901 0.9959432  0.9959432  0.9959432  0.9959432
 0.9969574  0.9969574  0.9969574  0.9969574  0.9969574  0.9969574
 0.9969574  0.9969574  0.9979716  0.9979716  0.9979716  0.9979716
 0.9979716  0.9979716  0.9979716  0.9979716  0.9979716  0.9989858
 0.9989858  1.         1.        ]
thresholds: [            inf  3.83593750e+00  3.02734375e+00  3.00781250e+00
  2.88281250e+00  2.84960938e+00  2.79882812e+00  2.78906250e+00
  2.74218750e+00  2.74023438e+00  2.70117188e+00  2.69531250e+00
  2.63085938e+00  2.62109375e+00  2.61132812e+00  2.60742188e+00
  2.52929688e+00  2.50195312e+00  2.47851562e+00  2.47656250e+00
  2.46484375e+00  2.46289062e+00  2.45703125e+00  2.45507812e+00
  2.43945312e+00  2.42578125e+00  2.41015625e+00  2.40820312e+00
  2.37890625e+00  2.37695312e+00  2.37304688e+00  2.36718750e+00
  2.29882812e+00  2.29492188e+00  2.27929688e+00  2.27734375e+00
  2.27343750e+00  2.23437500e+00  2.23242188e+00  2.22070312e+00
  2.21875000e+00  2.18750000e+00  2.18359375e+00  2.16601562e+00
  2.16406250e+00  2.14257812e+00  2.14062500e+00  2.09570312e+00
  2.08398438e+00  1.97167969e+00  1.96289062e+00  1.83300781e+00
  1.83007812e+00  1.61230469e+00  1.59375000e+00  1.56933594e+00
  1.56835938e+00  1.51171875e+00  1.50683594e+00  1.49511719e+00
  1.49414062e+00  1.46484375e+00  1.45507812e+00  1.36132812e+00
  1.34960938e+00  1.33105469e+00  1.32031250e+00  1.31835938e+00
  1.31738281e+00  1.28125000e+00  1.28027344e+00  1.25781250e+00
  1.25390625e+00  1.20019531e+00  1.19531250e+00  1.18554688e+00
  1.18457031e+00  1.16113281e+00  1.15136719e+00  1.13769531e+00
  1.13476562e+00  1.13378906e+00  1.12792969e+00  1.11132812e+00
  1.10449219e+00  1.09960938e+00  1.06933594e+00  1.06835938e+00
  1.06250000e+00  1.05566406e+00  1.05175781e+00  1.04687500e+00
  1.03515625e+00  1.03222656e+00  1.02832031e+00  1.02636719e+00
  1.02539062e+00  1.01953125e+00  1.01855469e+00  1.01367188e+00
  1.01269531e+00  1.00585938e+00  1.00390625e+00  9.91210938e-01
  9.90722656e-01  9.86816406e-01  9.85839844e-01  9.75585938e-01
  9.70214844e-01  9.62890625e-01  9.61914062e-01  9.52636719e-01
  9.51660156e-01  9.50683594e-01  9.45312500e-01  9.42382812e-01
  9.41406250e-01  9.03320312e-01  9.02832031e-01  8.60351562e-01
  8.58398438e-01  8.47656250e-01  8.45703125e-01  8.40332031e-01
  8.39355469e-01  8.28613281e-01  8.28125000e-01  8.02246094e-01
  7.95898438e-01  7.75878906e-01  7.71972656e-01  7.64160156e-01
  7.63183594e-01  7.62695312e-01  7.62207031e-01  7.55859375e-01
  7.52929688e-01  7.52441406e-01  7.51953125e-01  7.47070312e-01
  7.45117188e-01  7.43164062e-01  7.40722656e-01  7.39746094e-01
  7.38769531e-01  7.34863281e-01  7.33398438e-01  7.24609375e-01
  7.24121094e-01  7.07519531e-01  7.07031250e-01  7.02636719e-01
  6.99218750e-01  6.98730469e-01  6.84082031e-01  6.81152344e-01
  6.78222656e-01  6.75781250e-01  6.73828125e-01  6.71386719e-01
  6.68945312e-01  6.68457031e-01  6.64550781e-01  6.63574219e-01
  6.59667969e-01  6.58691406e-01  6.56738281e-01  6.52832031e-01
  6.51855469e-01  6.38183594e-01  6.37695312e-01  6.33300781e-01
  6.30859375e-01  6.27441406e-01  6.23535156e-01  6.21093750e-01
  6.18164062e-01  6.16699219e-01  6.15234375e-01  6.14257812e-01
  5.94238281e-01  5.93750000e-01  5.91796875e-01  5.90820312e-01
  5.88378906e-01  5.87890625e-01  5.86914062e-01  5.84960938e-01
  5.82031250e-01  5.80078125e-01  5.78125000e-01  5.76660156e-01
  5.73242188e-01  5.72265625e-01  5.70312500e-01  5.66406250e-01
  5.65917969e-01  5.65429688e-01  5.60058594e-01  5.59570312e-01
  5.53222656e-01  5.52734375e-01  5.49804688e-01  5.47851562e-01
  5.46386719e-01  5.44921875e-01  5.42968750e-01  5.38574219e-01
  5.38085938e-01  5.33691406e-01  5.32714844e-01  5.32226562e-01
  5.30761719e-01  5.29785156e-01  5.27832031e-01  5.25390625e-01
  5.24414062e-01  5.23437500e-01  5.22949219e-01  5.22460938e-01
  5.20996094e-01  5.19042969e-01  5.18554688e-01  5.16113281e-01
  5.13183594e-01  5.11718750e-01  5.05859375e-01  5.05371094e-01
  5.03906250e-01  5.00488281e-01  4.98535156e-01  4.98046875e-01
  4.91943359e-01  4.91699219e-01  4.90234375e-01  4.86083984e-01
  4.85839844e-01  4.83398438e-01  4.82910156e-01  4.82666016e-01
  4.79003906e-01  4.76562500e-01  4.71923828e-01  4.71191406e-01
  4.69970703e-01  4.69482422e-01  4.66064453e-01  4.65332031e-01
  4.63867188e-01  4.62158203e-01  4.56298828e-01  4.55566406e-01
  4.52392578e-01  4.49462891e-01  4.48730469e-01  4.46777344e-01
  4.46289062e-01  4.39941406e-01  4.37988281e-01  4.37500000e-01
  4.34570312e-01  4.33105469e-01  4.31640625e-01  4.31152344e-01
  4.28710938e-01  4.23339844e-01  4.22851562e-01  4.16992188e-01
  4.16259766e-01  4.14794922e-01  4.09912109e-01  4.09423828e-01
  4.06250000e-01  4.05273438e-01  4.04785156e-01  4.04296875e-01
  4.03320312e-01  3.99902344e-01  3.97949219e-01  3.93554688e-01
  3.92089844e-01  3.91601562e-01  3.90869141e-01  3.89892578e-01
  3.89160156e-01  3.86230469e-01  3.83544922e-01  3.83056641e-01
  3.81347656e-01  3.80859375e-01  3.80615234e-01  3.80371094e-01
  3.79882812e-01  3.77441406e-01  3.65234375e-01  3.64990234e-01
  3.63525391e-01  3.62548828e-01  3.61328125e-01  3.60107422e-01
  3.59130859e-01  3.55468750e-01  3.55224609e-01  3.49121094e-01
  3.46923828e-01  3.45458984e-01  3.45214844e-01  3.43505859e-01
  3.40820312e-01  3.40576172e-01  3.37890625e-01  3.31298828e-01
  3.29833984e-01  3.28125000e-01  3.27392578e-01  3.26904297e-01
  3.25683594e-01  3.24951172e-01  3.23242188e-01  3.22753906e-01
  3.21777344e-01  3.20312500e-01  3.19580078e-01  3.15185547e-01
  3.09814453e-01  3.08349609e-01  3.07861328e-01  3.05664062e-01
  3.03466797e-01  2.98828125e-01  2.96875000e-01  2.93701172e-01
  2.91503906e-01  2.89306641e-01  2.89062500e-01  2.87353516e-01
  2.86376953e-01  2.85400391e-01  2.84912109e-01  2.84179688e-01
  2.83935547e-01  2.82714844e-01  2.82470703e-01  2.81494141e-01
  2.80029297e-01  2.79785156e-01  2.78564453e-01  2.77587891e-01
  2.76123047e-01  2.75634766e-01  2.74169922e-01  2.69287109e-01
  2.67822266e-01  2.66845703e-01  2.65869141e-01  2.63427734e-01
  2.63183594e-01  2.62695312e-01  2.60986328e-01  2.60009766e-01
  2.58789062e-01  2.53173828e-01  2.52929688e-01  2.50000000e-01
  2.49755859e-01  2.49511719e-01  2.46704102e-01  2.45483398e-01
  2.43896484e-01  2.40234375e-01  2.38037109e-01  2.37182617e-01
  2.34863281e-01  2.31323242e-01  2.27416992e-01  2.27050781e-01
  2.24853516e-01  2.13378906e-01  2.12402344e-01  2.10449219e-01
  2.08374023e-01  2.08251953e-01  2.07031250e-01  2.06787109e-01
  2.06298828e-01  1.98974609e-01  1.98852539e-01  1.98608398e-01
  1.94824219e-01  1.94702148e-01  1.94580078e-01  1.90795898e-01
  1.90063477e-01  1.89941406e-01  1.88842773e-01  1.84692383e-01
  1.84448242e-01  1.82617188e-01  1.80908203e-01  1.78466797e-01
  1.78222656e-01  1.77124023e-01  1.75781250e-01  1.75415039e-01
  1.74072266e-01  1.71997070e-01  1.70166016e-01  1.60034180e-01
  1.59179688e-01  1.53930664e-01  1.48803711e-01  1.48681641e-01
  1.47583008e-01  1.46484375e-01  1.39526367e-01  1.37451172e-01
  1.35864258e-01  1.35009766e-01  1.30004883e-01  1.28662109e-01
  1.24938965e-01  1.24572754e-01  1.23046875e-01  1.21948242e-01
  1.20971680e-01  1.17126465e-01  1.16455078e-01  1.16027832e-01
  1.15783691e-01  1.15295410e-01  1.13281250e-01  1.09985352e-01
  1.09680176e-01  1.04187012e-01  1.03149414e-01  1.01257324e-01
  1.00830078e-01  9.68627930e-02  9.66186523e-02  9.36889648e-02
  9.18579102e-02  8.92333984e-02  8.79516602e-02  8.77685547e-02
  8.74023438e-02  8.28857422e-02  8.28247070e-02  7.56225586e-02
  7.55004883e-02  7.12890625e-02  7.05566406e-02  6.84814453e-02
  6.74438477e-02  6.73217773e-02  6.69555664e-02  6.65283203e-02
  6.61010742e-02  6.43310547e-02  6.41479492e-02  6.25610352e-02
  6.16455078e-02  6.14624023e-02  6.09741211e-02  5.76477051e-02
  5.71899414e-02  5.44738770e-02  5.40161133e-02  4.77294922e-02
  4.69360352e-02  4.52880859e-02  4.42810059e-02  4.26025391e-02
  4.15039062e-02  3.90930176e-02  3.81469727e-02  1.83868408e-02
  1.80511475e-02  1.70593262e-02  1.63116455e-02  8.89587402e-03
  7.58743286e-03  5.45120239e-03  4.12368774e-03  3.84140015e-03
  3.52859497e-03  2.09426880e-03  1.31607056e-03 -1.37710571e-03
 -1.59072876e-03 -9.01794434e-03 -1.08947754e-02 -1.19781494e-02
 -1.30767822e-02 -1.44500732e-02 -1.50604248e-02 -2.12249756e-02
 -2.30560303e-02 -3.35083008e-02 -3.35693359e-02 -3.47595215e-02
 -3.65295410e-02 -3.69873047e-02 -3.80554199e-02 -4.10156250e-02
 -4.19006348e-02 -4.47998047e-02 -4.49218750e-02 -4.49829102e-02
 -4.51049805e-02 -4.71496582e-02 -4.72717285e-02 -5.02929688e-02
 -5.05065918e-02 -5.67321777e-02 -5.67932129e-02 -7.69653320e-02
 -7.72094727e-02 -8.64868164e-02 -8.65478516e-02 -8.89892578e-02
 -8.94775391e-02 -9.04541016e-02 -9.05151367e-02 -9.24682617e-02
 -9.42382812e-02 -9.87548828e-02 -1.00524902e-01 -1.04797363e-01
 -1.05285645e-01 -1.05834961e-01 -1.06811523e-01 -1.07543945e-01
 -1.11938477e-01 -1.21704102e-01 -1.21887207e-01 -1.24267578e-01
 -1.24328613e-01 -1.32080078e-01 -1.32202148e-01 -1.34033203e-01
 -1.34765625e-01 -1.39160156e-01 -1.40991211e-01 -1.42089844e-01
 -1.44897461e-01 -1.51855469e-01 -1.54418945e-01 -1.56372070e-01
 -1.56616211e-01 -1.60522461e-01 -1.61987305e-01 -1.81030273e-01
 -1.81518555e-01 -1.86157227e-01 -1.87011719e-01 -1.96899414e-01
 -1.98730469e-01 -1.99584961e-01 -2.00439453e-01 -2.05444336e-01
 -2.05810547e-01 -2.17651367e-01 -2.18750000e-01 -2.27172852e-01
 -2.27661133e-01 -2.31201172e-01 -2.31323242e-01 -2.40234375e-01
 -2.40966797e-01 -2.49145508e-01 -2.50244141e-01 -2.51220703e-01
 -2.52441406e-01 -2.67578125e-01 -2.69287109e-01 -2.69775391e-01
 -2.71972656e-01 -2.80517578e-01 -2.80761719e-01 -2.82470703e-01
 -2.85156250e-01 -2.92724609e-01 -2.93212891e-01 -2.98828125e-01
 -3.00292969e-01 -3.07617188e-01 -3.08349609e-01 -3.08837891e-01
 -3.10058594e-01 -3.15917969e-01 -3.17626953e-01 -3.17871094e-01
 -3.25683594e-01 -3.28857422e-01 -3.30566406e-01 -3.31542969e-01
 -3.34716797e-01 -3.34960938e-01 -3.37158203e-01 -3.38623047e-01
 -3.39111328e-01 -3.46191406e-01 -3.48876953e-01 -3.59375000e-01
 -3.60839844e-01 -3.64501953e-01 -3.65234375e-01 -3.66943359e-01
 -3.72070312e-01 -3.77441406e-01 -3.79882812e-01 -3.99658203e-01
 -4.02832031e-01 -4.07226562e-01 -4.08447266e-01 -4.14794922e-01
 -4.16015625e-01 -4.26757812e-01 -4.29199219e-01 -4.72412109e-01
 -4.76318359e-01 -4.79980469e-01 -4.81445312e-01 -4.82666016e-01
 -4.83642578e-01 -4.91210938e-01 -4.93652344e-01 -5.01464844e-01
 -5.02441406e-01 -5.13671875e-01 -5.17089844e-01 -5.32714844e-01
 -5.34667969e-01 -5.37109375e-01 -5.37597656e-01 -5.38574219e-01
 -5.40039062e-01 -5.41992188e-01 -5.43457031e-01 -5.53710938e-01
 -5.57128906e-01 -5.62011719e-01 -5.63964844e-01 -5.67871094e-01
 -5.71289062e-01 -6.29882812e-01 -6.32324219e-01 -6.33789062e-01
 -6.34277344e-01 -6.43066406e-01 -6.44531250e-01 -6.59667969e-01
 -7.15332031e-01 -7.16796875e-01 -7.96386719e-01 -8.00292969e-01
 -9.25292969e-01 -9.28710938e-01 -1.04003906e+00 -1.05078125e+00
 -1.22558594e+00 -1.23632812e+00 -1.54785156e+00]
