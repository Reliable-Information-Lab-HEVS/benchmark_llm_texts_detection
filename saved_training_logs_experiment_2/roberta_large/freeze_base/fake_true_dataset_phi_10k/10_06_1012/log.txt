log_loss_steps: 256
eval_steps: 512
check_degradation: 0
----------------Epoch 1/1----------------
Epoch 1/1, Loss after 192 samples: 0.7105
Epoch 1/1, Loss after 448 samples: 0.6745
Mean accuracy: 0.7350, std: 0.0099, lower bound: 0.7154, upper bound: 0.7540 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 448 samples: 0.7349 with eval loss: 0.6439
Best model with eval loss 0.6439074277877808 and eval accuracy 0.7349397590361446 with 448 samples seen is saved
Epoch 1/1, Loss after 704 samples: 0.6436
Epoch 1/1, Loss after 960 samples: 0.6059
Mean accuracy: 0.7585, std: 0.0097, lower bound: 0.7395, upper bound: 0.7771 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 960 samples: 0.7580 with eval loss: 0.5306
Best model with eval loss 0.5306437164545059 and eval accuracy 0.7580321285140562 with 960 samples seen is saved
Epoch 1/1, Loss after 1216 samples: 0.5772
Epoch 1/1, Loss after 1472 samples: 0.5491
Mean accuracy: 0.7978, std: 0.0092, lower bound: 0.7796, upper bound: 0.8163 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 1472 samples: 0.7982 with eval loss: 0.4743
Best model with eval loss 0.47428772039711475 and eval accuracy 0.7981927710843374 with 1472 samples seen is saved
Epoch 1/1, Loss after 1728 samples: 0.4837
Epoch 1/1, Loss after 1984 samples: 0.5550
Mean accuracy: 0.8178, std: 0.0087, lower bound: 0.8002, upper bound: 0.8333 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 1984 samples: 0.8183 with eval loss: 0.4322
Best model with eval loss 0.4322146065533161 and eval accuracy 0.8182730923694779 with 1984 samples seen is saved
Epoch 1/1, Loss after 2240 samples: 0.4809
Epoch 1/1, Loss after 2496 samples: 0.4751
Mean accuracy: 0.8472, std: 0.0079, lower bound: 0.8313, upper bound: 0.8619 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 2496 samples: 0.8474 with eval loss: 0.4065
Best model with eval loss 0.4065232574939728 and eval accuracy 0.8473895582329317 with 2496 samples seen is saved
Epoch 1/1, Loss after 2752 samples: 0.5161
Epoch 1/1, Loss after 3008 samples: 0.4799
Mean accuracy: 0.6481, std: 0.0107, lower bound: 0.6275, upper bound: 0.6687 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 3008 samples: 0.6481 with eval loss: 0.5744
Epoch 1/1, Loss after 3264 samples: 0.4614
Epoch 1/1, Loss after 3520 samples: 0.4662
Mean accuracy: 0.8341, std: 0.0085, lower bound: 0.8178, upper bound: 0.8509 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 3520 samples: 0.8343 with eval loss: 0.3852
Best model with eval loss 0.3852456621825695 and eval accuracy 0.8343373493975904 with 3520 samples seen is saved
Epoch 1/1, Loss after 3776 samples: 0.5004
Epoch 1/1, Loss after 4032 samples: 0.4767
Mean accuracy: 0.8438, std: 0.0081, lower bound: 0.8273, upper bound: 0.8589 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 4032 samples: 0.8439 with eval loss: 0.3811
Best model with eval loss 0.3811305910348892 and eval accuracy 0.8438755020080321 with 4032 samples seen is saved
Epoch 1/1, Loss after 4288 samples: 0.4706
Epoch 1/1, Loss after 4544 samples: 0.5177
Mean accuracy: 0.8621, std: 0.0076, lower bound: 0.8464, upper bound: 0.8770 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 4544 samples: 0.8619 with eval loss: 0.3652
Best model with eval loss 0.3651806125417352 and eval accuracy 0.8619477911646586 with 4544 samples seen is saved
Epoch 1/1, Loss after 4800 samples: 0.4686
Epoch 1/1, Loss after 5056 samples: 0.4534
Mean accuracy: 0.7779, std: 0.0093, lower bound: 0.7600, upper bound: 0.7962 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 5056 samples: 0.7776 with eval loss: 0.4220
Epoch 1/1, Loss after 5312 samples: 0.4528
Epoch 1/1, Loss after 5568 samples: 0.4412
Mean accuracy: 0.8555, std: 0.0078, lower bound: 0.8404, upper bound: 0.8705 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 5568 samples: 0.8554 with eval loss: 0.3551
Best model with eval loss 0.35509845800697803 and eval accuracy 0.8554216867469879 with 5568 samples seen is saved
Epoch 1/1, Loss after 5824 samples: 0.5046
Epoch 1/1, Loss after 6080 samples: 0.4644
Mean accuracy: 0.8525, std: 0.0078, lower bound: 0.8378, upper bound: 0.8685 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 6080 samples: 0.8524 with eval loss: 0.3547
Best model with eval loss 0.3547275299206376 and eval accuracy 0.8524096385542169 with 6080 samples seen is saved
Epoch 1/1, Loss after 6336 samples: 0.4422
Epoch 1/1, Loss after 6592 samples: 0.4127
Mean accuracy: 0.8571, std: 0.0076, lower bound: 0.8424, upper bound: 0.8720 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 6592 samples: 0.8569 with eval loss: 0.3477
Best model with eval loss 0.34766088612377644 and eval accuracy 0.8569277108433735 with 6592 samples seen is saved
Epoch 1/1, Loss after 6848 samples: 0.4909
Epoch 1/1, Loss after 7104 samples: 0.4698
Mean accuracy: 0.8574, std: 0.0077, lower bound: 0.8429, upper bound: 0.8720 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 7104 samples: 0.8574 with eval loss: 0.3471
Best model with eval loss 0.34711993858218193 and eval accuracy 0.857429718875502 with 7104 samples seen is saved
Epoch 1/1, Loss after 7360 samples: 0.4152
Epoch 1/1, Loss after 7616 samples: 0.4441
Mean accuracy: 0.8677, std: 0.0077, lower bound: 0.8529, upper bound: 0.8830 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 7616 samples: 0.8680 with eval loss: 0.3417
Best model with eval loss 0.34170645102858543 and eval accuracy 0.8679718875502008 with 7616 samples seen is saved
Epoch 1/1, Loss after 7872 samples: 0.4373
Epoch 1/1, Loss after 8128 samples: 0.4308
Mean accuracy: 0.8678, std: 0.0075, lower bound: 0.8534, upper bound: 0.8825 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 8128 samples: 0.8680 with eval loss: 0.3389
Best model with eval loss 0.3389428253285587 and eval accuracy 0.8679718875502008 with 8128 samples seen is saved
Epoch 1/1, Loss after 8384 samples: 0.4180
Epoch 1/1, Loss after 8640 samples: 0.4660
Mean accuracy: 0.8776, std: 0.0076, lower bound: 0.8624, upper bound: 0.8926 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 8640 samples: 0.8775 with eval loss: 0.3354
Best model with eval loss 0.3354147826321423 and eval accuracy 0.8775100401606426 with 8640 samples seen is saved
Epoch 1/1, Loss after 8896 samples: 0.4126
Epoch 1/1, Loss after 9152 samples: 0.4877
Mean accuracy: 0.8530, std: 0.0082, lower bound: 0.8368, upper bound: 0.8680 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 9152 samples: 0.8539 with eval loss: 0.3557
Epoch 1/1, Loss after 9408 samples: 0.4235
Epoch 1/1, Loss after 9664 samples: 0.4202
Mean accuracy: 0.7539, std: 0.0095, lower bound: 0.7354, upper bound: 0.7731 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 9664 samples: 0.7540 with eval loss: 0.4593
Epoch 1/1, Loss after 9920 samples: 0.5041
Epoch 1/1, Loss after 10176 samples: 0.4322
Mean accuracy: 0.7247, std: 0.0095, lower bound: 0.7058, upper bound: 0.7435 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 10176 samples: 0.7249 with eval loss: 0.4913
Epoch 1/1, Loss after 10432 samples: 0.4597
Epoch 1/1, Loss after 10688 samples: 0.4286
Mean accuracy: 0.8430, std: 0.0083, lower bound: 0.8263, upper bound: 0.8589 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 10688 samples: 0.8429 with eval loss: 0.3526
Epoch 1/1, Loss after 10944 samples: 0.4928
Epoch 1/1, Loss after 11200 samples: 0.4158
Mean accuracy: 0.8395, std: 0.0082, lower bound: 0.8243, upper bound: 0.8549 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 11200 samples: 0.8394 with eval loss: 0.3479
Epoch 1/1, Loss after 11456 samples: 0.4171
Epoch 1/1, Loss after 11712 samples: 0.4844
Mean accuracy: 0.8592, std: 0.0079, lower bound: 0.8439, upper bound: 0.8750 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 11712 samples: 0.8594 with eval loss: 0.3325
Best model with eval loss 0.3324584299698472 and eval accuracy 0.8594377510040161 with 11712 samples seen is saved
Epoch 1/1, Loss after 11968 samples: 0.4248
Epoch 1/1, Loss after 12224 samples: 0.4884
Mean accuracy: 0.8221, std: 0.0092, lower bound: 0.8032, upper bound: 0.8389 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 12224 samples: 0.8223 with eval loss: 0.3783
Epoch 1/1, Loss after 12480 samples: 0.4253
Epoch 1/1, Loss after 12736 samples: 0.4261
Mean accuracy: 0.8291, std: 0.0084, lower bound: 0.8128, upper bound: 0.8454 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 12736 samples: 0.8293 with eval loss: 0.3759
Epoch 1/1, Loss after 12992 samples: 0.4257
Epoch 1/1, Loss after 13248 samples: 0.3797
Mean accuracy: 0.8789, std: 0.0072, lower bound: 0.8645, upper bound: 0.8931 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 13248 samples: 0.8785 with eval loss: 0.3268
Best model with eval loss 0.32680560601875186 and eval accuracy 0.8785140562248996 with 13248 samples seen is saved
Epoch 1/1, Loss after 13504 samples: 0.3810
Epoch 1/1, Loss after 13760 samples: 0.3785
Mean accuracy: 0.8297, std: 0.0083, lower bound: 0.8133, upper bound: 0.8454 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 13760 samples: 0.8298 with eval loss: 0.3774
Epoch 1/1, Loss after 14016 samples: 0.3670
Epoch 1/1, Loss after 14272 samples: 0.3899
Mean accuracy: 0.8744, std: 0.0074, lower bound: 0.8599, upper bound: 0.8881 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 14272 samples: 0.8745 with eval loss: 0.3232
Best model with eval loss 0.3231624965555966 and eval accuracy 0.8744979919678715 with 14272 samples seen is saved
Epoch 1/1, Loss after 14528 samples: 0.4408
Epoch 1/1, Loss after 14784 samples: 0.3705
Mean accuracy: 0.8461, std: 0.0080, lower bound: 0.8298, upper bound: 0.8614 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 14784 samples: 0.8464 with eval loss: 0.3503
Epoch 1/1, Loss after 15040 samples: 0.4066
Epoch 1/1, Loss after 15296 samples: 0.4403
Mean accuracy: 0.8665, std: 0.0074, lower bound: 0.8524, upper bound: 0.8810 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 15296 samples: 0.8665 with eval loss: 0.3310
Epoch 1/1, Loss after 15552 samples: 0.3926
Epoch 1/1, Loss after 15808 samples: 0.4358
Mean accuracy: 0.8736, std: 0.0075, lower bound: 0.8594, upper bound: 0.8891 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 15808 samples: 0.8730 with eval loss: 0.3274
Training signal is False, stopping training
Training finished
Best model: {'eval_acc': 0.8744979919678715, 'nb_samples': 14272, 'eval_loss': 0.3231624965555966}
Training loss logs: [{'samples': 192, 'loss': 0.7105007171630859}, {'samples': 448, 'loss': 0.6745443344116211}, {'samples': 704, 'loss': 0.6436378955841064}, {'samples': 960, 'loss': 0.6058827638626099}, {'samples': 1216, 'loss': 0.5772081613540649}, {'samples': 1472, 'loss': 0.5490964651107788}, {'samples': 1728, 'loss': 0.4836954176425934}, {'samples': 1984, 'loss': 0.5550243258476257}, {'samples': 2240, 'loss': 0.4809299409389496}, {'samples': 2496, 'loss': 0.47507011890411377}, {'samples': 2752, 'loss': 0.5160770937800407}, {'samples': 3008, 'loss': 0.4798816964030266}, {'samples': 3264, 'loss': 0.46137145161628723}, {'samples': 3520, 'loss': 0.4661925584077835}, {'samples': 3776, 'loss': 0.5004391372203827}, {'samples': 4032, 'loss': 0.4767066389322281}, {'samples': 4288, 'loss': 0.4706129804253578}, {'samples': 4544, 'loss': 0.5177080258727074}, {'samples': 4800, 'loss': 0.46858230978250504}, {'samples': 5056, 'loss': 0.4533744305372238}, {'samples': 5312, 'loss': 0.45280902832746506}, {'samples': 5568, 'loss': 0.44121239334344864}, {'samples': 5824, 'loss': 0.5046307742595673}, {'samples': 6080, 'loss': 0.464434877038002}, {'samples': 6336, 'loss': 0.44219981133937836}, {'samples': 6592, 'loss': 0.412680484354496}, {'samples': 6848, 'loss': 0.4908945709466934}, {'samples': 7104, 'loss': 0.4698266535997391}, {'samples': 7360, 'loss': 0.41517373919487}, {'samples': 7616, 'loss': 0.4441210553050041}, {'samples': 7872, 'loss': 0.4373069778084755}, {'samples': 8128, 'loss': 0.4307979717850685}, {'samples': 8384, 'loss': 0.4179832860827446}, {'samples': 8640, 'loss': 0.4659740924835205}, {'samples': 8896, 'loss': 0.4126352444291115}, {'samples': 9152, 'loss': 0.4877375215291977}, {'samples': 9408, 'loss': 0.4234885573387146}, {'samples': 9664, 'loss': 0.42021988332271576}, {'samples': 9920, 'loss': 0.5040954574942589}, {'samples': 10176, 'loss': 0.43220172822475433}, {'samples': 10432, 'loss': 0.45966411381959915}, {'samples': 10688, 'loss': 0.4286191910505295}, {'samples': 10944, 'loss': 0.4928003177046776}, {'samples': 11200, 'loss': 0.41576550155878067}, {'samples': 11456, 'loss': 0.41713055968284607}, {'samples': 11712, 'loss': 0.4843749552965164}, {'samples': 11968, 'loss': 0.4247887283563614}, {'samples': 12224, 'loss': 0.4883587807416916}, {'samples': 12480, 'loss': 0.4252640977501869}, {'samples': 12736, 'loss': 0.4260801002383232}, {'samples': 12992, 'loss': 0.42570810765028}, {'samples': 13248, 'loss': 0.3796786591410637}, {'samples': 13504, 'loss': 0.38097044080495834}, {'samples': 13760, 'loss': 0.3785296604037285}, {'samples': 14016, 'loss': 0.3670261725783348}, {'samples': 14272, 'loss': 0.3898567259311676}, {'samples': 14528, 'loss': 0.440833255648613}, {'samples': 14784, 'loss': 0.3705306127667427}, {'samples': 15040, 'loss': 0.4065564349293709}, {'samples': 15296, 'loss': 0.4403325915336609}, {'samples': 15552, 'loss': 0.39259790629148483}, {'samples': 15808, 'loss': 0.43578624725341797}]
Evaluation accuracy logs: [{'samples': 448, 'accuracy': 0.7350401606425703, 'std': 0.00991798149797841, 'lower_bound': 0.7153614457831325, 'upper_bound': 0.7540286144578313}, {'samples': 960, 'accuracy': 0.7584764056224899, 'std': 0.009676857863461013, 'lower_bound': 0.7394578313253012, 'upper_bound': 0.7771209839357429}, {'samples': 1472, 'accuracy': 0.7978192771084337, 'std': 0.009213816418473878, 'lower_bound': 0.7796059236947791, 'upper_bound': 0.8162650602409639}, {'samples': 1984, 'accuracy': 0.8178368473895582, 'std': 0.008669430744925379, 'lower_bound': 0.8001882530120481, 'upper_bound': 0.8333333333333334}, {'samples': 2496, 'accuracy': 0.847187751004016, 'std': 0.007879485769208643, 'lower_bound': 0.8313253012048193, 'upper_bound': 0.8619477911646586}, {'samples': 3008, 'accuracy': 0.6480712851405621, 'std': 0.010732131442733453, 'lower_bound': 0.6274974899598393, 'upper_bound': 0.6686746987951807}, {'samples': 3520, 'accuracy': 0.8340853413654619, 'std': 0.008470915069488183, 'lower_bound': 0.8177710843373494, 'upper_bound': 0.8509036144578314}, {'samples': 4032, 'accuracy': 0.8437530120481926, 'std': 0.008100214440138373, 'lower_bound': 0.8273092369477911, 'upper_bound': 0.8589357429718876}, {'samples': 4544, 'accuracy': 0.8620712851405623, 'std': 0.007622649116941374, 'lower_bound': 0.8463855421686747, 'upper_bound': 0.8770080321285141}, {'samples': 5056, 'accuracy': 0.7778925702811246, 'std': 0.00930785919329539, 'lower_bound': 0.7600401606425703, 'upper_bound': 0.7961847389558233}, {'samples': 5568, 'accuracy': 0.8554769076305221, 'std': 0.007785235019983792, 'lower_bound': 0.8403614457831325, 'upper_bound': 0.8704819277108434}, {'samples': 6080, 'accuracy': 0.8525311244979918, 'std': 0.00780801539403436, 'lower_bound': 0.8378388554216867, 'upper_bound': 0.8684738955823293}, {'samples': 6592, 'accuracy': 0.8571340361445782, 'std': 0.00755628118018565, 'lower_bound': 0.8423694779116466, 'upper_bound': 0.8719879518072289}, {'samples': 7104, 'accuracy': 0.8574211847389559, 'std': 0.007741554896854545, 'lower_bound': 0.8428714859437751, 'upper_bound': 0.8719879518072289}, {'samples': 7616, 'accuracy': 0.8677098393574297, 'std': 0.007677630096266586, 'lower_bound': 0.8529116465863453, 'upper_bound': 0.8830321285140562}, {'samples': 8128, 'accuracy': 0.8677771084337349, 'std': 0.007500669582966283, 'lower_bound': 0.8534136546184738, 'upper_bound': 0.8825426706827308}, {'samples': 8640, 'accuracy': 0.8776044176706828, 'std': 0.00755922514602585, 'lower_bound': 0.8624497991967871, 'upper_bound': 0.892570281124498}, {'samples': 9152, 'accuracy': 0.8529779116465863, 'std': 0.008174050142020326, 'lower_bound': 0.8368473895582329, 'upper_bound': 0.867984437751004}, {'samples': 9664, 'accuracy': 0.7539241967871486, 'std': 0.009525905144111812, 'lower_bound': 0.7354417670682731, 'upper_bound': 0.773117469879518}, {'samples': 10176, 'accuracy': 0.7246566265060241, 'std': 0.009538571962880342, 'lower_bound': 0.7058232931726908, 'upper_bound': 0.7434738955823293}, {'samples': 10688, 'accuracy': 0.843039156626506, 'std': 0.008300311442203844, 'lower_bound': 0.8263052208835341, 'upper_bound': 0.8589357429718876}, {'samples': 11200, 'accuracy': 0.8395476907630521, 'std': 0.008241488338656379, 'lower_bound': 0.8242971887550201, 'upper_bound': 0.8549447791164658}, {'samples': 11712, 'accuracy': 0.8591546184738955, 'std': 0.00792810247117219, 'lower_bound': 0.8438755020080321, 'upper_bound': 0.875}, {'samples': 12224, 'accuracy': 0.8221420682730923, 'std': 0.009163938358349277, 'lower_bound': 0.8032003012048192, 'upper_bound': 0.838855421686747}, {'samples': 12736, 'accuracy': 0.8290923694779117, 'std': 0.008360073708163636, 'lower_bound': 0.8127510040160643, 'upper_bound': 0.8453815261044176}, {'samples': 13248, 'accuracy': 0.8789051204819277, 'std': 0.0071542425391857515, 'lower_bound': 0.8644578313253012, 'upper_bound': 0.8930722891566265}, {'samples': 13760, 'accuracy': 0.8296511044176706, 'std': 0.00832499423989523, 'lower_bound': 0.8132530120481928, 'upper_bound': 0.8453815261044176}, {'samples': 14272, 'accuracy': 0.8743539156626506, 'std': 0.007448767894685723, 'lower_bound': 0.8599397590361446, 'upper_bound': 0.8880647590361446}, {'samples': 14784, 'accuracy': 0.8461385542168676, 'std': 0.007969255936207054, 'lower_bound': 0.8298192771084337, 'upper_bound': 0.8614457831325302}, {'samples': 15296, 'accuracy': 0.8664804216867469, 'std': 0.007419572725194137, 'lower_bound': 0.8523970883534137, 'upper_bound': 0.8810240963855421}, {'samples': 15808, 'accuracy': 0.8735737951807229, 'std': 0.007523251870739491, 'lower_bound': 0.8594377510040161, 'upper_bound': 0.8890562248995983}]
Evaluating the best model on the test set of dataset ./fake_true_datasets/fake_true_dataset_phi_10k...
Test metrics:
accuracy: 0.8714533132530121
precision: 0.8582266748891753
recall: 0.8901724121256186
f1_score: 0.873848923947834
fp_rate: 0.14729714775106628
tp_rate: 0.8901724121256186
std_accuracy: 0.0075111336095662964
std_precision: 0.011015103699744037
std_recall: 0.009981049394385051
std_f1_score: 0.007742406266508275
std_fp_rate: 0.011685579241939026
std_tp_rate: 0.009981049394385051
TP: 887.283
TN: 848.652
FP: 146.59
FN: 109.475
roc_auc: 0.9427554595893615
fpr: [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00100402 0.00100402 0.00100402
 0.00100402 0.00100402 0.00100402 0.00100402 0.00100402 0.00100402
 0.00100402 0.00100402 0.00100402 0.00100402 0.00100402 0.00100402
 0.00100402 0.00100402 0.00100402 0.00200803 0.00200803 0.00200803
 0.00200803 0.00200803 0.00200803 0.00301205 0.00301205 0.00401606
 0.00401606 0.00401606 0.00401606 0.00502008 0.00502008 0.00502008
 0.00502008 0.00502008 0.00502008 0.00502008 0.00502008 0.00502008
 0.00502008 0.00502008 0.00502008 0.0060241  0.0060241  0.00702811
 0.00702811 0.00702811 0.00702811 0.00702811 0.00702811 0.00702811
 0.00702811 0.00803213 0.00803213 0.00803213 0.00803213 0.00803213
 0.00803213 0.00803213 0.00803213 0.00803213 0.00803213 0.00903614
 0.00903614 0.00903614 0.01004016 0.01004016 0.01104418 0.01104418
 0.01104418 0.01104418 0.01104418 0.01104418 0.01204819 0.01204819
 0.01305221 0.01405622 0.01405622 0.01606426 0.01606426 0.01606426
 0.01606426 0.01606426 0.01606426 0.01706827 0.01706827 0.01907631
 0.01907631 0.02008032 0.02008032 0.02108434 0.02108434 0.02208835
 0.02208835 0.02208835 0.02208835 0.02309237 0.02309237 0.02409639
 0.02409639 0.0251004  0.0251004  0.02610442 0.02610442 0.02710843
 0.02710843 0.02811245 0.02811245 0.02811245 0.02811245 0.02811245
 0.02811245 0.02811245 0.02811245 0.02911647 0.02911647 0.03012048
 0.03012048 0.03012048 0.03012048 0.03212851 0.03212851 0.03212851
 0.03313253 0.03313253 0.03413655 0.03413655 0.03413655 0.03413655
 0.03413655 0.03514056 0.03514056 0.03614458 0.03614458 0.04016064
 0.04016064 0.04116466 0.04116466 0.04216867 0.04216867 0.04317269
 0.04317269 0.04518072 0.04518072 0.04718876 0.04718876 0.0502008
 0.0502008  0.0502008  0.0502008  0.0502008  0.0502008  0.05120482
 0.05120482 0.05120482 0.05220884 0.05220884 0.05321285 0.05321285
 0.0562249  0.0562249  0.05722892 0.05722892 0.05923695 0.05923695
 0.062249   0.06325301 0.06325301 0.06425703 0.06425703 0.06526104
 0.06526104 0.06626506 0.06626506 0.06726908 0.06726908 0.06827309
 0.06827309 0.06927711 0.06927711 0.07028112 0.07028112 0.07329317
 0.07329317 0.07429719 0.07429719 0.07429719 0.0753012  0.0753012
 0.0753012  0.07630522 0.07630522 0.07730924 0.07730924 0.07831325
 0.08032129 0.08032129 0.0813253  0.08232932 0.08232932 0.08333333
 0.08333333 0.08433735 0.08433735 0.08634538 0.08634538 0.0873494
 0.0873494  0.08835341 0.08835341 0.08935743 0.08935743 0.09036145
 0.09036145 0.09136546 0.09136546 0.09236948 0.09236948 0.09337349
 0.09337349 0.09437751 0.09437751 0.09538153 0.09538153 0.09638554
 0.09638554 0.09939759 0.09939759 0.10040161 0.10441767 0.10441767
 0.10542169 0.10542169 0.1064257  0.1064257  0.10843373 0.10843373
 0.10943775 0.10943775 0.11144578 0.1124498  0.1124498  0.11345382
 0.11445783 0.11546185 0.11546185 0.11646586 0.11646586 0.1184739
 0.1184739  0.11947791 0.11947791 0.12048193 0.12048193 0.12248996
 0.12248996 0.12349398 0.12449799 0.12550201 0.12550201 0.12650602
 0.12650602 0.12751004 0.12751004 0.12951807 0.12951807 0.13052209
 0.13052209 0.13253012 0.13353414 0.13353414 0.13353414 0.13453815
 0.13453815 0.1375502  0.1375502  0.14056225 0.14056225 0.14257028
 0.14257028 0.14558233 0.14558233 0.15160643 0.15160643 0.15562249
 0.15562249 0.16465863 0.16465863 0.16566265 0.16566265 0.16967871
 0.16967871 0.17269076 0.17269076 0.17369478 0.17369478 0.1746988
 0.1746988  0.17771084 0.17771084 0.17871486 0.17871486 0.18172691
 0.18172691 0.18273092 0.18273092 0.18975904 0.18975904 0.1937751
 0.1937751  0.19477912 0.19477912 0.1997992  0.1997992  0.20180723
 0.20381526 0.20682731 0.20682731 0.20783133 0.20783133 0.21084337
 0.21084337 0.21586345 0.21586345 0.2188755  0.2188755  0.22991968
 0.23092369 0.23192771 0.23192771 0.23293173 0.23293173 0.23493976
 0.23493976 0.23594378 0.23795181 0.23795181 0.24196787 0.24196787
 0.24799197 0.24799197 0.25100402 0.25401606 0.25401606 0.26104418
 0.26104418 0.26204819 0.26204819 0.26405622 0.26405622 0.26506024
 0.26506024 0.26907631 0.26907631 0.27409639 0.27409639 0.2811245
 0.28212851 0.28212851 0.28313253 0.28313253 0.29116466 0.29116466
 0.29216867 0.29216867 0.29317269 0.29317269 0.29518072 0.29518072
 0.29819277 0.29819277 0.29919679 0.30120482 0.30120482 0.30421687
 0.3062249  0.30722892 0.30923695 0.31325301 0.31325301 0.31425703
 0.31526104 0.31526104 0.31626506 0.31626506 0.31827309 0.31827309
 0.32028112 0.34036145 0.34036145 0.34136546 0.34337349 0.34337349
 0.35040161 0.35040161 0.35140562 0.36044177 0.36044177 0.36646586
 0.36646586 0.37851406 0.37851406 0.38554217 0.38554217 0.3875502
 0.38855422 0.38855422 0.39056225 0.39457831 0.39658635 0.41064257
 0.4126506  0.42168675 0.42168675 0.42771084 0.42971888 0.43072289
 0.43273092 0.46485944 0.46485944 0.46787149 0.46987952 0.47891566
 0.48092369 0.48092369 0.48293173 0.48493976 0.48895582 0.48995984
 0.49096386 0.49297189 0.49497992 0.49698795 0.50301205 0.50301205
 0.50401606 0.50401606 0.50502008 0.50803213 0.51004016 0.51204819
 0.51405622 0.51405622 0.51606426 0.51706827 0.51706827 0.53313253
 0.53413655 0.54116466 0.54116466 0.55321285 0.55722892 0.55923695
 0.56024096 0.56425703 0.56626506 0.57429719 0.57429719 0.59839357
 0.60240964 0.60843373 0.6124498  0.61445783 0.61546185 0.61546185
 0.62148594 0.62349398 0.6315261  0.6315261  0.63353414 0.63554217
 0.6375502  0.64457831 0.64658635 0.64859438 0.65060241 0.65160643
 0.65160643 0.65361446 0.65662651 0.65863454 0.6686747  0.6686747
 0.6746988  0.67670683 0.67971888 0.68172691 0.68574297 0.687751
 0.69176707 0.6937751  0.69578313 0.69779116 0.70080321 0.70281124
 0.70481928 0.70481928 0.70983936 0.71385542 0.71787149 0.72188755
 0.73192771 0.73192771 0.73393574 0.73493976 0.74799197 0.75
 0.75401606 0.7560241  0.76405622 0.76606426 0.76706827 0.76907631
 0.77309237 0.77409639 0.77811245 0.78012048 0.78012048 0.7811245
 0.78313253 0.8062249  0.80823293 0.80823293 0.80923695 0.81124498
 0.81325301 0.81526104 0.82128514 0.82329317 0.8253012  0.82730924
 0.82730924 0.82931727 0.83232932 0.84638554 0.84839357 0.85240964
 0.85542169 0.86947791 0.87349398 0.87550201 0.87751004 0.92168675
 0.92369478 0.9246988  0.92670683 0.9437751  0.94578313 0.95783133
 0.95983936 0.96184739 0.96385542 1.        ]
tpr: [0.         0.00100402 0.01606426 0.01807229 0.02108434 0.02309237
 0.02811245 0.03012048 0.03815261 0.04016064 0.04417671 0.04718876
 0.05823293 0.06024096 0.08433735 0.08634538 0.08835341 0.09036145
 0.09337349 0.09638554 0.09839357 0.10240964 0.11546185 0.11746988
 0.12449799 0.12650602 0.12851406 0.13052209 0.13253012 0.13453815
 0.15060241 0.15261044 0.17971888 0.18172691 0.18373494 0.18574297
 0.19076305 0.1937751  0.2248996  0.22791165 0.23493976 0.23895582
 0.2439759  0.24598394 0.24698795 0.24899598 0.25100402 0.25301205
 0.26004016 0.26204819 0.26305221 0.26305221 0.29518072 0.29718876
 0.31024096 0.31325301 0.32630522 0.32831325 0.34337349 0.34538153
 0.34638554 0.34839357 0.34939759 0.35140562 0.35240964 0.35441767
 0.35542169 0.35742972 0.36646586 0.36646586 0.37148594 0.37349398
 0.38253012 0.38554217 0.38855422 0.38855422 0.39759036 0.39759036
 0.39959839 0.40160643 0.41666667 0.41666667 0.41767068 0.42168675
 0.42269076 0.42570281 0.42771084 0.43473896 0.43674699 0.43975904
 0.44578313 0.44678715 0.45080321 0.45080321 0.45481928 0.45481928
 0.46485944 0.46686747 0.46987952 0.47188755 0.4748996  0.47690763
 0.47991968 0.47991968 0.48393574 0.48594378 0.48995984 0.49196787
 0.50200803 0.50401606 0.51004016 0.51204819 0.51606426 0.51606426
 0.51807229 0.52309237 0.52309237 0.52409639 0.5251004  0.52610442
 0.52811245 0.53212851 0.53413655 0.53614458 0.53614458 0.54016064
 0.54116466 0.54116466 0.54518072 0.54518072 0.54618474 0.54819277
 0.55321285 0.55522088 0.5562249  0.5562249  0.55722892 0.55722892
 0.55923695 0.55923695 0.56325301 0.56325301 0.57228916 0.57228916
 0.57730924 0.57931727 0.58032129 0.58032129 0.59538153 0.59538153
 0.59939759 0.59939759 0.60240964 0.60240964 0.60341365 0.60341365
 0.60542169 0.60542169 0.60843373 0.61044177 0.61646586 0.6184739
 0.62449799 0.62650602 0.6315261  0.6315261  0.63253012 0.63253012
 0.63654618 0.63855422 0.65261044 0.65261044 0.65461847 0.65863454
 0.65863454 0.6626506  0.6626506  0.66566265 0.66767068 0.6686747
 0.67068273 0.67068273 0.67369478 0.67369478 0.67670683 0.67670683
 0.67871486 0.67871486 0.68172691 0.68273092 0.68875502 0.68875502
 0.69678715 0.69879518 0.70682731 0.70682731 0.70883534 0.70883534
 0.70983936 0.71184739 0.71787149 0.71987952 0.72389558 0.72389558
 0.73092369 0.73293173 0.73293173 0.73493976 0.73493976 0.73694779
 0.73694779 0.73795181 0.73895582 0.74196787 0.74196787 0.74297189
 0.74297189 0.7439759  0.74497992 0.74497992 0.75       0.75
 0.75502008 0.75502008 0.7560241  0.7560241  0.75803213 0.75803213
 0.75903614 0.75903614 0.76004016 0.76004016 0.76405622 0.76405622
 0.77309237 0.77309237 0.77409639 0.77610442 0.77710843 0.77911647
 0.78614458 0.78614458 0.78714859 0.78714859 0.79216867 0.79317269
 0.79317269 0.79518072 0.79518072 0.79618474 0.79919679 0.79919679
 0.8002008  0.80120482 0.80321285 0.80321285 0.80722892 0.80722892
 0.80823293 0.80823293 0.81325301 0.81325301 0.81626506 0.81626506
 0.82028112 0.82028112 0.8253012  0.8253012  0.82630522 0.82630522
 0.83835341 0.83935743 0.84036145 0.84036145 0.84136546 0.84136546
 0.84337349 0.84337349 0.84437751 0.84538153 0.84538153 0.84738956
 0.84738956 0.84939759 0.84939759 0.85040161 0.85040161 0.85140562
 0.85140562 0.85542169 0.85542169 0.85542169 0.8564257  0.8564257
 0.85742972 0.85742972 0.85843373 0.85843373 0.8624498  0.8624498
 0.86445783 0.86445783 0.86646586 0.86646586 0.86746988 0.86746988
 0.86947791 0.86947791 0.87048193 0.87048193 0.87349398 0.87349398
 0.87449799 0.87449799 0.87550201 0.87550201 0.87650602 0.87650602
 0.87851406 0.87851406 0.87851406 0.88253012 0.88453815 0.88453815
 0.88554217 0.88554217 0.88654618 0.88654618 0.8875502  0.8875502
 0.88955823 0.88955823 0.89257028 0.89257028 0.89558233 0.89558233
 0.89759036 0.89759036 0.90060241 0.90060241 0.90261044 0.90261044
 0.90461847 0.90461847 0.90662651 0.90662651 0.90763052 0.90763052
 0.90863454 0.90863454 0.91064257 0.91064257 0.9126506  0.9126506
 0.91365462 0.91365462 0.91566265 0.91566265 0.9186747  0.9186747
 0.91967871 0.91967871 0.92068273 0.92068273 0.92168675 0.92168675
 0.92168675 0.92168675 0.92369478 0.92369478 0.9246988  0.9246988
 0.92570281 0.92570281 0.92771084 0.92771084 0.92871486 0.92871486
 0.92971888 0.92971888 0.93072289 0.93072289 0.93172691 0.93172691
 0.93273092 0.93373494 0.93373494 0.93473896 0.93473896 0.937751
 0.937751   0.93975904 0.93975904 0.93975904 0.94076305 0.94076305
 0.94176707 0.94176707 0.94277108 0.94277108 0.9437751  0.9437751
 0.94477912 0.94477912 0.94578313 0.94578313 0.94678715 0.94678715
 0.94779116 0.94879518 0.94879518 0.9497992  0.9497992  0.95180723
 0.95180723 0.95281124 0.95281124 0.95381526 0.95381526 0.95481928
 0.95481928 0.95682731 0.95682731 0.95682731 0.95783133 0.95783133
 0.95783133 0.95783133 0.95783133 0.95783133 0.95983936 0.95983936
 0.96084337 0.96184739 0.96184739 0.96285141 0.96285141 0.96385542
 0.96485944 0.96485944 0.96586345 0.96586345 0.96586345 0.96686747
 0.96686747 0.96787149 0.9688755  0.9688755  0.96987952 0.96987952
 0.97088353 0.97088353 0.97188755 0.97188755 0.97289157 0.97289157
 0.97289157 0.97389558 0.97389558 0.97389558 0.97389558 0.97389558
 0.97389558 0.97389558 0.9748996  0.9748996  0.9748996  0.97590361
 0.97590361 0.97590361 0.97690763 0.97690763 0.97690763 0.97690763
 0.97791165 0.97891566 0.97891566 0.97891566 0.97891566 0.97991968
 0.97991968 0.97991968 0.97991968 0.97991968 0.97991968 0.98092369
 0.98092369 0.98192771 0.98293173 0.98293173 0.98293173 0.98293173
 0.98293173 0.98393574 0.98393574 0.98393574 0.98493976 0.98493976
 0.98594378 0.98594378 0.98694779 0.98694779 0.98694779 0.98694779
 0.98795181 0.98795181 0.98795181 0.98795181 0.98895582 0.98895582
 0.98895582 0.98895582 0.98895582 0.98895582 0.98895582 0.98995984
 0.98995984 0.98995984 0.98995984 0.99096386 0.99096386 0.99096386
 0.99096386 0.99096386 0.99096386 0.99096386 0.99096386 0.99096386
 0.99196787 0.99196787 0.99196787 0.99196787 0.99196787 0.99297189
 0.99297189 0.99297189 0.99297189 0.99297189 0.99297189 0.99297189
 0.99297189 0.99297189 0.99297189 0.99297189 0.99297189 0.99297189
 0.99297189 0.9939759  0.9939759  0.9939759  0.9939759  0.9939759
 0.9939759  0.99497992 0.99497992 0.99598394 0.99598394 0.99598394
 0.99598394 0.99598394 0.99598394 0.99598394 0.99598394 0.99598394
 0.99598394 0.99698795 0.99698795 0.99698795 0.99799197 0.99799197
 0.99799197 0.99799197 0.99799197 0.99899598 0.99899598 0.99899598
 0.99899598 0.99899598 0.99899598 0.99899598 0.99899598 0.99899598
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.        ]
thresholds: [            inf  3.89062500e+00  3.30078125e+00  3.22656250e+00
  3.20898438e+00  3.17578125e+00  3.10937500e+00  3.09765625e+00
  2.97851562e+00  2.95898438e+00  2.91601562e+00  2.91406250e+00
  2.85546875e+00  2.84960938e+00  2.67187500e+00  2.66992188e+00
  2.65429688e+00  2.64648438e+00  2.61132812e+00  2.60937500e+00
  2.59960938e+00  2.58593750e+00  2.46484375e+00  2.45312500e+00
  2.40429688e+00  2.38671875e+00  2.37109375e+00  2.36523438e+00
  2.35742188e+00  2.35546875e+00  2.22460938e+00  2.22265625e+00
  2.09375000e+00  2.08593750e+00  2.08007812e+00  2.05859375e+00
  2.02734375e+00  2.01757812e+00  1.86230469e+00  1.86035156e+00
  1.81542969e+00  1.78613281e+00  1.76269531e+00  1.75683594e+00
  1.75195312e+00  1.74707031e+00  1.74511719e+00  1.74316406e+00
  1.68652344e+00  1.68554688e+00  1.67871094e+00  1.67578125e+00
  1.54394531e+00  1.53808594e+00  1.48535156e+00  1.47949219e+00
  1.41210938e+00  1.41113281e+00  1.33496094e+00  1.33300781e+00
  1.32714844e+00  1.32617188e+00  1.32519531e+00  1.32031250e+00
  1.31835938e+00  1.31738281e+00  1.30664062e+00  1.30468750e+00
  1.26953125e+00  1.26855469e+00  1.25097656e+00  1.24316406e+00
  1.21777344e+00  1.21679688e+00  1.20800781e+00  1.20410156e+00
  1.18750000e+00  1.17968750e+00  1.17578125e+00  1.17285156e+00
  1.13964844e+00  1.13867188e+00  1.13378906e+00  1.12304688e+00
  1.11816406e+00  1.11718750e+00  1.11523438e+00  1.10058594e+00
  1.09960938e+00  1.09082031e+00  1.08496094e+00  1.08203125e+00
  1.07910156e+00  1.07421875e+00  1.05761719e+00  1.05175781e+00
  1.03515625e+00  1.03417969e+00  1.02636719e+00  1.02246094e+00
  1.01171875e+00  1.01074219e+00  9.98535156e-01  9.97558594e-01
  9.84375000e-01  9.82910156e-01  9.79003906e-01  9.77539062e-01
  9.53125000e-01  9.49218750e-01  9.39941406e-01  9.37988281e-01
  9.22363281e-01  9.18457031e-01  9.16015625e-01  9.02832031e-01
  9.01855469e-01  8.96484375e-01  8.95507812e-01  8.95019531e-01
  8.94531250e-01  8.85742188e-01  8.84765625e-01  8.69140625e-01
  8.67675781e-01  8.54492188e-01  8.53027344e-01  8.50585938e-01
  8.44238281e-01  8.40332031e-01  8.39355469e-01  8.36914062e-01
  8.23242188e-01  8.22753906e-01  8.21289062e-01  8.20800781e-01
  8.20312500e-01  8.17871094e-01  8.15917969e-01  8.14453125e-01
  8.06152344e-01  8.05664062e-01  7.73925781e-01  7.72460938e-01
  7.66601562e-01  7.60742188e-01  7.55859375e-01  7.54882812e-01
  7.22656250e-01  7.20703125e-01  7.17285156e-01  7.16796875e-01
  7.08496094e-01  7.08007812e-01  7.05566406e-01  7.03613281e-01
  7.00683594e-01  7.00195312e-01  6.93847656e-01  6.92382812e-01
  6.79687500e-01  6.79199219e-01  6.72851562e-01  6.71386719e-01
  6.62109375e-01  6.61132812e-01  6.58203125e-01  6.54785156e-01
  6.48925781e-01  6.45996094e-01  6.17675781e-01  6.14746094e-01
  6.12304688e-01  6.07910156e-01  6.04003906e-01  5.98144531e-01
  5.97167969e-01  5.93261719e-01  5.84960938e-01  5.83007812e-01
  5.80566406e-01  5.80078125e-01  5.70800781e-01  5.69824219e-01
  5.59082031e-01  5.53222656e-01  5.42968750e-01  5.39550781e-01
  5.33691406e-01  5.29785156e-01  5.19531250e-01  5.18554688e-01
  4.99023438e-01  4.98046875e-01  4.84130859e-01  4.81201172e-01
  4.80224609e-01  4.75341797e-01  4.74365234e-01  4.72167969e-01
  4.63867188e-01  4.63623047e-01  4.56298828e-01  4.50683594e-01
  4.41894531e-01  4.40185547e-01  4.39453125e-01  4.38232422e-01
  4.37011719e-01  4.35058594e-01  4.22851562e-01  4.22119141e-01
  4.20410156e-01  4.14306641e-01  4.11132812e-01  4.10156250e-01
  4.04052734e-01  4.03564453e-01  4.02832031e-01  4.02343750e-01
  3.94287109e-01  3.93066406e-01  3.86718750e-01  3.85986328e-01
  3.81347656e-01  3.80615234e-01  3.77197266e-01  3.75244141e-01
  3.73046875e-01  3.70849609e-01  3.66943359e-01  3.66210938e-01
  3.57421875e-01  3.54003906e-01  3.44238281e-01  3.41064453e-01
  3.37646484e-01  3.31542969e-01  3.31298828e-01  3.30078125e-01
  3.22998047e-01  3.22021484e-01  3.19580078e-01  3.19091797e-01
  3.09570312e-01  3.07861328e-01  3.01269531e-01  2.98095703e-01
  2.96386719e-01  2.93212891e-01  2.85644531e-01  2.82714844e-01
  2.80273438e-01  2.79785156e-01  2.77099609e-01  2.74658203e-01
  2.67822266e-01  2.65625000e-01  2.64648438e-01  2.64404297e-01
  2.53417969e-01  2.49389648e-01  2.42309570e-01  2.41821289e-01
  2.36328125e-01  2.35717773e-01  2.26196289e-01  2.25219727e-01
  2.23999023e-01  2.22656250e-01  2.00927734e-01  1.98974609e-01
  1.98730469e-01  1.98486328e-01  1.96777344e-01  1.96411133e-01
  1.95190430e-01  1.80908203e-01  1.80786133e-01  1.80175781e-01
  1.74072266e-01  1.72973633e-01  1.71875000e-01  1.68945312e-01
  1.64550781e-01  1.61376953e-01  1.59912109e-01  1.57714844e-01
  1.52709961e-01  1.44042969e-01  1.42211914e-01  1.41235352e-01
  1.40991211e-01  1.40136719e-01  1.39892578e-01  1.38793945e-01
  1.36230469e-01  1.35498047e-01  1.24023438e-01  1.16210938e-01
  1.10656738e-01  1.10351562e-01  1.08947754e-01  1.08459473e-01
  1.06933594e-01  1.03515625e-01  1.02539062e-01  1.01684570e-01
  1.00585938e-01  9.96704102e-02  9.72900391e-02  9.64355469e-02
  9.63134766e-02  9.58251953e-02  9.39331055e-02  9.10644531e-02
  9.02099609e-02  8.92944336e-02  8.70361328e-02  8.56323242e-02
  8.21533203e-02  7.59277344e-02  7.58056641e-02  7.42797852e-02
  7.40966797e-02  7.28149414e-02  7.15332031e-02  6.67724609e-02
  6.40869141e-02  6.07910156e-02  5.70983887e-02  5.47790527e-02
  5.19104004e-02  4.33959961e-02  3.62548828e-02  3.17993164e-02
  2.92358398e-02  1.72424316e-02  1.35040283e-02  1.12380981e-02
  9.38415527e-03  1.23882294e-03 -2.71224976e-03 -1.06811523e-02
 -2.55126953e-02 -2.59857178e-02 -2.79541016e-02 -2.87780762e-02
 -2.91900635e-02 -3.42407227e-02 -4.14733887e-02 -4.18090820e-02
 -4.40979004e-02 -5.06286621e-02 -5.09338379e-02 -5.20629883e-02
 -5.28259277e-02 -5.87768555e-02 -6.37207031e-02 -7.15942383e-02
 -7.39746094e-02 -7.40966797e-02 -7.50122070e-02 -8.66699219e-02
 -8.80126953e-02 -9.00878906e-02 -9.03320312e-02 -9.87548828e-02
 -1.01928711e-01 -1.03576660e-01 -1.06750488e-01 -1.10412598e-01
 -1.13220215e-01 -1.20361328e-01 -1.22985840e-01 -1.23901367e-01
 -1.24633789e-01 -1.34277344e-01 -1.36840820e-01 -1.36962891e-01
 -1.37207031e-01 -1.39648438e-01 -1.39892578e-01 -1.47460938e-01
 -1.47949219e-01 -1.50268555e-01 -1.51000977e-01 -1.53686523e-01
 -1.59179688e-01 -1.66870117e-01 -1.71997070e-01 -1.73706055e-01
 -1.74316406e-01 -1.78588867e-01 -1.79077148e-01 -1.86035156e-01
 -1.87255859e-01 -1.89453125e-01 -1.89575195e-01 -1.90429688e-01
 -1.93847656e-01 -1.94335938e-01 -1.95678711e-01 -2.07397461e-01
 -2.09472656e-01 -2.15209961e-01 -2.17041016e-01 -2.25952148e-01
 -2.26684570e-01 -2.30712891e-01 -2.30957031e-01 -2.31323242e-01
 -2.41943359e-01 -2.43164062e-01 -2.43286133e-01 -2.45361328e-01
 -2.46459961e-01 -2.47680664e-01 -2.48657227e-01 -2.49267578e-01
 -2.52929688e-01 -2.53662109e-01 -2.56347656e-01 -2.57568359e-01
 -2.59765625e-01 -2.62451172e-01 -2.62939453e-01 -2.66601562e-01
 -2.67578125e-01 -2.71728516e-01 -2.74169922e-01 -2.75390625e-01
 -2.76611328e-01 -2.77587891e-01 -2.78076172e-01 -2.79296875e-01
 -2.81250000e-01 -2.82226562e-01 -2.84423828e-01 -3.18359375e-01
 -3.19580078e-01 -3.20556641e-01 -3.21777344e-01 -3.22021484e-01
 -3.33496094e-01 -3.34472656e-01 -3.39355469e-01 -3.51074219e-01
 -3.51318359e-01 -3.60839844e-01 -3.61328125e-01 -3.72558594e-01
 -3.72802734e-01 -3.80615234e-01 -3.80859375e-01 -3.86718750e-01
 -3.87939453e-01 -3.88427734e-01 -3.88916016e-01 -4.00146484e-01
 -4.00634766e-01 -4.20898438e-01 -4.23828125e-01 -4.34326172e-01
 -4.35302734e-01 -4.40917969e-01 -4.42382812e-01 -4.43603516e-01
 -4.45312500e-01 -4.82910156e-01 -4.86083984e-01 -4.89501953e-01
 -4.92187500e-01 -5.03906250e-01 -5.04882812e-01 -5.06835938e-01
 -5.09277344e-01 -5.11718750e-01 -5.13671875e-01 -5.14648438e-01
 -5.15136719e-01 -5.18066406e-01 -5.21484375e-01 -5.21972656e-01
 -5.34179688e-01 -5.34667969e-01 -5.35644531e-01 -5.36132812e-01
 -5.40039062e-01 -5.42968750e-01 -5.43945312e-01 -5.45898438e-01
 -5.46386719e-01 -5.48339844e-01 -5.48828125e-01 -5.49316406e-01
 -5.49804688e-01 -5.70312500e-01 -5.71777344e-01 -5.80566406e-01
 -5.81542969e-01 -6.02539062e-01 -6.07421875e-01 -6.14257812e-01
 -6.17187500e-01 -6.22558594e-01 -6.27441406e-01 -6.38183594e-01
 -6.38671875e-01 -6.78222656e-01 -6.80664062e-01 -6.90429688e-01
 -6.94335938e-01 -6.94824219e-01 -6.95312500e-01 -6.95800781e-01
 -7.01171875e-01 -7.02148438e-01 -7.10937500e-01 -7.12402344e-01
 -7.13378906e-01 -7.18750000e-01 -7.20214844e-01 -7.30957031e-01
 -7.31445312e-01 -7.33886719e-01 -7.34375000e-01 -7.35351562e-01
 -7.36328125e-01 -7.39257812e-01 -7.42675781e-01 -7.43164062e-01
 -7.62695312e-01 -7.63671875e-01 -7.72460938e-01 -7.73437500e-01
 -7.82714844e-01 -7.85644531e-01 -7.95898438e-01 -7.96875000e-01
 -8.00781250e-01 -8.01757812e-01 -8.04199219e-01 -8.04687500e-01
 -8.07617188e-01 -8.10058594e-01 -8.11035156e-01 -8.11523438e-01
 -8.19824219e-01 -8.22265625e-01 -8.29101562e-01 -8.32031250e-01
 -8.48632812e-01 -8.51074219e-01 -8.54980469e-01 -8.55468750e-01
 -8.87695312e-01 -8.90625000e-01 -9.02343750e-01 -9.04785156e-01
 -9.22851562e-01 -9.26269531e-01 -9.27246094e-01 -9.27734375e-01
 -9.35058594e-01 -9.36523438e-01 -9.39941406e-01 -9.41894531e-01
 -9.42871094e-01 -9.43359375e-01 -9.44824219e-01 -9.78027344e-01
 -9.81933594e-01 -9.84375000e-01 -9.84863281e-01 -9.85351562e-01
 -9.88281250e-01 -9.89257812e-01 -1.00390625e+00 -1.00976562e+00
 -1.01367188e+00 -1.01660156e+00 -1.01953125e+00 -1.02246094e+00
 -1.02539062e+00 -1.07812500e+00 -1.08105469e+00 -1.09179688e+00
 -1.10058594e+00 -1.13574219e+00 -1.14550781e+00 -1.15136719e+00
 -1.15332031e+00 -1.31640625e+00 -1.31835938e+00 -1.32910156e+00
 -1.33007812e+00 -1.41894531e+00 -1.44140625e+00 -1.53808594e+00
 -1.54589844e+00 -1.55566406e+00 -1.56347656e+00 -2.17968750e+00]
