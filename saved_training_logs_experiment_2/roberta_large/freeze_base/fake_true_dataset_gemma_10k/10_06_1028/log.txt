log_loss_steps: 256
eval_steps: 512
check_degradation: 0
----------------Epoch 1/1----------------
Epoch 1/1, Loss after 192 samples: 0.7123
Epoch 1/1, Loss after 448 samples: 0.6897
Mean accuracy: 0.5248, std: 0.0111, lower bound: 0.5035, upper bound: 0.5465 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 448 samples: 0.5248 with eval loss: 0.6680
Best model with eval loss 0.6679855700462095 and eval accuracy 0.5247724974721941 with 448 samples seen is saved
Epoch 1/1, Loss after 704 samples: 0.6671
Epoch 1/1, Loss after 960 samples: 0.6241
Mean accuracy: 0.6266, std: 0.0108, lower bound: 0.6041, upper bound: 0.6476 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 960 samples: 0.6264 with eval loss: 0.6147
Best model with eval loss 0.6147145532792614 and eval accuracy 0.6263902932254802 with 960 samples seen is saved
Epoch 1/1, Loss after 1216 samples: 0.5956
Epoch 1/1, Loss after 1472 samples: 0.6046
Mean accuracy: 0.7570, std: 0.0097, lower bound: 0.7386, upper bound: 0.7750 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 1472 samples: 0.7573 with eval loss: 0.5225
Best model with eval loss 0.5224819067985781 and eval accuracy 0.7573306370070778 with 1472 samples seen is saved
Epoch 1/1, Loss after 1728 samples: 0.5489
Epoch 1/1, Loss after 1984 samples: 0.4896
Mean accuracy: 0.7373, std: 0.0098, lower bound: 0.7184, upper bound: 0.7568 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 1984 samples: 0.7371 with eval loss: 0.5069
Best model with eval loss 0.5068847671631844 and eval accuracy 0.737108190091001 with 1984 samples seen is saved
Epoch 1/1, Loss after 2240 samples: 0.5797
Epoch 1/1, Loss after 2496 samples: 0.4930
Mean accuracy: 0.7501, std: 0.0103, lower bound: 0.7300, upper bound: 0.7705 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 2496 samples: 0.7508 with eval loss: 0.4934
Best model with eval loss 0.4933900756220664 and eval accuracy 0.7507583417593529 with 2496 samples seen is saved
Epoch 1/1, Loss after 2752 samples: 0.4947
Epoch 1/1, Loss after 3008 samples: 0.5315
Mean accuracy: 0.7533, std: 0.0093, lower bound: 0.7346, upper bound: 0.7715 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 3008 samples: 0.7538 with eval loss: 0.4829
Best model with eval loss 0.48293348666160335 and eval accuracy 0.7537917087967644 with 3008 samples seen is saved
Epoch 1/1, Loss after 3264 samples: 0.5244
Epoch 1/1, Loss after 3520 samples: 0.5686
Mean accuracy: 0.7336, std: 0.0096, lower bound: 0.7139, upper bound: 0.7528 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 3520 samples: 0.7331 with eval loss: 0.4918
Epoch 1/1, Loss after 3776 samples: 0.5293
Epoch 1/1, Loss after 4032 samples: 0.4923
Mean accuracy: 0.8112, std: 0.0090, lower bound: 0.7942, upper bound: 0.8291 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 4032 samples: 0.8109 with eval loss: 0.4421
Best model with eval loss 0.4421455302546101 and eval accuracy 0.8109201213346815 with 4032 samples seen is saved
Epoch 1/1, Loss after 4288 samples: 0.4881
Epoch 1/1, Loss after 4544 samples: 0.4769
Mean accuracy: 0.6126, std: 0.0107, lower bound: 0.5915, upper bound: 0.6335 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 4544 samples: 0.6122 with eval loss: 0.5917
Epoch 1/1, Loss after 4800 samples: 0.6319
Epoch 1/1, Loss after 5056 samples: 0.4979
Mean accuracy: 0.8312, std: 0.0084, lower bound: 0.8144, upper bound: 0.8473 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 5056 samples: 0.8311 with eval loss: 0.4301
Best model with eval loss 0.4301359336222372 and eval accuracy 0.8311425682507584 with 5056 samples seen is saved
Epoch 1/1, Loss after 5312 samples: 0.5035
Epoch 1/1, Loss after 5568 samples: 0.4181
Mean accuracy: 0.8155, std: 0.0086, lower bound: 0.7988, upper bound: 0.8337 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 5568 samples: 0.8155 with eval loss: 0.4248
Best model with eval loss 0.4247628738803248 and eval accuracy 0.8154701718907987 with 5568 samples seen is saved
Epoch 1/1, Loss after 5824 samples: 0.4975
Epoch 1/1, Loss after 6080 samples: 0.5027
Mean accuracy: 0.7610, std: 0.0096, lower bound: 0.7417, upper bound: 0.7791 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 6080 samples: 0.7609 with eval loss: 0.4836
Epoch 1/1, Loss after 6336 samples: 0.5603
Epoch 1/1, Loss after 6592 samples: 0.4673
Mean accuracy: 0.8206, std: 0.0087, lower bound: 0.8033, upper bound: 0.8372 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 6592 samples: 0.8205 with eval loss: 0.4148
Best model with eval loss 0.41483121918093774 and eval accuracy 0.820525783619818 with 6592 samples seen is saved
Epoch 1/1, Loss after 6848 samples: 0.5696
Epoch 1/1, Loss after 7104 samples: 0.5092
Mean accuracy: 0.7799, std: 0.0090, lower bound: 0.7619, upper bound: 0.7973 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 7104 samples: 0.7796 with eval loss: 0.4556
Epoch 1/1, Loss after 7360 samples: 0.5343
Epoch 1/1, Loss after 7616 samples: 0.4957
Mean accuracy: 0.8306, std: 0.0085, lower bound: 0.8129, upper bound: 0.8458 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 7616 samples: 0.8306 with eval loss: 0.4178
Epoch 1/1, Loss after 7872 samples: 0.4715
Epoch 1/1, Loss after 8128 samples: 0.4658
Mean accuracy: 0.8322, std: 0.0088, lower bound: 0.8155, upper bound: 0.8498 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 8128 samples: 0.8322 with eval loss: 0.4149
Epoch 1/1, Loss after 8384 samples: 0.4567
Epoch 1/1, Loss after 8640 samples: 0.5058
Mean accuracy: 0.8280, std: 0.0085, lower bound: 0.8109, upper bound: 0.8448 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 8640 samples: 0.8281 with eval loss: 0.4118
Best model with eval loss 0.41175779700279236 and eval accuracy 0.8281092012133469 with 8640 samples seen is saved
Epoch 1/1, Loss after 8896 samples: 0.4456
Epoch 1/1, Loss after 9152 samples: 0.5374
Mean accuracy: 0.8329, std: 0.0080, lower bound: 0.8180, upper bound: 0.8488 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 9152 samples: 0.8327 with eval loss: 0.4108
Best model with eval loss 0.410831586007149 and eval accuracy 0.8326592517694641 with 9152 samples seen is saved
Epoch 1/1, Loss after 9408 samples: 0.4835
Epoch 1/1, Loss after 9664 samples: 0.4745
Mean accuracy: 0.8379, std: 0.0084, lower bound: 0.8215, upper bound: 0.8534 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 9664 samples: 0.8382 with eval loss: 0.4095
Best model with eval loss 0.40953368429214726 and eval accuracy 0.8382204246713852 with 9664 samples seen is saved
Epoch 1/1, Loss after 9920 samples: 0.4848
Epoch 1/1, Loss after 10176 samples: 0.4330
Mean accuracy: 0.8247, std: 0.0087, lower bound: 0.8079, upper bound: 0.8413 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 10176 samples: 0.8246 with eval loss: 0.4202
Epoch 1/1, Loss after 10432 samples: 0.4541
Epoch 1/1, Loss after 10688 samples: 0.4443
Mean accuracy: 0.8277, std: 0.0084, lower bound: 0.8109, upper bound: 0.8443 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 10688 samples: 0.8276 with eval loss: 0.4143
Epoch 1/1, Loss after 10944 samples: 0.4276
Epoch 1/1, Loss after 11200 samples: 0.4638
Mean accuracy: 0.8382, std: 0.0082, lower bound: 0.8220, upper bound: 0.8549 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 11200 samples: 0.8382 with eval loss: 0.3974
Best model with eval loss 0.397377198742282 and eval accuracy 0.8382204246713852 with 11200 samples seen is saved
Epoch 1/1, Loss after 11456 samples: 0.4406
Epoch 1/1, Loss after 11712 samples: 0.4389
Mean accuracy: 0.8370, std: 0.0084, lower bound: 0.8205, upper bound: 0.8534 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 11712 samples: 0.8372 with eval loss: 0.3930
Best model with eval loss 0.39300892333830556 and eval accuracy 0.8372093023255814 with 11712 samples seen is saved
Epoch 1/1, Loss after 11968 samples: 0.4661
Epoch 1/1, Loss after 12224 samples: 0.4678
Mean accuracy: 0.8253, std: 0.0087, lower bound: 0.8084, upper bound: 0.8423 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 12224 samples: 0.8251 with eval loss: 0.4036
Epoch 1/1, Loss after 12480 samples: 0.3684
Epoch 1/1, Loss after 12736 samples: 0.4535
Mean accuracy: 0.8318, std: 0.0084, lower bound: 0.8145, upper bound: 0.8478 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 12736 samples: 0.8316 with eval loss: 0.3966
Epoch 1/1, Loss after 12992 samples: 0.4412
Epoch 1/1, Loss after 13248 samples: 0.4298
Mean accuracy: 0.8459, std: 0.0080, lower bound: 0.8301, upper bound: 0.8615 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 13248 samples: 0.8463 with eval loss: 0.3883
Best model with eval loss 0.38826961190469805 and eval accuracy 0.846309403437816 with 13248 samples seen is saved
Epoch 1/1, Loss after 13504 samples: 0.3907
Epoch 1/1, Loss after 13760 samples: 0.4746
Mean accuracy: 0.8450, std: 0.0082, lower bound: 0.8286, upper bound: 0.8610 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 13760 samples: 0.8453 with eval loss: 0.3875
Best model with eval loss 0.3875196883755346 and eval accuracy 0.8452982810920121 with 13760 samples seen is saved
Epoch 1/1, Loss after 14016 samples: 0.4335
Epoch 1/1, Loss after 14272 samples: 0.4699
Mean accuracy: 0.8340, std: 0.0082, lower bound: 0.8185, upper bound: 0.8493 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 14272 samples: 0.8337 with eval loss: 0.3950
Epoch 1/1, Loss after 14528 samples: 0.4591
Epoch 1/1, Loss after 14784 samples: 0.5005
Mean accuracy: 0.8450, std: 0.0081, lower bound: 0.8286, upper bound: 0.8610 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 14784 samples: 0.8453 with eval loss: 0.3867
Best model with eval loss 0.3867101054037771 and eval accuracy 0.8452982810920121 with 14784 samples seen is saved
Epoch 1/1, Loss after 15040 samples: 0.4496
Epoch 1/1, Loss after 15296 samples: 0.4748
Mean accuracy: 0.8444, std: 0.0082, lower bound: 0.8276, upper bound: 0.8595 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 15296 samples: 0.8443 with eval loss: 0.3850
Best model with eval loss 0.3849823148019852 and eval accuracy 0.8442871587462083 with 15296 samples seen is saved
Epoch 1/1, Loss after 15552 samples: 0.4763
Epoch 1/1, Loss after 15808 samples: 0.4824
Mean accuracy: 0.8455, std: 0.0082, lower bound: 0.8296, upper bound: 0.8620 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 15808 samples: 0.8453 with eval loss: 0.3859
Training signal is False, stopping training
Training finished
Best model: {'eval_acc': 0.8442871587462083, 'nb_samples': 15296, 'eval_loss': 0.3849823148019852}
Training loss logs: [{'samples': 192, 'loss': 0.7123270034790039}, {'samples': 448, 'loss': 0.6897001266479492}, {'samples': 704, 'loss': 0.6670622825622559}, {'samples': 960, 'loss': 0.624061107635498}, {'samples': 1216, 'loss': 0.5955895185470581}, {'samples': 1472, 'loss': 0.6046330332756042}, {'samples': 1728, 'loss': 0.5489497482776642}, {'samples': 1984, 'loss': 0.48960351943969727}, {'samples': 2240, 'loss': 0.5797379910945892}, {'samples': 2496, 'loss': 0.4930432289838791}, {'samples': 2752, 'loss': 0.49469003081321716}, {'samples': 3008, 'loss': 0.5314973890781403}, {'samples': 3264, 'loss': 0.5243512690067291}, {'samples': 3520, 'loss': 0.5686243176460266}, {'samples': 3776, 'loss': 0.5293142795562744}, {'samples': 4032, 'loss': 0.492298424243927}, {'samples': 4288, 'loss': 0.48805880546569824}, {'samples': 4544, 'loss': 0.4769371747970581}, {'samples': 4800, 'loss': 0.631888672709465}, {'samples': 5056, 'loss': 0.49786072969436646}, {'samples': 5312, 'loss': 0.5035004839301109}, {'samples': 5568, 'loss': 0.41808604449033737}, {'samples': 5824, 'loss': 0.4974658489227295}, {'samples': 6080, 'loss': 0.5027435198426247}, {'samples': 6336, 'loss': 0.5603128969669342}, {'samples': 6592, 'loss': 0.467319592833519}, {'samples': 6848, 'loss': 0.5696296989917755}, {'samples': 7104, 'loss': 0.5091876685619354}, {'samples': 7360, 'loss': 0.5342789739370346}, {'samples': 7616, 'loss': 0.4956729859113693}, {'samples': 7872, 'loss': 0.471493698656559}, {'samples': 8128, 'loss': 0.46584074199199677}, {'samples': 8384, 'loss': 0.4566522389650345}, {'samples': 8640, 'loss': 0.505796954035759}, {'samples': 8896, 'loss': 0.44561532139778137}, {'samples': 9152, 'loss': 0.5373960584402084}, {'samples': 9408, 'loss': 0.48350922763347626}, {'samples': 9664, 'loss': 0.47445618361234665}, {'samples': 9920, 'loss': 0.4847627133131027}, {'samples': 10176, 'loss': 0.4329887330532074}, {'samples': 10432, 'loss': 0.45413145422935486}, {'samples': 10688, 'loss': 0.4443042278289795}, {'samples': 10944, 'loss': 0.4275665283203125}, {'samples': 11200, 'loss': 0.46382027864456177}, {'samples': 11456, 'loss': 0.44055134803056717}, {'samples': 11712, 'loss': 0.4389412924647331}, {'samples': 11968, 'loss': 0.46612395346164703}, {'samples': 12224, 'loss': 0.4677596017718315}, {'samples': 12480, 'loss': 0.3684111163020134}, {'samples': 12736, 'loss': 0.4535309225320816}, {'samples': 12992, 'loss': 0.4411945641040802}, {'samples': 13248, 'loss': 0.4298301264643669}, {'samples': 13504, 'loss': 0.3907106816768646}, {'samples': 13760, 'loss': 0.47461601346731186}, {'samples': 14016, 'loss': 0.43345891684293747}, {'samples': 14272, 'loss': 0.46985164284706116}, {'samples': 14528, 'loss': 0.45905836671590805}, {'samples': 14784, 'loss': 0.500466413795948}, {'samples': 15040, 'loss': 0.4495847299695015}, {'samples': 15296, 'loss': 0.4747973382472992}, {'samples': 15552, 'loss': 0.4763115867972374}, {'samples': 15808, 'loss': 0.4823751673102379}]
Evaluation accuracy logs: [{'samples': 448, 'accuracy': 0.5248068756319515, 'std': 0.011134740956047752, 'lower_bound': 0.5035389282103134, 'upper_bound': 0.5465242669362993}, {'samples': 960, 'accuracy': 0.6266304347826087, 'std': 0.010754741587554358, 'lower_bound': 0.6041456016177957, 'upper_bound': 0.6476238624873609}, {'samples': 1472, 'accuracy': 0.7570080889787665, 'std': 0.009673552802968326, 'lower_bound': 0.7386122345803842, 'upper_bound': 0.7750252780586451}, {'samples': 1984, 'accuracy': 0.737303842264914, 'std': 0.00983650083382279, 'lower_bound': 0.71840242669363, 'upper_bound': 0.7568250758341759}, {'samples': 2496, 'accuracy': 0.7501102123356927, 'std': 0.010267678283329796, 'lower_bound': 0.7300176946410515, 'upper_bound': 0.7704752275025278}, {'samples': 3008, 'accuracy': 0.7533250758341759, 'std': 0.009343518268214444, 'lower_bound': 0.7345803842264914, 'upper_bound': 0.7714989888776542}, {'samples': 3520, 'accuracy': 0.7335995955510617, 'std': 0.009645921698767672, 'lower_bound': 0.7138523761375126, 'upper_bound': 0.7527805864509606}, {'samples': 4032, 'accuracy': 0.8111582406471183, 'std': 0.009022123484726155, 'lower_bound': 0.7942239635995956, 'upper_bound': 0.8291203235591507}, {'samples': 4544, 'accuracy': 0.6125631951466127, 'std': 0.010690622669822665, 'lower_bound': 0.5915065722952477, 'upper_bound': 0.6334681496461072}, {'samples': 5056, 'accuracy': 0.831196157735086, 'std': 0.008366735963337937, 'lower_bound': 0.8144464105156723, 'upper_bound': 0.8473205257836198}, {'samples': 5568, 'accuracy': 0.8154615773508594, 'std': 0.008597494154140328, 'lower_bound': 0.7987866531850354, 'upper_bound': 0.8336703741152679}, {'samples': 6080, 'accuracy': 0.7610278058645096, 'std': 0.009596651499770964, 'lower_bound': 0.7416582406471183, 'upper_bound': 0.7790697674418605}, {'samples': 6592, 'accuracy': 0.8206359959555107, 'std': 0.008737502403068182, 'lower_bound': 0.80332406471183, 'upper_bound': 0.8372093023255814}, {'samples': 7104, 'accuracy': 0.7798771486349849, 'std': 0.009048818977152625, 'lower_bound': 0.7618680485338726, 'upper_bound': 0.7972699696663297}, {'samples': 7616, 'accuracy': 0.8305824064711831, 'std': 0.008548714086558596, 'lower_bound': 0.8129423660262892, 'upper_bound': 0.845803842264914}, {'samples': 8128, 'accuracy': 0.8321809908998989, 'std': 0.008791955672279255, 'lower_bound': 0.8154575328614762, 'upper_bound': 0.8498483316481295}, {'samples': 8640, 'accuracy': 0.8280288169868554, 'std': 0.008469133833516001, 'lower_bound': 0.8109201213346815, 'upper_bound': 0.8447927199191102}, {'samples': 9152, 'accuracy': 0.8329307381193124, 'std': 0.007956553653860257, 'lower_bound': 0.8179979777553084, 'upper_bound': 0.8488372093023255}, {'samples': 9664, 'accuracy': 0.8379034378159758, 'std': 0.008373745565889774, 'lower_bound': 0.8215369059656218, 'upper_bound': 0.8533998988877655}, {'samples': 10176, 'accuracy': 0.8246814964610718, 'std': 0.008668149655542813, 'lower_bound': 0.80788675429727, 'upper_bound': 0.8412537917087968}, {'samples': 10688, 'accuracy': 0.8277335692618807, 'std': 0.00837180835023021, 'lower_bound': 0.8109201213346815, 'upper_bound': 0.8442997977755309}, {'samples': 11200, 'accuracy': 0.8382436804853388, 'std': 0.00821377263792161, 'lower_bound': 0.8220424671385238, 'upper_bound': 0.8549039433771486}, {'samples': 11712, 'accuracy': 0.8369600606673407, 'std': 0.008420408578378754, 'lower_bound': 0.820525783619818, 'upper_bound': 0.8533872598584429}, {'samples': 12224, 'accuracy': 0.8253139534883721, 'std': 0.00865061051721267, 'lower_bound': 0.8083923154701719, 'upper_bound': 0.8422649140546006}, {'samples': 12736, 'accuracy': 0.8317891809908999, 'std': 0.008411893165164781, 'lower_bound': 0.8144590495449949, 'upper_bound': 0.8478260869565217}, {'samples': 13248, 'accuracy': 0.8458518705763397, 'std': 0.007987354154766589, 'lower_bound': 0.8301314459049545, 'upper_bound': 0.8614888776541962}, {'samples': 13760, 'accuracy': 0.844979271991911, 'std': 0.008178299176439936, 'lower_bound': 0.8286147623862488, 'upper_bound': 0.8609706774519716}, {'samples': 14272, 'accuracy': 0.8339716885743176, 'std': 0.00817489108698652, 'lower_bound': 0.8185035389282103, 'upper_bound': 0.8493427704752275}, {'samples': 14784, 'accuracy': 0.8449858442871588, 'std': 0.008088474390240812, 'lower_bound': 0.8286147623862488, 'upper_bound': 0.8609706774519716}, {'samples': 15296, 'accuracy': 0.8444100101112235, 'std': 0.008224812974336743, 'lower_bound': 0.8276036400404448, 'upper_bound': 0.8594666329625885}, {'samples': 15808, 'accuracy': 0.8455374115267948, 'std': 0.00822060610863414, 'lower_bound': 0.8296258847320526, 'upper_bound': 0.8619817997977756}]
Evaluating the best model on the test set of dataset ./fake_true_datasets/fake_true_dataset_gemma_10k...
Test metrics:
accuracy: 0.8518326592517694
precision: 0.8663415629124748
recall: 0.832091193923519
f1_score: 0.8488029107370719
fp_rate: 0.12840904238351922
tp_rate: 0.832091193923519
std_accuracy: 0.007838862116097063
std_precision: 0.01089900644062252
std_recall: 0.01182692030744381
std_f1_score: 0.008505773073144679
std_fp_rate: 0.010472413960886865
std_tp_rate: 0.01182692030744381
TP: 823.04
TN: 861.885
FP: 126.979
FN: 166.096
roc_auc: 0.9244658891895788
fpr: [0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00101112
 0.00101112 0.00202224 0.00202224 0.00202224 0.00202224 0.00202224
 0.00202224 0.00202224 0.00202224 0.00303337 0.00303337 0.00404449
 0.00404449 0.00404449 0.00404449 0.00404449 0.00404449 0.00404449
 0.00404449 0.00404449 0.00404449 0.00505561 0.00505561 0.00606673
 0.00606673 0.00707786 0.00707786 0.00808898 0.00808898 0.00808898
 0.0091001  0.0091001  0.0091001  0.0091001  0.0091001  0.0091001
 0.0091001  0.0091001  0.01011122 0.01011122 0.01112235 0.01112235
 0.01213347 0.01213347 0.01314459 0.01314459 0.01314459 0.01314459
 0.01516684 0.01516684 0.01617796 0.01617796 0.01718908 0.01718908
 0.0182002  0.0182002  0.0182002  0.0182002  0.0182002  0.0182002
 0.01921132 0.01921132 0.01921132 0.01921132 0.02022245 0.02022245
 0.02123357 0.02123357 0.02224469 0.02224469 0.02325581 0.02325581
 0.02426694 0.02426694 0.02426694 0.02426694 0.02527806 0.02527806
 0.02527806 0.02628918 0.02628918 0.0273003  0.0273003  0.02831143
 0.02831143 0.02932255 0.02932255 0.02932255 0.02932255 0.03033367
 0.03033367 0.03134479 0.03134479 0.03235592 0.03336704 0.03336704
 0.03437816 0.03437816 0.03538928 0.03538928 0.0364004  0.0364004
 0.0364004  0.0364004  0.03741153 0.03741153 0.03842265 0.03842265
 0.03943377 0.03943377 0.04044489 0.04246714 0.04246714 0.04347826
 0.04651163 0.04651163 0.04752275 0.04752275 0.04853387 0.04853387
 0.04954499 0.04954499 0.05257836 0.05257836 0.05358948 0.05358948
 0.05358948 0.05358948 0.05460061 0.05460061 0.05460061 0.05561173
 0.05561173 0.05662285 0.05662285 0.05662285 0.05662285 0.05763397
 0.05763397 0.0586451  0.0586451  0.05965622 0.05965622 0.06268959
 0.06268959 0.06370071 0.06370071 0.06471183 0.06471183 0.06572295
 0.06673407 0.06673407 0.0677452  0.0677452  0.06875632 0.06875632
 0.07178969 0.07178969 0.07178969 0.07178969 0.07280081 0.07280081
 0.07482305 0.07482305 0.07583418 0.07583418 0.0768453  0.0768453
 0.07785642 0.07987867 0.07987867 0.08190091 0.08190091 0.08291203
 0.08392315 0.08392315 0.08493428 0.08493428 0.08695652 0.08695652
 0.08796764 0.08796764 0.08998989 0.08998989 0.0950455  0.09605662
 0.09605662 0.09706775 0.09706775 0.09807887 0.09807887 0.10010111
 0.10010111 0.10111223 0.10111223 0.10313448 0.10313448 0.1041456
 0.10515672 0.10717897 0.10717897 0.10819009 0.10819009 0.10819009
 0.11021234 0.11021234 0.11122346 0.11122346 0.11122346 0.11425683
 0.11425683 0.11526795 0.11627907 0.11627907 0.11729019 0.11729019
 0.1223458  0.1223458  0.12335693 0.12335693 0.12335693 0.12436805
 0.12436805 0.12537917 0.12537917 0.12639029 0.12639029 0.12841254
 0.12841254 0.12942366 0.12942366 0.13043478 0.13043478 0.1314459
 0.1314459  0.13549039 0.13549039 0.13650152 0.13650152 0.13953488
 0.13953488 0.14357937 0.14357937 0.14661274 0.14762386 0.14762386
 0.14863498 0.14964611 0.14964611 0.15065723 0.15065723 0.15166835
 0.1536906  0.15470172 0.15470172 0.15571284 0.15571284 0.15672396
 0.15672396 0.15773509 0.15773509 0.15874621 0.15874621 0.16885743
 0.16885743 0.16986855 0.16986855 0.1718908  0.1718908  0.17694641
 0.17694641 0.17795753 0.17997978 0.17997978 0.1809909  0.1809909
 0.18200202 0.18200202 0.18503539 0.18503539 0.18806876 0.18806876
 0.19615774 0.19615774 0.19817998 0.19817998 0.20323559 0.20323559
 0.20728008 0.20728008 0.21334681 0.21334681 0.21638018 0.21638018
 0.21840243 0.21840243 0.21941355 0.21941355 0.22143579 0.22143579
 0.22446916 0.22548028 0.22750253 0.22750253 0.22851365 0.22851365
 0.23761375 0.23761375 0.239636   0.239636   0.24064712 0.24064712
 0.24266936 0.24266936 0.24671385 0.24671385 0.24772497 0.24772497
 0.24974722 0.24974722 0.25075834 0.25075834 0.25581395 0.2578362
 0.25884732 0.26491405 0.26491405 0.2669363  0.2669363  0.2760364
 0.2760364  0.27704752 0.27704752 0.27805865 0.27805865 0.28311426
 0.28311426 0.28412538 0.28412538 0.28614762 0.28614762 0.28715875
 0.28715875 0.29120324 0.29120324 0.29524772 0.29524772 0.29726997
 0.29726997 0.30030334 0.30030334 0.30434783 0.30434783 0.30535895
 0.30535895 0.30738119 0.31547017 0.31547017 0.31648129 0.31648129
 0.31850354 0.32052578 0.32457027 0.32457027 0.3255814  0.32760364
 0.33063701 0.33164813 0.33367037 0.33367037 0.33569262 0.33670374
 0.34580384 0.34782609 0.34883721 0.34883721 0.3528817  0.35490394
 0.36299292 0.36299292 0.37209302 0.37209302 0.37613751 0.37815976
 0.37917088 0.37917088 0.38624874 0.39029323 0.39029323 0.39231547
 0.39231547 0.40040445 0.40040445 0.41051567 0.41051567 0.41860465
 0.41961577 0.4206269  0.4206269  0.42163802 0.42163802 0.42366026
 0.429727   0.429727   0.43073812 0.43073812 0.43377149 0.43377149
 0.44084934 0.44084934 0.44388271 0.44590495 0.44590495 0.44893832
 0.45298281 0.45601618 0.45803842 0.46006067 0.46208291 0.47017189
 0.47219414 0.4752275  0.4752275  0.47623862 0.47826087 0.48634985
 0.48634985 0.48736097 0.48736097 0.49241658 0.49443883 0.49544995
 0.49544995 0.49646107 0.49646107 0.49848332 0.49848332 0.50455005
 0.5065723  0.50960566 0.51263903 0.51769464 0.51971689 0.52072801
 0.52275025 0.52780586 0.52881699 0.52881699 0.53083923 0.53083923
 0.53488372 0.53488372 0.53892821 0.53892821 0.54095046 0.5429727
 0.55510617 0.55712841 0.55813953 0.55915066 0.56521739 0.56622851
 0.570273   0.57229525 0.57431749 0.57532861 0.57532861 0.57633974
 0.59049545 0.59150657 0.60262892 0.60262892 0.60667341 0.60667341
 0.60869565 0.61678463 0.61678463 0.619818   0.62285137 0.62487361
 0.62689585 0.63195147 0.63397371 0.64812942 0.65217391 0.65318504
 0.65520728 0.65722952 0.65925177 0.66127401 0.66127401 0.66430738
 0.66632963 0.66734075 0.66734075 0.66936299 0.67138524 0.67340748
 0.67542973 0.67644085 0.67846309 0.68149646 0.68149646 0.68452983
 0.68857432 0.69362993 0.69464105 0.69969666 0.70171891 0.70374115
 0.70374115 0.7057634  0.70778564 0.70879676 0.71284125 0.71284125
 0.71385238 0.71385238 0.71688574 0.71890799 0.71890799 0.7239636
 0.72699697 0.72901921 0.72901921 0.73104146 0.7330637  0.73508595
 0.73710819 0.74620829 0.74924166 0.75429727 0.75631951 0.75733064
 0.75733064 0.75935288 0.76744186 0.76946411 0.77047523 0.77350859
 0.77755308 0.77856421 0.78361982 0.78361982 0.78867543 0.78867543
 0.79271992 0.79373104 0.79575329 0.79575329 0.80788675 0.809909
 0.8190091  0.82103134 0.82204247 0.82608696 0.8372093  0.83923155
 0.84327604 0.84529828 0.85136502 0.85338726 0.86147624 0.86147624
 0.86349848 0.87158746 0.87360971 0.87360971 0.87866532 0.88068756
 0.88372093 0.88776542 0.89282103 0.89686552 0.90899899 0.91203236
 0.91809909 0.91809909 0.91911021 0.92113246 0.92416582 0.92719919
 0.9322548  0.93427705 0.94034378 0.94236603 0.94337715 0.94539939
 0.94742164 0.94944388 0.95045501 0.95247725 0.96258847 0.96461072
 0.96764408 0.96966633 0.98988878 0.99191102 1.        ]
tpr: [0.         0.00101112 0.0091001  0.01112235 0.01314459 0.01617796
 0.0182002  0.02022245 0.03134479 0.03336704 0.04448938 0.04651163
 0.05561173 0.05763397 0.06066734 0.06268959 0.06471183 0.07077856
 0.07785642 0.07987867 0.08088979 0.08493428 0.08897877 0.09100101
 0.0950455  0.09706775 0.10920121 0.11526795 0.13650152 0.13852376
 0.14155713 0.14661274 0.14964611 0.1536906  0.15773509 0.15975733
 0.16177958 0.16380182 0.16582406 0.17087968 0.17492417 0.17593529
 0.17795753 0.18705763 0.18907988 0.19413549 0.19615774 0.20323559
 0.20525784 0.24772497 0.24974722 0.25075834 0.25278059 0.2669363
 0.26895854 0.28715875 0.29120324 0.30030334 0.30232558 0.31951466
 0.32153691 0.32962588 0.33367037 0.34580384 0.34782609 0.34883721
 0.35085945 0.35490394 0.35793731 0.3619818  0.36501517 0.36501517
 0.36703741 0.36804853 0.36905966 0.3710819  0.37411527 0.37613751
 0.37714863 0.37917088 0.38220425 0.38220425 0.42264914 0.42264914
 0.43276036 0.43680485 0.4479272  0.44994944 0.45197169 0.45399393
 0.46410516 0.4661274  0.47118301 0.47118301 0.48028311 0.48028311
 0.4934277  0.4934277  0.50758342 0.50758342 0.51466127 0.51668352
 0.51769464 0.51870576 0.52072801 0.52578362 0.52780586 0.53185035
 0.5338726  0.53690597 0.53690597 0.53791709 0.53892821 0.54095046
 0.54196158 0.54802831 0.54802831 0.55005056 0.5520728  0.55308392
 0.55308392 0.55409505 0.55510617 0.55611729 0.55611729 0.55712841
 0.55712841 0.5611729  0.56319515 0.56723964 0.56926188 0.570273
 0.57128413 0.57431749 0.57633974 0.58240647 0.58240647 0.58341759
 0.58442872 0.58746208 0.58746208 0.59251769 0.59352882 0.59555106
 0.59555106 0.59858443 0.60262892 0.60364004 0.60364004 0.60566229
 0.61779575 0.61779575 0.619818   0.62184024 0.62386249 0.62386249
 0.6289181  0.6289181  0.63902932 0.64105157 0.64307381 0.64307381
 0.64509606 0.64610718 0.64914055 0.65015167 0.65015167 0.65217391
 0.65217391 0.65318504 0.65318504 0.65520728 0.65520728 0.66329626
 0.6653185  0.66936299 0.66936299 0.67138524 0.67138524 0.67239636
 0.67239636 0.67542973 0.67542973 0.67644085 0.68048534 0.68149646
 0.68149646 0.68351871 0.68351871 0.68452983 0.68452983 0.68554095
 0.68554095 0.68655207 0.68655207 0.6875632  0.6875632  0.69059656
 0.69261881 0.69362993 0.69464105 0.69868554 0.70171891 0.70171891
 0.70273003 0.70475228 0.7057634  0.70778564 0.70879676 0.70879676
 0.71789687 0.71789687 0.71890799 0.71890799 0.72093023 0.72093023
 0.72295248 0.7239636  0.72497472 0.72497472 0.72699697 0.72699697
 0.72800809 0.73205258 0.73205258 0.7421638  0.7421638  0.74317492
 0.74317492 0.74519717 0.74721941 0.75227503 0.75227503 0.75328615
 0.75328615 0.75631951 0.75631951 0.75733064 0.75733064 0.75935288
 0.760364   0.760364   0.76238625 0.76238625 0.76744186 0.76744186
 0.76845298 0.77148635 0.77148635 0.77350859 0.77350859 0.77654196
 0.77654196 0.77856421 0.77856421 0.77957533 0.77957533 0.78058645
 0.7826087  0.7826087  0.78766431 0.78766431 0.78968655 0.78968655
 0.79474216 0.79474216 0.79676441 0.79676441 0.79777553 0.79777553
 0.79878665 0.79878665 0.80283114 0.80283114 0.80485339 0.80788675
 0.80788675 0.80889788 0.80889788 0.81092012 0.81193124 0.81193124
 0.81294237 0.81395349 0.81395349 0.81496461 0.81597573 0.81698686
 0.81698686 0.81799798 0.81799798 0.82002022 0.82103134 0.82103134
 0.82305359 0.82305359 0.82507583 0.82507583 0.82912032 0.82912032
 0.83013145 0.83013145 0.83114257 0.83114257 0.83417594 0.83417594
 0.83619818 0.83619818 0.8372093  0.8372093  0.84226491 0.84226491
 0.84327604 0.84327604 0.84428716 0.84428716 0.84529828 0.8463094
 0.84732053 0.84732053 0.84833165 0.84833165 0.84934277 0.84934277
 0.84934277 0.84934277 0.85035389 0.85035389 0.85136502 0.85136502
 0.85338726 0.85338726 0.85439838 0.85439838 0.8554095  0.8554095
 0.85844287 0.85844287 0.85945399 0.85945399 0.86248736 0.86248736
 0.86450961 0.86552073 0.86552073 0.86754297 0.86754297 0.86956522
 0.86956522 0.87158746 0.87158746 0.87664307 0.87664307 0.8776542
 0.8776542  0.87866532 0.87866532 0.88068756 0.88068756 0.88169869
 0.88169869 0.88473205 0.88473205 0.88574317 0.88574317 0.88776542
 0.88776542 0.88877654 0.88877654 0.89079879 0.89079879 0.89180991
 0.89180991 0.89282103 0.89282103 0.89484328 0.89484328 0.89686552
 0.89686552 0.89787664 0.89787664 0.89888777 0.89888777 0.89989889
 0.89989889 0.90091001 0.90091001 0.90293225 0.90293225 0.90394338
 0.90394338 0.9049545  0.9049545  0.90697674 0.90697674 0.90697674
 0.90798787 0.90798787 0.91102123 0.91102123 0.91304348 0.91304348
 0.91506572 0.91506572 0.91607685 0.91607685 0.91809909 0.91809909
 0.91911021 0.91911021 0.92012133 0.92012133 0.92113246 0.92113246
 0.92214358 0.92214358 0.9231547  0.9231547  0.92416582 0.92416582
 0.92517695 0.92517695 0.92618807 0.92618807 0.92719919 0.92719919
 0.92922144 0.92922144 0.92922144 0.93023256 0.93023256 0.93124368
 0.93124368 0.93124368 0.93124368 0.9322548  0.9322548  0.9322548
 0.9322548  0.93326593 0.93326593 0.93528817 0.93528817 0.93629929
 0.93629929 0.93629929 0.93731041 0.93832154 0.93832154 0.93832154
 0.93832154 0.93933266 0.93933266 0.94034378 0.94034378 0.94034378
 0.94034378 0.9413549  0.9413549  0.9413549  0.94236603 0.94236603
 0.94337715 0.94337715 0.94438827 0.94438827 0.94539939 0.94539939
 0.94641052 0.94641052 0.94742164 0.94742164 0.94843276 0.94843276
 0.94843276 0.94944388 0.95045501 0.95146613 0.95146613 0.95348837
 0.95348837 0.95449949 0.95449949 0.95449949 0.95551062 0.95551062
 0.95551062 0.95551062 0.95551062 0.95551062 0.95551062 0.95551062
 0.95551062 0.95551062 0.95652174 0.95652174 0.95652174 0.95652174
 0.95753286 0.95753286 0.95955511 0.95955511 0.95955511 0.95955511
 0.96157735 0.96157735 0.96258847 0.96258847 0.9635996  0.9635996
 0.9635996  0.9635996  0.9635996  0.9635996  0.9635996  0.9635996
 0.9635996  0.9635996  0.96461072 0.96562184 0.96562184 0.96663296
 0.96663296 0.96764408 0.96764408 0.96865521 0.96865521 0.96865521
 0.96865521 0.96865521 0.96865521 0.96966633 0.96966633 0.97067745
 0.97067745 0.97067745 0.97067745 0.97067745 0.97371082 0.97472194
 0.97472194 0.97674419 0.97674419 0.97775531 0.97775531 0.97876643
 0.97876643 0.97876643 0.97977755 0.97977755 0.97977755 0.97977755
 0.97977755 0.97977755 0.97977755 0.97977755 0.97977755 0.97977755
 0.97977755 0.97977755 0.97977755 0.97977755 0.98078868 0.98078868
 0.98078868 0.98078868 0.9817998  0.9817998  0.9817998  0.9817998
 0.9817998  0.98281092 0.98281092 0.98281092 0.98382204 0.98382204
 0.98382204 0.98382204 0.98483316 0.98483316 0.98483316 0.98483316
 0.98584429 0.98584429 0.98584429 0.98685541 0.98685541 0.98786653
 0.98786653 0.98887765 0.98887765 0.98887765 0.98988878 0.98988878
 0.98988878 0.98988878 0.9908999  0.9908999  0.9908999  0.9908999
 0.9908999  0.9908999  0.9908999  0.9908999  0.9908999  0.9908999
 0.99191102 0.99191102 0.99191102 0.99191102 0.99191102 0.99191102
 0.99191102 0.99292214 0.99292214 0.99393327 0.99393327 0.99494439
 0.99494439 0.99595551 0.99595551 0.99696663 0.99696663 0.99696663
 0.99696663 0.99696663 0.99696663 0.99696663 0.99696663 0.99696663
 0.99696663 0.99696663 0.99696663 0.99696663 0.99696663 0.99797776
 0.99797776 0.99797776 0.99797776 0.99898888 0.99898888 0.99898888
 0.99898888 0.99898888 0.99898888 0.99898888 0.99898888 0.99898888
 0.99898888 1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.        ]
thresholds: [            inf  4.61718750e+00  3.43164062e+00  3.38085938e+00
  3.30273438e+00  3.29296875e+00  3.24414062e+00  3.22460938e+00
  2.94140625e+00  2.92968750e+00  2.81640625e+00  2.81250000e+00
  2.74218750e+00  2.73828125e+00  2.69921875e+00  2.68945312e+00
  2.68554688e+00  2.65429688e+00  2.61523438e+00  2.61328125e+00
  2.60742188e+00  2.60156250e+00  2.57617188e+00  2.57031250e+00
  2.51757812e+00  2.51367188e+00  2.46484375e+00  2.45507812e+00
  2.28125000e+00  2.27929688e+00  2.27343750e+00  2.25000000e+00
  2.24609375e+00  2.20898438e+00  2.19921875e+00  2.19335938e+00
  2.18554688e+00  2.17578125e+00  2.17382812e+00  2.15820312e+00
  2.13671875e+00  2.12890625e+00  2.12695312e+00  2.09179688e+00
  2.08007812e+00  2.03906250e+00  2.03515625e+00  1.98144531e+00
  1.97558594e+00  1.70019531e+00  1.69238281e+00  1.69140625e+00
  1.68945312e+00  1.58398438e+00  1.57226562e+00  1.46777344e+00
  1.43066406e+00  1.38476562e+00  1.38281250e+00  1.29882812e+00
  1.29589844e+00  1.26171875e+00  1.25000000e+00  1.17773438e+00
  1.17187500e+00  1.15820312e+00  1.15527344e+00  1.13769531e+00
  1.13671875e+00  1.12597656e+00  1.11425781e+00  1.11132812e+00
  1.10546875e+00  1.10351562e+00  1.10156250e+00  1.09960938e+00
  1.08593750e+00  1.07812500e+00  1.07714844e+00  1.07421875e+00
  1.06933594e+00  1.06347656e+00  9.29199219e-01  9.27246094e-01
  9.03320312e-01  8.89648438e-01  8.61816406e-01  8.57421875e-01
  8.55468750e-01  8.51074219e-01  8.20800781e-01  8.12988281e-01
  8.07617188e-01  8.07128906e-01  7.89550781e-01  7.87109375e-01
  7.63183594e-01  7.53417969e-01  7.15332031e-01  7.08984375e-01
  6.82128906e-01  6.81152344e-01  6.79199219e-01  6.78222656e-01
  6.77734375e-01  6.66015625e-01  6.65527344e-01  6.55273438e-01
  6.54296875e-01  6.51367188e-01  6.48437500e-01  6.47460938e-01
  6.43554688e-01  6.36230469e-01  6.35742188e-01  6.26953125e-01
  6.24511719e-01  6.18652344e-01  6.17675781e-01  6.15722656e-01
  6.15234375e-01  6.12304688e-01  6.10839844e-01  6.05957031e-01
  6.05468750e-01  5.98632812e-01  5.97656250e-01  5.88378906e-01
  5.86425781e-01  5.82519531e-01  5.79589844e-01  5.74707031e-01
  5.73730469e-01  5.67871094e-01  5.63964844e-01  5.56152344e-01
  5.55175781e-01  5.53710938e-01  5.52734375e-01  5.46386719e-01
  5.45898438e-01  5.37597656e-01  5.35156250e-01  5.34667969e-01
  5.31250000e-01  5.27343750e-01  5.21484375e-01  5.20996094e-01
  5.20019531e-01  5.17578125e-01  4.98291016e-01  4.91455078e-01
  4.88281250e-01  4.86816406e-01  4.85839844e-01  4.85107422e-01
  4.76562500e-01  4.75341797e-01  4.52392578e-01  4.50927734e-01
  4.49707031e-01  4.47265625e-01  4.46044922e-01  4.43359375e-01
  4.34814453e-01  4.34082031e-01  4.32617188e-01  4.31396484e-01
  4.30175781e-01  4.29931641e-01  4.27734375e-01  4.23828125e-01
  4.20410156e-01  4.10644531e-01  4.10400391e-01  4.08691406e-01
  4.08447266e-01  4.05273438e-01  4.03076172e-01  4.00634766e-01
  4.00146484e-01  3.94775391e-01  3.94531250e-01  3.93554688e-01
  3.91845703e-01  3.91357422e-01  3.86962891e-01  3.82568359e-01
  3.82324219e-01  3.81835938e-01  3.81591797e-01  3.80859375e-01
  3.80126953e-01  3.79882812e-01  3.70117188e-01  3.68896484e-01
  3.66699219e-01  3.62792969e-01  3.62548828e-01  3.61816406e-01
  3.61572266e-01  3.52539062e-01  3.49365234e-01  3.48876953e-01
  3.47900391e-01  3.47167969e-01  3.46191406e-01  3.45703125e-01
  3.44482422e-01  3.43261719e-01  3.26171875e-01  3.18847656e-01
  3.17871094e-01  3.17138672e-01  3.13476562e-01  3.11279297e-01
  3.10302734e-01  3.08593750e-01  3.07617188e-01  3.05908203e-01
  3.03710938e-01  3.02490234e-01  2.96875000e-01  2.93457031e-01
  2.91503906e-01  2.78076172e-01  2.76611328e-01  2.74658203e-01
  2.70507812e-01  2.65625000e-01  2.64160156e-01  2.59033203e-01
  2.58056641e-01  2.56835938e-01  2.52685547e-01  2.48291016e-01
  2.47192383e-01  2.46948242e-01  2.45483398e-01  2.41943359e-01
  2.40722656e-01  2.39501953e-01  2.39013672e-01  2.35595703e-01
  2.25097656e-01  2.24609375e-01  2.22900391e-01  2.17651367e-01
  2.17407227e-01  2.15087891e-01  2.14355469e-01  2.13134766e-01
  2.11914062e-01  2.11547852e-01  2.07031250e-01  2.06909180e-01
  2.02392578e-01  2.01904297e-01  1.99584961e-01  1.94824219e-01
  1.83715820e-01  1.82983398e-01  1.81518555e-01  1.80053711e-01
  1.76757812e-01  1.76025391e-01  1.73706055e-01  1.73461914e-01
  1.72607422e-01  1.72485352e-01  1.72241211e-01  1.69799805e-01
  1.60278320e-01  1.60034180e-01  1.59667969e-01  1.57836914e-01
  1.55395508e-01  1.55151367e-01  1.53808594e-01  1.51855469e-01
  1.51367188e-01  1.48681641e-01  1.46606445e-01  1.46484375e-01
  1.46240234e-01  1.45751953e-01  1.42089844e-01  1.41967773e-01
  1.38061523e-01  1.36840820e-01  1.35131836e-01  1.34155273e-01
  1.34033203e-01  1.33422852e-01  1.27807617e-01  1.25732422e-01
  1.23413086e-01  1.22985840e-01  1.21154785e-01  1.18713379e-01
  1.15661621e-01  1.15356445e-01  1.14501953e-01  1.12060547e-01
  1.10778809e-01  1.10534668e-01  1.10168457e-01  1.08093262e-01
  1.07482910e-01  1.06628418e-01  1.01867676e-01  9.77172852e-02
  9.68627930e-02  9.48486328e-02  9.44213867e-02  8.95385742e-02
  8.81958008e-02  8.78295898e-02  8.75854492e-02  8.67919922e-02
  8.53271484e-02  8.24584961e-02  8.05664062e-02  7.91625977e-02
  7.72705078e-02  7.69653320e-02  7.52563477e-02  7.51342773e-02
  7.47680664e-02  7.46459961e-02  7.45239258e-02  7.33642578e-02
  6.69555664e-02  6.68334961e-02  6.67114258e-02  5.82275391e-02
  5.60302734e-02  5.49621582e-02  5.38024902e-02  5.01403809e-02
  4.49218750e-02  3.94592285e-02  3.76586914e-02  3.55224609e-02
  3.49731445e-02  3.35388184e-02  3.25622559e-02  3.06243896e-02
  2.93273926e-02  2.47955322e-02  2.14080811e-02  1.84173584e-02
  1.57623291e-02  1.40838623e-02  6.28280640e-03  5.18417358e-03
  2.92587280e-03  1.87301636e-03 -1.63650513e-03 -1.88064575e-03
 -6.85501099e-03 -9.33837891e-03 -1.38244629e-02 -1.42211914e-02
 -1.77001953e-02 -2.06146240e-02 -2.25219727e-02 -2.32238770e-02
 -2.33306885e-02 -2.53448486e-02 -3.02124023e-02 -3.03039551e-02
 -3.10821533e-02 -3.13720703e-02 -3.25927734e-02 -3.34472656e-02
 -3.41796875e-02 -3.74755859e-02 -4.71801758e-02 -4.91027832e-02
 -5.07202148e-02 -5.17272949e-02 -5.18188477e-02 -5.29785156e-02
 -5.49316406e-02 -5.56030273e-02 -5.74645996e-02 -5.78613281e-02
 -5.80749512e-02 -5.91430664e-02 -6.06079102e-02 -6.07910156e-02
 -6.16760254e-02 -6.20117188e-02 -6.68945312e-02 -6.75659180e-02
 -6.85424805e-02 -7.31201172e-02 -7.91015625e-02 -8.05664062e-02
 -8.46557617e-02 -9.79003906e-02 -1.05590820e-01 -1.06628418e-01
 -1.07910156e-01 -1.09130859e-01 -1.09924316e-01 -1.15356445e-01
 -1.15844727e-01 -1.16149902e-01 -1.19445801e-01 -1.21704102e-01
 -1.22619629e-01 -1.23657227e-01 -1.24694824e-01 -1.26708984e-01
 -1.28417969e-01 -1.30981445e-01 -1.33789062e-01 -1.35498047e-01
 -1.35620117e-01 -1.38916016e-01 -1.39892578e-01 -1.45141602e-01
 -1.45629883e-01 -1.46728516e-01 -1.49047852e-01 -1.49658203e-01
 -1.63208008e-01 -1.64550781e-01 -1.65161133e-01 -1.66137695e-01
 -1.67358398e-01 -1.67480469e-01 -1.70654297e-01 -1.71752930e-01
 -1.74072266e-01 -1.74804688e-01 -1.77490234e-01 -1.79077148e-01
 -1.82495117e-01 -1.83715820e-01 -1.85180664e-01 -1.86645508e-01
 -1.96899414e-01 -1.97265625e-01 -1.97998047e-01 -1.99462891e-01
 -2.03613281e-01 -2.03735352e-01 -2.14477539e-01 -2.14599609e-01
 -2.24243164e-01 -2.24365234e-01 -2.30346680e-01 -2.32299805e-01
 -2.32421875e-01 -2.32543945e-01 -2.39501953e-01 -2.42187500e-01
 -2.43774414e-01 -2.46459961e-01 -2.47680664e-01 -2.53906250e-01
 -2.54394531e-01 -2.65136719e-01 -2.65380859e-01 -2.75390625e-01
 -2.75634766e-01 -2.75878906e-01 -2.76123047e-01 -2.76611328e-01
 -2.78564453e-01 -2.79052734e-01 -2.84179688e-01 -2.84912109e-01
 -2.87353516e-01 -2.88330078e-01 -2.93945312e-01 -2.95410156e-01
 -3.03222656e-01 -3.03466797e-01 -3.05908203e-01 -3.06640625e-01
 -3.08105469e-01 -3.11523438e-01 -3.12744141e-01 -3.15185547e-01
 -3.15673828e-01 -3.17382812e-01 -3.17871094e-01 -3.26660156e-01
 -3.29101562e-01 -3.36181641e-01 -3.36669922e-01 -3.36914062e-01
 -3.37890625e-01 -3.49853516e-01 -3.50585938e-01 -3.52050781e-01
 -3.54248047e-01 -3.58154297e-01 -3.59130859e-01 -3.59375000e-01
 -3.64746094e-01 -3.67187500e-01 -3.68164062e-01 -3.70361328e-01
 -3.70605469e-01 -3.77197266e-01 -3.79150391e-01 -3.82080078e-01
 -3.83300781e-01 -3.92089844e-01 -3.93554688e-01 -3.93798828e-01
 -3.95263672e-01 -3.99414062e-01 -4.00146484e-01 -4.00634766e-01
 -4.02832031e-01 -4.03564453e-01 -4.07226562e-01 -4.07714844e-01
 -4.10156250e-01 -4.10400391e-01 -4.13574219e-01 -4.13818359e-01
 -4.29199219e-01 -4.29687500e-01 -4.30664062e-01 -4.32128906e-01
 -4.37744141e-01 -4.37988281e-01 -4.40185547e-01 -4.45800781e-01
 -4.46044922e-01 -4.48486328e-01 -4.52392578e-01 -4.53369141e-01
 -4.67773438e-01 -4.68017578e-01 -4.73388672e-01 -4.76562500e-01
 -4.81933594e-01 -4.82421875e-01 -4.84375000e-01 -4.91943359e-01
 -4.95117188e-01 -5.02929688e-01 -5.04394531e-01 -5.07812500e-01
 -5.09277344e-01 -5.19531250e-01 -5.23925781e-01 -5.35156250e-01
 -5.39550781e-01 -5.41992188e-01 -5.42480469e-01 -5.46875000e-01
 -5.48339844e-01 -5.49804688e-01 -5.50781250e-01 -5.58105469e-01
 -5.59082031e-01 -5.60546875e-01 -5.62011719e-01 -5.62500000e-01
 -5.66894531e-01 -5.67382812e-01 -5.68847656e-01 -5.70312500e-01
 -5.70800781e-01 -5.74218750e-01 -5.74707031e-01 -5.77636719e-01
 -5.82519531e-01 -5.91308594e-01 -5.91796875e-01 -6.03027344e-01
 -6.04980469e-01 -6.06933594e-01 -6.07910156e-01 -6.10351562e-01
 -6.10839844e-01 -6.12792969e-01 -6.16210938e-01 -6.16699219e-01
 -6.19628906e-01 -6.20117188e-01 -6.30371094e-01 -6.30859375e-01
 -6.34765625e-01 -6.45507812e-01 -6.46484375e-01 -6.48437500e-01
 -6.51855469e-01 -6.54785156e-01 -6.57714844e-01 -6.60156250e-01
 -6.61132812e-01 -6.74804688e-01 -6.75292969e-01 -6.87011719e-01
 -6.89941406e-01 -6.90429688e-01 -6.91406250e-01 -6.93359375e-01
 -7.07519531e-01 -7.08496094e-01 -7.11914062e-01 -7.14843750e-01
 -7.22167969e-01 -7.22656250e-01 -7.29003906e-01 -7.31445312e-01
 -7.41699219e-01 -7.45605469e-01 -7.53906250e-01 -7.56835938e-01
 -7.59765625e-01 -7.62207031e-01 -7.81738281e-01 -7.83203125e-01
 -7.94433594e-01 -7.95898438e-01 -7.97363281e-01 -8.06640625e-01
 -8.25683594e-01 -8.27148438e-01 -8.36914062e-01 -8.38378906e-01
 -8.48632812e-01 -8.54003906e-01 -8.73046875e-01 -8.75976562e-01
 -8.76464844e-01 -8.92089844e-01 -8.93554688e-01 -8.95019531e-01
 -9.07714844e-01 -9.11621094e-01 -9.17480469e-01 -9.22363281e-01
 -9.37500000e-01 -9.43847656e-01 -9.88281250e-01 -9.93164062e-01
 -1.00585938e+00 -1.01171875e+00 -1.01269531e+00 -1.01464844e+00
 -1.02343750e+00 -1.02441406e+00 -1.05273438e+00 -1.06054688e+00
 -1.08105469e+00 -1.08300781e+00 -1.08691406e+00 -1.08789062e+00
 -1.10839844e+00 -1.12207031e+00 -1.12695312e+00 -1.14257812e+00
 -1.21191406e+00 -1.21679688e+00 -1.23730469e+00 -1.24023438e+00
 -1.44531250e+00 -1.47265625e+00 -1.99023438e+00]
