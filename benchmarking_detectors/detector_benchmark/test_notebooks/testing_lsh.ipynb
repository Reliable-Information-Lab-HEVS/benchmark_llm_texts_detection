{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Jaccard Similarity: 1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import hashlib\n",
    "\n",
    "class MinHash:\n",
    "    def __init__(self, num_hashes):\n",
    "        self.num_hashes = num_hashes\n",
    "        self.max_hash = (1 << 32) - 1\n",
    "        self.hash_funcs = [self._hash_func(i) for i in range(num_hashes)]\n",
    "\n",
    "    def _hash_func(self, seed):\n",
    "        def hash(x):\n",
    "            return int(hashlib.md5((str(x) + str(seed)).encode('utf8')).hexdigest(), 16)\n",
    "        return hash\n",
    "\n",
    "    def compute(self, set_data):\n",
    "        min_hashes = [self.max_hash] * self.num_hashes\n",
    "        for item in set_data:\n",
    "            for i, hash_func in enumerate(self.hash_funcs):\n",
    "                min_hashes[i] = min(min_hashes[i], hash_func(item))\n",
    "        return min_hashes\n",
    "\n",
    "# Example usage\n",
    "data1 = {'apple', 'banana', 'cherry'}\n",
    "data2 = {'apple', 'banana', 'date'}\n",
    "\n",
    "minhash = MinHash(num_hashes=200)\n",
    "signature1 = minhash.compute(data1)\n",
    "signature2 = minhash.compute(data2)\n",
    "\n",
    "# Estimating Jaccard Similarity\n",
    "similarity = np.mean([x == y for x, y in zip(signature1, signature2)])\n",
    "print(f\"Estimated Jaccard Similarity: {similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295,\n",
       " 4294967295]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signature2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Cosine Similarity: 0.2\n",
      "[1 0 0 0 1]\n",
      "[0 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SimHash:\n",
    "    def __init__(self, dim):\n",
    "        self.dim = dim\n",
    "        self.random_vectors = np.random.randn(dim, dim)\n",
    "\n",
    "    def compute(self, vector):\n",
    "        projections = np.dot(vector, self.random_vectors)\n",
    "        simhash = (projections > 0).astype(int)\n",
    "        return simhash\n",
    "\n",
    "# Example usage\n",
    "vector1 = np.array([1, 0, 1, 0, 1])\n",
    "vector2 = np.array([1, 1, 0, 1, 0])\n",
    "\n",
    "simhash = SimHash(dim=5)\n",
    "signature1 = simhash.compute(vector1)\n",
    "signature2 = simhash.compute(vector2)\n",
    "\n",
    "# Estimating Cosine Similarity\n",
    "similarity = np.dot(signature1, signature2) / len(signature1)\n",
    "print(f\"Estimated Cosine Similarity: {similarity}\")\n",
    "\n",
    "print(signature1)\n",
    "print(signature2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signature1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated seed 1: 88739654438713744637394983803097507872644302151361883387078094910037439175419\n",
      "Generated seed 2: 90552439634884501211528977791671251485636081624297019446735676370132102697723\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "from collections import Counter\n",
    "\n",
    "def hash_feature(feature):\n",
    "    \"\"\"Hash a feature using SHA-256 and return it as a bit string.\"\"\"\n",
    "    return bin(int(hashlib.sha256(feature.encode()).hexdigest(), 16))[2:].zfill(256)\n",
    "\n",
    "def simhash(text):\n",
    "    \"\"\"Generate a SimHash for the given text.\"\"\"\n",
    "    features = text.split()  # Simple split by whitespace; you can use other methods to extract features\n",
    "    feature_weights = Counter(features)\n",
    "    \n",
    "    # Initialize a list of sums for each bit position\n",
    "    bit_sums = [0] * 256\n",
    "    \n",
    "    for feature, weight in feature_weights.items():\n",
    "        feature_hash = hash_feature(feature)\n",
    "        \n",
    "        for i, bit in enumerate(feature_hash):\n",
    "            if bit == '1':\n",
    "                bit_sums[i] += weight\n",
    "            else:\n",
    "                bit_sums[i] -= weight\n",
    "    \n",
    "    # Generate the final SimHash\n",
    "    simhash_bits = ['1' if bit_sum > 0 else '0' for bit_sum in bit_sums]\n",
    "    \n",
    "    # Convert the bit string to an integer\n",
    "    simhash_value = int(''.join(simhash_bits), 2)\n",
    "    \n",
    "    return simhash_value\n",
    "\n",
    "def generate_seed(text):\n",
    "    \"\"\"Generate a seed based on SimHash.\"\"\"\n",
    "    simhash_value = simhash(text)\n",
    "    \n",
    "    # Optionally, convert the SimHash value to a different format or range if needed\n",
    "    # For example, we can use it directly as an integer seed\n",
    "    \n",
    "    return simhash_value\n",
    "\n",
    "# Example usage\n",
    "text1 = \"This is an example text to generate a SimHash based seed.\"\n",
    "text2 = \"This is an example text to generate a SimHash based seed\"\n",
    "seed1 = generate_seed(text1)\n",
    "seed2 = generate_seed(text2)\n",
    "print(f\"Generated seed 1: {seed1}\")\n",
    "print(f\"Generated seed 2: {seed2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's be more concrete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marluxiaboss/anaconda3/envs/llm_detector/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "control_str = [\"This is a test\"]\n",
    "test_str1 = [\"Three members of the same family who died in a static caravan from carbon monoxide poisoning would have been unconscious 'within minutes', investigators said today. The bodies of married couple John and Audrey Cook were discovered alongside their daughter, Maureen, at the mobile home they shared on Tremarle Home Park in Camborne, west Cornwall. The inquests have now opened into the deaths last Saturday, with investigators saying the three died along with the family's pet dog, of carbon monoxide\"]\n",
    "test_str2 = [\"Three members of the same family who died in a static caravan from carbon monoxide poisoning would have been unconscious 'within minutes', investigators told today. The bodies of married couple John and Audrey Cook were discovered alongside their daughter, Maureen, at the mobile home they shared on Tremarle Home Park in Camborne, west Cornwall. The inquests have now opened into the deaths last Saturday, with investigators saying the three died along with the family's pet dog, of carbon monoxide\"]\n",
    "\n",
    "control_str_embed = model.encode(control_str)\n",
    "test_str_embed1 = model.encode(test_str1)\n",
    "test_str_embed2 = model.encode(test_str2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 384), (1, 384), (1, 384))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "control_str_embed.shape, test_str_embed1.shape, test_str_embed2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "138.62943611198904"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eps = 0.2\n",
    "new_dim = 8 * np.log(2) / eps**2\n",
    "new_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.2916666666666667, 0.2942708333333333, 0.5390625)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simhash = SimHash(dim=384)\n",
    "sign1 = simhash.compute(control_str_embed[0])\n",
    "sign2 = simhash.compute(test_str_embed1[0])\n",
    "sign3 = simhash.compute(test_str_embed2[0])\n",
    "\n",
    "# compute similarity of each pair\n",
    "similarity1 = np.dot(sign1, sign2) / len(sign1)\n",
    "similarity2 = np.dot(sign1, sign3) / len(sign1)\n",
    "similarity3 = np.dot(sign2, sign3) / len(sign1)\n",
    "\n",
    "similarity1, similarity2, similarity3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing new KGW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import hashlib\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import (AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForMaskedLM, AutoModelForCausalLM,\n",
    "        LogitsProcessor, LogitsProcessorList, set_seed)\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "os.chdir(\"../\")\n",
    "\n",
    "SRC_PATH = [\"src\"]\n",
    "for module_path in SRC_PATH:\n",
    "    if module_path not in sys.path:\n",
    "        sys.path.append(module_path)\n",
    "\n",
    "from watermark.auto_watermark import AutoWatermark\n",
    "from utils.gen_utils import transform_chat_template_with_prompt\n",
    "from generation.article_generator import ArticleGenerator\n",
    "from utils.gen_utils import transform_chat_template_with_prompt\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    def __init__(self, tokenizer, use_chat_template, chat_template_type, gen_params, model_name, device):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.use_chat_template = use_chat_template\n",
    "        self.chat_template_type = chat_template_type\n",
    "        self.gen_params = gen_params\n",
    "        self.model_name = model_name\n",
    "        self.device = device\n",
    "\n",
    "@dataclass\n",
    "class PromptConfig:\n",
    "    def __init__(self, system_prompt, user_prompt):\n",
    "        self.system_prompt = system_prompt\n",
    "        self.user_prompt = user_prompt\n",
    "\n",
    "\n",
    "class LLMGenerator(nn.Module):\n",
    "    def __init__(self, model: AutoModelForCausalLM, model_config: ModelConfig) -> None:\n",
    "        \"\"\"\n",
    "        Class for generating text using a model from Huggingface.\n",
    "        \n",
    "        Parameters:\n",
    "            model: AutoModelForCausalLM\n",
    "                The pretrained language model (Transformers) to be used for text generation.\n",
    "            model_config: ModelConfig\n",
    "                The configuration of the model.\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        # gpt should already be trained\n",
    "        self.generator = model\n",
    "        self.tokenizer = model_config.tokenizer\n",
    "        self.device = model_config.device\n",
    "        self.gen_params = model_config.gen_params\n",
    "\n",
    "    def forward(self, samples: list, batch_size: int=1, watermarking_scheme=None) -> list[str]:\n",
    "        \"\"\"\n",
    "        Takes a list of input contexts and generates text using the model.\n",
    "        \n",
    "        Parameters:\n",
    "            samples: list\n",
    "                A list of input contexts for text generation.\n",
    "            batch_size: int\n",
    "                The batch size to use for generation.\n",
    "            watermarking_scheme: LogitsProcessor\n",
    "                The watermarking scheme to use for generation.\n",
    "        \"\"\"\n",
    "        \n",
    "        outputs_list = []\n",
    "        for i in tqdm(range(0, len(samples), batch_size), desc=\"Generating text\"):\n",
    "            \n",
    "            batch_samples = samples[i:i+batch_size]\n",
    "            encoding = self.tokenizer.batch_encode_plus(\n",
    "                batch_samples, return_tensors='pt', padding=True, truncation=True)\n",
    "            input_ids = encoding['input_ids'].to(self.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                if watermarking_scheme is not None:\n",
    "                    output_ids = self.generator.generate(\n",
    "                        input_ids, pad_token_id=self.tokenizer.pad_token_id, \n",
    "                        logits_processor=LogitsProcessorList([watermarking_scheme]), **self.gen_params)\n",
    "                else:     \n",
    "                    output_ids = self.generator.generate(\n",
    "                        input_ids, pad_token_id=self.tokenizer.pad_token_id, **self.gen_params)\n",
    "\n",
    "            # decode the generated text\n",
    "            decoded_outputs = self.tokenizer.batch_decode(\n",
    "                output_ids[:, input_ids.shape[1]:])\n",
    "                \n",
    "            outputs_list.extend(decoded_outputs)\n",
    "            \n",
    "        # remove special tokens from the generated text\n",
    "        special_tokens = self.tokenizer.additional_special_tokens + \\\n",
    "            [self.tokenizer.pad_token] + [self.tokenizer.eos_token]\n",
    "            \n",
    "        for i, sample in enumerate(samples):\n",
    "            output = outputs_list[i]\n",
    "            for special_token in special_tokens:\n",
    "                output = output.replace(special_token, \"\")\n",
    "                output = output.strip()\n",
    "            outputs_list[i] = output\n",
    "        \n",
    "        return outputs_list\n",
    "    def hash(self, token: int):\n",
    "        \n",
    "        return token\n",
    "        \n",
    "    \n",
    "    def forward_special(self, samples: list, batch_size: int=1, watermarking_scheme=None) -> list[str]:\n",
    "        \"\"\"\n",
    "        Takes a list of input contexts and generates text using the model.\n",
    "        \n",
    "        Parameters:\n",
    "            samples: list\n",
    "                A list of input contexts for text generation.\n",
    "            batch_size: int\n",
    "                The batch size to use for generation.\n",
    "            watermarking_scheme: LogitsProcessor\n",
    "                The watermarking scheme to use for generation.\n",
    "        \"\"\"\n",
    "        \n",
    "        outputs_list = []\n",
    "        for i in tqdm(range(0, len(samples), batch_size), desc=\"Generating text\"):\n",
    "            \n",
    "            batch_samples = samples[i:i+batch_size]\n",
    "            encoding = self.tokenizer.batch_encode_plus(\n",
    "                batch_samples, return_tensors='pt', padding=True, truncation=True)\n",
    "            input_ids = encoding['input_ids'].to(self.device)\n",
    "\n",
    "            # set max_new_tokens to 1 to generate one token at a time\n",
    "            self.gen_params['max_new_tokens'] = 1\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                text_len = 10\n",
    "                for i in range(text_len):\n",
    "                    \n",
    "                    # find the last token\n",
    "                    last_token = input_ids[0][-1].item()\n",
    "                    \n",
    "                    # hash it to get a seed\n",
    "                    seed = self.hash(last_token)\n",
    "                    print(\"seed:\", seed)\n",
    "                    \n",
    "                    # set seed for generation\n",
    "                    set_seed(int(seed))\n",
    "                    \n",
    "                    #torch.manual_seed(seed)\n",
    "                    output_ids = self.generator.generate(\n",
    "                        input_ids, pad_token_id=self.tokenizer.pad_token_id, **self.gen_params)\n",
    "                                        \n",
    "\n",
    "            # decode the generated text\n",
    "            decoded_outputs = self.tokenizer.batch_decode(\n",
    "                output_ids[:, input_ids.shape[1]:])\n",
    "                \n",
    "            outputs_list.extend(decoded_outputs)\n",
    "            \n",
    "        # remove special tokens from the generated text\n",
    "        special_tokens = self.tokenizer.additional_special_tokens + \\\n",
    "            [self.tokenizer.pad_token] + [self.tokenizer.eos_token]\n",
    "            \n",
    "        for i, sample in enumerate(samples):\n",
    "            output = outputs_list[i]\n",
    "            for special_token in special_tokens:\n",
    "                output = output.replace(special_token, \"\")\n",
    "                output = output.strip()\n",
    "            outputs_list[i] = output\n",
    "        \n",
    "        return outputs_list\n",
    "    \n",
    "    \n",
    "class PromptAttack(ArticleGenerator):\n",
    "    \n",
    "    def __init__(self, gen_model: ModelConfig, gen_config: LLMGenerator, gen_prompt_config: PromptConfig,\n",
    "                adversarial_prompt_config: PromptConfig, max_sample_len: int, watermarking_scheme: AutoWatermark=None) -> None:\n",
    "        \"\"\"\n",
    "        Class for generating text using a model from Huggingface with adversarial prompt.\n",
    "        This class can also be used to generate text with a specific prompt in an non-adversarial way.\n",
    "        Here, we add the adversarial_gen_params parameter to make it explicit that we are changing the generation parameters.\n",
    "        \n",
    "        Parameters:\n",
    "            gen_model: LLMGenerator\n",
    "                The pretrained language model (Transformers) to be used for text generation.\n",
    "            gen_config: ModelConfig\n",
    "                The configuration of the model.\n",
    "            gen_prompt_config: PromptConfig\n",
    "                The configuration of the prompt.\n",
    "            adversarial_prompt_config: PromptConfig\n",
    "                The adversarial prompt configuration to use for generation.\n",
    "            max_sample_len: int\n",
    "                The maximum length of the generated text.\n",
    "            watermarking_scheme: AutoWatermark\n",
    "                The optional watermarking scheme to use for generation. Default is None.\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__(gen_model, gen_config, gen_prompt_config, max_sample_len, watermarking_scheme)\n",
    "\n",
    "        # Set adversarial prompts\n",
    "        self.adversarial_prompt_config = adversarial_prompt_config\n",
    "        \n",
    "        self.attack_name = \"prompt_attack\"\n",
    "    \n",
    "    def generate_adversarial_text(self, prefixes: list[str], batch_size: int=1) -> list[str]:\n",
    "        \"\"\"\n",
    "        Generate text with an (adversarial) prompt.\n",
    "        \n",
    "        Parameters:\n",
    "            prefixes: list[str]\n",
    "                A list of input contexts for text generation.\n",
    "            batch_size: int\n",
    "                The batch size to use for generation.\n",
    "                \n",
    "        Returns:\n",
    "            fake_articles: list[str]\n",
    "                A list of generated text.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Create adversarial prompt configuration\n",
    "        self.gen_prompt_config = self.adversarial_prompt_config\n",
    "        \n",
    "        # generate text\n",
    "        fake_articles = self.generate_text(prefixes, batch_size=batch_size)\n",
    "        \n",
    "        # cut to max_sample_len\n",
    "        fake_articles = [text[:self.max_sample_len] for text in fake_articles]\n",
    "        \n",
    "        return fake_articles\n",
    "    \n",
    "    def generate_text_special(self, prefixes, batch_size=1) -> list[str]:\n",
    "        \"\"\"\n",
    "        Takes a list of input contexts and generates text using the model.\n",
    "        \n",
    "        Parameters:\n",
    "            prefixes: list\n",
    "                A list of input contexts for text generation.\n",
    "            batch_size: int\n",
    "                The batch size to use for generation.\n",
    "                \n",
    "        Returns:\n",
    "            fake_articles: list\n",
    "                A list of generated text.\n",
    "        \"\"\"\n",
    "        \n",
    "        # assumption: all attacks will generate text\n",
    "        gen_model = self.gen_model\n",
    "\n",
    "        # apply the chat template with the prompt\n",
    "        system_prompt = self.gen_prompt_config.system_prompt\n",
    "        user_prompt = self.gen_prompt_config.user_prompt\n",
    "        gen_tokenizer = self.gen_model_config.tokenizer\n",
    "        use_chat_template = self.gen_model_config.use_chat_template\n",
    "        template_type = self.gen_model_config.chat_template_type\n",
    "        \n",
    "        # apply the chat template with the prompt\n",
    "        prefixes_with_prompt = [transform_chat_template_with_prompt(\n",
    "            prefix, user_prompt, gen_tokenizer,\n",
    "            use_chat_template, template_type, system_prompt, forced_prefix=prefix) for prefix in prefixes]\n",
    "\n",
    "        # generate articles\n",
    "        fake_articles = []\n",
    "        fake_articles = gen_model.forward_special(prefixes_with_prompt, batch_size=batch_size, watermarking_scheme=self.watermarking_scheme)\n",
    "            \n",
    "        # add the prefix back to the generated text since generation cuts the first \"input_size\" tokens from the input\n",
    "        # if we force the prefix in the generation, it is counted in the \"input_size\" tokens\n",
    "        # We have to be careful though because sometimes fake articles starts with a space, sometimes not\n",
    "        prefixes = [prefix.strip() for prefix in prefixes]\n",
    "        fake_articles = [fake_article.strip() for fake_article in fake_articles]\n",
    "        fake_articles = [f\"{prefixes[i]} {fake_articles[i]}\" for i in range(len(fake_articles))]\n",
    "        \n",
    "        # cut to max_sample_len\n",
    "        fake_articles = [text[:self.max_sample_len] for text in fake_articles]\n",
    "        \n",
    "        return fake_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marluxiaboss/anaconda3/envs/llm_detector/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Generating text:   0%|          | 0/1 [00:00<?, ?it/s]/home/marluxiaboss/anaconda3/envs/llm_detector/lib/python3.12/site-packages/transformers/generation/utils.py:1164: UserWarning: Unfeasible length constraints: `min_new_tokens` (100), when added to the prompt length (42), is larger than the maximum possible length (43). Generation will stop at the defined maximum length. You should decrease the minimum length and/or increase the maximum length. Note that `max_length` is set to 43, its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed: 220\n",
      "input_ids: tensor([[151644,   8948,    198,   2610,    525,    264,  10950,  17847,     13,\n",
      "         151645,    198, 151644,    872,    198,   7985,    264,   3669,   4549,\n",
      "           5916,    448,     25,   9645,    752,    264,   3364,    911,  50436,\n",
      "             25,    220, 151645,    198, 151644,  77091,    198,   9645,    752,\n",
      "            264,   3364,    911,  50436,     25,    220]], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating text:   0%|          | 0/1 [00:01<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_ids: tensor([[151644,   8948,    198,   2610,    525,    264,  10950,  17847,     13,\n",
      "         151645,    198, 151644,    872,    198,   7985,    264,   3669,   4549,\n",
      "           5916,    448,     25,   9645,    752,    264,   3364,    911,  50436,\n",
      "             25,    220, 151645,    198, 151644,  77091,    198,   9645,    752,\n",
      "            264,   3364,    911,  50436,     25,    220,     16]],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'fwfewf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_372763/1245516372.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0mprefixes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"Write me a story about dragons: \"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0mgen_texts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprompt_attack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_text_special\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefixes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0mgen_texts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_372763/3941289368.py\u001b[0m in \u001b[0;36mgenerate_text_special\u001b[0;34m(self, prefixes, batch_size)\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0;31m# generate articles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0mfake_articles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m         \u001b[0mfake_articles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_special\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefixes_with_prompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwatermarking_scheme\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwatermarking_scheme\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0;31m# add the prefix back to the generated text since generation cuts the first \"input_size\" tokens from the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_372763/3941289368.py\u001b[0m in \u001b[0;36mforward_special\u001b[0;34m(self, samples, batch_size, watermarking_scheme)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"output_ids:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m                     \u001b[0mfwfewf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0;31m# decode the generated text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fwfewf' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import (AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForMaskedLM, AutoModelForCausalLM,\n",
    "                          ElectraForSequenceClassification, ElectraTokenizer, AutoConfig)\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "default_gen_params = {\n",
    "    #\"max_length\": 100,\n",
    "    \"max_new_tokens\": 100,\n",
    "    \"min_new_tokens\": 100,\n",
    "    \"temperature\": 0.8,\n",
    "    \"top_p\": 0.95,\n",
    "    \"repetition_penalty\": 1,\n",
    "    \"do_sample\": True,\n",
    "    \"top_k\": 50\n",
    "}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"Qwen/Qwen2-0.5B-Instruct\",\n",
    "    pad_token='<|extra_0|>',\n",
    "    eos_token='<|endoftext|>',\n",
    "    padding_side='left',\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen2-0.5B-Instruct\",\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    ").to(device)\n",
    "\n",
    "\n",
    "model_config = ModelConfig(tokenizer,\n",
    "    use_chat_template=True, chat_template_type=\"system_user\", gen_params=default_gen_params, model_name=\"qwen\", device=device)\n",
    "\n",
    "gen = LLMGenerator(model, model_config)\n",
    "\n",
    "gen_prompt = \"You are a helpful assistant.\"\n",
    "user_prompt = \"Write a news article starting with:\"\n",
    "gen_prompt_config = PromptConfig(system_prompt=gen_prompt, user_prompt=user_prompt)\n",
    "max_sample_len = 500\n",
    "\n",
    "prompt_attack = PromptAttack(gen, model_config, gen_prompt_config, gen_prompt_config, max_sample_len)\n",
    "\n",
    "prefixes = [\"Write me a story about dragons: \"]\n",
    "\n",
    "gen_texts = prompt_attack.generate_text_special(prefixes, batch_size=1)\n",
    "gen_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_detector",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
