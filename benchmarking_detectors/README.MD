# LLM-generated news benchmark  :mag: ðŸ“°
The purpose of this benchmark is to evaluate LLM detectors, especially against evasion attacks. So far, the benchmark is based on the detection of short LLM-generated news articles, but it can be extended to cover different detection tasks.  
The main consideration is to make the benchmark easy to extend with different datasets, detectors and evasion attacks

## Features
- Detectors and watermark detection benchmarking (adversarial + non-adversarial)
- LLM-generated watermarked text quality benchmarking
- Modularity: possible to add new datasets, detectors, attacks and watermarking schemes without much effort

## Table of content

- [Table of content](#table-of-content)
- [Repo structure](#repo-structure)
    - [0. Class folders](#0-class-folders)
    - [1. Config files](#1-config-files) 
    - ...
- [Reproducing the experiments and the plots](#reproducing-the-experiments-and-the-plots)
- [Running the python scripts + external libraries benchmarks](#running-the-python-scripts--external-libraries-benchmarks)
- [Adding a watermarking scheme, attack, detector or dataset](#adding-a-watermarking-scheme-attack-detector-or-dataset)
- [Loading an LLM, detector or watermarking scheme and using them](#loading-an-llm-detector-or-watermarking-scheme-and-using-them)

## Repo structure

### 0. Class folders
- `detector_benchmark/dataset_loader`: folder for the dataset_loader classes
- `detector_benchmark/detector`: folder for the detector classes and detector loader
- `detector_benchmark/generation`: folder for the bases generation + adversarial generation classes and also the generator loader
- `detector_benchmark/pipeline`: folder for the pipeline classes (text generation, testing detection and evaluating generated text quality)
- `detector_benchmark/text_quality_evaluation`: folder for text quality evaluator classes
- `detector_benchmark/watermark`: folder for watermark classes (general and different watermark schemes)

### 1. Config files
Configuration files (hydra configuration) are located in:
- `detector_benchmark/conf/` for the detection, generation, pipeline and watermark configurations.

### 2. Python scripts
- `detector_benchmark/create_dataset.py`: script to create a dataset using a dataset loader and a generation config
- `detector_benchmark/test_detector.py`: script to test a detector on a dataset created using the script above
- `detector_benchmark/test_text_quality.py`: script to run basic text quality evaluation on generated text (non-watermarked, watermarked or even human written)

### 3. External libraries submodules
Some existing github repositories were forked and adapted to work with watermarking, namely:
- `lm-evaluation-harness/` from EleutherAI that supports a wide range of benchmarking tasks. 
- `bigcode-evaluation-harness/` from BigCode that supports a wide range of code benchmarking tasks. 
- `prometheus-eval/BiGGen-Bench/` from the authors of the following paper <https://arxiv.org/abs/2406.05761> which benchmarks LLMs with an open source LLM-as-Judge.

Our fork of the respecitives repos allows to run theirs benchmark with text generated with a watermarking scheme. See [in the following section](#evaluation-with-lm-harness) more details about how to run the benchmarks from the external libraries with a watermaking scheme.

### 4. Bash scripts
Bash scripts for running the different experiments can be found under `bash_scripts`. It contains the following subfolder corresponding to the different experiment types:
- `bash_scripts/big_gen_bench` for running the evaluation from [BiGGen-Bench](https://github.com/prometheus-eval/prometheus-eval/tree/main/BiGGen-Bench) with different watermarking schemes (one bash script per watermarking scheme).
- `bash_scripts/bigcode_eval` for running the evaluation from [bigcode-evaluation-harness](https://github.com/bigcode-project/bigcode-evaluation-harness) on selected tasks with different watermarking schemes. There is one subfolder `bash_scripts/bigcode_eval/generation` for generating the text to be evaluated and `bash_scripts/bigcode_eval/evaluation` for running the evaluations. The latter will actually run the code generated by the LLM, it's therefore advised to launch it inside a sandbox.
- `bash_scripts/generating_datasets` for generating the different datasets using the different watermarking schemes.
- `bash_scripts/lm_harness` for running the evaluation from [lm-evluation-harness](https://github.com/EleutherAI/lm-evaluation-harness) on selected tasks with different watemarking schemes (one bash scripts for all the watermarking schemes).
- `bash_scripts/test_detectors` for running the detection evaluation scritps with the detectors/watermark detectors on the different generated datasets. There is one subfolder `bash_scripts/test_detectors/test_watermark_detectors` for testing the watermark detectors on the respective text generated with watermark and `bash_scripts/test_detectors/test_zero_shot_detectors` for testing zero shot dectors.
- `bash_scripts/text_quality_pipeline` for running basic text quality evaluation such as computing the perlexity.
Currently only has the subfolder `bash_scripts/text_quality_pipeline/ppl_scorer` for computing the perplexity of the text generated by different watermarking schemes and also human text


### 5. Jupyter notebooks
- `big_gen_bench_res.ipynb` to visualize the results after running the benchmark from BiGGen-Bench with different watermarking schemes
- `lm_harness_bench_res.ipynb` to visualize the results after running the benchmark from lm-harness with different watermarking schemes see [the following section](#evaluation-with-lm-harness) for the tasks to run in lm-harness
- `watermark_detection_results.ipynb` to visualize the detection results after having run the script `detector_benchmark/test_detector.py`
- `bigcode_eval_res.ipnyb` TODO

### 6. Data folders
- `detector_benchmark/data/generated_datasets` for the datasets adversarial + non-adversarial datasets generated with the LLMs with or without watermarking. The arboresence is as follows:

```
generated_datasets  
â”‚
â””â”€â”€â”€{source_dataset} (e.g. cnn_dailymail)
    â”‚
    â””â”€â”€â”€{adversarial_attack} (e.g. no_attack)
        â”‚ 
        â””â”€â”€â”€{watermarking_scheme} (e.g. no_watermark)
            â”‚  
            â””â”€â”€â”€log 
            â”‚   log.txt (terminal logs)
            â”‚
            â”‚   
            â””â”€â”€â”€{generator_name}_{experiment_name} (actual generated hugginface dataset)

```
Where `source_dataset` is the name of the dataset used to obtain the true human written samples and the prefixes for the fake AI-written samples, `adversarial_attack` and `watermarking_scheme` are the respective attack and watermark used to generate the datasets. `experiment_name` is the name used for the specific run of the generation.

### 7. Results folders
Here are the following folders used to save the results from the scripts/benchmarks:

- `detector_benchmark/detection_test_results` containing the results when running the detection script `detector_benchmark/test_detector.py`.
- `detector_benchmark/text_quality_eval_results` containing the results when running the text quality evaluation script `detector_benchmark/test_text_quality.py`
- TODO: add info about the respective results folder for the external libraries


## Reproducing the experiments and the plots


## Running the python scripts + external libraries benchmarks
See the corresponding bash script in [the bash script section](#4-bash-scripts) for examples.

## Adding a watermarking scheme, attack, detector or dataset

### How to add a watermarking scheme on the benchmark
Credits to <https://github.com/THU-BPM/MarkLLM> for most of the watermarking code structure and classes.

To add a watermarking scheme, 4 files needs to be added/modified inside the:
- add: a {watermarking_scheme}.py file inside its own `detector_benchmark/watermark/{watermarking_scheme}` folder.
- add: a corresponding `__init__.py` inside the same folder
- add: a configuration filde under `detector_benchmark/conf/watermark` to configure the watermarking scheme.
- modify: the `WATERMARK_MAPPING_NAMES` dictionnary variable inside `detector_benchmark/watermark/auto_watermark.py`.

See examples of already added watermarking schemes to understand what functions the{watermarking_scheme}.py should implement. The core of the watermarking scheme is a class `{watermarking_scheme}` inheriting from LogitsProcessor having at least a `__init__` constructor method and a `__call__` method with the following signature:
```
def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:
```
Taking as input a context (input_ids) and the logits (scores) as returned by the LLM. The watermarking scheme then modifies the logits and returns the new logits.

### How to test a new detector on the detection benchmark
To add a new detectors, 3 files needs to be added/modified:
- add: a .py file in `detector_benchmark/detector` containing the class for the new detector, inheriting the base detector class.
- add: a configuration file under `detector_benchmark/conf/detection` to configure the new detector.
- modify: `detector_benchmark/detector/detector_loader.py` to be able to load the detector from the .py file created above.


The added detector class should have at least a constructor and a `detect` function with the following signature:
```
def detect(self, texts: list, batch_size: int, detection_threshold: float) -> tuple[list[int], list[float], list[int]]:
```
Where `texts` is a list of text to be detected as LLM-written or not and it should return the following variables:
- `preds`: a list of 0s (human-written) and 1s (LLM-generated) with the predicted labels computed as argmax of the logits of both classes
- `logits_pos_class`: the list of logits or probabilities for the positive class (softmaxed or not) 
- `preds_at_threshold`: same as `preds` but where the prediction is made using a threshold on the logits rather than argmax  

You can look at already added detectors for guiding examples.

### How to add an attack
To add an attack, 3 files needs to be added/modified:
- add: a .py file in `generation` containing the class for the attack, inheriting the base ArticleGenerator class
- add: a configuration file under `detector_benchmark/conf/generation` to configure the new attack.
- modify: `detector_benchmark/generation/attack_loader.py` to be able to load the attack.

The added detector class should have at least a constructor and a `generate_adversarial_text` function with the following signature:
```
def generate_adversarial_text(self, prefixes: list[str], batch_size: int) -> list[str]:
```
Where `prefixes` is a list of inputs text to continue for the generation (see how the dataset samples look like) and it should return the list of generated text which is the continuation of the prefixes.

You can look at already added detectors for guiding examples.

Note: for any attack that involves either using a specific prompt for the generation or modifying a prompt parameter, the already existing attacks in `detector_benchmark/generation/gen_params_attack.py` and `detector_benchmark/generation/prompt_attack.py` can be used for this purpose by only modifying the related configuration file in `conf`.

### How to add a dataset
First, let's describe the datasets format used for testing the detectors. The datasets are huggingface Datasets where each samples has the following fields:
- `label`: the label of the text (0 for human written, 1 for AI-written)
- `text`: contains the full text of the sample
- `prefix`: prefix from the human written texts used to generated the AI-written text. For each prefix, we always have the corresponding true sample (label 0) and the fake one (label 1) sharing the same prefix.
- `generation_config`: the config (data from the config file in `conf`) used to generate the text AI-generated text for the dataset
- `watermark_config`: the watermarking config used to generate the text (which watermarking algorithm if any,...)

Now to add a different dataset than the existing ones, we need to add dataset loader class inheriting the base `FakeTruePairsDataLoader` class inside the `dataset_loader` folder.
This class should implement the `load_data` function with the following signature:
```
def load_data(self) -> DatasetDict:
```
This function should return a DatasetDict (huggingface dataset format) with a train, eval and test split and respecting the dataset format. 
To see how to apply the correct format to the dataset, see the existing dataset loaders.

Note: the load_data returns a dataset where the fake samples (label 1) have an empty `text` and no `generation_config` nor `watermark_config` fields since the AI texts have not been generated yet. The only fields that should be fully field for AI texts are the label and the prefix.

## Loading an LLM, detector or watermarking scheme and using them

### Loading and using an LLM

Use the GenLoader class from `detector_benchmark/generation/gen_loader.py` and LLMGenerator class from `detector_benchmark/generation/generator.py` in the following way:

```
model_name = "qwen2_chat_0_5B"
gen_params = {
    "max_new_tokens": 220,
    "min_new_tokens": 200,
    "temperature": 0.8,
    "top_p": 0.95,
    "repetition_penalty": 1,
    "do_sample": True,
    "top_k": 50
}
device = "cuda" if torch.cuda.is_available() else "cpu"

gen_loader = GenLoader(model_name, gen_params, device)
gen, gen_model, gen_config = gen_loader.load()

user_prompt="Continue writing the following news article starting with: "

prefixes_with_prompt = [transform_chat_template_with_prompt(
    prefix, user_prompt, gen_tokenizer,
    use_chat_template, template_type, system_prompt, forced_prefix=prefix) for prefix in prefixes]

# generate articles
fake_articles = []
fake_articles = gen_model(prefixes_with_prompt, batch_size=batch_size,      watermarking_scheme=self.watermarking_scheme)
```
where:
- `model_name` is the name of the LLM to be loaded (see in gen_loader.py the list of supported generators)
- `gen_params` is a dictionnary containing the generation parameters for the model (same as gen_kwargs when using .generate() from hugging face).
- `gen` is the huggingface transformer model
- `gen_model` is the `LLMGenerator` class instance.

### Loading and using a detector

```
detector_loader = DetectorLoader(cfg, detector_name, device,
                weights_checkpoint, local_weights)
detector = detector_loader.load()

fake_true_articles = dataset["text"][:]

preds, logits, preds_at_threshold = self.detector.detect(fake_true_articles, batch_size, detection_threshold)
```


### Loading a watermarking scheme

```
algorithm_config = cfg.watermark

watermarking_scheme = AutoWatermark.load(watermarking_scheme_name,
                algorithm_config=algorithm_config,
                gen_model=gen,
                model_config=model_config)

output_ids = self.generator.generate(
        input_ids,  
        logits_processor=LogitsProcessorList([watermarking_scheme.logits_processor]), **self.gen_params
    )

# or directly using our LLMGenerator class:
fake_articles = gen_model(prefixes_with_prompt, batch_size=batch_size, watermarking_scheme=self.watermarking_scheme)
```



## Class descriptions


## Future work


