{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/marluxiaboss/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from datasets import concatenate_datasets, load_from_disk, DatasetDict\n",
    "import argparse\n",
    "import os\n",
    "import pandas as pd\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import nltk.data\n",
    "nltk.download('punkt')\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForMaskedLM, AutoModelForCausalLM\n",
    "from datasets import load_from_disk, concatenate_datasets, Dataset\n",
    "\n",
    "from abc import ABC, abstractmethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    def __init__(self, tokenizer, use_chat_template, chat_template_type, gen_params, device):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.use_chat_template = use_chat_template\n",
    "        self.chat_template_type = chat_template_type\n",
    "        self.gen_params = gen_params\n",
    "        self.device = device\n",
    "\n",
    "@dataclass\n",
    "class PromptConfig:\n",
    "    def __init__(self, system_prompt, user_prompt):\n",
    "        self.system_prompt = system_prompt\n",
    "        self.user_prompt = user_prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMGenerator(nn.Module):\n",
    "    def __init__(self, model, model_config):\n",
    "        super().__init__()\n",
    "\n",
    "        # gpt should already be trained\n",
    "        self.generator = model\n",
    "        self.tokenizer = model_config.tokenizer\n",
    "        self.device = model_config.device\n",
    "        self.gen_params = model_config.gen_params\n",
    "\n",
    "    def forward(self, samples: list, max_new_tokens: int = None, min_new_tokens: int = None, batch_size: int = 1):\n",
    "        \n",
    "        \n",
    "        outputs_list = []\n",
    "        for i in range(0, len(samples), batch_size):\n",
    "            \n",
    "            batch_samples = samples[i:i+batch_size]\n",
    "\n",
    "            # handle generation parameters for the model and tokenizer\n",
    "            self.gen_params[\"max_new_tokens\"] = max_new_tokens\n",
    "            self.gen_params[\"min_new_tokens\"] = min_new_tokens\n",
    "            encoding = self.tokenizer.batch_encode_plus(\n",
    "                batch_samples, return_tensors='pt', padding=True, truncation=True)\n",
    "            input_ids = encoding['input_ids'].to(self.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output_ids = self.generator.generate(\n",
    "                    input_ids, pad_token_id=self.tokenizer.pad_token_id, **self.gen_params)\n",
    "\n",
    "            # decode the generated text\n",
    "            decoded_outputs = self.tokenizer.batch_decode(\n",
    "                output_ids[:, input_ids.shape[1]:])\n",
    "                \n",
    "            outputs_list.extend(decoded_outputs)\n",
    "            \n",
    "        # remove special tokens from the generated text\n",
    "        special_tokens = self.tokenizer.additional_special_tokens + \\\n",
    "            [self.tokenizer.pad_token] + [self.tokenizer.eos_token]\n",
    "            \n",
    "        for i, sample in enumerate(samples):\n",
    "            output = outputs_list[i]\n",
    "            for special_token in special_tokens:\n",
    "                output = output.replace(special_token, \"\")\n",
    "            outputs_list[i] = output\n",
    "        \n",
    "        return outputs_list\n",
    "    \n",
    "    \n",
    "def transform_chat_template_with_prompt(prefix: str, prompt: str, tokenizer: AutoTokenizer,\n",
    "                                        use_chat_template: bool = False, template_type: str = None,\n",
    "                                        system_prompt: str = \"\", forced_prefix: str = \"\") -> str:\n",
    "    \n",
    "    # TODO: To bet put in a utils file\n",
    "    \n",
    "    \"\"\"\n",
    "    Transform a prefix with a prompt into a chat template\n",
    "    \n",
    "    Parameters:\n",
    "    prefix : str\n",
    "        The prefix to use\n",
    "    prompt : str\n",
    "        The prompt to use\n",
    "    tokenizer : AutoTokenizer\n",
    "        The tokenizer to use\n",
    "    use_chat_template : bool, optional\n",
    "        Whether to use a chat template, by default False\n",
    "    template_type : str, optional\n",
    "        The type of template to use, by default None\n",
    "    system_prompt : str, optional\n",
    "        The system prompt to use, by default \"\"\n",
    "        \n",
    "    Returns:\n",
    "    str\n",
    "        The transformed prefix\n",
    "    \"\"\"\n",
    "        \n",
    "\n",
    "    if prefix != \"\":\n",
    "        text_instruction = f\"{prompt} {prefix}\"\n",
    "    else:\n",
    "        text_instruction = prompt\n",
    "        \n",
    "    if use_chat_template:\n",
    "        if system_prompt == \"\":\n",
    "            sys_prompt = \"You are a helpful assistant.\"\n",
    "        else:\n",
    "            sys_prompt = system_prompt\n",
    "        match template_type:\n",
    "            case \"system_user\":\n",
    "                messages = [\n",
    "                {\"role\": \"system\", \"content\": f\"{sys_prompt}\"},\n",
    "                {\"role\": \"user\", \"content\": f\"{text_instruction}\"},\n",
    "                ]\n",
    "            case \"user\":\n",
    "                messages = [\n",
    "                {\"role\": \"user\", \"content\": f\"{text_instruction}\"},\n",
    "                ]\n",
    "            case _:\n",
    "                raise ValueError(\"Template type not supported\")\n",
    "\n",
    "        text_template = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "\n",
    "        # force prefix on the generated response\n",
    "        #text_template = f\"{text_template}\\n{forced_prefix}\"\n",
    "        text_template = f\"{text_template} {forced_prefix}\"\n",
    "\n",
    "    else:\n",
    "        text_template = text_instruction\n",
    "\n",
    "    return text_template\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attacks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idea: \n",
    "# start with general attack def\n",
    "# implement current attacks with this def\n",
    "# test that this works and its practical\n",
    "# iterate on the general attack def to make it better\n",
    "\n",
    "# support batched gen!\n",
    "\n",
    "class Attack(ABC):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def generate_text(self, prefix):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "class ParaphrasingAttack(Attack):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def paraphrase(self, text):\n",
    "        pass\n",
    "    \n",
    "    def generate_text(self, prefixes: list):\n",
    "        \n",
    "        # generate text according to prefix\n",
    "        \n",
    "        # paraphrase the text\n",
    "        pass\n",
    "    \n",
    "class PromptParaphrasingAttack(Attack):\n",
    "    \n",
    "    def __init__(self, gen_model, gen_config, gen_prompt_config, paraphraser_model, paraphraser_config, paraphraser_prompt_config):\n",
    "        \n",
    "        # Generator LLM\n",
    "        self.gen_model = gen_model\n",
    "        self.gen_system_prompt = gen_prompt_config.system_prompt\n",
    "        self.gen_user_prompt = gen_prompt_config.user_prompt\n",
    "        self.gen_model_config = gen_config\n",
    "                \n",
    "        # Paraphraser LLM\n",
    "        self.paraphraser_model = paraphraser_model\n",
    "        self.system_paraphrasing_prompt = paraphraser_prompt_config.system_prompt\n",
    "        self.user_paraphrasing_prompt = paraphraser_prompt_config.user_prompt\n",
    "        self.model_config = paraphraser_config\n",
    "    \n",
    "    def paraphrase(self, texts, nb_paraphrasing=1, batch_size=1) -> list:\n",
    "        \n",
    "        # Get all the parameters\n",
    "        model_config = self.model_config\n",
    "        tokenizer = model_config.tokenizer\n",
    "        use_chat_template = model_config.use_chat_template\n",
    "        template_type = model_config.chat_template_type\n",
    "        system_paraphrasing_prompt = self.system_paraphrasing_prompt\n",
    "        user_paraphrasing_prompt = self.user_paraphrasing_prompt\n",
    "        \n",
    "        # paraphrasing parameters\n",
    "        max_new_tokens = model_config.gen_params[\"max_new_tokens\"]\n",
    "        min_new_tokens = model_config.gen_params[\"min_new_tokens\"]\n",
    "        fake_articles = texts\n",
    "\n",
    "        # generate articles\n",
    "        for i in range(nb_paraphrasing):\n",
    "            \n",
    "            #user_paraphrasing_prompts = [f\"INPUT: {fake_text}\" for fake_text in fake_articles]\n",
    "        \n",
    "            prefixes_with_prompt = [transform_chat_template_with_prompt(\n",
    "                fake_article, user_paraphrasing_prompt, tokenizer,\n",
    "                use_chat_template, template_type, system_paraphrasing_prompt, forced_prefix=\"OUTPUT:\")\n",
    "                for fake_article in fake_articles]\n",
    "            \n",
    "            fake_articles = []\n",
    "            \n",
    "            # generate the articles\n",
    "            for i in range(0, len(prefixes_with_prompt), batch_size):\n",
    "                samples = prefixes_with_prompt[i:i+batch_size]\n",
    "                outputs = self.paraphraser_model(samples, max_new_tokens=max_new_tokens, min_new_tokens=min_new_tokens)\n",
    "                fake_articles.extend(outputs)\n",
    "                    \n",
    "        return fake_articles\n",
    "    \n",
    "    def generate_text(self, prefixes, batch_size=1):\n",
    "\n",
    "        gen_model = self.gen_model\n",
    "\n",
    "        # apply the chat template with the prompt\n",
    "        system_prompt = self.gen_system_prompt\n",
    "        user_prompt = self.gen_user_prompt\n",
    "        gen_tokenizer = self.gen_model_config.tokenizer\n",
    "        use_chat_template = self.gen_model_config.use_chat_template\n",
    "        template_type = self.gen_model_config.chat_template_type\n",
    "        \n",
    "        # gen params\n",
    "        max_new_tokens = self.gen_model_config.gen_params[\"max_new_tokens\"]\n",
    "        min_new_tokens = self.gen_model_config.gen_params[\"min_new_tokens\"]\n",
    "        \n",
    "        # apply the chat template with the prompt\n",
    "        prefixes_with_prompt = [transform_chat_template_with_prompt(\n",
    "            prefix, user_prompt, gen_tokenizer,\n",
    "            use_chat_template, template_type, system_prompt, forced_prefix=prefix) for prefix in prefixes]\n",
    "\n",
    "        # generate articles\n",
    "        fake_articles = []\n",
    "        fake_articles = gen_model(prefixes_with_prompt, max_new_tokens=max_new_tokens, min_new_tokens=min_new_tokens, batch_size=2)\n",
    "            \n",
    "        # add the prefix back to the generated text since generation cuts the first \"input_size\" tokens from the input\n",
    "        # if we force the prefix in the generation, it is counted in the \"input_size\" tokens\n",
    "        fake_articles = [f\"{prefixes[i]} {fake_articles[i]}\" for i in range(len(fake_articles))]\n",
    "    \n",
    "        #fake_articles = ['INPUT: Why are cats better than dogs Cats are often considered more intelligent and social than dogs. They have a higher IQ, which means they are able to learn new things quickly and adapt to new situations better than dogs. Cats also have a more natural tendency to socialize, which means they have a more complex social hierarchy. Additionally, cats are often kept as pets for a longer period of time than dogs, which can make them more comfortable and less stressed.', 'INPUT: Why are dogs better than cats ?Dogs are known for their loyalty and adaptability. They are also intelligent and trainable, making them ideal companions for families. Dogs are also highly social animals and are known for their loyalty to their owners. Additionally, dogs are known for their natural ability to follow their humans and provide a sense of comfort and security.']\n",
    "        #fake_articles = ['Why are cats better than dogs varies based on factors such as size, shape, and temperament. Generally, cats are smaller in size than dogs and are therefore less likely to cause injury or harm to humans. Cats tend to be more independent and less likely to wander off and cause accidents, making them less likely to pose a risk to humans.Additionally, cats are more social animals and tend to be more affectionate towards humans. This makes them more likely to engage in companion animal behavior and increase their chances of getting along well with humans', 'Why are dogs better than cats ? Dogs have been around for centuries and have proven to be highly intelligent and loyal companions. They are more social animals and thrive in large and loving families. Dogs are also known for their strength, intelligence, and willingness to work hard for their owners.On the other hand, cats are more independent and self-reliant animals. They are also more social animals and thrive in small and loving homes. Cats are known for their independent personality and tend to be more independent and self-reliant']\n",
    "        \n",
    "        print(\"Original generated text: \", fake_articles)\n",
    "        # paraphrase the text\n",
    "        paraphrased_fake_articles = self.paraphrase(fake_articles, batch_size=batch_size, nb_paraphrasing=1)\n",
    "        #paraphrased_fake_articles = fake_articles  \n",
    "        \n",
    "        print(\"Paraphrased generated text: \", paraphrased_fake_articles)    \n",
    "        \n",
    "        return paraphrased_fake_articles\n",
    "    \n",
    "class PromptAttack(Attack):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def generate_text(self, prefix):\n",
    "        \n",
    "        # change the prefix to add the prompt\n",
    "        pass\n",
    "    \n",
    "class GenParamsAttack(Attack):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def generate_text(self, prefix):\n",
    "        \n",
    "        # select gen params for the attack\n",
    "        pass\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set generation parameters\n",
    "default_gen_params = {\n",
    "    #\"max_length\": 100,\n",
    "    \"max_new_tokens\": 100,\n",
    "    \"min_new_tokens\": 100,\n",
    "    \"temperature\": 0.8,\n",
    "    \"top_p\": 0.95,\n",
    "    \"repetition_penalty\": 1,\n",
    "    \"do_sample\": True,\n",
    "    \"top_k\": 50\n",
    "}\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marluxiaboss/anaconda3/envs/llm_detector/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/home/marluxiaboss/anaconda3/envs/llm_detector/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original generated text:  ['Why are cats better than dogs ?\\nOne reason is that cats are known for their independence and loyalty. They are more independent and self-sufficient than dogs, which can be overly dependent on their owners.\\nCats also have a more positive and positive attitude towards their owners, which can make them more approachable and comfortable in certain situations.\\nAnother reason is that cats are generally more docile and peaceful animals. They are also known for their peaceful nature, which can make them more gentle and kind to people.\\nOverall, cats are a', 'Why are dogs better than cats ?\\nDogs have long been regarded as the \"cat\\'s best friend\" for several reasons. However, there are a few factors that can make dogs excel in certain situations, even when they are not the dominant breed. In this article, we will explore why dogs are better than cats in certain situations.\\n\\nOne of the main reasons why dogs excel in certain situations is their ability to adapt to different environments. Cats, on the other hand, are naturally territorial and may struggle to adapt to different environments.']\n",
      "Paraphrased generated text:  [' Cats and dogs are both great animals, but cats are known for their independence and loyalty. They are also known for their positive attitude towards their owners and peaceful nature. Additionally, cats are generally more docile and peaceful animals. Their peaceful nature makes them more gentle and kind to people. The main reason why cats are better than dogs is because they are known for their independence and loyalty, and they are also known for their positive attitude towards their owners and peaceful nature. They are also generally more docile', ' Dogs have been recognized as \"the best friend\" for several reasons, and they excel in different situations, even when they are not the dominant breeds. Factors such as their adaptability to different environments, and the fact that they are naturally territorial can contribute to their success. Dogs are considered better than cats in certain situations because they can adapt to different environments and may struggle in the presence of cats. This makes dogs more suitable for companionship and protection. They are the best friend for many people who']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[' Cats and dogs are both great animals, but cats are known for their independence and loyalty. They are also known for their positive attitude towards their owners and peaceful nature. Additionally, cats are generally more docile and peaceful animals. Their peaceful nature makes them more gentle and kind to people. The main reason why cats are better than dogs is because they are known for their independence and loyalty, and they are also known for their positive attitude towards their owners and peaceful nature. They are also generally more docile',\n",
       " ' Dogs have been recognized as \"the best friend\" for several reasons, and they excel in different situations, even when they are not the dominant breeds. Factors such as their adaptability to different environments, and the fact that they are naturally territorial can contribute to their success. Dogs are considered better than cats in certain situations because they can adapt to different environments and may struggle in the presence of cats. This makes dogs more suitable for companionship and protection. They are the best friend for many people who']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paraphraser_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"Qwen/Qwen2-0.5B-Instruct\",\n",
    "    pad_token='<|extra_0|>',\n",
    "    eos_token='<|endoftext|>',\n",
    "    padding_side='left',\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "paraphraser = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen2-0.5B-Instruct\",\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    "    pad_token_id=paraphraser_tokenizer.pad_token_id,\n",
    ").to(device)\n",
    "\n",
    "\n",
    "paraphraser_config = ModelConfig(paraphraser_tokenizer,\n",
    "    use_chat_template=True, chat_template_type=\"system_user\", gen_params=default_gen_params, device=device)\n",
    "paraphraser_model = LLMGenerator(paraphraser, paraphraser_config)\n",
    "\n",
    "gen_model = paraphraser_model\n",
    "gen_tokenizer = paraphraser_tokenizer\n",
    "gen_config = paraphraser_config\n",
    "\n",
    "\n",
    "dataset_list = [\"Why are cats better than dogs\", \"Why are dogs better than cats\"]\n",
    "\n",
    "system_paraphrasing_prompt = \"\"\"You are a paraphraser. You are given an input passage ‘INPUT’. You should paraphrase ‘INPUT’ to print ‘OUTPUT’.\"\n",
    "\"‘OUTPUT’ shoud be diverse and different as much as possible from ‘INPUT’ and should not copy any part verbatim from ‘INPUT’.\"\n",
    "\"‘OUTPUT’ should preserve the meaning and content of ’INPUT’ while maintaining text quality and grammar.\"\n",
    "\"‘OUTPUT’ should not be much longer than ‘INPUT’. You should print ‘OUTPUT’ and nothing else so that its easy for me to parse.\"\"\"\n",
    "user_paraphrasing_prompt = \"INPUT:\"\n",
    "paraphraser_prompt_config = PromptConfig(system_prompt=system_paraphrasing_prompt, user_prompt=\"\")\n",
    "\n",
    "\n",
    "gen_prompt = \"You are a helpful assistant.\"\n",
    "user_prompt = \"Write a news article starting with:\"\n",
    "gen_prompt_config = PromptConfig(system_prompt=gen_prompt, user_prompt=user_prompt)\n",
    "\n",
    "prompt_paraphrasing_attack = PromptParaphrasingAttack(gen_model, gen_config, gen_prompt_config, paraphraser_model, paraphraser_config, paraphraser_prompt_config)\n",
    "paraphrased_fake_articles = prompt_paraphrasing_attack.generate_text(dataset_list, batch_size=2)\n",
    "\n",
    "paraphrased_fake_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_detector",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
