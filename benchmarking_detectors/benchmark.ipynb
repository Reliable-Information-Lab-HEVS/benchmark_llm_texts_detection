{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/marluxiaboss/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/marluxiaboss/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/marluxiaboss/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from datasets import concatenate_datasets, load_from_disk, DatasetDict\n",
    "import argparse\n",
    "import os\n",
    "import pandas as pd\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import nltk.data\n",
    "nltk.download('punkt')\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForMaskedLM, AutoModelForCausalLM\n",
    "from datasets import load_from_disk, concatenate_datasets, Dataset\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "from evasion_attack import *\n",
    "from utils import *\n",
    "from generator import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    def __init__(self, tokenizer, use_chat_template, chat_template_type, gen_params, device):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.use_chat_template = use_chat_template\n",
    "        self.chat_template_type = chat_template_type\n",
    "        self.gen_params = gen_params\n",
    "        self.device = device\n",
    "\n",
    "@dataclass\n",
    "class PromptConfig:\n",
    "    def __init__(self, system_prompt, user_prompt):\n",
    "        self.system_prompt = system_prompt\n",
    "        self.user_prompt = user_prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMGenerator(nn.Module):\n",
    "    def __init__(self, model, model_config):\n",
    "        super().__init__()\n",
    "\n",
    "        # gpt should already be trained\n",
    "        self.generator = model\n",
    "        self.tokenizer = model_config.tokenizer\n",
    "        self.device = model_config.device\n",
    "        self.gen_params = model_config.gen_params\n",
    "\n",
    "    def forward(self, samples: list, batch_size: int = 1):\n",
    "        \n",
    "        \n",
    "        outputs_list = []\n",
    "        for i in range(0, len(samples), batch_size):\n",
    "            \n",
    "            batch_samples = samples[i:i+batch_size]\n",
    "            encoding = self.tokenizer.batch_encode_plus(\n",
    "                batch_samples, return_tensors='pt', padding=True, truncation=True)\n",
    "            input_ids = encoding['input_ids'].to(self.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output_ids = self.generator.generate(\n",
    "                    input_ids, pad_token_id=self.tokenizer.pad_token_id, **self.gen_params)\n",
    "\n",
    "            # decode the generated text\n",
    "            decoded_outputs = self.tokenizer.batch_decode(\n",
    "                output_ids[:, input_ids.shape[1]:])\n",
    "                \n",
    "            outputs_list.extend(decoded_outputs)\n",
    "            \n",
    "        # remove special tokens from the generated text\n",
    "        special_tokens = self.tokenizer.additional_special_tokens + \\\n",
    "            [self.tokenizer.pad_token] + [self.tokenizer.eos_token]\n",
    "            \n",
    "        for i, sample in enumerate(samples):\n",
    "            output = outputs_list[i]\n",
    "            for special_token in special_tokens:\n",
    "                output = output.replace(special_token, \"\")\n",
    "            outputs_list[i] = output\n",
    "        \n",
    "        return outputs_list\n",
    "    \n",
    "    \n",
    "def transform_chat_template_with_prompt(prefix: str, prompt: str, tokenizer: AutoTokenizer,\n",
    "                                        use_chat_template: bool = False, template_type: str = None,\n",
    "                                        system_prompt: str = \"\", forced_prefix: str = \"\") -> str:\n",
    "    \n",
    "    \"\"\"\n",
    "    Transform a prefix with a prompt into a chat template\n",
    "    \n",
    "    Parameters:\n",
    "    prefix : str\n",
    "        The prefix to use\n",
    "    prompt : str\n",
    "        The prompt to use\n",
    "    tokenizer : AutoTokenizer\n",
    "        The tokenizer to use\n",
    "    use_chat_template : bool, optional\n",
    "        Whether to use a chat template, by default False\n",
    "    template_type : str, optional\n",
    "        The type of template to use, by default None\n",
    "    system_prompt : str, optional\n",
    "        The system prompt to use, by default \"\"\n",
    "        \n",
    "    Returns:\n",
    "    str\n",
    "        The transformed prefix\n",
    "    \"\"\"\n",
    "        \n",
    "\n",
    "    if prefix != \"\":\n",
    "        text_instruction = f\"{prompt} {prefix}\"\n",
    "    else:\n",
    "        text_instruction = prompt\n",
    "        \n",
    "    if use_chat_template:\n",
    "        if system_prompt == \"\":\n",
    "            sys_prompt = \"You are a helpful assistant.\"\n",
    "        else:\n",
    "            sys_prompt = system_prompt\n",
    "        match template_type:\n",
    "            case \"system_user\":\n",
    "                messages = [\n",
    "                {\"role\": \"system\", \"content\": f\"{sys_prompt}\"},\n",
    "                {\"role\": \"user\", \"content\": f\"{text_instruction}\"},\n",
    "                ]\n",
    "            case \"user\":\n",
    "                messages = [\n",
    "                {\"role\": \"user\", \"content\": f\"{text_instruction}\"},\n",
    "                ]\n",
    "            case _:\n",
    "                raise ValueError(\"Template type not supported\")\n",
    "\n",
    "        text_template = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "\n",
    "        # force prefix on the generated response\n",
    "        #text_template = f\"{text_template}\\n{forced_prefix}\"\n",
    "        text_template = f\"{text_template} {forced_prefix}\"\n",
    "\n",
    "    else:\n",
    "        text_template = text_instruction\n",
    "\n",
    "    return text_template\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attacks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idea: \n",
    "# start with general attack def\n",
    "# implement current attacks with this def\n",
    "# test that this works and its practical\n",
    "# iterate on the general attack def to make it better\n",
    "\n",
    "# support batched gen!\n",
    "\n",
    "class Attack(ABC):\n",
    "    \n",
    "    def __init__(self, gen_model, gen_config, gen_prompt_config):\n",
    "        \n",
    "        # Generator LLM\n",
    "        self.gen_model = gen_model\n",
    "        self.gen_prompt_config = gen_prompt_config\n",
    "        self.gen_model_config = gen_config\n",
    "        \n",
    "        \n",
    "    def generate_text(self, prefixes, batch_size=1):\n",
    "        \n",
    "        # assumption: all attacks will generate text\n",
    "\n",
    "        gen_model = self.gen_model\n",
    "\n",
    "        # apply the chat template with the prompt\n",
    "        system_prompt = self.gen_prompt_config.system_prompt\n",
    "        user_prompt = self.gen_prompt_config.user_prompt\n",
    "        gen_tokenizer = self.gen_model_config.tokenizer\n",
    "        use_chat_template = self.gen_model_config.use_chat_template\n",
    "        template_type = self.gen_model_config.chat_template_type\n",
    "        \n",
    "        # apply the chat template with the prompt\n",
    "        prefixes_with_prompt = [transform_chat_template_with_prompt(\n",
    "            prefix, user_prompt, gen_tokenizer,\n",
    "            use_chat_template, template_type, system_prompt, forced_prefix=prefix) for prefix in prefixes]\n",
    "\n",
    "        # generate articles\n",
    "        fake_articles = []\n",
    "        fake_articles = gen_model(prefixes_with_prompt, batch_size=batch_size)\n",
    "            \n",
    "        # add the prefix back to the generated text since generation cuts the first \"input_size\" tokens from the input\n",
    "        # if we force the prefix in the generation, it is counted in the \"input_size\" tokens\n",
    "        fake_articles = [f\"{prefixes[i]} {fake_articles[i]}\" for i in range(len(fake_articles))]\n",
    "        \n",
    "        return fake_articles\n",
    "    \n",
    "    @abstractmethod \n",
    "    def generate_adversarial_text(self, prefixes, batch_size=1):\n",
    "        \"\"\"\n",
    "        This is the adversarial version of text generation. \n",
    "        All attack should generate text at some point. Either generate text in a specific way or modify the generated text.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "class ParaphrasingAttack(Attack):\n",
    "    \n",
    "    def paraphrase(self, texts, nb_paraphrasing=1, batch_size=1) -> list:\n",
    "        pass\n",
    "    \n",
    "class PromptParaphrasingAttack(Attack):\n",
    "    \n",
    "    def __init__(self, gen_model, gen_config, gen_prompt_config, paraphraser_model, paraphraser_config, paraphraser_prompt_config):\n",
    "        \n",
    "        # Generator LLM\n",
    "        self.gen_model = gen_model\n",
    "        self.gen_prompt_config = gen_prompt_config\n",
    "        self.gen_model_config = gen_config\n",
    "                \n",
    "        # Paraphraser LLM\n",
    "        self.paraphraser_model = paraphraser_model\n",
    "        self.paraphraser_prompt_config = paraphraser_prompt_config\n",
    "        self.model_config = paraphraser_config\n",
    "    \n",
    "    def paraphrase(self, texts, nb_paraphrasing=1, batch_size=1) -> list:\n",
    "        \n",
    "        # Get all the parameters\n",
    "        model_config = self.model_config\n",
    "        tokenizer = model_config.tokenizer\n",
    "        use_chat_template = model_config.use_chat_template\n",
    "        template_type = model_config.chat_template_type\n",
    "        system_paraphrasing_prompt = self.paraphraser_prompt_config.system_prompt\n",
    "        user_paraphrasing_prompt = self.paraphraser_prompt_config.user_prompt\n",
    "        \n",
    "        fake_articles = texts\n",
    "\n",
    "        # generate articles\n",
    "        for i in range(nb_paraphrasing):\n",
    "            \n",
    "            #user_paraphrasing_prompts = [f\"INPUT: {fake_text}\" for fake_text in fake_articles]\n",
    "        \n",
    "            prefixes_with_prompt = [transform_chat_template_with_prompt(\n",
    "                fake_article, user_paraphrasing_prompt, tokenizer,\n",
    "                use_chat_template, template_type, system_paraphrasing_prompt, forced_prefix=\"OUTPUT:\")\n",
    "                for fake_article in fake_articles]\n",
    "            \n",
    "            fake_articles = []\n",
    "            \n",
    "            # generate the articles\n",
    "            for i in range(0, len(prefixes_with_prompt), batch_size):\n",
    "                samples = prefixes_with_prompt[i:i+batch_size]\n",
    "                outputs = self.paraphraser_model(samples)\n",
    "                fake_articles.extend(outputs)\n",
    "                    \n",
    "        return fake_articles\n",
    "    \n",
    "    \"\"\"\n",
    "    def generate_text(self, prefixes, batch_size=1):\n",
    "\n",
    "        gen_model = self.gen_model\n",
    "\n",
    "        # apply the chat template with the prompt\n",
    "        system_prompt = self.gen_system_prompt\n",
    "        user_prompt = self.gen_user_prompt\n",
    "        gen_tokenizer = self.gen_model_config.tokenizer\n",
    "        use_chat_template = self.gen_model_config.use_chat_template\n",
    "        template_type = self.gen_model_config.chat_template_type\n",
    "        \n",
    "        # apply the chat template with the prompt\n",
    "        prefixes_with_prompt = [transform_chat_template_with_prompt(\n",
    "            prefix, user_prompt, gen_tokenizer,\n",
    "            use_chat_template, template_type, system_prompt, forced_prefix=prefix) for prefix in prefixes]\n",
    "\n",
    "        # generate articles\n",
    "        fake_articles = []\n",
    "        fake_articles = gen_model(prefixes_with_prompt, batch_size=batch_size)\n",
    "            \n",
    "        # add the prefix back to the generated text since generation cuts the first \"input_size\" tokens from the input\n",
    "        # if we force the prefix in the generation, it is counted in the \"input_size\" tokens\n",
    "        fake_articles = [f\"{prefixes[i]} {fake_articles[i]}\" for i in range(len(fake_articles))]\n",
    "        \n",
    "\n",
    "        \n",
    "        return paraphrased_fake_articles\n",
    "    \"\"\"\n",
    "    \n",
    "    def generate_adversarial_text(self, prefixes, batch_size=1):\n",
    "\n",
    "        # generate news articles in a \"normal\" way\n",
    "        fake_articles = self.generate_text(prefixes, batch_size=batch_size)\n",
    "        \n",
    "        # paraphrase the texts\n",
    "        paraphrased_fake_articles = self.paraphrase(fake_articles, batch_size=batch_size, nb_paraphrasing=1)\n",
    "        \n",
    "        return paraphrased_fake_articles\n",
    "    \n",
    "class PromptAttack(Attack):\n",
    "    \n",
    "    def __init__(self, gen_model, gen_config, gen_prompt_config, adversarial_prompt_config):\n",
    "        \n",
    "        # Generator LLM\n",
    "        self.gen_model = gen_model\n",
    "        self.gen_prompt_config = gen_prompt_config\n",
    "        self.gen_model_config = gen_config\n",
    "        \n",
    "        # Set adversarial prompts\n",
    "        self.adversarial_prompt_config = adversarial_prompt_config\n",
    "    \n",
    "    def generate_adversarial_text(self, prefixes, batch_size=1):\n",
    "        \n",
    "        # Create adversarial prompt configuration\n",
    "        self.gen_prompt_config = self.adversarial_prompt_config\n",
    "        \n",
    "        # generate text\n",
    "        fake_articles = self.generate_text(prefixes, batch_size=batch_size)\n",
    "        \n",
    "        return fake_articles\n",
    "    \n",
    "class GenParamsAttack(Attack):\n",
    "    \n",
    "    def __init__(self, gen_model, gen_config, gen_prompt_config, adversarial_gen_params):\n",
    "        \n",
    "        # Generator LLM\n",
    "        self.gen_model = gen_model\n",
    "        self.gen_prompt_config = gen_prompt_config\n",
    "        self.gen_model_config = gen_config\n",
    "\n",
    "        self.adversarial_gen_params = adversarial_gen_params\n",
    "    \n",
    "    def generate_adversarial_text(self, prefixes, batch_size=1):\n",
    "    \n",
    "        # Change specific generation parameters compared to base model\n",
    "        for key, value in self.adversarial_gen_params.items():\n",
    "            self.gen_model_config.gen_params[key] = value\n",
    "\n",
    "        # generate text\n",
    "        fake_articles = self.generate_text(prefixes, batch_size=batch_size)\n",
    "        \n",
    "        return fake_articles\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set generation parameters\n",
    "default_gen_params = {\n",
    "    #\"max_length\": 100,\n",
    "    \"max_new_tokens\": 100,\n",
    "    \"min_new_tokens\": 100,\n",
    "    \"temperature\": 0.8,\n",
    "    \"top_p\": 0.95,\n",
    "    \"repetition_penalty\": 1,\n",
    "    \"do_sample\": True,\n",
    "    \"top_k\": 50\n",
    "}\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marluxiaboss/anaconda3/envs/llm_detector/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/home/marluxiaboss/anaconda3/envs/llm_detector/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[' Cats are generally more relaxed and can live independently, which makes them better-suited for being home owners. Dogs, on the other hand, are more prone to anxiety and other behavioral problems. So, cats are better because they can be more independent and less prone to anxiety. It\\'s a matter of personal preference and behavior. The answer to the question \"Why are cats better than dogs?\" is that cats have lower energy levels and are more laid-back, which makes them better-suited for being home',\n",
       " ' Dogs are better than cats because they have a unique personality and loyalty towards their owners. While cats are known for their cuddly and affectionate personality, dogs are known for their loyalty and love of their families. Additionally, dogs are social animals that enjoy being with their owners, and they are known for their intelligence and intelligence. Therefore, dogs are an important part of many households, and they are also known for their companionship. Furthermore, they are considered to be more intelligent than cats. However,']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paraphraser_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"Qwen/Qwen2-0.5B-Instruct\",\n",
    "    pad_token='<|extra_0|>',\n",
    "    eos_token='<|endoftext|>',\n",
    "    padding_side='left',\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "paraphraser = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen2-0.5B-Instruct\",\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    "    pad_token_id=paraphraser_tokenizer.pad_token_id,\n",
    ").to(device)\n",
    "\n",
    "\n",
    "paraphraser_config = ModelConfig(paraphraser_tokenizer,\n",
    "    use_chat_template=True, chat_template_type=\"system_user\", gen_params=default_gen_params, device=device)\n",
    "paraphraser_model = LLMGenerator(paraphraser, paraphraser_config)\n",
    "\n",
    "gen_model = paraphraser_model\n",
    "gen_tokenizer = paraphraser_tokenizer\n",
    "gen_config = paraphraser_config\n",
    "\n",
    "\n",
    "dataset_list = [\"Why are cats better than dogs\", \"Why are dogs better than cats\"]\n",
    "\n",
    "system_paraphrasing_prompt = \"\"\"You are a paraphraser. You are given an input passage ‘INPUT’. You should paraphrase ‘INPUT’ to print ‘OUTPUT’.\"\n",
    "    \"‘OUTPUT’ shoud be diverse and different as much as possible from ‘INPUT’ and should not copy any part verbatim from ‘INPUT’.\"\n",
    "    \"‘OUTPUT’ should preserve the meaning and content of ’INPUT’ while maintaining text quality and grammar.\"\n",
    "    \"‘OUTPUT’ should not be much longer than ‘INPUT’. You should print ‘OUTPUT’ and nothing else so that its easy for me to parse.\"\"\"\n",
    "user_paraphrasing_prompt = \"INPUT:\"\n",
    "paraphraser_prompt_config = PromptConfig(system_prompt=system_paraphrasing_prompt, user_prompt=\"\")\n",
    "\n",
    "\n",
    "gen_prompt = \"You are a helpful assistant.\"\n",
    "user_prompt = \"Write a news article starting with:\"\n",
    "gen_prompt_config = PromptConfig(system_prompt=gen_prompt, user_prompt=user_prompt)\n",
    "\n",
    "prompt_paraphrasing_attack = PromptParaphrasingAttack(gen_model, gen_config, gen_prompt_config, paraphraser_model, paraphraser_config, paraphraser_prompt_config)\n",
    "paraphrased_fake_articles = prompt_paraphrasing_attack.generate_adversarial_text(dataset_list, batch_size=2)\n",
    "\n",
    "paraphrased_fake_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Why are cats better than dogs ? Some people believe that cats are better at managing their emotions and instincts than dogs, while others argue that dogs have a more intuitive sense of the world and can be trained more effectively. However, there is no conclusive proof of this claim, and cats and dogs are truly different creatures with their own unique traits and abilities. Whether or not cats are better than dogs is a matter of personal opinion and can vary from person to person. Ultimately, it's up to each person to decide which pet they want\",\n",
       " 'Why are dogs better than cats ?\\nIn the world of pet ownership, there is no denying the fact that dogs and cats are two of the most beloved pets. While cats have their unique quirks and traits that make them special, dogs are a breed that can bring joy and companionship to many people. In this article, we will explore why dogs are better than cats in a few key ways.\\nFirst and foremost, dogs are highly social animals. Cats, while they are also known to be social, can be more reserved and']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adversarial_system_prompt = \"You are a helpful assistant.\"\n",
    "advesarial_user_prompt = \"Write a news article in the CNN news article style starting with:\"\n",
    "adversarial_prompt_config = PromptConfig(system_prompt=adversarial_system_prompt, user_prompt=advesarial_user_prompt)\n",
    "\n",
    "prompt_attack = PromptAttack(gen_model, gen_config, gen_prompt_config, adversarial_prompt_config)\n",
    "prompt_attack_fake_articles = prompt_attack.generate_adversarial_text(dataset_list, batch_size=2)\n",
    "prompt_attack_fake_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Why are cats better than dogs ?\\n\\nIt's safe to say that cats are not only adorable pets, but also known for their independence, leadership, and intelligence. While dogs have had a long history of making dog lovers proud, some studies suggest that cats may be able to outsmart dogs on certain skills.\\n\\nTo understand the differences between cats and dogs, let's consider their physical adaptations. Cats have long fingers, a rounded shape to their ears, and a short, round body shape that can accommodate various activities, such as crawling\",\n",
       " 'Why are dogs better than cats ?\\nIf you\\'re looking for reasons why dogs are the \"best\" pet option, the simple answer is that they are intelligent, loyal, playful, and affectionate. Of course, dogs have their flaws too, but this lack of flaws can often make them much easier to care for. However, the quality of relationships that can be formed between two individuals is greatly dependent on how well those individuals interact. In contrast, the \"better\" of cats has been scrutinized for a long time, with']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adversarial_gen_param = {\n",
    "    \"temperature\": 1.2\n",
    "}\n",
    "\n",
    "gen_parameters_attack = GenParamsAttack(gen_model, gen_config, gen_prompt_config, adversarial_gen_param)\n",
    "gen_parameters_fake_articles = gen_parameters_attack.generate_adversarial_text(dataset_list, batch_size=2)\n",
    "gen_parameters_fake_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Why are cats better than dogs  When assessing the overall performance statistics at this cross of various breed divisions versus its opposant (typically that dog which could have won and done such orda that dogs could barely take home). For instance dogs on short terms such which could easily dominate can no match to short in human lives .On many top ten contenders like Golden reunion/1 who proved an equal score against cats. While I wouldn categor a variety here may conclude a strong stance but ultimately I am strongly suggesting by all scores Cats on average beat',\n",
       " 'Why are dogs better than cats : After many dogs have already beaten out kittens every Christmas with canine teeth clouthing noses red and purveycing the all-youcare-your-time party downworm season in the month before its first born arrival year, scientists concluded for weeks ahead as many saw that animals which weigh even lesser have larger mouths that swallow up more water over night, their large digestive organs allow up all these smaller quantities quickly.\\nIn conclusion their small jaw means the animals they gob and consume is far, and quick when put up']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adversarial_gen_param = {\n",
    "    \"temperature\": 10.0\n",
    "}\n",
    "\n",
    "gen_parameters_attack = GenParamsAttack(gen_model, gen_config, gen_prompt_config, adversarial_gen_param)\n",
    "gen_parameters_fake_articles = gen_parameters_attack.generate_adversarial_text(dataset_list, batch_size=2)\n",
    "gen_parameters_fake_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_detector",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
