{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [Errno -3] Temporary\n",
      "[nltk_data]     failure in name resolution>\n",
      "[nltk_data] Error loading punkt: <urlopen error [Errno -3] Temporary\n",
      "[nltk_data]     failure in name resolution>\n"
     ]
    }
   ],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from datasets import concatenate_datasets, load_from_disk, DatasetDict\n",
    "import argparse\n",
    "import os\n",
    "import pandas as pd\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import nltk.data\n",
    "nltk.download('punkt')\n",
    "from transformers import (AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForMaskedLM, AutoModelForCausalLM,\n",
    "                          ElectraForSequenceClassification, ElectraTokenizer, AutoConfig)\n",
    "from datasets import load_from_disk, concatenate_datasets, Dataset\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "from evasion_attack import PromptParaphrasingAttack, PromptAttack, GenParamsAttack\n",
    "from utils import ModelConfig, PromptConfig\n",
    "from generator import LLMGenerator\n",
    "from detector import Detector, BertDetector\n",
    "from fast_detect_gpt import FastDetectGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set generation parameters\n",
    "default_gen_params = {\n",
    "    #\"max_length\": 100,\n",
    "    \"max_new_tokens\": 100,\n",
    "    \"min_new_tokens\": 100,\n",
    "    \"temperature\": 0.8,\n",
    "    \"top_p\": 0.95,\n",
    "    \"repetition_penalty\": 1,\n",
    "    \"do_sample\": True,\n",
    "    \"top_k\": 50\n",
    "}\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marluxiaboss/anaconda3/envs/llm_detector/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/home/marluxiaboss/anaconda3/envs/llm_detector/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[' Many people argue that cats are simply more independent and independent in their behavior compared to dogs because they are more agile and agile than dogs. Cats can easily run and jump over obstacles while dogs are typically slower and more likely to get into trouble. Additionally, cats are often more social than dogs. This is why they are often the better option for humans. However, it is important to note that this argument is not based on any scientific evidence or studies. Additionally, it is important to note that it is',\n",
       " \" Dogs and cats are both great pets but dogs are generally considered to be more reliable and loyal. They have better physical abilities, such as running longer distances and being more alert and focused than cats. Additionally, dogs are known to be more intelligent than cats and have been shown to be more docile and trainable. However, there are some differences in their personalities and behaviors, and it's important to take into account the specific needs and preferences of the person or family member adopting the dog. Ultimately, the\",\n",
       " \" Cats are better than dogs because they are more independent and can handle certain tasks on their own, while dogs are more social and require less attention. While it's true that cats are more independent and may need more space and attention than dogs, they can also be more playful and social. It's worth noting that cats and dogs are both pets and can be trained to behave in certain ways. For example, some people believe that dogs are more independent and can handle certain tasks on their own, while others\",\n",
       " \" Dogs can lead to better behavior in dogs, including improved obedience and loyalty. They are trained with positive reinforcement, which leads to a greater willingness to perform certain tasks, such as retrieving toys, than dogs trained with negative reinforcement. Furthermore, the study found that dogs who were trained with positive reinforcement were more likely to exhibit positive behavior in various situations, such as when they were approached by strangers. This suggests that dogs are better than cats in some ways, but it's important to note that all dogs are\",\n",
       " ' Cats are generally considered superior because they are generally more social animals. Their social interaction is often seen as a defining feature of their breed, and their closeness to their companions is a hallmark of their loyalty. In contrast, dogs are generally considered inferior because they are more independent animals and their social interaction is often seen as a hindrance. The debate about whether cats are better or worse than dogs is a matter of opinion and personal preference. Ultimately, it is up to each individual to weigh the benefits and']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paraphraser_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"Qwen/Qwen2-0.5B-Instruct\",\n",
    "    pad_token='<|extra_0|>',\n",
    "    eos_token='<|endoftext|>',\n",
    "    padding_side='left',\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "paraphraser = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen2-0.5B-Instruct\",\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    "    pad_token_id=paraphraser_tokenizer.pad_token_id,\n",
    ").to(device)\n",
    "\n",
    "\n",
    "paraphraser_config = ModelConfig(paraphraser_tokenizer,\n",
    "    use_chat_template=True, chat_template_type=\"system_user\", gen_params=default_gen_params, device=device)\n",
    "paraphraser_model = LLMGenerator(paraphraser, paraphraser_config)\n",
    "\n",
    "gen_model = paraphraser_model\n",
    "gen_tokenizer = paraphraser_tokenizer\n",
    "gen_config = paraphraser_config\n",
    "\n",
    "\n",
    "dataset_list = [\"Why are cats better than dogs\", \"Why are dogs better than cats\", \"Why are cats better than dogs\",\n",
    "                \"Why are dogs better than cats\", \"Why are cats better than dogs\"]\n",
    "\n",
    "system_paraphrasing_prompt = \"\"\"You are a paraphraser. You are given an input passage ‘INPUT’. You should paraphrase ‘INPUT’ to print ‘OUTPUT’.\"\n",
    "    \"‘OUTPUT’ shoud be diverse and different as much as possible from ‘INPUT’ and should not copy any part verbatim from ‘INPUT’.\"\n",
    "    \"‘OUTPUT’ should preserve the meaning and content of ’INPUT’ while maintaining text quality and grammar.\"\n",
    "    \"‘OUTPUT’ should not be much longer than ‘INPUT’. You should print ‘OUTPUT’ and nothing else so that its easy for me to parse.\"\"\"\n",
    "user_paraphrasing_prompt = \"INPUT:\"\n",
    "paraphraser_prompt_config = PromptConfig(system_prompt=system_paraphrasing_prompt, user_prompt=\"\")\n",
    "\n",
    "\n",
    "gen_prompt = \"You are a helpful assistant.\"\n",
    "user_prompt = \"Write a news article starting with:\"\n",
    "gen_prompt_config = PromptConfig(system_prompt=gen_prompt, user_prompt=user_prompt)\n",
    "\n",
    "prompt_paraphrasing_attack = PromptParaphrasingAttack(gen_model, gen_config, gen_prompt_config, paraphraser_model, paraphraser_config, paraphraser_prompt_config)\n",
    "paraphrased_fake_articles = prompt_paraphrasing_attack.generate_adversarial_text(dataset_list, batch_size=2)\n",
    "\n",
    "paraphrased_fake_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Why are cats better than dogs ?\\nThe question of whether cats are better than dogs is a topic that has been debated for centuries. While both animals are intelligent and capable of achieving success in various pursuits, there are several factors that could explain why cats are often considered more well-adjusted and intelligent than dogs. \\nOne reason that cats may outperform dogs in certain tasks is that they are often social and have a natural inclination towards nurturing and caring for others. Dogs, on the other hand, are often social animals that are more independent',\n",
       " 'Why are dogs better than cats ?\\nIn many households, dogs are the most commonly owned pets, but they are also beloved companions for many families. While cats may be a smaller pet, they offer a unique way to bond with humans and provide a loyal and affectionate friend.\\nAccording to a recent study published in the journal Nature, dogs are more social animals than cats and are known for their loyalty and intelligence. The study found that dogs can detect and respond to the presence of humans, which is a unique trait that many people find']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adversarial_system_prompt = \"You are a helpful assistant.\"\n",
    "advesarial_user_prompt = \"Write a news article in the CNN news article style starting with:\"\n",
    "adversarial_prompt_config = PromptConfig(system_prompt=adversarial_system_prompt, user_prompt=advesarial_user_prompt)\n",
    "\n",
    "prompt_attack = PromptAttack(gen_model, gen_config, gen_prompt_config, adversarial_prompt_config)\n",
    "prompt_attack_fake_articles = prompt_attack.generate_adversarial_text(dataset_list, batch_size=2)\n",
    "prompt_attack_fake_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Why are cats better than dogs ?\\nThe topic of cats being more intelligent and affectionate than dogs is a widely debated subject, with different opinions within the media, scientists, and among pet owners themselves. Some argue that cats are superior, as they are capable of better cognitive functions and possess unique personality traits. Others believe that dogs are more capable and docile.\\nRegardless of one's perspective, research has consistently shown that cats excel in certain tasks and interactions with their owners, and that they possess a keen sense of smell, a keen\",\n",
       " 'Why are dogs better than cats ?\\nThere are many reasons why dogs are often considered to be superior to cats. However, one key factor that stands out is their exceptional ability to sense their surroundings and react appropriately. In general, dogs are more alert and responsive to their environment, which means they are able to adapt to a wide range of situations quickly and effectively.\\nFurthermore, dogs are known for their ability to learn new skills and behaviors quickly, which is one of their primary advantages over cats. As a result, dogs are often more']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adversarial_gen_param = {\n",
    "    \"temperature\": 1.2\n",
    "}\n",
    "\n",
    "gen_parameters_attack = GenParamsAttack(gen_model, gen_config, gen_prompt_config, adversarial_gen_param)\n",
    "gen_parameters_fake_articles = gen_parameters_attack.generate_adversarial_text(dataset_list, batch_size=2)\n",
    "gen_parameters_fake_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Why are cats better than dogs ... It all begins now as The \"Alo !!g \" breeds one and every once and every another discovers we must take that leap that only some things do (it says...) In an experiment known to lovers there, scientists find it takes three bites rather just... A couple dogs decide between finding themselves cats instead, just a new way at trying and it happens, right? We then found and had found one in which, this morning The little brown one comes for it A dog named Jodie decides',\n",
       " \"Why are dogs better than cats  when coming from animal福利? Why and Can. When one breeds or socialize to another?\\nThis is one reason dog owners have long debated: While both of species excel in all physical behaviors: Intelligence vs Motion Dog Behavior The research shows:\\nMost experts, if all of breed's qualities were transferred as their ability towards social relationships would greatly evolve from breed A dog may learn tricks well if bred A because those that grew the best with human beings trained at dogs' level are generally higher social competence skills\"]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adversarial_gen_param = {\n",
    "    \"temperature\": 10.0\n",
    "}\n",
    "\n",
    "gen_parameters_attack = GenParamsAttack(gen_model, gen_config, gen_prompt_config, adversarial_gen_param)\n",
    "gen_parameters_fake_articles = gen_parameters_attack.generate_adversarial_text(dataset_list, batch_size=2)\n",
    "gen_parameters_fake_articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marluxiaboss/anaconda3/envs/llm_detector/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ElectraForSequenceClassification(\n",
       "  (electra): ElectraModel(\n",
       "    (embeddings): ElectraEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 1024, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 1024)\n",
       "      (token_type_embeddings): Embedding(2, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): ElectraEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): ElectraClassificationHead(\n",
       "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (activation): GELUActivation()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=1024, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "detector_path = \"google/electra-large-discriminator\"\n",
    "config = AutoConfig.from_pretrained(detector_path)\n",
    "detector_model = ElectraForSequenceClassification(config)\n",
    "bert_tokenizer = ElectraTokenizer.from_pretrained(detector_path)\n",
    "\n",
    "model_path = \"../saved_training_logs_experiment_2/electra_large/full_finetuning/fake_true_dataset_round_robin_10k/10_06_1308/saved_models/best_model.pt\"\n",
    "detector_model.load_state_dict(torch.load(model_path))\n",
    "detector_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'detector_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_489345/579843562.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtext_to_detect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"I am AI generated text\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"I am human generated text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdetector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertDetector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetector_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbert_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_to_detect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetection_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'detector_model' is not defined"
     ]
    }
   ],
   "source": [
    "text_to_detect = [\"I am AI generated text\", \"I am human generated text\"]\n",
    "detector = BertDetector(detector_model, bert_tokenizer, device)\n",
    "preds, logits = detector.detect(text_to_detect, detection_threshold=0.5, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 0], [0.14659571647644043, -0.6974161863327026])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds, logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fast-DetectGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import glob\n",
    "import json\n",
    "\n",
    "class FastDetectGPT(Detector):\n",
    "    def __init__(self, ref_model, scoring_model, ref_tokenizer, scoring_tokenizer, device):\n",
    "        self.ref_model = ref_model\n",
    "        self.scoring_model = scoring_model\n",
    "        self.ref_tokenizer = ref_tokenizer\n",
    "        self.scoring_tokenizer = scoring_tokenizer\n",
    "        self.device = device\n",
    "        \n",
    "    def get_samples(logits, labels):\n",
    "        assert logits.shape[0] == 1\n",
    "        assert labels.shape[0] == 1\n",
    "        nsamples = 10000\n",
    "        lprobs = torch.log_softmax(logits, dim=-1)\n",
    "        distrib = torch.distributions.categorical.Categorical(logits=lprobs)\n",
    "        samples = distrib.sample([nsamples]).permute([1, 2, 0])\n",
    "        return samples\n",
    "\n",
    "    def get_likelihood(logits, labels):\n",
    "        assert logits.shape[0] == 1\n",
    "        assert labels.shape[0] == 1\n",
    "        labels = labels.unsqueeze(-1) if labels.ndim == logits.ndim - 1 else labels\n",
    "        lprobs = torch.log_softmax(logits, dim=-1)\n",
    "        log_likelihood = lprobs.gather(dim=-1, index=labels)\n",
    "        return log_likelihood.mean(dim=1)\n",
    "\n",
    "    def get_sampling_discrepancy(self, logits_ref, logits_score, labels):\n",
    "        assert logits_ref.shape[0] == 1\n",
    "        assert logits_score.shape[0] == 1\n",
    "        assert labels.shape[0] == 1\n",
    "        if logits_ref.size(-1) != logits_score.size(-1):\n",
    "            # print(f\"WARNING: vocabulary size mismatch {logits_ref.size(-1)} vs {logits_score.size(-1)}.\")\n",
    "            vocab_size = min(logits_ref.size(-1), logits_score.size(-1))\n",
    "            logits_ref = logits_ref[:, :, :vocab_size]\n",
    "            logits_score = logits_score[:, :, :vocab_size]\n",
    "\n",
    "        samples = self.get_samples(logits_ref, labels)\n",
    "        log_likelihood_x = (logits_score, labels)\n",
    "        log_likelihood_x_tilde = self.get_likelihood(logits_score, samples)\n",
    "        miu_tilde = log_likelihood_x_tilde.mean(dim=-1)\n",
    "        sigma_tilde = log_likelihood_x_tilde.std(dim=-1)\n",
    "        discrepancy = (log_likelihood_x.squeeze(-1) - miu_tilde) / sigma_tilde\n",
    "        return discrepancy.item()\n",
    "    \n",
    "    def get_sampling_discrepancy_analytic(self, logits_ref, logits_score, labels):\n",
    "        assert logits_ref.shape[0] == 1\n",
    "        assert logits_score.shape[0] == 1\n",
    "        assert labels.shape[0] == 1\n",
    "        if logits_ref.size(-1) != logits_score.size(-1):\n",
    "            # print(f\"WARNING: vocabulary size mismatch {logits_ref.size(-1)} vs {logits_score.size(-1)}.\")\n",
    "            vocab_size = min(logits_ref.size(-1), logits_score.size(-1))\n",
    "            logits_ref = logits_ref[:, :, :vocab_size]\n",
    "            logits_score = logits_score[:, :, :vocab_size]\n",
    "\n",
    "        labels = labels.unsqueeze(-1) if labels.ndim == logits_score.ndim - 1 else labels\n",
    "        lprobs_score = torch.log_softmax(logits_score, dim=-1)\n",
    "        probs_ref = torch.softmax(logits_ref, dim=-1)\n",
    "        log_likelihood = lprobs_score.gather(dim=-1, index=labels).squeeze(-1)\n",
    "        mean_ref = (probs_ref * lprobs_score).sum(dim=-1)\n",
    "        var_ref = (probs_ref * torch.square(lprobs_score)).sum(dim=-1) - torch.square(mean_ref)\n",
    "        discrepancy = (log_likelihood.sum(dim=-1) - mean_ref.sum(dim=-1)) / var_ref.sum(dim=-1).sqrt()\n",
    "        discrepancy = discrepancy.mean()\n",
    "        return discrepancy.item()\n",
    "    \n",
    "    class ProbEstimatorFastDetectGPT:\n",
    "        def __init__(self, args=None, ref_path=None):\n",
    "            if args is None:\n",
    "                ref_path = ref_path\n",
    "            else:\n",
    "                ref_path = args.ref_path\n",
    "            self.real_crits = []\n",
    "            self.fake_crits = []\n",
    "            for result_file in glob.glob(os.path.join(ref_path, '*.json')):\n",
    "                with open(result_file, 'r') as fin:\n",
    "                    res = json.load(fin)\n",
    "                    self.real_crits.extend(res['predictions']['real'])\n",
    "                    self.fake_crits.extend(res['predictions']['samples'])\n",
    "            print(f'ProbEstimator: total {len(self.real_crits) * 2} samples.')\n",
    "\n",
    "        def crit_to_prob(self, crit):\n",
    "            offset = np.sort(np.abs(np.array(self.real_crits + self.fake_crits) - crit))[100]\n",
    "            cnt_real = np.sum((np.array(self.real_crits) > crit - offset) & (np.array(self.real_crits) < crit + offset))\n",
    "            cnt_fake = np.sum((np.array(self.fake_crits) > crit - offset) & (np.array(self.fake_crits) < crit + offset))\n",
    "            return cnt_fake / (cnt_real + cnt_fake)\n",
    "\n",
    "            \n",
    "    def detect(self, texts: list) -> list:\n",
    "        reference_model_name = \"gpt-neo-2.7B\"\n",
    "        scoring_model_name = \"gpt-neo-2.7B\"\n",
    "        \n",
    "        ref_path = \"local_infer_ref\"\n",
    "        \n",
    "        # create experiment folder\n",
    "\n",
    "        # load model\n",
    "        \"\"\"\n",
    "        scoring_tokenizer = load_tokenizer(scoring_model_name, dataset, cache_dir)\n",
    "        scoring_model = load_model(scoring_model_name,device,cache_dir)\n",
    "        scoring_model.eval()\n",
    "        if reference_model_name != scoring_model_name:\n",
    "            reference_tokenizer = load_tokenizer(reference_model_name, dataset, cache_dir)\n",
    "            reference_model = load_model(reference_model_name, device, cache_dir)\n",
    "            reference_model.eval()\n",
    "        \"\"\"\n",
    "        ref_model = self.ref_model\n",
    "        ref_tokenizer = self.ref_tokenizer\n",
    "        \n",
    "        scoring_model = self.scoring_model\n",
    "        scoring_tokenizer = self.scoring_tokenizer\n",
    "        \n",
    "\n",
    "                \n",
    "        # evaluate criterion\n",
    "        #name = \"sampling_discrepancy_analytic\"\n",
    "        criterion_fn = self.get_sampling_discrepancy_analytic\n",
    "        prob_estimator = self.ProbEstimatorFastDetectGPT(ref_path=ref_path)\n",
    "\n",
    "\n",
    "        # iterate over the dataset and do detection on each sample\n",
    "        dataset = Dataset.from_dict({\"text\": texts})\n",
    "\n",
    "        # create dataloader\n",
    "        batch_size = 1\n",
    "        \n",
    "        test_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "        preds = []\n",
    "        probs = []\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(test_loader, desc=\"Performing detection on dataset...\"):\n",
    "                text = batch[\"text\"]\n",
    "\n",
    "                tokenized = scoring_tokenizer(text, return_tensors=\"pt\", padding=True, return_token_type_ids=False).to(device)\n",
    "                labels = tokenized.input_ids[:, 1:]\n",
    "                logits_score = scoring_model(**tokenized).logits[:, :-1]\n",
    "\n",
    "                if reference_model_name == scoring_model_name:\n",
    "                    logits_ref = logits_score\n",
    "                else:\n",
    "                    tokenized = ref_tokenizer(text, return_tensors=\"pt\", padding=True, return_token_type_ids=False).to(device)\n",
    "                    assert torch.all(tokenized.input_ids[:, 1:] == labels), \"Tokenizer is mismatch.\"\n",
    "                    logits_ref = ref_model(**tokenized).logits[:, :-1]\n",
    "\n",
    "                for i in range(batch_size):\n",
    "                    crit = criterion_fn(logits_ref[i:i+1], logits_score[i:i+1], labels[i:i+1])\n",
    "                    prob = prob_estimator.crit_to_prob(crit)\n",
    "                    pred = 1 if prob > 0.5 else 0\n",
    "                    \n",
    "                    probs.append(prob)\n",
    "                    preds.append(pred)\n",
    "\n",
    "        preds = np.array(preds)\n",
    "        probs = np.array(probs)\n",
    "        \n",
    "        return preds, probs\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marluxiaboss/anaconda3/envs/llm_detector/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "ref_model_path = \"openai-community/gpt2\"\n",
    "ref_model = AutoModelForCausalLM.from_pretrained(ref_model_path, torch_dtype=\"auto\").to(device)\n",
    "ref_tokenizer = AutoTokenizer.from_pretrained(ref_model_path, trust_remote_code=True, padding_side=\"left\")\n",
    "\n",
    "# special for gpt2\n",
    "ref_tokenizer.pad_token = ref_tokenizer.eos_token\n",
    "ref_tokenizer.padding_side = 'left'\n",
    "\n",
    "scoring_model = ref_model\n",
    "scoring_tokenizer = ref_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ProbEstimator: total 1800 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Performing detection on dataset...:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Performing detection on dataset...: 100%|██████████| 2/2 [00:01<00:00,  1.59it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0, 0]), array([0., 0.]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast_detector = FastDetectGPT(ref_model, scoring_model, ref_tokenizer, scoring_tokenizer, device)\n",
    "\n",
    "texts = [\"I am AI generated text\", \"I am human generated text\"]\n",
    "preds, probs = fast_detector.detect(texts)\n",
    "preds, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WatermarkingDetector(Detector):\n",
    "    def __init__(self, watermark):\n",
    "        self.watermark = watermark\n",
    "        \n",
    "    def detect(self, text: str) -> bool:\n",
    "        return self.watermark in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WatermarkingGenerator(Generator):\n",
    "    \n",
    "    def __init__(self, model, tokenizer, watermark):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.watermark = watermark\n",
    "        \n",
    "    def generate(self, prompt: str) -> str:\n",
    "        text = self.watermark + \" \" + prompt\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        outputs = self.model.generate(**inputs)\n",
    "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_detector",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
