Parameter 'function'=<function DetectorTrainer.load_fact_checking_dataset.<locals>.<lambda> at 0x153d9f9a1080> of the transform datasets.arrow_dataset.Dataset.filter@2.0.1 couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.


Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Answering fact completion questions...: 100%|██████████| 32/32 [00:01<00:00, 26.38it/s]
Answering fact completion questions...:  81%|████████▏ | 26/32 [00:01<00:00, 38.59it/s]
Answering fact completion questions...:  66%|██████▌   | 21/32 [00:00<00:00, 68.61it/s]
Average Loss on fact answering task after 0 samples: 6.1816
Average Loss on fact answering task after 0 samples: 6.2771
Average Loss on fact answering task after 0 samples: 6.1164
Answering fact completion questions...: 100%|██████████| 32/32 [00:00<00:00, 68.58it/s]
Answering fact completion questions...: 100%|██████████| 32/32 [00:00<00:00, 68.48it/s]
Answering fact completion questions...: 100%|██████████| 32/32 [00:00<00:00, 69.89it/s]
Answering fact completion questions...:  88%|████████▊ | 28/32 [00:00<00:00, 69.06it/s]
Average Loss on fact answering task after 0 samples: 6.2850
----------------Epoch 1/1----------------
Epoch 1/1, Loss after 192 samples: 0.6985
Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Answering fact completion questions...: 100%|██████████| 32/32 [00:00<00:00, 69.46it/s]
Answering fact completion questions...:  88%|████████▊ | 28/32 [00:00<00:00, 68.99it/s]
Answering fact completion questions...:  22%|██▏       | 7/32 [00:00<00:00, 69.38it/s]
Average Loss on fact answering task after 480 samples: 6.3571
Average Loss on fact answering task after 480 samples: 6.2088
Answering fact completion questions...: 100%|██████████| 32/32 [00:00<00:00, 69.56it/s]
Answering fact completion questions...: 100%|██████████| 32/32 [00:00<00:00, 70.25it/s]
Answering fact completion questions...: 100%|██████████| 32/32 [00:00<00:00, 69.43it/s]
Answering fact completion questions...:  88%|████████▊ | 28/32 [00:00<00:00, 68.69it/s]
Average Loss on fact answering task after 480 samples: 6.2049
Average Loss on fact answering task after 480 samples: 6.2378
Mean accuracy: 0.6452, std: 0.0105, lower bound: 0.6242, upper bound: 0.6648 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 480 samples: 0.6455
Best model with eval accuracy 0.6455375253549696 with 480 samples seen is saved
Epoch 1/1, Loss after 640 samples: 0.6823
Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Answering fact completion questions...: 100%|██████████| 32/32 [00:00<00:00, 62.25it/s]
Answering fact completion questions...: 100%|██████████| 32/32 [00:00<00:00, 69.49it/s]
Answering fact completion questions...:  88%|████████▊ | 28/32 [00:00<00:00, 68.45it/s]
Answering fact completion questions...:  66%|██████▌   | 21/32 [00:00<00:00, 67.97it/s]
Average Loss on fact answering task after 992 samples: 6.3145
Average Loss on fact answering task after 992 samples: 6.2823
Average Loss on fact answering task after 992 samples: 6.2716
Answering fact completion questions...: 100%|██████████| 32/32 [00:00<00:00, 69.98it/s]
Answering fact completion questions...: 100%|██████████| 32/32 [00:00<00:00, 69.60it/s]
Answering fact completion questions...:  88%|████████▊ | 28/32 [00:00<00:00, 68.98it/s]
Average Loss on fact answering task after 992 samples: 6.3342
Mean accuracy: 0.7484, std: 0.0095, lower bound: 0.7297, upper bound: 0.7683 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 992 samples: 0.7480
Best model with eval accuracy 0.7479716024340771 with 992 samples seen is saved
Epoch 1/1, Loss after 1088 samples: 0.6436
Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Average Loss on fact answering task after 1504 samples: 6.7516
Answering fact completion questions...: 100%|██████████| 32/32 [00:00<00:00, 62.04it/s]
Answering fact completion questions...: 100%|██████████| 32/32 [00:00<00:00, 60.08it/s]
Answering fact completion questions...:  78%|███████▊  | 25/32 [00:00<00:00, 59.94it/s]
Answering fact completion questions...: 100%|██████████| 32/32 [00:00<00:00, 61.49it/s]
Answering fact completion questions...:  81%|████████▏ | 26/32 [00:00<00:00, 61.88it/s]
Average Loss on fact answering task after 1504 samples: 6.5658
Answering fact completion questions...:  38%|███▊      | 12/32 [00:00<00:00, 50.61it/s]

Answering fact completion questions...:  78%|███████▊  | 25/32 [00:00<00:00, 59.37it/s]
Answering fact completion questions...: 100%|██████████| 32/32 [00:00<00:00, 60.14it/s]
Answering fact completion questions...:  78%|███████▊  | 25/32 [00:00<00:00, 60.20it/s]
Mean accuracy: 0.8039, std: 0.0091, lower bound: 0.7870, upper bound: 0.8210 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 1504 samples: 0.8038
Best model with eval accuracy 0.8037525354969574 with 1504 samples seen is saved
Epoch 1/1, Loss after 1536 samples: 0.4255
Epoch 1/1, Loss after 1760 samples: 0.4000
Epoch 1/1, Loss after 1984 samples: 0.3635
Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Answering fact completion questions...: 100%|██████████| 32/32 [00:00<00:00, 63.07it/s]
Answering fact completion questions...: 100%|██████████| 32/32 [00:00<00:00, 69.54it/s]
Answering fact completion questions...: 100%|██████████| 32/32 [00:00<00:00, 69.85it/s]
Answering fact completion questions...:  88%|████████▊ | 28/32 [00:00<00:00, 69.29it/s]
Answering fact completion questions...:  44%|████▍     | 14/32 [00:00<00:00, 69.18it/s]
Average Loss on fact answering task after 2016 samples: 6.5191
Average Loss on fact answering task after 2016 samples: 6.5147
Average Loss on fact answering task after 2016 samples: 6.7137
Answering fact completion questions...: 100%|██████████| 32/32 [00:00<00:00, 68.91it/s]
Answering fact completion questions...:  88%|████████▊ | 28/32 [00:00<00:00, 68.18it/s]
Mean accuracy: 0.7659, std: 0.0089, lower bound: 0.7475, upper bound: 0.7830 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 2016 samples: 0.7662
Epoch 1/1, Loss after 2208 samples: 0.3497
Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Answering fact completion questions...: 100%|██████████| 32/32 [00:00<00:00, 69.19it/s]
Answering fact completion questions...: 100%|██████████| 32/32 [00:00<00:00, 68.44it/s]
Answering fact completion questions...:  88%|████████▊ | 28/32 [00:00<00:00, 67.94it/s]
Answering fact completion questions...:   0%|          | 0/32 [00:00<?, ?it/s]
Average Loss on fact answering task after 2528 samples: 6.2346
Average Loss on fact answering task after 2528 samples: 6.2740
Average Loss on fact answering task after 2528 samples: 6.6488
Answering fact completion questions...: 100%|██████████| 32/32 [00:00<00:00, 69.25it/s]
Answering fact completion questions...: 100%|██████████| 32/32 [00:00<00:00, 70.08it/s]
Answering fact completion questions...:  91%|█████████ | 29/32 [00:00<00:00, 69.42it/s]
Average Loss on fact answering task after 2528 samples: 6.5530
Mean accuracy: 0.8431, std: 0.0081, lower bound: 0.8276, upper bound: 0.8600 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 2528 samples: 0.8433
Best model with eval accuracy 0.8433062880324543 with 2528 samples seen is saved
Epoch 1/1, Loss after 2656 samples: 0.2896
Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Answering fact completion questions...: 100%|██████████| 32/32 [00:00<00:00, 60.85it/s]
Answering fact completion questions...: 100%|██████████| 32/32 [00:00<00:00, 70.26it/s]
Answering fact completion questions...:  91%|█████████ | 29/32 [00:00<00:00, 69.52it/s]
Answering fact completion questions...:  22%|██▏       | 7/32 [00:00<00:00, 67.50it/s]
Average Loss on fact answering task after 3040 samples: 6.4707
Average Loss on fact answering task after 3040 samples: 6.5831
Average Loss on fact answering task after 3040 samples: 6.6020
Answering fact completion questions...: 100%|██████████| 32/32 [00:00<00:00, 69.23it/s]
Answering fact completion questions...: 100%|██████████| 32/32 [00:00<00:00, 70.25it/s]
Answering fact completion questions...:  94%|█████████▍| 30/32 [00:00<00:00, 69.41it/s]
Average Loss on fact answering task after 3040 samples: 6.2290
Mean accuracy: 0.8354, std: 0.0083, lower bound: 0.8190, upper bound: 0.8514 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 3040 samples: 0.8352
Epoch 1/1, Loss after 3104 samples: 0.2793
Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Answering fact completion questions...:  88%|████████▊ | 28/32 [00:00<00:00, 68.61it/s]
Epoch 1/1, Loss after 3552 samples: 0.3408
Average Loss on fact answering task after 3552 samples: 6.3136
Average Loss on fact answering task after 3552 samples: 6.4011
Answering fact completion questions...: 100%|██████████| 32/32 [00:00<00:00, 69.79it/s]
Answering fact completion questions...: 100%|██████████| 32/32 [00:00<00:00, 69.28it/s]
Answering fact completion questions...: 100%|██████████| 32/32 [00:00<00:00, 68.88it/s]
Answering fact completion questions...: 100%|██████████| 32/32 [00:00<00:00, 69.82it/s]
Answering fact completion questions...:  91%|█████████ | 29/32 [00:00<00:00, 69.36it/s]
Average Loss on fact answering task after 3552 samples: 6.2777
Average Loss on fact answering task after 3552 samples: 6.3517
Mean accuracy: 0.8892, std: 0.0071, lower bound: 0.8757, upper bound: 0.9026 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 3552 samples: 0.8895
Best model with eval accuracy 0.8894523326572008 with 3552 samples seen is saved
Epoch 1/1, Loss after 3776 samples: 0.2843
Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Answering fact completion questions...: 100%|██████████| 32/32 [00:00<00:00, 69.18it/s]
Answering fact completion questions...: 100%|██████████| 32/32 [00:00<00:00, 69.66it/s]
Answering fact completion questions...:  91%|█████████ | 29/32 [00:00<00:00, 68.89it/s]
Answering fact completion questions...:   0%|          | 0/32 [00:00<?, ?it/s]
Average Loss on fact answering task after 4064 samples: 6.1994
Average Loss on fact answering task after 4064 samples: 6.4701
Average Loss on fact answering task after 4064 samples: 6.5010
Answering fact completion questions...: 100%|██████████| 32/32 [00:00<00:00, 69.35it/s]
Answering fact completion questions...: 100%|██████████| 32/32 [00:00<00:00, 69.18it/s]
Answering fact completion questions...:  91%|█████████ | 29/32 [00:00<00:00, 68.42it/s]
Average Loss on fact answering task after 4064 samples: 6.3561
Mean accuracy: 0.8930, std: 0.0069, lower bound: 0.8793, upper bound: 0.9057 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 4064 samples: 0.8935
Best model with eval accuracy 0.8935091277890467 with 4064 samples seen is saved
Epoch 1/1, Loss after 4224 samples: 0.2800
Epoch 1/1, Loss after 4448 samples: 0.2252
Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Answering fact completion questions...: 100%|██████████| 32/32 [00:00<00:00, 68.75it/s]
Answering fact completion questions...: 100%|██████████| 32/32 [00:00<00:00, 68.68it/s]
Answering fact completion questions...: 100%|██████████| 32/32 [00:00<00:00, 69.69it/s]
Answering fact completion questions...:  88%|████████▊ | 28/32 [00:00<00:00, 68.92it/s]
Answering fact completion questions...:   0%|          | 0/32 [00:00<?, ?it/s]
Average Loss on fact answering task after 4576 samples: 6.4350
Average Loss on fact answering task after 4576 samples: 6.4308
Average Loss on fact answering task after 4576 samples: 6.5755
Answering fact completion questions...: 100%|██████████| 32/32 [00:00<00:00, 69.37it/s]
Answering fact completion questions...:  88%|████████▊ | 28/32 [00:00<00:00, 68.44it/s]
Mean accuracy: 0.8058, std: 0.0090, lower bound: 0.7870, upper bound: 0.8230 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 4576 samples: 0.8058
Epoch 1/1, Loss after 4672 samples: 0.2462
Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Answering fact completion questions...: 100%|██████████| 32/32 [00:00<00:00, 69.76it/s]
Answering fact completion questions...:  88%|████████▊ | 28/32 [00:00<00:00, 69.26it/s]
Answering fact completion questions...:  66%|██████▌   | 21/32 [00:00<00:00, 68.75it/s]
Average Loss on fact answering task after 5088 samples: 6.7206
Average Loss on fact answering task after 5088 samples: 6.6556
Answering fact completion questions...: 100%|██████████| 32/32 [00:00<00:00, 69.55it/s]
Answering fact completion questions...: 100%|██████████| 32/32 [00:00<00:00, 69.33it/s]
Answering fact completion questions...: 100%|██████████| 32/32 [00:00<00:00, 69.21it/s]
Answering fact completion questions...:  88%|████████▊ | 28/32 [00:00<00:00, 69.01it/s]
Average Loss on fact answering task after 5088 samples: 6.3425
Average Loss on fact answering task after 5088 samples: 6.4732
Mean accuracy: 0.8973, std: 0.0070, lower bound: 0.8828, upper bound: 0.9102 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 5088 samples: 0.8976
Best model with eval accuracy 0.8975659229208925 with 5088 samples seen is saved
Epoch 1/1, Loss after 5120 samples: 0.2770
Epoch 1/1, Loss after 5344 samples: 0.2351
Epoch 1/1, Loss after 5568 samples: 0.2468
Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Answering fact completion questions...: 100%|██████████| 32/32 [00:00<00:00, 69.48it/s]
Answering fact completion questions...: 100%|██████████| 32/32 [00:00<00:00, 69.85it/s]
Answering fact completion questions...: 100%|██████████| 32/32 [00:00<00:00, 68.69it/s]
Answering fact completion questions...:  88%|████████▊ | 28/32 [00:00<00:00, 68.58it/s]
Answering fact completion questions...:  22%|██▏       | 7/32 [00:00<00:00, 67.44it/s]
Average Loss on fact answering task after 5600 samples: 6.5661
Average Loss on fact answering task after 5600 samples: 6.7792
Average Loss on fact answering task after 5600 samples: 6.3115
Answering fact completion questions...: 100%|██████████| 32/32 [00:00<00:00, 68.91it/s]
Answering fact completion questions...:  88%|████████▊ | 28/32 [00:00<00:00, 67.69it/s]
Mean accuracy: 0.8969, std: 0.0070, lower bound: 0.8829, upper bound: 0.9108 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 5600 samples: 0.8971
Epoch 1/1, Loss after 5792 samples: 0.2688
Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Answering fact completion questions...: 100%|██████████| 32/32 [00:00<00:00, 69.61it/s]
Answering fact completion questions...: 100%|██████████| 32/32 [00:00<00:00, 69.85it/s]
Answering fact completion questions...:  88%|████████▊ | 28/32 [00:00<00:00, 69.14it/s]
Answering fact completion questions...:   0%|          | 0/32 [00:00<?, ?it/s]
Average Loss on fact answering task after 6112 samples: 6.2784
Average Loss on fact answering task after 6112 samples: 6.5039
Average Loss on fact answering task after 6112 samples: 6.6593
Answering fact completion questions...: 100%|██████████| 32/32 [00:00<00:00, 69.36it/s]
Answering fact completion questions...: 100%|██████████| 32/32 [00:00<00:00, 69.71it/s]
Answering fact completion questions...:  88%|████████▊ | 28/32 [00:00<00:00, 68.99it/s]
Answering fact completion questions...: 100%|██████████| 32/32 [00:00<00:00, 69.36it/s]
Average Loss on fact answering task after 6112 samples: 6.6230
Mean accuracy: 0.8758, std: 0.0074, lower bound: 0.8611, upper bound: 0.8895 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 6112 samples: 0.8758
Epoch 1/1, Loss after 6240 samples: 0.2825
Epoch 1/1, Loss after 6464 samples: 0.2241
Answering fact completion questions...: 100%|██████████| 32/32 [00:00<00:00, 69.36it/s]
Answering fact completion questions...: 100%|██████████| 32/32 [00:00<00:00, 69.16it/s]
Answering fact completion questions...: 100%|██████████| 32/32 [00:00<00:00, 69.16it/s]
Average Loss on fact answering task after 6624 samples: 6.4510
Average Loss on fact answering task after 6624 samples: 6.5443
Answering fact completion questions...:  88%|████████▊ | 28/32 [00:00<00:00, 69.03it/s]
Answering fact completion questions...:  88%|████████▊ | 28/32 [00:00<00:00, 69.03it/s]
Average Loss on fact answering task after 6624 samples: 6.3415
Mean accuracy: 0.8977, std: 0.0068, lower bound: 0.8839, upper bound: 0.9103 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 6624 samples: 0.8976
Epoch 1/1, Loss after 6688 samples: 0.2070
Epoch 1/1, Loss after 6912 samples: 0.2373
Epoch 1/1, Loss after 7136 samples: 0.1789
Average Loss on fact answering task after 7136 samples: 6.5637
Answering fact completion questions...: 100%|██████████| 32/32 [00:00<00:00, 69.52it/s]ing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
Answering fact completion questions...: 100%|██████████| 32/32 [00:00<00:00, 69.52it/s]ing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
Average Loss on fact answering task after 7136 samples: 6.7161
Average Loss on fact answering task after 7136 samples: 6.4762
Average Loss on fact answering task after 7136 samples: 6.6205
Answering fact completion questions...: 100%|██████████| 32/32 [00:00<00:00, 69.52it/s]ing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
Mean accuracy: 0.8888, std: 0.0072, lower bound: 0.8742, upper bound: 0.9021 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 7136 samples: 0.8889
Epoch 1/1, Loss after 7360 samples: 0.2428
- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Average Loss on fact answering task after 7648 samples: 6.5879
Average Loss on fact answering task after 7648 samples: 6.3708
Average Loss on fact answering task after 7648 samples: 6.1533
Answering fact completion questions...: 100%|██████████| 32/32 [00:00<00:00, 69.45it/s]f a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Answering fact completion questions...: 100%|██████████| 32/32 [00:00<00:00, 69.45it/s]f a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Average Loss on fact answering task after 7648 samples: 6.4274
Mean accuracy: 0.9044, std: 0.0066, lower bound: 0.8910, upper bound: 0.9173 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 7648 samples: 0.9042
Best model with eval accuracy 0.904158215010142 with 7648 samples seen is saved
Epoch 1/1, Loss after 7808 samples: 0.2190
Epoch 1/1, Loss after 8032 samples: 0.1851
Answering fact completion questions...: 100%|██████████| 32/32 [00:00<00:00, 69.34it/s]f a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Answering fact completion questions...: 100%|██████████| 32/32 [00:00<00:00, 69.34it/s]f a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Average Loss on fact answering task after 8160 samples: 6.3594
Average Loss on fact answering task after 8160 samples: 6.6426
Average Loss on fact answering task after 8160 samples: 6.4871
Average Loss on fact answering task after 8160 samples: 6.3813
Answering fact completion questions...: 100%|██████████| 32/32 [00:00<00:00, 69.34it/s]f a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Mean accuracy: 0.8267, std: 0.0082, lower bound: 0.8103, upper bound: 0.8433 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 8160 samples: 0.8271
Epoch 1/1, Loss after 8256 samples: 0.2456
Answering fact completion questions...:   0%|          | 0/32 [00:00<?, ?it/s]int of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Answering fact completion questions...:   0%|          | 0/32 [00:00<?, ?it/s]int of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Average Loss on fact answering task after 8672 samples: 6.7314
Average Loss on fact answering task after 8672 samples: 6.3097
Average Loss on fact answering task after 8672 samples: 6.7627
Answering fact completion questions...: 100%|██████████| 32/32 [00:00<00:00, 69.17it/s]model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Answering fact completion questions...: 100%|██████████| 32/32 [00:00<00:00, 69.17it/s]model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Average Loss on fact answering task after 8672 samples: 6.5382
Mean accuracy: 0.8086, std: 0.0091, lower bound: 0.7916, upper bound: 0.8266 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 8672 samples: 0.8088
Epoch 1/1, Loss after 8704 samples: 0.2469
Epoch 1/1, Loss after 8928 samples: 0.2216
Answering fact completion questions...: 100%|██████████| 32/32 [00:00<00:00, 69.17it/s]model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Epoch 1/1, Loss after 9152 samples: 0.1827
Average Loss on fact answering task after 9184 samples: 6.4589
Answering fact completion questions...:  91%|█████████ | 29/32 [00:00<00:00, 69.28it/s]model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Answering fact completion questions...:  91%|█████████ | 29/32 [00:00<00:00, 69.28it/s]model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Average Loss on fact answering task after 9184 samples: 6.3175
Average Loss on fact answering task after 9184 samples: 6.2557
Average Loss on fact answering task after 9184 samples: 6.5438
Answering fact completion questions...:  91%|█████████ | 29/32 [00:00<00:00, 69.28it/s]model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Mean accuracy: 0.8966, std: 0.0066, lower bound: 0.8839, upper bound: 0.9108 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 9184 samples: 0.8966
Epoch 1/1, Loss after 9376 samples: 0.2241
Epoch 1/1, Loss after 9600 samples: 0.1671
Answering fact completion questions...: 100%|██████████| 32/32 [00:00<00:00, 69.16it/s]ing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ForSequenceClassification model from a BertForPreTraining model).
Answering fact completion questions...: 100%|██████████| 32/32 [00:00<00:00, 69.16it/s]ing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ForSequenceClassification model from a BertForPreTraining model).
Average Loss on fact answering task after 9696 samples: 6.4903
Average Loss on fact answering task after 9696 samples: 6.5158
Average Loss on fact answering task after 9696 samples: 6.6127
Average Loss on fact answering task after 9696 samples: 6.2794
Answering fact completion questions...: 100%|██████████| 32/32 [00:00<00:00, 69.16it/s]ing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ForSequenceClassification model from a BertForPreTraining model).
Mean accuracy: 0.9002, std: 0.0066, lower bound: 0.8874, upper bound: 0.9128 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 9696 samples: 0.9001
Epoch 1/1, Loss after 9824 samples: 0.2563
- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
Average Loss on fact answering task after 10208 samples: 6.5323
Answering fact completion questions...:  88%|████████▊ | 28/32 [00:00<00:00, 69.39it/s]f a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
Answering fact completion questions...:  88%|████████▊ | 28/32 [00:00<00:00, 69.39it/s]f a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
Average Loss on fact answering task after 10208 samples: 6.3249
Average Loss on fact answering task after 10208 samples: 6.1496
Answering fact completion questions...: 100%|██████████| 32/32 [00:00<00:00, 69.70it/s]f a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 32/32 [00:00<00:00, 69.70it/s]f a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
Mean accuracy: 0.8890, std: 0.0072, lower bound: 0.8742, upper bound: 0.9026 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 10208 samples: 0.8895
Epoch 1/1, Loss after 10272 samples: 0.2110
Epoch 1/1, Loss after 10496 samples: 0.2017
Epoch 1/1, Loss after 10720 samples: 0.2196
Answering fact completion questions...: 100%|██████████| 32/32 [00:00<00:00, 68.85it/s]f a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 32/32 [00:00<00:00, 68.85it/s]f a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
Average Loss on fact answering task after 10720 samples: 6.6072
Average Loss on fact answering task after 10720 samples: 6.3505
Average Loss on fact answering task after 10720 samples: 6.2885
Average Loss on fact answering task after 10720 samples: 6.3198
Answering fact completion questions...: 100%|██████████| 32/32 [00:00<00:00, 68.85it/s]f a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
Mean accuracy: 0.8911, std: 0.0073, lower bound: 0.8768, upper bound: 0.9047 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 10720 samples: 0.8910
Epoch 1/1, Loss after 10944 samples: 0.1798
- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Average Loss on fact answering task after 11232 samples: 6.5521
Average Loss on fact answering task after 11232 samples: 6.4161
Answering fact completion questions...: 100%|██████████| 32/32 [00:00<00:00, 69.37it/s]model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Answering fact completion questions...: 100%|██████████| 32/32 [00:00<00:00, 69.37it/s]model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Average Loss on fact answering task after 11232 samples: 6.4486
Average Loss on fact answering task after 11232 samples: 6.3724
Mean accuracy: 0.9045, std: 0.0067, lower bound: 0.8910, upper bound: 0.9168 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 11232 samples: 0.9047
Best model with eval accuracy 0.9046653144016227 with 11232 samples seen is saved
Epoch 1/1, Loss after 11392 samples: 0.2006
Answering fact completion questions...: 100%|██████████| 32/32 [00:00<00:00, 67.45it/s]model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Answering fact completion questions...: 100%|██████████| 32/32 [00:00<00:00, 67.45it/s]model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Average Loss on fact answering task after 11744 samples: 6.4843
Average Loss on fact answering task after 11744 samples: 6.4075
Average Loss on fact answering task after 11744 samples: 6.4724
Answering fact completion questions...:  88%|████████▊ | 28/32 [00:00<00:00, 67.54it/s]model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Answering fact completion questions...:  88%|████████▊ | 28/32 [00:00<00:00, 67.54it/s]model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Average Loss on fact answering task after 11744 samples: 6.5938
Mean accuracy: 0.8823, std: 0.0072, lower bound: 0.8676, upper bound: 0.8960 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 11744 samples: 0.8824
Epoch 1/1, Loss after 11840 samples: 0.1623
Answering fact completion questions...:  25%|██▌       | 8/32 [00:00<00:00, 70.49it/s]zing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ForSequenceClassification model from a BertForPreTraining model).
Answering fact completion questions...:  25%|██▌       | 8/32 [00:00<00:00, 70.49it/s]zing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ForSequenceClassification model from a BertForPreTraining model).
Average Loss on fact answering task after 12256 samples: 6.4947
Average Loss on fact answering task after 12256 samples: 6.5087
Answering fact completion questions...: 100%|██████████| 32/32 [00:00<00:00, 69.23it/s]ing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ForSequenceClassification model from a BertForPreTraining model).
Answering fact completion questions...: 100%|██████████| 32/32 [00:00<00:00, 69.23it/s]ing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ForSequenceClassification model from a BertForPreTraining model).
Average Loss on fact answering task after 12256 samples: 6.5193
Average Loss on fact answering task after 12256 samples: 6.5048
Mean accuracy: 0.8547, std: 0.0078, lower bound: 0.8403, upper bound: 0.8697 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 12256 samples: 0.8545
Epoch 1/1, Loss after 12288 samples: 0.1793
Epoch 1/1, Loss after 12512 samples: 0.1543
Epoch 1/1, Loss after 12736 samples: 0.2054
Average Loss on fact answering task after 12768 samples: 6.7067
Answering fact completion questions...:  91%|█████████ | 29/32 [00:00<00:00, 69.37it/s]f a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
Answering fact completion questions...:  91%|█████████ | 29/32 [00:00<00:00, 69.37it/s]f a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
Average Loss on fact answering task after 12768 samples: 6.6567
Average Loss on fact answering task after 12768 samples: 6.6037
Answering fact completion questions...: 100%|██████████| 32/32 [00:00<00:00, 69.96it/s]f a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 32/32 [00:00<00:00, 69.96it/s]f a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
Mean accuracy: 0.8901, std: 0.0071, lower bound: 0.8763, upper bound: 0.9037 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 12768 samples: 0.8905
Epoch 1/1, Loss after 12960 samples: 0.2227
Answering fact completion questions...: 100%|██████████| 32/32 [00:00<00:00, 69.40it/s]f a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 32/32 [00:00<00:00, 69.40it/s]f a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
Average Loss on fact answering task after 13280 samples: 6.5117
Average Loss on fact answering task after 13280 samples: 6.4306
Average Loss on fact answering task after 13280 samples: 6.4420
Average Loss on fact answering task after 13280 samples: 6.5443
Answering fact completion questions...: 100%|██████████| 32/32 [00:00<00:00, 69.40it/s]f a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
Average Loss on fact answering task after 13280 samples: 6.5216
Mean accuracy: 0.8717, std: 0.0075, lower bound: 0.8570, upper bound: 0.8864 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 13280 samples: 0.8717
- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Epoch 1/1, Loss after 13632 samples: 0.1452
- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Average Loss on fact answering task after 13792 samples: 6.4336
- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Answering fact completion questions...:  41%|████      | 13/32 [00:00<00:00, 58.53it/s]model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Answering fact completion questions...:  41%|████      | 13/32 [00:00<00:00, 58.53it/s]model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Answering fact completion questions...:  84%|████████▍ | 27/32 [00:00<00:00, 64.27it/s]model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Answering fact completion questions...:  84%|████████▍ | 27/32 [00:00<00:00, 64.27it/s]model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Answering fact completion questions...: 100%|██████████| 32/32 [00:00<00:00, 61.42it/s]model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Answering fact completion questions...: 100%|██████████| 32/32 [00:00<00:00, 61.42it/s]model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Average Loss on fact answering task after 13792 samples: 6.3929
Answering fact completion questions...: 100%|██████████| 32/32 [00:00<00:00, 61.42it/s]model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Mean accuracy: 0.8399, std: 0.0085, lower bound: 0.8240, upper bound: 0.8560 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 13792 samples: 0.8398
Epoch 1/1, Loss after 13856 samples: 0.1721
Epoch 1/1, Loss after 14080 samples: 0.2018
Epoch 1/1, Loss after 14304 samples: 0.1688
Answering fact completion questions...: 100%|██████████| 32/32 [00:00<00:00, 61.42it/s]model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Average Loss on fact answering task after 14304 samples: 6.3369
Answering fact completion questions...: 100%|██████████| 32/32 [00:00<00:00, 59.97it/s]model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Answering fact completion questions...: 100%|██████████| 32/32 [00:00<00:00, 59.97it/s]model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Answering fact completion questions...:   0%|          | 0/32 [00:00<?, ?it/s]9.97it/s]model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Answering fact completion questions...:   0%|          | 0/32 [00:00<?, ?it/s]9.97it/s]model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Average Loss on fact answering task after 14304 samples: 6.3654
Answering fact completion questions...:  88%|████████▊ | 28/32 [00:00<00:00, 68.45it/s]model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Answering fact completion questions...:  88%|████████▊ | 28/32 [00:00<00:00, 68.45it/s]model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Mean accuracy: 0.8859, std: 0.0069, lower bound: 0.8722, upper bound: 0.8986 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 14304 samples: 0.8859
Epoch 1/1, Loss after 14528 samples: 0.1345
Epoch 1/1, Loss after 14752 samples: 0.2052
Answering fact completion questions...:  88%|████████▊ | 28/32 [00:00<00:00, 67.87it/s]ing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ForSequenceClassification model from a BertForPreTraining model).
Answering fact completion questions...:  88%|████████▊ | 28/32 [00:00<00:00, 67.87it/s]ing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ForSequenceClassification model from a BertForPreTraining model).
Average Loss on fact answering task after 14816 samples: 6.6355
Average Loss on fact answering task after 14816 samples: 6.5195
Average Loss on fact answering task after 14816 samples: 6.4042
Answering fact completion questions...: 100%|██████████| 32/32 [00:00<00:00, 69.14it/s]ing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ForSequenceClassification model from a BertForPreTraining model).
Answering fact completion questions...: 100%|██████████| 32/32 [00:00<00:00, 69.14it/s]ing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ForSequenceClassification model from a BertForPreTraining model).
Mean accuracy: 0.8795, std: 0.0073, lower bound: 0.8656, upper bound: 0.8930 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 14816 samples: 0.8798
Epoch 1/1, Loss after 14976 samples: 0.1533
- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
Average Loss on fact answering task after 15328 samples: 6.4519
Average Loss on fact answering task after 15328 samples: 6.6760
Answering fact completion questions...:  88%|████████▊ | 28/32 [00:00<00:00, 68.10it/s]f a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
Answering fact completion questions...:  88%|████████▊ | 28/32 [00:00<00:00, 68.10it/s]f a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
Average Loss on fact answering task after 15328 samples: 6.7341
Answering fact completion questions...: 100%|██████████| 32/32 [00:00<00:00, 68.34it/s]f a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 32/32 [00:00<00:00, 68.34it/s]f a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
Mean accuracy: 0.8471, std: 0.0083, lower bound: 0.8296, upper bound: 0.8636 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 15328 samples: 0.8469
Epoch 1/1, Loss after 15424 samples: 0.2028
Epoch 1/1, Loss after 15648 samples: 0.1613
Training signal is False, stopping training
Training finished
Best model: {'eval_acc': 0.9046653144016227, 'nb_samples': 11232}
Training loss logs: [{'samples': 192, 'loss': 0.6985179356166294}, {'samples': 416, 'loss': 0.6907108851841518}, {'samples': 640, 'loss': 0.6823316301618304}, {'samples': 864, 'loss': 0.6772548130580357}, {'samples': 1088, 'loss': 0.6436407906668526}, {'samples': 1312, 'loss': 0.5378808294023786}, {'samples': 1536, 'loss': 0.42545043570654734}, {'samples': 1760, 'loss': 0.4000239372253418}, {'samples': 1984, 'loss': 0.36351380603654043}, {'samples': 2208, 'loss': 0.34968274406024386}, {'samples': 2432, 'loss': 0.3179038805621011}, {'samples': 2656, 'loss': 0.2895506066935403}, {'samples': 2880, 'loss': 0.27479320232357296}, {'samples': 3104, 'loss': 0.27933469840458464}, {'samples': 3328, 'loss': 0.2828832502876009}, {'samples': 3552, 'loss': 0.3408484288624355}, {'samples': 3776, 'loss': 0.28428400840078083}, {'samples': 4000, 'loss': 0.2917353915316718}, {'samples': 4224, 'loss': 0.28000703028270174}, {'samples': 4448, 'loss': 0.22521394065448216}, {'samples': 4672, 'loss': 0.24615099387509481}, {'samples': 4896, 'loss': 0.26892709732055664}, {'samples': 5120, 'loss': 0.2770013234445027}, {'samples': 5344, 'loss': 0.2350882121494838}, {'samples': 5568, 'loss': 0.24678338212626322}, {'samples': 5792, 'loss': 0.2688008759702955}, {'samples': 6016, 'loss': 0.279280566743442}, {'samples': 6240, 'loss': 0.28248865476676394}, {'samples': 6464, 'loss': 0.22406152955123357}, {'samples': 6688, 'loss': 0.20702426774161203}, {'samples': 6912, 'loss': 0.23730927918638503}, {'samples': 7136, 'loss': 0.17891437347446168}, {'samples': 7360, 'loss': 0.2428271078637668}, {'samples': 7584, 'loss': 0.20251416202102387}, {'samples': 7808, 'loss': 0.2190130991595132}, {'samples': 8032, 'loss': 0.18506652116775513}, {'samples': 8256, 'loss': 0.24558049546820776}, {'samples': 8480, 'loss': 0.1636595263012818}, {'samples': 8704, 'loss': 0.246926092675754}, {'samples': 8928, 'loss': 0.22157120491777146}, {'samples': 9152, 'loss': 0.18273572836603438}, {'samples': 9376, 'loss': 0.2241170502134732}, {'samples': 9600, 'loss': 0.1670935239110674}, {'samples': 9824, 'loss': 0.25628522464207243}, {'samples': 10048, 'loss': 0.2075357245547431}, {'samples': 10272, 'loss': 0.2109579358782087}, {'samples': 10496, 'loss': 0.20173632885728562}, {'samples': 10720, 'loss': 0.21955774937357223}, {'samples': 10944, 'loss': 0.17975566004003798}, {'samples': 11168, 'loss': 0.17276238011462347}, {'samples': 11392, 'loss': 0.20055988005229405}, {'samples': 11616, 'loss': 0.19281109741755895}, {'samples': 11840, 'loss': 0.16233909662280763}, {'samples': 12064, 'loss': 0.16812128041471755}, {'samples': 12288, 'loss': 0.17930698607649123}, {'samples': 12512, 'loss': 0.15430724301508494}, {'samples': 12736, 'loss': 0.20544422205005372}, {'samples': 12960, 'loss': 0.22267183022839682}, {'samples': 13184, 'loss': 0.1337252924484866}, {'samples': 13408, 'loss': 0.1889966811452593}, {'samples': 13632, 'loss': 0.14519312605261803}, {'samples': 13856, 'loss': 0.1720587717635291}, {'samples': 14080, 'loss': 0.2018132678100041}, {'samples': 14304, 'loss': 0.16878426128200122}, {'samples': 14528, 'loss': 0.13451094499656133}, {'samples': 14752, 'loss': 0.20517584149326598}, {'samples': 14976, 'loss': 0.15327378575290954}, {'samples': 15200, 'loss': 0.24132880994251796}, {'samples': 15424, 'loss': 0.20283798021929605}, {'samples': 15648, 'loss': 0.1613188087940216}]
Evaluation accuracy logs: [{'samples': 480, 'accuracy': 0.6451779918864097, 'std': 0.010532251946615523, 'lower_bound': 0.6242266734279919, 'upper_bound': 0.6648199797160244}, {'samples': 992, 'accuracy': 0.7483767748478701, 'std': 0.009529213896203764, 'lower_bound': 0.7297033468559837, 'upper_bound': 0.7682555780933062}, {'samples': 1504, 'accuracy': 0.8038813387423935, 'std': 0.009056850270928738, 'lower_bound': 0.7870182555780934, 'upper_bound': 0.8209939148073022}, {'samples': 2016, 'accuracy': 0.7658610547667343, 'std': 0.008932447695994994, 'lower_bound': 0.7474645030425964, 'upper_bound': 0.7829741379310345}, {'samples': 2528, 'accuracy': 0.8431227180527384, 'std': 0.008145363686557982, 'lower_bound': 0.8275862068965517, 'upper_bound': 0.8600405679513184}, {'samples': 3040, 'accuracy': 0.8354487829614604, 'std': 0.00831850106243765, 'lower_bound': 0.8189528397565923, 'upper_bound': 0.8514198782961461}, {'samples': 3552, 'accuracy': 0.8892292089249493, 'std': 0.007110835459048181, 'lower_bound': 0.875747971602434, 'upper_bound': 0.9026369168356998}, {'samples': 4064, 'accuracy': 0.8929852941176469, 'std': 0.00687054948935999, 'lower_bound': 0.8792976673427991, 'upper_bound': 0.9056795131845842}, {'samples': 4576, 'accuracy': 0.8058184584178499, 'std': 0.009008182501164886, 'lower_bound': 0.7870182555780934, 'upper_bound': 0.8230223123732252}, {'samples': 5088, 'accuracy': 0.8972854969574037, 'std': 0.0069851204684592905, 'lower_bound': 0.8828473630831642, 'upper_bound': 0.9102434077079108}, {'samples': 5600, 'accuracy': 0.8969153144016228, 'std': 0.007026738339166213, 'lower_bound': 0.8828600405679513, 'upper_bound': 0.9107505070993914}, {'samples': 6112, 'accuracy': 0.8757647058823529, 'std': 0.007361573166430728, 'lower_bound': 0.8610547667342799, 'upper_bound': 0.8894523326572008}, {'samples': 6624, 'accuracy': 0.8977150101419877, 'std': 0.006805409184785918, 'lower_bound': 0.8838742393509128, 'upper_bound': 0.9102560851926977}, {'samples': 7136, 'accuracy': 0.8887525354969573, 'std': 0.0072403707231092, 'lower_bound': 0.8742393509127789, 'upper_bound': 0.902129817444219}, {'samples': 7648, 'accuracy': 0.9043661257606491, 'std': 0.006596189751179858, 'lower_bound': 0.890960953346856, 'upper_bound': 0.9173427991886409}, {'samples': 8160, 'accuracy': 0.8267200811359027, 'std': 0.0081607203671961, 'lower_bound': 0.8103321501014198, 'upper_bound': 0.8433062880324543}, {'samples': 8672, 'accuracy': 0.8085856997971602, 'std': 0.009092152642918051, 'lower_bound': 0.7915821501014199, 'upper_bound': 0.8265720081135902}, {'samples': 9184, 'accuracy': 0.8965983772819472, 'std': 0.006638927260882879, 'lower_bound': 0.8838742393509128, 'upper_bound': 0.9107505070993914}, {'samples': 9696, 'accuracy': 0.9001541582150102, 'std': 0.0065769673648394976, 'lower_bound': 0.8874239350912779, 'upper_bound': 0.9127789046653144}, {'samples': 10208, 'accuracy': 0.8890425963488845, 'std': 0.007166538728519241, 'lower_bound': 0.8742393509127789, 'upper_bound': 0.9026369168356998}, {'samples': 10720, 'accuracy': 0.8911419878296145, 'std': 0.007337428134341537, 'lower_bound': 0.8767748478701826, 'upper_bound': 0.9046653144016227}, {'samples': 11232, 'accuracy': 0.904450304259635, 'std': 0.006738551945399611, 'lower_bound': 0.890973630831643, 'upper_bound': 0.9168356997971603}, {'samples': 11744, 'accuracy': 0.8822555780933062, 'std': 0.007237520633732141, 'lower_bound': 0.8676343813387424, 'upper_bound': 0.8960446247464503}, {'samples': 12256, 'accuracy': 0.8547358012170385, 'std': 0.007807066118158815, 'lower_bound': 0.84026369168357, 'upper_bound': 0.8696881338742394}, {'samples': 12768, 'accuracy': 0.8900958417849898, 'std': 0.007063962579158993, 'lower_bound': 0.8762677484787018, 'upper_bound': 0.9036511156186613}, {'samples': 13280, 'accuracy': 0.8717276876267748, 'std': 0.00747641985679941, 'lower_bound': 0.856985294117647, 'upper_bound': 0.8864097363083164}, {'samples': 13792, 'accuracy': 0.8398909736308315, 'std': 0.008515356591250212, 'lower_bound': 0.8240365111561866, 'upper_bound': 0.8559837728194726}, {'samples': 14304, 'accuracy': 0.8859310344827586, 'std': 0.006889778322742834, 'lower_bound': 0.8722109533468559, 'upper_bound': 0.898592799188641}, {'samples': 14816, 'accuracy': 0.8794558823529413, 'std': 0.0072543333991942766, 'lower_bound': 0.8656059837728194, 'upper_bound': 0.8930020283975659}, {'samples': 15328, 'accuracy': 0.8471227180527383, 'std': 0.008313667168337632, 'lower_bound': 0.8296146044624746, 'upper_bound': 0.8635902636916836}]