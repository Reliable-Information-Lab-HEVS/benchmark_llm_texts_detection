Model: distil_roberta-base
dataset_path: ./fake_true_datasets/fake_true_dataset_mistral_10k
num_epochs: 1
batch_size: 8
learning_rate: 0.0003
warmup_ratio: 0.1
weight_decay: 0.01
device: cuda
evaluation: False
model_path: model
log_mode: online
freeze_base: True
save_dir: ./saved_training_logs_experiment_1
fp16: True
check_degradation: 500
add_more_layers: False
use_adapter: True
