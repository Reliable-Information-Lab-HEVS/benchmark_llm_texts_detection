Parameter 'function'=<function DetectorTrainer.load_fact_checking_dataset.<locals>.<lambda> at 0x14cb22989080> of the transform datasets.arrow_dataset.Dataset.filter@2.0.1 couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.




Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).

Answering fact completion questions...:  92%|█████████▏| 115/125 [00:02<00:00, 55.75it/s]
Answering fact completion questions...:  97%|█████████▋| 121/125 [00:02<00:00, 55.69it/s]
Answering fact completion questions...:  82%|████████▏ | 102/125 [00:01<00:00, 56.01it/s]
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:02<00:00, 56.15it/s]
Answering fact completion questions...:  72%|███████▏  | 90/125 [00:01<00:00, 56.32it/s]
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:02<00:00, 55.72it/s]
Answering fact completion questions...:  58%|█████▊    | 72/125 [00:01<00:00, 56.46it/s]
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:02<00:00, 56.29it/s]
Answering fact completion questions...:  48%|████▊     | 60/125 [00:01<00:01, 55.99it/s]
Average Loss on fact answering task after 0 samples: 5.9736

Answering fact completion questions...:  96%|█████████▌| 120/125 [00:02<00:00, 56.21it/s]
Epoch 1/1, Loss after 192 samples: 0.6952
Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).

Answering fact completion questions...:  91%|█████████ | 114/125 [00:02<00:00, 55.97it/s]
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:02<00:00, 55.80it/s]
Answering fact completion questions...:  34%|███▎      | 42/125 [00:00<00:01, 55.62it/s]

Answering fact completion questions...:  96%|█████████▌| 120/125 [00:02<00:00, 55.39it/s]
Answering fact completion questions...:  77%|███████▋  | 96/125 [00:01<00:00, 55.67it/s]
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:02<00:00, 55.84it/s]
Answering fact completion questions...:  62%|██████▏   | 78/125 [00:01<00:00, 55.53it/s]
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:02<00:00, 55.52it/s]
Answering fact completion questions...:  48%|████▊     | 60/125 [00:01<00:01, 55.77it/s]

Answering fact completion questions...:  96%|█████████▌| 120/125 [00:02<00:00, 55.54it/s]
Mean accuracy: 0.6355, std: 0.0111, lower bound: 0.6141, upper bound: 0.6567 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 496 samples: 0.6359
Best model with eval accuracy 0.6359026369168357 with 496 samples seen is saved
Epoch 1/1, Loss after 592 samples: 0.6784
Epoch 1/1, Loss after 792 samples: 0.6455
Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:02<00:00, 56.02it/s]
Answering fact completion questions...:  19%|█▉        | 24/125 [00:00<00:01, 56.41it/s]
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:02<00:00, 56.14it/s]
Answering fact completion questions...:   5%|▍         | 6/125 [00:00<00:02, 56.45it/s]

Answering fact completion questions...:  96%|█████████▌| 120/125 [00:02<00:00, 55.28it/s]
Average Loss on fact answering task after 1000 samples: 6.3030
Answering fact completion questions...:  82%|████████▏ | 102/125 [00:01<00:00, 56.09it/s]
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:02<00:00, 56.18it/s]
Answering fact completion questions...:  72%|███████▏  | 90/125 [00:01<00:00, 56.03it/s]

Answering fact completion questions...:  96%|█████████▌| 120/125 [00:02<00:00, 55.96it/s]
Mean accuracy: 0.8341, std: 0.0082, lower bound: 0.8179, upper bound: 0.8509 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 1000 samples: 0.8342
Best model with eval accuracy 0.8341784989858012 with 1000 samples seen is saved
Epoch 1/1, Loss after 1192 samples: 0.4326
Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Answering fact completion questions...:  53%|█████▎    | 66/125 [00:01<00:01, 55.68it/s]
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:02<00:00, 55.37it/s]

Answering fact completion questions...:  91%|█████████ | 114/125 [00:02<00:00, 56.15it/s]
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:02<00:00, 56.26it/s]
Answering fact completion questions...:  38%|███▊      | 48/125 [00:00<00:01, 55.89it/s]

Answering fact completion questions...:  96%|█████████▌| 120/125 [00:02<00:00, 56.10it/s]
Answering fact completion questions...:  72%|███████▏  | 90/125 [00:01<00:00, 56.16it/s]
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:02<00:00, 56.04it/s]

Answering fact completion questions...:  96%|█████████▌| 120/125 [00:02<00:00, 55.95it/s]
Average Loss on fact answering task after 1504 samples: 6.4144
Mean accuracy: 0.8618, std: 0.0080, lower bound: 0.8463, upper bound: 0.8768 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 1504 samples: 0.8616
Best model with eval accuracy 0.8615618661257607 with 1504 samples seen is saved
Epoch 1/1, Loss after 1592 samples: 0.3943
Epoch 1/1, Loss after 1792 samples: 0.3612
Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).

Answering fact completion questions...:  91%|█████████ | 114/125 [00:02<00:00, 56.04it/s]
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:02<00:00, 56.14it/s]
Answering fact completion questions...:  82%|████████▏ | 102/125 [00:01<00:00, 56.09it/s]
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:02<00:00, 56.32it/s]
Answering fact completion questions...:  67%|██████▋   | 84/125 [00:01<00:00, 55.83it/s]
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:02<00:00, 55.80it/s]
Answering fact completion questions...:  58%|█████▊    | 72/125 [00:01<00:00, 55.71it/s]
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:02<00:00, 55.63it/s]
Answering fact completion questions...:  43%|████▎     | 54/125 [00:00<00:01, 56.00it/s]

Answering fact completion questions...:  96%|█████████▌| 120/125 [00:02<00:00, 56.19it/s]
Mean accuracy: 0.7881, std: 0.0092, lower bound: 0.7693, upper bound: 0.8053 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 2008 samples: 0.7880
Epoch 1/1, Loss after 2192 samples: 0.2453
Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).

Answering fact completion questions...:  96%|█████████▌| 120/125 [00:02<00:00, 55.51it/s]
Average Loss on fact answering task after 2512 samples: 6.3372
Answering fact completion questions...:  82%|████████▏ | 102/125 [00:01<00:00, 56.12it/s]
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:02<00:00, 56.03it/s]
Answering fact completion questions...:  72%|███████▏  | 90/125 [00:01<00:00, 56.03it/s]
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:02<00:00, 55.89it/s]

Answering fact completion questions...:  96%|█████████▌| 120/125 [00:02<00:00, 56.32it/s]
Average Loss on fact answering task after 2512 samples: 6.3084
Answering fact completion questions...:  53%|█████▎    | 66/125 [00:01<00:01, 55.84it/s]

Answering fact completion questions...:  96%|█████████▌| 120/125 [00:02<00:00, 56.06it/s]
Mean accuracy: 0.7850, std: 0.0091, lower bound: 0.7672, upper bound: 0.8032 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 2512 samples: 0.7850
Epoch 1/1, Loss after 2592 samples: 0.2444
Epoch 1/1, Loss after 2792 samples: 0.2883
Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:02<00:00, 55.87it/s]
Answering fact completion questions...:   0%|          | 0/125 [00:00<?, ?it/s]

Answering fact completion questions...:  91%|█████████ | 114/125 [00:02<00:00, 56.36it/s]
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:02<00:00, 56.28it/s]
Answering fact completion questions...:  77%|███████▋  | 96/125 [00:01<00:00, 55.71it/s]
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:02<00:00, 56.29it/s]
Answering fact completion questions...:  67%|██████▋   | 84/125 [00:01<00:00, 55.99it/s]
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:02<00:00, 55.59it/s]
Answering fact completion questions...:  58%|█████▊    | 72/125 [00:01<00:00, 55.91it/s]

Answering fact completion questions...:  96%|█████████▌| 120/125 [00:02<00:00, 55.90it/s]
Mean accuracy: 0.8470, std: 0.0080, lower bound: 0.8322, upper bound: 0.8626 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 3016 samples: 0.8469
Epoch 1/1, Loss after 3192 samples: 0.2616
Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:02<00:00, 55.75it/s]
Answering fact completion questions...:   5%|▍         | 6/125 [00:00<00:02, 54.76it/s]

Answering fact completion questions...:  91%|█████████ | 114/125 [00:02<00:00, 56.02it/s]
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:02<00:00, 55.90it/s]
Answering fact completion questions...:  82%|████████▏ | 102/125 [00:01<00:00, 55.65it/s]
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:02<00:00, 55.66it/s]
Answering fact completion questions...:  67%|██████▋   | 84/125 [00:01<00:00, 55.93it/s]
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:02<00:00, 55.16it/s]
Answering fact completion questions...:  53%|█████▎    | 66/125 [00:01<00:01, 55.14it/s]

Answering fact completion questions...:  96%|█████████▌| 120/125 [00:02<00:00, 55.39it/s]
Mean accuracy: 0.8911, std: 0.0069, lower bound: 0.8778, upper bound: 0.9042 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 3520 samples: 0.8910
Best model with eval accuracy 0.890973630831643 with 3520 samples seen is saved
Epoch 1/1, Loss after 3592 samples: 0.3016
Epoch 1/1, Loss after 3792 samples: 0.2447
Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:02<00:00, 56.50it/s]
Answering fact completion questions...:  14%|█▍        | 18/125 [00:00<00:01, 56.08it/s]
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:02<00:00, 55.46it/s]
Answering fact completion questions...:   0%|          | 0/125 [00:00<?, ?it/s]

Answering fact completion questions...:  91%|█████████ | 114/125 [00:02<00:00, 55.96it/s]
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:02<00:00, 56.07it/s]
Answering fact completion questions...:  82%|████████▏ | 102/125 [00:01<00:00, 55.81it/s]
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:02<00:00, 56.10it/s]
Answering fact completion questions...:  67%|██████▋   | 84/125 [00:01<00:00, 56.19it/s]

Answering fact completion questions...:  96%|█████████▌| 120/125 [00:02<00:00, 56.06it/s]
Mean accuracy: 0.9089, std: 0.0064, lower bound: 0.8960, upper bound: 0.9219 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 4024 samples: 0.9087
Best model with eval accuracy 0.9087221095334685 with 4024 samples seen is saved
Epoch 1/1, Loss after 4192 samples: 0.3775
Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Answering fact completion questions...:  53%|█████▎    | 66/125 [00:01<00:01, 55.57it/s]
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:02<00:00, 55.61it/s]
Answering fact completion questions...:  38%|███▊      | 48/125 [00:00<00:01, 55.53it/s]
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:02<00:00, 55.93it/s]
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:02<00:00, 55.71it/s]
Answering fact completion questions...:  14%|█▍        | 18/125 [00:00<00:01, 55.58it/s]
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:02<00:00, 55.75it/s]
Answering fact completion questions...:   5%|▍         | 6/125 [00:00<00:02, 55.97it/s]

Answering fact completion questions...:  91%|█████████ | 114/125 [00:02<00:00, 56.07it/s]

Answering fact completion questions...:  96%|█████████▌| 120/125 [00:02<00:00, 55.78it/s]
Mean accuracy: 0.9023, std: 0.0067, lower bound: 0.8895, upper bound: 0.9148 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 4528 samples: 0.9026
Epoch 1/1, Loss after 4592 samples: 0.1851
Epoch 1/1, Loss after 4792 samples: 0.2400
Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Answering fact completion questions...:  53%|█████▎    | 66/125 [00:01<00:01, 55.99it/s]
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:02<00:00, 56.15it/s]
Answering fact completion questions...:  38%|███▊      | 48/125 [00:00<00:01, 56.11it/s]
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:02<00:00, 56.60it/s]
Answering fact completion questions...:  29%|██▉       | 36/125 [00:00<00:01, 56.52it/s]
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:02<00:00, 56.72it/s]
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:02<00:00, 55.88it/s]
Answering fact completion questions...:   5%|▍         | 6/125 [00:00<00:02, 56.04it/s]

Answering fact completion questions...:  96%|█████████▌| 120/125 [00:02<00:00, 56.34it/s]
Average Loss on fact answering task after 5032 samples: 6.5800
Mean accuracy: 0.8798, std: 0.0072, lower bound: 0.8656, upper bound: 0.8935 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 5032 samples: 0.8798
Epoch 1/1, Loss after 5192 samples: 0.2088
Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Answering fact completion questions...:  58%|█████▊    | 72/125 [00:01<00:00, 56.12it/s]
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:02<00:00, 56.04it/s]
Answering fact completion questions...:  48%|████▊     | 60/125 [00:01<00:01, 55.65it/s]
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:02<00:00, 55.98it/s]
Answering fact completion questions...:  34%|███▎      | 42/125 [00:00<00:01, 55.81it/s]
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:02<00:00, 56.07it/s]
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:02<00:00, 55.34it/s]
Answering fact completion questions...:  10%|▉         | 12/125 [00:00<00:02, 55.77it/s]

Answering fact completion questions...:  96%|█████████▌| 120/125 [00:02<00:00, 55.20it/s]
Average Loss on fact answering task after 5536 samples: 6.6600
Mean accuracy: 0.9203, std: 0.0062, lower bound: 0.9082, upper bound: 0.9331 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 5536 samples: 0.9204
Best model with eval accuracy 0.9203853955375254 with 5536 samples seen is saved
Epoch 1/1, Loss after 5592 samples: 0.2892
Epoch 1/1, Loss after 5792 samples: 0.2310
Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Answering fact completion questions...:  82%|████████▏ | 102/125 [00:01<00:00, 56.02it/s]
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:02<00:00, 55.84it/s]
Answering fact completion questions...:  67%|██████▋   | 84/125 [00:01<00:00, 55.66it/s]
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:02<00:00, 55.84it/s]
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 55.79it/s]
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 55.79it/s]
Answering fact completion questions...:  48%|████▊     | 60/125 [00:01<00:01, 55.91it/s]]
Answering fact completion questions...:  48%|████▊     | 60/125 [00:01<00:01, 55.91it/s]]
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:02<00:00, 55.87it/s]
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:02<00:00, 55.87it/s]
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:02<00:00, 55.87it/s]
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 55.63it/s]
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 55.63it/s]
Mean accuracy: 0.8384, std: 0.0082, lower bound: 0.8225, upper bound: 0.8535 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 6040 samples: 0.8382
Epoch 1/1, Loss after 6192 samples: 0.2898
- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Answering fact completion questions...:  10%|▉         | 12/125 [00:00<00:01, 56.63it/s] a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Answering fact completion questions...:  10%|▉         | 12/125 [00:00<00:01, 56.63it/s] a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:02<00:00, 56.44it/s]a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:02<00:00, 56.44it/s]a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Average Loss on fact answering task after 6544 samples: 6.6032
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:02<00:00, 56.44it/s]a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 56.24it/s]a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 56.24it/s]a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Answering fact completion questions...:  67%|██████▋   | 84/125 [00:01<00:00, 55.73it/s]]a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Answering fact completion questions...:  67%|██████▋   | 84/125 [00:01<00:00, 55.73it/s]]a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:02<00:00, 55.71it/s]a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:02<00:00, 55.71it/s]a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Mean accuracy: 0.8394, std: 0.0084, lower bound: 0.8225, upper bound: 0.8555 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 6544 samples: 0.8392
Epoch 1/1, Loss after 6592 samples: 0.2301
Epoch 1/1, Loss after 6792 samples: 0.1826
Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 55.51it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 55.51it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).
Answering fact completion questions...:   5%|▍         | 6/125 [00:00<00:02, 56.77it/s]s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).
Answering fact completion questions...:   5%|▍         | 6/125 [00:00<00:02, 56.77it/s]s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).
Answering fact completion questions...:  91%|█████████ | 114/125 [00:02<00:00, 55.86it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).
Answering fact completion questions...:  91%|█████████ | 114/125 [00:02<00:00, 55.86it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:02<00:00, 55.74it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:02<00:00, 55.74it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 55.77it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 55.77it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).
Average Loss on fact answering task after 7048 samples: 6.3488
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 55.77it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).
Mean accuracy: 0.9252, std: 0.0061, lower bound: 0.9133, upper bound: 0.9371 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 7048 samples: 0.9249
Best model with eval accuracy 0.9249492900608519 with 7048 samples seen is saved
Epoch 1/1, Loss after 7192 samples: 0.2501
- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Answering fact completion questions...:  77%|███████▋  | 96/125 [00:01<00:00, 56.16it/s]odel trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Answering fact completion questions...:  77%|███████▋  | 96/125 [00:01<00:00, 56.16it/s]odel trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:02<00:00, 56.29it/s]del trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 56.16it/s]del trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 56.16it/s]del trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Answering fact completion questions...:   0%|          | 0/125 [00:00<?, ?it/s]56.16it/s]del trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Answering fact completion questions...:   0%|          | 0/125 [00:00<?, ?it/s]56.16it/s]del trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Answering fact completion questions...:  86%|████████▋ | 108/125 [00:01<00:00, 56.16it/s]del trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Answering fact completion questions...:  86%|████████▋ | 108/125 [00:01<00:00, 56.16it/s]del trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:02<00:00, 56.35it/s]del trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:02<00:00, 56.35it/s]del trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 55.96it/s]del trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 55.96it/s]del trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Mean accuracy: 0.8502, std: 0.0083, lower bound: 0.8337, upper bound: 0.8661 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 7552 samples: 0.8504
Epoch 1/1, Loss after 7592 samples: 0.2043
Epoch 1/1, Loss after 7792 samples: 0.2144
- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
Answering fact completion questions...:  10%|▉         | 12/125 [00:00<00:02, 56.29it/s] a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:02<00:00, 56.07it/s]a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:02<00:00, 56.07it/s]a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
Average Loss on fact answering task after 8056 samples: 6.6409
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:02<00:00, 56.07it/s]a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 56.04it/s]a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 56.04it/s]a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
Answering fact completion questions...:  67%|██████▋   | 84/125 [00:01<00:00, 56.06it/s]]a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
Answering fact completion questions...:  67%|██████▋   | 84/125 [00:01<00:00, 56.06it/s]]a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:02<00:00, 55.52it/s]a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:02<00:00, 55.52it/s]a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
Mean accuracy: 0.8395, std: 0.0081, lower bound: 0.8235, upper bound: 0.8550 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 8056 samples: 0.8398
Epoch 1/1, Loss after 8192 samples: 0.1761
Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 55.27it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 55.27it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...:   0%|          | 0/125 [00:00<?, ?it/s]55.27it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...:   0%|          | 0/125 [00:00<?, ?it/s]55.27it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...:  91%|█████████ | 114/125 [00:02<00:00, 55.98it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...:  91%|█████████ | 114/125 [00:02<00:00, 55.98it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:02<00:00, 55.72it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:02<00:00, 55.72it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 55.85it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 55.85it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Average Loss on fact answering task after 8560 samples: 6.3995
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 55.85it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Mean accuracy: 0.8517, std: 0.0082, lower bound: 0.8357, upper bound: 0.8676 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 8560 samples: 0.8514
Epoch 1/1, Loss after 8592 samples: 0.1925
Epoch 1/1, Loss after 8792 samples: 0.1926
Answering fact completion questions...:  24%|██▍       | 30/125 [00:00<00:01, 56.24it/s]odel trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:02<00:00, 56.64it/s]del trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:02<00:00, 56.64it/s]del trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 55.77it/s]del trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 55.77it/s]del trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Average Loss on fact answering task after 9064 samples: 6.4760
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 55.77it/s]del trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Answering fact completion questions...:  82%|████████▏ | 102/125 [00:01<00:00, 55.95it/s]del trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Answering fact completion questions...:  82%|████████▏ | 102/125 [00:01<00:00, 55.95it/s]del trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:02<00:00, 55.83it/s]del trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:02<00:00, 55.83it/s]del trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 55.81it/s]del trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 55.81it/s]del trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Mean accuracy: 0.9051, std: 0.0069, lower bound: 0.8920, upper bound: 0.9184 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 9064 samples: 0.9052
Epoch 1/1, Loss after 9192 samples: 0.1397
- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
Answering fact completion questions...:  14%|█▍        | 18/125 [00:00<00:01, 56.40it/s] a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
Answering fact completion questions...:  14%|█▍        | 18/125 [00:00<00:01, 56.40it/s] a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:02<00:00, 56.12it/s]a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:02<00:00, 56.12it/s]a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
Average Loss on fact answering task after 9568 samples: 6.4205
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:02<00:00, 56.12it/s]a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 55.76it/s]a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 55.76it/s]a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 55.74it/s]a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 55.74it/s]a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 55.93it/s]a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 55.93it/s]a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
Mean accuracy: 0.8889, std: 0.0071, lower bound: 0.8753, upper bound: 0.9021 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 9568 samples: 0.8889
Epoch 1/1, Loss after 9592 samples: 0.1918
Epoch 1/1, Loss after 9792 samples: 0.2410
Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 55.83it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 55.83it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Average Loss on fact answering task after 10072 samples: 6.5317
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 55.83it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 55.66it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 55.66it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 55.60it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 55.60it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 55.64it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 55.64it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 55.44it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 55.44it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Mean accuracy: 0.9075, std: 0.0067, lower bound: 0.8945, upper bound: 0.9199 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 10072 samples: 0.9077
Epoch 1/1, Loss after 10192 samples: 0.1684
Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 56.48it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 56.48it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 56.30it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 56.30it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Average Loss on fact answering task after 10576 samples: 6.4367
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 56.30it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 56.19it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 56.19it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 56.32it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 56.32it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 56.09it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 56.09it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Mean accuracy: 0.8712, std: 0.0073, lower bound: 0.8570, upper bound: 0.8854 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 10576 samples: 0.8712
Epoch 1/1, Loss after 10592 samples: 0.1577
Epoch 1/1, Loss after 10792 samples: 0.1618
Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 56.09it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 56.09it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 55.93it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 55.93it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Average Loss on fact answering task after 11080 samples: 6.3515
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 55.93it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 55.60it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 55.60it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 55.77it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 55.77it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 55.22it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 55.22it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Mean accuracy: 0.9007, std: 0.0067, lower bound: 0.8879, upper bound: 0.9138 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 11080 samples: 0.9006
Epoch 1/1, Loss after 11192 samples: 0.1522
Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 55.89it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 55.89it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 55.78it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 55.78it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 55.50it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 55.50it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Average Loss on fact answering task after 11584 samples: 6.5050
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 55.50it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 55.74it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 55.74it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Mean accuracy: 0.9154, std: 0.0064, lower bound: 0.9036, upper bound: 0.9280 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 11584 samples: 0.9153
Epoch 1/1, Loss after 11592 samples: 0.1055
Epoch 1/1, Loss after 11792 samples: 0.1886
Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 56.22it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 56.22it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 56.38it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 56.38it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 56.12it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 56.12it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 56.01it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 56.01it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 55.89it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 55.89it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Mean accuracy: 0.8997, std: 0.0068, lower bound: 0.8864, upper bound: 0.9133 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 12088 samples: 0.8996
Epoch 1/1, Loss after 12192 samples: 0.1466
Epoch 1/1, Loss after 12392 samples: 0.1509
Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 56.01it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 56.01it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 55.91it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 55.91it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 56.01it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 56.01it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 55.61it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 55.61it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 55.24it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 55.24it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Mean accuracy: 0.8302, std: 0.0086, lower bound: 0.8129, upper bound: 0.8463 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 12592 samples: 0.8301
Epoch 1/1, Loss after 12792 samples: 0.2041
Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 55.98it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 55.98it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 55.98it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 56.03it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 56.03it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 56.44it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 56.44it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 56.05it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 56.05it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 56.25it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 56.25it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Mean accuracy: 0.8720, std: 0.0075, lower bound: 0.8575, upper bound: 0.8864 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 13096 samples: 0.8717
Epoch 1/1, Loss after 13192 samples: 0.1064
Epoch 1/1, Loss after 13392 samples: 0.1493
Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 55.98it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 55.98it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 56.08it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 56.08it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 55.78it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 55.78it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 55.75it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 55.75it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 55.82it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 55.82it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Mean accuracy: 0.8511, std: 0.0083, lower bound: 0.8352, upper bound: 0.8666 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 13600 samples: 0.8514
Epoch 1/1, Loss after 13792 samples: 0.1540
Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 55.74it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 55.74it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 48.19it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 48.19it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 48.15it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 47.85it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 47.85it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Average Loss on fact answering task after 14104 samples: 6.3815
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 47.85it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 48.18it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 48.18it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Mean accuracy: 0.9037, std: 0.0064, lower bound: 0.8910, upper bound: 0.9158 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 14104 samples: 0.9037
Epoch 1/1, Loss after 14192 samples: 0.1672
Epoch 1/1, Loss after 14392 samples: 0.1878
Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 49.76it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 48.36it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 48.36it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Average Loss on fact answering task after 14608 samples: 6.2429
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 48.36it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 48.56it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 48.56it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 48.32it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 48.32it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 48.43it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 48.43it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Mean accuracy: 0.9269, std: 0.0056, lower bound: 0.9153, upper bound: 0.9376 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 14608 samples: 0.9265
Best model with eval accuracy 0.9264705882352942 with 14608 samples seen is saved
Epoch 1/1, Loss after 14792 samples: 0.1924
Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 48.55it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 48.55it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 48.19it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 48.22it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 48.22it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Average Loss on fact answering task after 15112 samples: 6.3912
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 48.22it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 48.31it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 48.31it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 48.22it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 48.22it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Mean accuracy: 0.8841, std: 0.0071, lower bound: 0.8702, upper bound: 0.8991 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 15112 samples: 0.8844
Epoch 1/1, Loss after 15192 samples: 0.1716
Epoch 1/1, Loss after 15392 samples: 0.2132
Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 48.29it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 48.24it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 48.24it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Average Loss on fact answering task after 15616 samples: 6.4808
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 48.24it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 48.03it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 48.03it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 50.66it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 50.66it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 55.96it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:02<00:00, 55.96it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Mean accuracy: 0.8656, std: 0.0079, lower bound: 0.8499, upper bound: 0.8803 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 15616 samples: 0.8656
Training signal is False, stopping training
Training finished
Best model: {'eval_acc': 0.9264705882352942, 'nb_samples': 14608}
Training loss logs: [{'samples': 192, 'loss': 0.695234375}, {'samples': 392, 'loss': 0.68927490234375}, {'samples': 592, 'loss': 0.67841796875}, {'samples': 792, 'loss': 0.64550048828125}, {'samples': 992, 'loss': 0.555655089020729}, {'samples': 1192, 'loss': 0.4325718343257904}, {'samples': 1392, 'loss': 0.3982010525465012}, {'samples': 1592, 'loss': 0.3942621469497681}, {'samples': 1792, 'loss': 0.36124906957149505}, {'samples': 1992, 'loss': 0.3467442685365677}, {'samples': 2192, 'loss': 0.2453138554096222}, {'samples': 2392, 'loss': 0.3367198312282562}, {'samples': 2592, 'loss': 0.24441503763198852}, {'samples': 2792, 'loss': 0.2882876396179199}, {'samples': 2992, 'loss': 0.32989797949790955}, {'samples': 3192, 'loss': 0.26158484995365144}, {'samples': 3392, 'loss': 0.23888576209545134}, {'samples': 3592, 'loss': 0.30158922493457796}, {'samples': 3792, 'loss': 0.24472817480564119}, {'samples': 3992, 'loss': 0.2744120842218399}, {'samples': 4192, 'loss': 0.3774751842021942}, {'samples': 4392, 'loss': 0.30671006083488467}, {'samples': 4592, 'loss': 0.18507031738758087}, {'samples': 4792, 'loss': 0.23995872855186462}, {'samples': 4992, 'loss': 0.2691565257310867}, {'samples': 5192, 'loss': 0.20878717303276062}, {'samples': 5392, 'loss': 0.23642607629299164}, {'samples': 5592, 'loss': 0.28923292756080626}, {'samples': 5792, 'loss': 0.2310483920574188}, {'samples': 5992, 'loss': 0.22979062736034395}, {'samples': 6192, 'loss': 0.289766326546669}, {'samples': 6392, 'loss': 0.19914751827716828}, {'samples': 6592, 'loss': 0.23013051807880402}, {'samples': 6792, 'loss': 0.1825515478849411}, {'samples': 6992, 'loss': 0.23927495121955872}, {'samples': 7192, 'loss': 0.2501272666454315}, {'samples': 7392, 'loss': 0.1711496204137802}, {'samples': 7592, 'loss': 0.2043429881334305}, {'samples': 7792, 'loss': 0.2143772542476654}, {'samples': 7992, 'loss': 0.23756962895393371}, {'samples': 8192, 'loss': 0.17612809240818023}, {'samples': 8392, 'loss': 0.1922716647386551}, {'samples': 8592, 'loss': 0.19245449542999268}, {'samples': 8792, 'loss': 0.1925892150402069}, {'samples': 8992, 'loss': 0.1620829313993454}, {'samples': 9192, 'loss': 0.13965295493602753}, {'samples': 9392, 'loss': 0.155446098446846}, {'samples': 9592, 'loss': 0.19182763278484344}, {'samples': 9792, 'loss': 0.24100447475910186}, {'samples': 9992, 'loss': 0.1883550488948822}, {'samples': 10192, 'loss': 0.1683671748638153}, {'samples': 10392, 'loss': 0.18761814177036285}, {'samples': 10592, 'loss': 0.15773594498634338}, {'samples': 10792, 'loss': 0.16183504462242126}, {'samples': 10992, 'loss': 0.18751472890377044}, {'samples': 11192, 'loss': 0.15223634839057923}, {'samples': 11392, 'loss': 0.1517445933818817}, {'samples': 11592, 'loss': 0.10553302824497222}, {'samples': 11792, 'loss': 0.18855331599712372}, {'samples': 11992, 'loss': 0.1399747759103775}, {'samples': 12192, 'loss': 0.1465592271089554}, {'samples': 12392, 'loss': 0.15088256180286408}, {'samples': 12592, 'loss': 0.13445233464241027}, {'samples': 12792, 'loss': 0.20410361766815185}, {'samples': 12992, 'loss': 0.19168010234832764}, {'samples': 13192, 'loss': 0.10636185586452485}, {'samples': 13392, 'loss': 0.1493239277601242}, {'samples': 13592, 'loss': 0.11963112592697143}, {'samples': 13792, 'loss': 0.1539561152458191}, {'samples': 13992, 'loss': 0.16458190023899077}, {'samples': 14192, 'loss': 0.1671522378921509}, {'samples': 14392, 'loss': 0.18780393183231353}, {'samples': 14592, 'loss': 0.09436387479305268}, {'samples': 14792, 'loss': 0.1924475222826004}, {'samples': 14992, 'loss': 0.15907410621643067}, {'samples': 15192, 'loss': 0.17157977163791657}, {'samples': 15392, 'loss': 0.2131525433063507}, {'samples': 15592, 'loss': 0.15981434226036073}]
Evaluation accuracy logs: [{'samples': 496, 'accuracy': 0.6355182555780932, 'std': 0.01105159616404826, 'lower_bound': 0.6140973630831643, 'upper_bound': 0.6566937119675457}, {'samples': 1000, 'accuracy': 0.8340786004056795, 'std': 0.008210566984527117, 'lower_bound': 0.8179386409736308, 'upper_bound': 0.8509127789046653}, {'samples': 1504, 'accuracy': 0.8617712981744421, 'std': 0.008010219275490746, 'lower_bound': 0.8463488843813387, 'upper_bound': 0.8767875253549696}, {'samples': 2008, 'accuracy': 0.7881196754563895, 'std': 0.009172170893540093, 'lower_bound': 0.7692697768762677, 'upper_bound': 0.8052865111561867}, {'samples': 2512, 'accuracy': 0.7850425963488843, 'std': 0.00912721409052382, 'lower_bound': 0.7672287018255578, 'upper_bound': 0.8032454361054767}, {'samples': 3016, 'accuracy': 0.8470005070993915, 'std': 0.008043398443565017, 'lower_bound': 0.8321501014198783, 'upper_bound': 0.8625760649087221}, {'samples': 3520, 'accuracy': 0.891104462474645, 'std': 0.006871850451304369, 'lower_bound': 0.877776369168357, 'upper_bound': 0.904170892494929}, {'samples': 4024, 'accuracy': 0.9089274847870183, 'std': 0.006399284407748458, 'lower_bound': 0.8960446247464503, 'upper_bound': 0.9219066937119675}, {'samples': 4528, 'accuracy': 0.9022631845841784, 'std': 0.0066785554133652805, 'lower_bound': 0.8894523326572008, 'upper_bound': 0.9148073022312373}, {'samples': 5032, 'accuracy': 0.8798057809330628, 'std': 0.007188846568444256, 'lower_bound': 0.8656186612576064, 'upper_bound': 0.8935091277890467}, {'samples': 5536, 'accuracy': 0.9202789046653145, 'std': 0.006226344252770848, 'lower_bound': 0.9082150101419878, 'upper_bound': 0.9330628803245437}, {'samples': 6040, 'accuracy': 0.8383620689655172, 'std': 0.00820776826267496, 'lower_bound': 0.8225152129817445, 'upper_bound': 0.853473630831643}, {'samples': 6544, 'accuracy': 0.8394097363083165, 'std': 0.008397972327526648, 'lower_bound': 0.8225152129817445, 'upper_bound': 0.8554766734279919}, {'samples': 7048, 'accuracy': 0.9252074036511156, 'std': 0.006067882728703796, 'lower_bound': 0.9132860040567952, 'upper_bound': 0.9371196754563894}, {'samples': 7552, 'accuracy': 0.8501663286004056, 'std': 0.008280099489886922, 'lower_bound': 0.8336587221095334, 'upper_bound': 0.8661257606490872}, {'samples': 8056, 'accuracy': 0.8394685598377282, 'std': 0.008089616203406263, 'lower_bound': 0.8235167342799188, 'upper_bound': 0.8549822515212983}, {'samples': 8560, 'accuracy': 0.8516693711967546, 'std': 0.008172641600774367, 'lower_bound': 0.8356997971602435, 'upper_bound': 0.8676470588235294}, {'samples': 9064, 'accuracy': 0.9050826572008114, 'std': 0.006942367929645602, 'lower_bound': 0.8919751521298174, 'upper_bound': 0.9183569979716024}, {'samples': 9568, 'accuracy': 0.8889421906693712, 'std': 0.007050067063617496, 'lower_bound': 0.8752535496957403, 'upper_bound': 0.902129817444219}, {'samples': 10072, 'accuracy': 0.9074944219066938, 'std': 0.006661204776994136, 'lower_bound': 0.894510649087221, 'upper_bound': 0.9198782961460447}, {'samples': 10576, 'accuracy': 0.8712018255578093, 'std': 0.0073155287281054795, 'lower_bound': 0.8569979716024341, 'upper_bound': 0.8853955375253549}, {'samples': 11080, 'accuracy': 0.9006987829614604, 'std': 0.006653644835328288, 'lower_bound': 0.8879183569979716, 'upper_bound': 0.9138057809330629}, {'samples': 11584, 'accuracy': 0.9153762677484788, 'std': 0.0064174247640389846, 'lower_bound': 0.9036384381338742, 'upper_bound': 0.9279918864097363}, {'samples': 12088, 'accuracy': 0.8996703853955376, 'std': 0.006849587661223465, 'lower_bound': 0.8864097363083164, 'upper_bound': 0.9132860040567952}, {'samples': 12592, 'accuracy': 0.8302347870182556, 'std': 0.008612979355338788, 'lower_bound': 0.8128803245436106, 'upper_bound': 0.8463488843813387}, {'samples': 13096, 'accuracy': 0.872048681541582, 'std': 0.007536418279517998, 'lower_bound': 0.8575050709939148, 'upper_bound': 0.8864097363083164}, {'samples': 13600, 'accuracy': 0.8510730223123733, 'std': 0.008322027008899863, 'lower_bound': 0.8351800202839756, 'upper_bound': 0.8666328600405679}, {'samples': 14104, 'accuracy': 0.9036501014198783, 'std': 0.006398983441493312, 'lower_bound': 0.890973630831643, 'upper_bound': 0.9158215010141988}, {'samples': 14608, 'accuracy': 0.9268838742393509, 'std': 0.00561596893539711, 'lower_bound': 0.915301724137931, 'upper_bound': 0.9376267748478702}, {'samples': 15112, 'accuracy': 0.8840689655172413, 'std': 0.007133476277064966, 'lower_bound': 0.8701825557809331, 'upper_bound': 0.8990872210953347}, {'samples': 15616, 'accuracy': 0.8656135902636917, 'std': 0.007924000385004504, 'lower_bound': 0.8498859026369168, 'upper_bound': 0.8803245436105477}]