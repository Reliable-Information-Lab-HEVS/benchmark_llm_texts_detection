Parameter 'function'=<function DetectorTrainer.load_fact_checking_dataset.<locals>.<lambda> at 0x1459dc4e5260> of the transform datasets.arrow_dataset.Dataset.filter@2.0.1 couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.


Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Answering fact completion questions...: 100%|██████████| 63/63 [00:01<00:00, 51.87it/s]
Answering fact completion questions...:  98%|█████████▊| 62/63 [00:01<00:00, 89.01it/s]
Answering fact completion questions...:  67%|██████▋   | 42/63 [00:00<00:00, 132.74it/s]
Average Loss on fact answering task after 0 samples: 6.0821
Average Loss on fact answering task after 0 samples: 6.0401
Average Loss on fact answering task after 0 samples: 6.0629
Answering fact completion questions...: 100%|██████████| 63/63 [00:00<00:00, 132.74it/s]
Answering fact completion questions...: 100%|██████████| 63/63 [00:00<00:00, 134.85it/s]
Answering fact completion questions...: 100%|██████████| 63/63 [00:00<00:00, 133.95it/s]
Answering fact completion questions...:  89%|████████▉ | 56/63 [00:00<00:00, 133.67it/s]
Average Loss on fact answering task after 0 samples: 6.1072
----------------Epoch 1/1----------------
Epoch 1/1, Loss after 192 samples: 0.6988
Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Answering fact completion questions...:   0%|          | 0/63 [00:00<?, ?it/s]
Average Loss on fact answering task after 496 samples: 5.6746
Average Loss on fact answering task after 496 samples: 5.8210
Answering fact completion questions...: 100%|██████████| 63/63 [00:00<00:00, 132.54it/s]
Answering fact completion questions...: 100%|██████████| 63/63 [00:00<00:00, 132.16it/s]
Answering fact completion questions...:  89%|████████▉ | 56/63 [00:00<00:00, 131.99it/s]
Answering fact completion questions...:  89%|████████▉ | 56/63 [00:00<00:00, 131.39it/s]
Average Loss on fact answering task after 496 samples: 5.8552
Answering fact completion questions...: 100%|██████████| 63/63 [00:00<00:00, 131.92it/s]
Answering fact completion questions...:  89%|████████▉ | 56/63 [00:00<00:00, 131.55it/s]
Mean accuracy: 0.6292, std: 0.0110, lower bound: 0.6080, upper bound: 0.6501 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 496 samples: 0.6288
Best model with eval accuracy 0.6288032454361054 with 496 samples seen is saved
Epoch 1/1, Loss after 608 samples: 0.6881
Epoch 1/1, Loss after 816 samples: 0.6457
Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Answering fact completion questions...: 100%|██████████| 63/63 [00:00<00:00, 133.08it/s]
Answering fact completion questions...: 100%|██████████| 63/63 [00:00<00:00, 132.66it/s]
Answering fact completion questions...:  89%|████████▉ | 56/63 [00:00<00:00, 132.06it/s]
Answering fact completion questions...:  44%|████▍     | 28/63 [00:00<00:00, 130.69it/s]
Average Loss on fact answering task after 1008 samples: 6.2326
Average Loss on fact answering task after 1008 samples: 6.2332
Average Loss on fact answering task after 1008 samples: 6.3267
Answering fact completion questions...: 100%|██████████| 63/63 [00:00<00:00, 134.54it/s]
Answering fact completion questions...: 100%|██████████| 63/63 [00:00<00:00, 132.86it/s]
Answering fact completion questions...:  89%|████████▉ | 56/63 [00:00<00:00, 132.13it/s]
Mean accuracy: 0.5564, std: 0.0111, lower bound: 0.5340, upper bound: 0.5776 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 1008 samples: 0.5563
Epoch 1/1, Loss after 1024 samples: 0.6314
Epoch 1/1, Loss after 1232 samples: 0.5148
Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Answering fact completion questions...:  41%|████▏     | 26/63 [00:00<00:00, 129.59it/s]
Average Loss on fact answering task after 1520 samples: 6.6822
Average Loss on fact answering task after 1520 samples: 6.7269
Answering fact completion questions...: 100%|██████████| 63/63 [00:00<00:00, 132.61it/s]
Answering fact completion questions...: 100%|██████████| 63/63 [00:00<00:00, 133.70it/s]
Answering fact completion questions...: 100%|██████████| 63/63 [00:00<00:00, 133.65it/s]
Answering fact completion questions...:  89%|████████▉ | 56/63 [00:00<00:00, 133.80it/s]
Answering fact completion questions...:  44%|████▍     | 28/63 [00:00<00:00, 132.26it/s]
Average Loss on fact answering task after 1520 samples: 6.4010

Answering fact completion questions...:  89%|████████▉ | 56/63 [00:00<00:00, 131.30it/s]
Mean accuracy: 0.8391, std: 0.0082, lower bound: 0.8220, upper bound: 0.8545 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 1520 samples: 0.8392
Best model with eval accuracy 0.8392494929006086 with 1520 samples seen is saved
Epoch 1/1, Loss after 1648 samples: 0.4473
Epoch 1/1, Loss after 1856 samples: 0.3876
Average Loss on fact answering task after 2032 samples: 6.8611
Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Answering fact completion questions...: 100%|██████████| 63/63 [00:00<00:00, 130.46it/s]
Answering fact completion questions...: 100%|██████████| 63/63 [00:00<00:00, 133.29it/s]
Answering fact completion questions...: 100%|██████████| 63/63 [00:00<00:00, 131.61it/s]
Answering fact completion questions...:  89%|████████▉ | 56/63 [00:00<00:00, 131.61it/s]
Answering fact completion questions...:  44%|████▍     | 28/63 [00:00<00:00, 131.57it/s]
Average Loss on fact answering task after 2032 samples: 6.8187
Average Loss on fact answering task after 2032 samples: 6.9038
Answering fact completion questions...: 100%|██████████| 63/63 [00:00<00:00, 132.49it/s]
Answering fact completion questions...:  89%|████████▉ | 56/63 [00:00<00:00, 132.66it/s]
Mean accuracy: 0.8724, std: 0.0075, lower bound: 0.8580, upper bound: 0.8859 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 2032 samples: 0.8722
Best model with eval accuracy 0.8722109533468559 with 2032 samples seen is saved
Epoch 1/1, Loss after 2064 samples: 0.2865
Epoch 1/1, Loss after 2272 samples: 0.3990
Epoch 1/1, Loss after 2480 samples: 0.2595
Average Loss on fact answering task after 2544 samples: 6.8514
Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Answering fact completion questions...: 100%|██████████| 63/63 [00:00<00:00, 125.03it/s]
Answering fact completion questions...: 100%|██████████| 63/63 [00:00<00:00, 132.54it/s]
Answering fact completion questions...:  89%|████████▉ | 56/63 [00:00<00:00, 132.47it/s]
Answering fact completion questions...:  67%|██████▋   | 42/63 [00:00<00:00, 131.23it/s]
Average Loss on fact answering task after 2544 samples: 6.8862
Average Loss on fact answering task after 2544 samples: 6.7516
Answering fact completion questions...: 100%|██████████| 63/63 [00:00<00:00, 133.49it/s]
Answering fact completion questions...: 100%|██████████| 63/63 [00:00<00:00, 132.36it/s]
Answering fact completion questions...:  89%|████████▉ | 56/63 [00:00<00:00, 132.04it/s]
Mean accuracy: 0.8486, std: 0.0083, lower bound: 0.8311, upper bound: 0.8646 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 2544 samples: 0.8484
Epoch 1/1, Loss after 2688 samples: 0.2734
Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Answering fact completion questions...:  60%|██████    | 38/63 [00:00<00:00, 120.48it/s]
Average Loss on fact answering task after 3056 samples: 6.8835
Average Loss on fact answering task after 3056 samples: 7.0066
Answering fact completion questions...: 100%|██████████| 63/63 [00:00<00:00, 132.40it/s]
Answering fact completion questions...: 100%|██████████| 63/63 [00:00<00:00, 131.63it/s]
Answering fact completion questions...: 100%|██████████| 63/63 [00:00<00:00, 132.24it/s]
Answering fact completion questions...:  89%|████████▉ | 56/63 [00:00<00:00, 131.55it/s]
Answering fact completion questions...:  65%|██████▌   | 41/63 [00:00<00:00, 131.29it/s]
Average Loss on fact answering task after 3056 samples: 6.9477

Answering fact completion questions...:  87%|████████▋ | 55/63 [00:00<00:00, 132.02it/s]
Mean accuracy: 0.7782, std: 0.0095, lower bound: 0.7596, upper bound: 0.7967 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 3056 samples: 0.7784
Epoch 1/1, Loss after 3104 samples: 0.2412
Epoch 1/1, Loss after 3312 samples: 0.2402
Epoch 1/1, Loss after 3520 samples: 0.3682
Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Answering fact completion questions...: 100%|██████████| 63/63 [00:00<00:00, 128.82it/s]
Answering fact completion questions...: 100%|██████████| 63/63 [00:00<00:00, 131.99it/s]
Answering fact completion questions...:  87%|████████▋ | 55/63 [00:00<00:00, 131.11it/s]
Answering fact completion questions...:  44%|████▍     | 28/63 [00:00<00:00, 132.71it/s]
Average Loss on fact answering task after 3568 samples: 6.7137
Average Loss on fact answering task after 3568 samples: 6.7203
Average Loss on fact answering task after 3568 samples: 6.6030
Answering fact completion questions...: 100%|██████████| 63/63 [00:00<00:00, 132.50it/s]
Answering fact completion questions...: 100%|██████████| 63/63 [00:00<00:00, 133.32it/s]
Answering fact completion questions...:  89%|████████▉ | 56/63 [00:00<00:00, 133.74it/s]
Mean accuracy: 0.9064, std: 0.0065, lower bound: 0.8935, upper bound: 0.9189 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 3568 samples: 0.9062
Best model with eval accuracy 0.9061866125760649 with 3568 samples seen is saved
Epoch 1/1, Loss after 3728 samples: 0.2582
Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Answering fact completion questions...: 100%|██████████| 63/63 [00:00<00:00, 132.24it/s]
Answering fact completion questions...:  87%|████████▋ | 55/63 [00:00<00:00, 132.50it/s]
Answering fact completion questions...:  44%|████▍     | 28/63 [00:00<00:00, 130.82it/s]
Average Loss on fact answering task after 4080 samples: 6.8919
Average Loss on fact answering task after 4080 samples: 6.9187
Average Loss on fact answering task after 4080 samples: 6.7990
Answering fact completion questions...: 100%|██████████| 63/63 [00:00<00:00, 132.87it/s]
Answering fact completion questions...: 100%|██████████| 63/63 [00:00<00:00, 133.08it/s]
Answering fact completion questions...: 100%|██████████| 63/63 [00:00<00:00, 133.84it/s]
Answering fact completion questions...:  89%|████████▉ | 56/63 [00:00<00:00, 133.52it/s]
Average Loss on fact answering task after 4080 samples: 6.6971
Mean accuracy: 0.9201, std: 0.0063, lower bound: 0.9077, upper bound: 0.9320 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 4080 samples: 0.9204
Best model with eval accuracy 0.9203853955375254 with 4080 samples seen is saved
Epoch 1/1, Loss after 4144 samples: 0.2662
Epoch 1/1, Loss after 4352 samples: 0.2588
Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Answering fact completion questions...:  89%|████████▉ | 56/63 [00:00<00:00, 129.06it/s]
Average Loss on fact answering task after 4592 samples: 7.1622
Average Loss on fact answering task after 4592 samples: 7.3316
Average Loss on fact answering task after 4592 samples: 7.1486
Answering fact completion questions...: 100%|██████████| 63/63 [00:00<00:00, 133.38it/s]
Answering fact completion questions...: 100%|██████████| 63/63 [00:00<00:00, 131.89it/s]
Answering fact completion questions...: 100%|██████████| 63/63 [00:00<00:00, 131.49it/s]
Answering fact completion questions...:  87%|████████▋ | 55/63 [00:00<00:00, 131.22it/s]
Answering fact completion questions...:  89%|████████▉ | 56/63 [00:00<00:00, 132.22it/s]
Average Loss on fact answering task after 4592 samples: 7.2709
Mean accuracy: 0.8785, std: 0.0073, lower bound: 0.8641, upper bound: 0.8940 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 4592 samples: 0.8783
Epoch 1/1, Loss after 4768 samples: 0.1602
Epoch 1/1, Loss after 4976 samples: 0.2311
Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Answering fact completion questions...: 100%|██████████| 63/63 [00:00<00:00, 122.35it/s]
Answering fact completion questions...: 100%|██████████| 63/63 [00:00<00:00, 132.23it/s]
Answering fact completion questions...:  89%|████████▉ | 56/63 [00:00<00:00, 132.24it/s]
Answering fact completion questions...:  43%|████▎     | 27/63 [00:00<00:00, 132.44it/s]
Average Loss on fact answering task after 5104 samples: 7.1207
Average Loss on fact answering task after 5104 samples: 7.1029
Average Loss on fact answering task after 5104 samples: 6.9527
Answering fact completion questions...: 100%|██████████| 63/63 [00:00<00:00, 133.68it/s]
Answering fact completion questions...: 100%|██████████| 63/63 [00:00<00:00, 133.51it/s]
Answering fact completion questions...:  89%|████████▉ | 56/63 [00:00<00:00, 133.10it/s]
Mean accuracy: 0.9152, std: 0.0064, lower bound: 0.9026, upper bound: 0.9275 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 5104 samples: 0.9153
Epoch 1/1, Loss after 5184 samples: 0.1590
Epoch 1/1, Loss after 5392 samples: 0.2165
Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Answering fact completion questions...:  17%|█▋        | 11/63 [00:00<00:00, 97.39it/s]
Average Loss on fact answering task after 5616 samples: 7.1466
Average Loss on fact answering task after 5616 samples: 7.2046
Answering fact completion questions...: 100%|██████████| 63/63 [00:00<00:00, 132.71it/s]
Answering fact completion questions...: 100%|██████████| 63/63 [00:00<00:00, 132.89it/s]
Answering fact completion questions...: 100%|██████████| 63/63 [00:00<00:00, 132.33it/s]
Answering fact completion questions...:  89%|████████▉ | 56/63 [00:00<00:00, 131.67it/s]
Answering fact completion questions...:  21%|██        | 13/63 [00:00<00:00, 129.65it/s]
Average Loss on fact answering task after 5616 samples: 7.1546

Answering fact completion questions...:  87%|████████▋ | 55/63 [00:00<00:00, 132.60it/s]
Mean accuracy: 0.9025, std: 0.0066, lower bound: 0.8884, upper bound: 0.9143 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 5616 samples: 0.9026
Epoch 1/1, Loss after 5808 samples: 0.2181
Epoch 1/1, Loss after 6016 samples: 0.2138
Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Answering fact completion questions...: 100%|██████████| 63/63 [00:00<00:00, 129.42it/s]
Answering fact completion questions...: 100%|██████████| 63/63 [00:00<00:00, 131.30it/s]
Answering fact completion questions...:  89%|████████▉ | 56/63 [00:00<00:00, 130.10it/s]
Answering fact completion questions...:   0%|          | 0/63 [00:00<?, ?it/s]
Average Loss on fact answering task after 6128 samples: 7.3055
Average Loss on fact answering task after 6128 samples: 7.2929
Average Loss on fact answering task after 6128 samples: 7.2275
Answering fact completion questions...: 100%|██████████| 63/63 [00:00<00:00, 134.36it/s]
Answering fact completion questions...: 100%|██████████| 63/63 [00:00<00:00, 132.46it/s]
Answering fact completion questions...:  89%|████████▉ | 56/63 [00:00<00:00, 132.11it/s]
Answering fact completion questions...: 100%|██████████| 63/63 [00:00<00:00, 134.36it/s]
Mean accuracy: 0.9179, std: 0.0061, lower bound: 0.9047, upper bound: 0.9295 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 6128 samples: 0.9178
Epoch 1/1, Loss after 6224 samples: 0.2697
Epoch 1/1, Loss after 6432 samples: 0.2097
Epoch 1/1, Loss after 6640 samples: 0.1524
Average Loss on fact answering task after 6640 samples: 7.4931
Answering fact completion questions...: 100%|██████████| 63/63 [00:00<00:00, 132.12it/s]
Answering fact completion questions...: 100%|██████████| 63/63 [00:00<00:00, 131.67it/s]
Answering fact completion questions...: 100%|██████████| 63/63 [00:00<00:00, 131.20it/s]
Answering fact completion questions...:  89%|████████▉ | 56/63 [00:00<00:00, 130.12it/s]
Answering fact completion questions...:  44%|████▍     | 28/63 [00:00<00:00, 129.83it/s]
Answering fact completion questions...: 100%|██████████| 63/63 [00:00<00:00, 132.12it/s]
Average Loss on fact answering task after 6640 samples: 7.1119
Average Loss on fact answering task after 6640 samples: 7.3334
Answering fact completion questions...:  89%|████████▉ | 56/63 [00:00<00:00, 132.55it/s]
Answering fact completion questions...:  89%|████████▉ | 56/63 [00:00<00:00, 132.55it/s]
Mean accuracy: 0.8835, std: 0.0073, lower bound: 0.8697, upper bound: 0.8976 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 6640 samples: 0.8834
Epoch 1/1, Loss after 6848 samples: 0.1463
Answering fact completion questions...:  22%|██▏       | 14/63 [00:00<00:00, 132.28it/s]ng RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
Answering fact completion questions...:  22%|██▏       | 14/63 [00:00<00:00, 132.28it/s]ng RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
Average Loss on fact answering task after 7152 samples: 7.4233
Average Loss on fact answering task after 7152 samples: 7.3836
Average Loss on fact answering task after 7152 samples: 7.2160
Answering fact completion questions...: 100%|██████████| 63/63 [00:00<00:00, 132.77it/s]ng RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
Answering fact completion questions...: 100%|██████████| 63/63 [00:00<00:00, 132.77it/s]ng RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
Average Loss on fact answering task after 7152 samples: 7.2239
Mean accuracy: 0.9108, std: 0.0064, lower bound: 0.8981, upper bound: 0.9239 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 7152 samples: 0.9108
Epoch 1/1, Loss after 7264 samples: 0.2690
Epoch 1/1, Loss after 7472 samples: 0.1449
Average Loss on fact answering task after 7664 samples: 7.2070
Answering fact completion questions...:   0%|          | 0/63 [00:00<?, ?it/s]ckpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Answering fact completion questions...:   0%|          | 0/63 [00:00<?, ?it/s]ckpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Average Loss on fact answering task after 7664 samples: 7.2037
Average Loss on fact answering task after 7664 samples: 7.1216
Answering fact completion questions...: 100%|██████████| 63/63 [00:00<00:00, 133.05it/s] a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Answering fact completion questions...: 100%|██████████| 63/63 [00:00<00:00, 133.05it/s] a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Mean accuracy: 0.9368, std: 0.0052, lower bound: 0.9270, upper bound: 0.9468 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 7664 samples: 0.9371
Best model with eval accuracy 0.9371196754563894 with 7664 samples seen is saved
Epoch 1/1, Loss after 7680 samples: 0.1687
Epoch 1/1, Loss after 7888 samples: 0.1603
Answering fact completion questions...:  87%|████████▋ | 55/63 [00:00<00:00, 132.08it/s] a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Answering fact completion questions...:  87%|████████▋ | 55/63 [00:00<00:00, 132.08it/s] a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Average Loss on fact answering task after 8176 samples: 7.1782
Average Loss on fact answering task after 8176 samples: 6.8963
Average Loss on fact answering task after 8176 samples: 7.2985
Average Loss on fact answering task after 8176 samples: 6.9508
Answering fact completion questions...: 100%|██████████| 63/63 [00:00<00:00, 132.99it/s] a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Answering fact completion questions...: 100%|██████████| 63/63 [00:00<00:00, 132.99it/s] a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Mean accuracy: 0.9406, std: 0.0053, lower bound: 0.9295, upper bound: 0.9508 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 8176 samples: 0.9407
Best model with eval accuracy 0.9406693711967545 with 8176 samples seen is saved
Epoch 1/1, Loss after 8304 samples: 0.1888
- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Average Loss on fact answering task after 8688 samples: 7.3797
Average Loss on fact answering task after 8688 samples: 7.1741
Average Loss on fact answering task after 8688 samples: 7.3119
Answering fact completion questions...: 100%|██████████| 63/63 [00:00<00:00, 133.99it/s]odel trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Answering fact completion questions...: 100%|██████████| 63/63 [00:00<00:00, 133.99it/s]odel trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Average Loss on fact answering task after 8688 samples: 7.3136
Mean accuracy: 0.8961, std: 0.0067, lower bound: 0.8834, upper bound: 0.9092 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 8688 samples: 0.8960
Epoch 1/1, Loss after 8720 samples: 0.1603
Epoch 1/1, Loss after 8928 samples: 0.1887
Epoch 1/1, Loss after 9136 samples: 0.1322
Average Loss on fact answering task after 9200 samples: 7.1181
Answering fact completion questions...: 100%|██████████| 63/63 [00:00<00:00, 123.21it/s]odel trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Answering fact completion questions...: 100%|██████████| 63/63 [00:00<00:00, 123.21it/s]odel trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Average Loss on fact answering task after 9200 samples: 7.4320
Average Loss on fact answering task after 9200 samples: 7.3819
Answering fact completion questions...:  89%|████████▉ | 56/63 [00:00<00:00, 132.93it/s]odel trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Answering fact completion questions...:  89%|████████▉ | 56/63 [00:00<00:00, 132.93it/s]odel trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Mean accuracy: 0.9188, std: 0.0058, lower bound: 0.9077, upper bound: 0.9300 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 9200 samples: 0.9189
Epoch 1/1, Loss after 9344 samples: 0.1018
Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ForSequenceClassification model from a BertForPreTraining model).
Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ForSequenceClassification model from a BertForPreTraining model).
Average Loss on fact answering task after 9712 samples: 7.1238
Average Loss on fact answering task after 9712 samples: 7.1367
Answering fact completion questions...: 100%|██████████| 63/63 [00:00<00:00, 133.78it/s]ng RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ForSequenceClassification model from a BertForPreTraining model).
Answering fact completion questions...: 100%|██████████| 63/63 [00:00<00:00, 133.78it/s]ng RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ForSequenceClassification model from a BertForPreTraining model).
Average Loss on fact answering task after 9712 samples: 7.0724
Average Loss on fact answering task after 9712 samples: 7.1676
Answering fact completion questions...: 100%|██████████| 63/63 [00:00<00:00, 133.78it/s]ng RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ForSequenceClassification model from a BertForPreTraining model).
Mean accuracy: 0.9189, std: 0.0061, lower bound: 0.9072, upper bound: 0.9305 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 9712 samples: 0.9189
Epoch 1/1, Loss after 9760 samples: 0.2272
Epoch 1/1, Loss after 9968 samples: 0.2196
Epoch 1/1, Loss after 10176 samples: 0.2040
- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
Average Loss on fact answering task after 10224 samples: 6.9230
Average Loss on fact answering task after 10224 samples: 7.1748
Average Loss on fact answering task after 10224 samples: 7.1145
Answering fact completion questions...: 100%|██████████| 63/63 [00:00<00:00, 133.32it/s] a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 63/63 [00:00<00:00, 133.32it/s] a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
Mean accuracy: 0.8713, std: 0.0073, lower bound: 0.8560, upper bound: 0.8849 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 10224 samples: 0.8707
Epoch 1/1, Loss after 10384 samples: 0.1472
Answering fact completion questions...:  22%|██▏       | 14/63 [00:00<00:00, 130.00it/s] a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
Answering fact completion questions...:  22%|██▏       | 14/63 [00:00<00:00, 130.00it/s] a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
Average Loss on fact answering task after 10736 samples: 7.1437
Average Loss on fact answering task after 10736 samples: 7.0760
Answering fact completion questions...: 100%|██████████| 63/63 [00:00<00:00, 132.41it/s] a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 63/63 [00:00<00:00, 132.41it/s] a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
Average Loss on fact answering task after 10736 samples: 7.2963
Average Loss on fact answering task after 10736 samples: 7.0777
Answering fact completion questions...: 100%|██████████| 63/63 [00:00<00:00, 132.41it/s] a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
Mean accuracy: 0.9348, std: 0.0054, lower bound: 0.9249, upper bound: 0.9458 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 10736 samples: 0.9346
Epoch 1/1, Loss after 10800 samples: 0.1491
Epoch 1/1, Loss after 11008 samples: 0.1527
- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Average Loss on fact answering task after 11248 samples: 7.0875
Average Loss on fact answering task after 11248 samples: 7.3428
Average Loss on fact answering task after 11248 samples: 7.1393
Answering fact completion questions...: 100%|██████████| 63/63 [00:00<00:00, 132.48it/s]odel trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Answering fact completion questions...: 100%|██████████| 63/63 [00:00<00:00, 132.48it/s]odel trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Average Loss on fact answering task after 11248 samples: 7.1163
Mean accuracy: 0.9204, std: 0.0058, lower bound: 0.9092, upper bound: 0.9315 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 11248 samples: 0.9199
Epoch 1/1, Loss after 11424 samples: 0.2085
Epoch 1/1, Loss after 11632 samples: 0.1015
Average Loss on fact answering task after 11760 samples: 7.1276
Answering fact completion questions...: 100%|██████████| 63/63 [00:00<00:00, 130.50it/s]odel trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Answering fact completion questions...: 100%|██████████| 63/63 [00:00<00:00, 130.50it/s]odel trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Average Loss on fact answering task after 11760 samples: 7.0424
Average Loss on fact answering task after 11760 samples: 7.2235
Answering fact completion questions...:  89%|████████▉ | 56/63 [00:00<00:00, 131.74it/s]odel trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Answering fact completion questions...:  89%|████████▉ | 56/63 [00:00<00:00, 131.74it/s]odel trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Mean accuracy: 0.8855, std: 0.0071, lower bound: 0.8707, upper bound: 0.8986 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 11760 samples: 0.8854
Epoch 1/1, Loss after 11840 samples: 0.1342
Epoch 1/1, Loss after 12048 samples: 0.1635
Answering fact completion questions...:  21%|██        | 13/63 [00:00<00:00, 128.63it/s]ng RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ForSequenceClassification model from a BertForPreTraining model).
Answering fact completion questions...:  21%|██        | 13/63 [00:00<00:00, 128.63it/s]ng RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ForSequenceClassification model from a BertForPreTraining model).
Average Loss on fact answering task after 12272 samples: 7.2154
Average Loss on fact answering task after 12272 samples: 6.9903
Average Loss on fact answering task after 12272 samples: 7.2811
Answering fact completion questions...: 100%|██████████| 63/63 [00:00<00:00, 133.30it/s]ng RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ForSequenceClassification model from a BertForPreTraining model).
Answering fact completion questions...: 100%|██████████| 63/63 [00:00<00:00, 133.30it/s]ng RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ForSequenceClassification model from a BertForPreTraining model).
Average Loss on fact answering task after 12272 samples: 7.0932
Mean accuracy: 0.8445, std: 0.0080, lower bound: 0.8291, upper bound: 0.8595 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 12272 samples: 0.8443
Epoch 1/1, Loss after 12464 samples: 0.1298
Epoch 1/1, Loss after 12672 samples: 0.1572
Average Loss on fact answering task after 12784 samples: 6.9621
Answering fact completion questions...:   0%|          | 0/63 [00:00<?, ?it/s]ckpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
Answering fact completion questions...:   0%|          | 0/63 [00:00<?, ?it/s]ckpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
Average Loss on fact answering task after 12784 samples: 6.9317
Average Loss on fact answering task after 12784 samples: 6.9761
Answering fact completion questions...: 100%|██████████| 63/63 [00:00<00:00, 132.24it/s] a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 63/63 [00:00<00:00, 132.24it/s] a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
Mean accuracy: 0.9255, std: 0.0059, lower bound: 0.9138, upper bound: 0.9366 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 12784 samples: 0.9260
Epoch 1/1, Loss after 12880 samples: 0.1559
Epoch 1/1, Loss after 13088 samples: 0.1129
Answering fact completion questions...:  65%|██████▌   | 41/63 [00:00<00:00, 132.77it/s] a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
Answering fact completion questions...:  65%|██████▌   | 41/63 [00:00<00:00, 132.77it/s] a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
Average Loss on fact answering task after 13296 samples: 7.1813
Average Loss on fact answering task after 13296 samples: 7.0919
Answering fact completion questions...: 100%|██████████| 63/63 [00:00<00:00, 133.47it/s] a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 63/63 [00:00<00:00, 133.47it/s] a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
Average Loss on fact answering task after 13296 samples: 6.9408
Average Loss on fact answering task after 13296 samples: 6.9801
Answering fact completion questions...: 100%|██████████| 63/63 [00:00<00:00, 133.47it/s] a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
Mean accuracy: 0.9291, std: 0.0057, lower bound: 0.9178, upper bound: 0.9397 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 13296 samples: 0.9290
Epoch 1/1, Loss after 13504 samples: 0.0924
Epoch 1/1, Loss after 13712 samples: 0.1540
Answering fact completion questions...:   0%|          | 0/63 [00:00<?, ?it/s]int of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Answering fact completion questions...:   0%|          | 0/63 [00:00<?, ?it/s]int of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Average Loss on fact answering task after 13808 samples: 6.9040
Average Loss on fact answering task after 13808 samples: 7.0766
Average Loss on fact answering task after 13808 samples: 7.0476
Answering fact completion questions...: 100%|██████████| 63/63 [00:00<00:00, 134.95it/s]odel trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Answering fact completion questions...: 100%|██████████| 63/63 [00:00<00:00, 134.95it/s]odel trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Mean accuracy: 0.8997, std: 0.0070, lower bound: 0.8859, upper bound: 0.9133 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 13808 samples: 0.8996
Epoch 1/1, Loss after 13920 samples: 0.0743
Epoch 1/1, Loss after 14128 samples: 0.1595
Average Loss on fact answering task after 14320 samples: 6.9019
Answering fact completion questions...: 100%|██████████| 63/63 [00:00<00:00, 123.97it/s]odel trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Answering fact completion questions...: 100%|██████████| 63/63 [00:00<00:00, 123.97it/s]odel trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Average Loss on fact answering task after 14320 samples: 7.1154
Average Loss on fact answering task after 14320 samples: 7.1904
Answering fact completion questions...:  87%|████████▋ | 55/63 [00:00<00:00, 132.26it/s]odel trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Answering fact completion questions...:  87%|████████▋ | 55/63 [00:00<00:00, 132.26it/s]odel trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Mean accuracy: 0.9057, std: 0.0064, lower bound: 0.8925, upper bound: 0.9189 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 14320 samples: 0.9057
Epoch 1/1, Loss after 14336 samples: 0.1284
Epoch 1/1, Loss after 14544 samples: 0.0999
Answering fact completion questions...:  44%|████▍     | 28/63 [00:00<00:00, 133.79it/s]ng RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ForSequenceClassification model from a BertForPreTraining model).
Answering fact completion questions...:  44%|████▍     | 28/63 [00:00<00:00, 133.79it/s]ng RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ForSequenceClassification model from a BertForPreTraining model).
Average Loss on fact answering task after 14832 samples: 6.8547
Average Loss on fact answering task after 14832 samples: 7.1569
Average Loss on fact answering task after 14832 samples: 7.0069
Answering fact completion questions...: 100%|██████████| 63/63 [00:00<00:00, 132.98it/s]ng RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ForSequenceClassification model from a BertForPreTraining model).
Answering fact completion questions...: 100%|██████████| 63/63 [00:00<00:00, 132.98it/s]ng RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ForSequenceClassification model from a BertForPreTraining model).
Average Loss on fact answering task after 14832 samples: 6.9363
Mean accuracy: 0.8944, std: 0.0071, lower bound: 0.8803, upper bound: 0.9077 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 14832 samples: 0.8945
Epoch 1/1, Loss after 14960 samples: 0.0863
Epoch 1/1, Loss after 15168 samples: 0.1280
Average Loss on fact answering task after 15344 samples: 7.0917
Answering fact completion questions...:  44%|████▍     | 28/63 [00:00<00:00, 132.13it/s] a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
Answering fact completion questions...:  44%|████▍     | 28/63 [00:00<00:00, 132.13it/s] a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
Average Loss on fact answering task after 15344 samples: 7.1321
Average Loss on fact answering task after 15344 samples: 7.1066
Answering fact completion questions...: 100%|██████████| 63/63 [00:00<00:00, 131.62it/s] a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 63/63 [00:00<00:00, 131.62it/s] a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
Mean accuracy: 0.9021, std: 0.0066, lower bound: 0.8889, upper bound: 0.9153 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 15344 samples: 0.9021
Epoch 1/1, Loss after 15376 samples: 0.1594
Epoch 1/1, Loss after 15584 samples: 0.1186
Training signal is False, stopping training
Training finished
Best model: {'eval_acc': 0.9406693711967545, 'nb_samples': 8176}
Training loss logs: [{'samples': 192, 'loss': 0.6987750713641827}, {'samples': 400, 'loss': 0.6921128493088943}, {'samples': 608, 'loss': 0.6880868765024039}, {'samples': 816, 'loss': 0.6456815279447116}, {'samples': 1024, 'loss': 0.6313993013822116}, {'samples': 1232, 'loss': 0.5147834190955529}, {'samples': 1440, 'loss': 0.5377440819373498}, {'samples': 1648, 'loss': 0.44725740872896635}, {'samples': 1856, 'loss': 0.3876195320716271}, {'samples': 2064, 'loss': 0.28653797736534703}, {'samples': 2272, 'loss': 0.39896077376145583}, {'samples': 2480, 'loss': 0.2595243453979492}, {'samples': 2688, 'loss': 0.27342084737924427}, {'samples': 2896, 'loss': 0.23874557935274565}, {'samples': 3104, 'loss': 0.24121897037212664}, {'samples': 3312, 'loss': 0.2402129631776076}, {'samples': 3520, 'loss': 0.3681508486087506}, {'samples': 3728, 'loss': 0.258150183237516}, {'samples': 3936, 'loss': 0.2679672699708205}, {'samples': 4144, 'loss': 0.2661657608472384}, {'samples': 4352, 'loss': 0.25876696751667905}, {'samples': 4560, 'loss': 0.1752397234623249}, {'samples': 4768, 'loss': 0.16018532789670503}, {'samples': 4976, 'loss': 0.23113822020017183}, {'samples': 5184, 'loss': 0.15897585795475885}, {'samples': 5392, 'loss': 0.21650055509347182}, {'samples': 5600, 'loss': 0.2178274576480572}, {'samples': 5808, 'loss': 0.21808844346266526}, {'samples': 6016, 'loss': 0.2138361564049354}, {'samples': 6224, 'loss': 0.26967471837997437}, {'samples': 6432, 'loss': 0.2097352147102356}, {'samples': 6640, 'loss': 0.15242245564093956}, {'samples': 6848, 'loss': 0.14634167689543504}, {'samples': 7056, 'loss': 0.14509442448616028}, {'samples': 7264, 'loss': 0.2689561362449939}, {'samples': 7472, 'loss': 0.14494616710222685}, {'samples': 7680, 'loss': 0.16868083293621355}, {'samples': 7888, 'loss': 0.16026082405677208}, {'samples': 8096, 'loss': 0.16759119354761565}, {'samples': 8304, 'loss': 0.18880646045391375}, {'samples': 8512, 'loss': 0.11015436282524696}, {'samples': 8720, 'loss': 0.16025076921169573}, {'samples': 8928, 'loss': 0.1886795713351323}, {'samples': 9136, 'loss': 0.1322015363436479}, {'samples': 9344, 'loss': 0.10183145908208993}, {'samples': 9552, 'loss': 0.19943268024004424}, {'samples': 9760, 'loss': 0.22721829551916856}, {'samples': 9968, 'loss': 0.21957229880186227}, {'samples': 10176, 'loss': 0.20400808178461516}, {'samples': 10384, 'loss': 0.1472064462991861}, {'samples': 10592, 'loss': 0.1362407345038194}, {'samples': 10800, 'loss': 0.14913684129714966}, {'samples': 11008, 'loss': 0.15265784699183244}, {'samples': 11216, 'loss': 0.09777375367971566}, {'samples': 11424, 'loss': 0.208535380088366}, {'samples': 11632, 'loss': 0.10150503539122067}, {'samples': 11840, 'loss': 0.13417148016966307}, {'samples': 12048, 'loss': 0.163530537715325}, {'samples': 12256, 'loss': 0.14243219792842865}, {'samples': 12464, 'loss': 0.12982776417182043}, {'samples': 12672, 'loss': 0.1571966982804812}, {'samples': 12880, 'loss': 0.1559441192792012}, {'samples': 13088, 'loss': 0.11293369760880104}, {'samples': 13296, 'loss': 0.16990132859120002}, {'samples': 13504, 'loss': 0.09236721350596501}, {'samples': 13712, 'loss': 0.15399508865980002}, {'samples': 13920, 'loss': 0.07431022822856903}, {'samples': 14128, 'loss': 0.1595347420527385}, {'samples': 14336, 'loss': 0.12835717315857226}, {'samples': 14544, 'loss': 0.09988552217300121}, {'samples': 14752, 'loss': 0.1726896923321944}, {'samples': 14960, 'loss': 0.0863362206862523}, {'samples': 15168, 'loss': 0.12797869283419389}, {'samples': 15376, 'loss': 0.15936155101427665}, {'samples': 15584, 'loss': 0.1186008957716135}]
Evaluation accuracy logs: [{'samples': 496, 'accuracy': 0.6291678498985801, 'std': 0.011005809758861694, 'lower_bound': 0.6080121703853956, 'upper_bound': 0.6501140973630832}, {'samples': 1008, 'accuracy': 0.5564361054766734, 'std': 0.011140210024231703, 'lower_bound': 0.5339629817444219, 'upper_bound': 0.5775862068965517}, {'samples': 1520, 'accuracy': 0.8391115618661258, 'std': 0.008195674035916438, 'lower_bound': 0.8220081135902637, 'upper_bound': 0.8544624746450304}, {'samples': 2032, 'accuracy': 0.8724006085192698, 'std': 0.007465910592995351, 'lower_bound': 0.8580121703853956, 'upper_bound': 0.8859026369168357}, {'samples': 2544, 'accuracy': 0.8486075050709939, 'std': 0.008285015309160556, 'lower_bound': 0.8311359026369168, 'upper_bound': 0.8646171399594321}, {'samples': 3056, 'accuracy': 0.778183569979716, 'std': 0.009451411944032826, 'lower_bound': 0.7596348884381339, 'upper_bound': 0.7966531440162272}, {'samples': 3568, 'accuracy': 0.9064163286004057, 'std': 0.006490590744879848, 'lower_bound': 0.8934964503042596, 'upper_bound': 0.9188640973630832}, {'samples': 4080, 'accuracy': 0.9201029411764706, 'std': 0.006274783836980515, 'lower_bound': 0.907707910750507, 'upper_bound': 0.9320486815415822}, {'samples': 4592, 'accuracy': 0.8784959432048681, 'std': 0.007345956744355519, 'lower_bound': 0.8640973630831643, 'upper_bound': 0.8940162271805274}, {'samples': 5104, 'accuracy': 0.9152134888438134, 'std': 0.006352953623222675, 'lower_bound': 0.9026242393509127, 'upper_bound': 0.9274847870182555}, {'samples': 5616, 'accuracy': 0.9024695740365112, 'std': 0.006567575368202329, 'lower_bound': 0.8884254563894523, 'upper_bound': 0.9143128803245436}, {'samples': 6128, 'accuracy': 0.9178559837728195, 'std': 0.006121255882966371, 'lower_bound': 0.9046653144016227, 'upper_bound': 0.9295131845841785}, {'samples': 6640, 'accuracy': 0.8834721095334686, 'std': 0.0073432150649042184, 'lower_bound': 0.8696754563894523, 'upper_bound': 0.8975786004056795}, {'samples': 7152, 'accuracy': 0.9107961460446247, 'std': 0.006375590665879142, 'lower_bound': 0.8980730223123732, 'upper_bound': 0.9239477687626775}, {'samples': 7664, 'accuracy': 0.9368159229208924, 'std': 0.0051542905011948996, 'lower_bound': 0.9269776876267748, 'upper_bound': 0.9467545638945233}, {'samples': 8176, 'accuracy': 0.9405552738336713, 'std': 0.005334781401703399, 'lower_bound': 0.9295131845841785, 'upper_bound': 0.9508113590263692}, {'samples': 8688, 'accuracy': 0.896093813387424, 'std': 0.006670228780404273, 'lower_bound': 0.8833671399594321, 'upper_bound': 0.9092292089249493}, {'samples': 9200, 'accuracy': 0.9188397565922921, 'std': 0.005808705573929679, 'lower_bound': 0.907707910750507, 'upper_bound': 0.9300329614604462}, {'samples': 9712, 'accuracy': 0.918870182555781, 'std': 0.006066864216282457, 'lower_bound': 0.9072008113590264, 'upper_bound': 0.9305273833671399}, {'samples': 10224, 'accuracy': 0.8713260649087222, 'std': 0.00733876325982051, 'lower_bound': 0.8559837728194726, 'upper_bound': 0.8848884381338742}, {'samples': 10736, 'accuracy': 0.934830121703854, 'std': 0.00542311386091256, 'lower_bound': 0.9249492900608519, 'upper_bound': 0.945753042596349}, {'samples': 11248, 'accuracy': 0.9204447261663286, 'std': 0.005848360177815487, 'lower_bound': 0.9092165314401622, 'upper_bound': 0.9315415821501014}, {'samples': 11760, 'accuracy': 0.88551369168357, 'std': 0.007130703027848424, 'lower_bound': 0.8706896551724138, 'upper_bound': 0.898592799188641}, {'samples': 12272, 'accuracy': 0.8445334685598377, 'std': 0.007954603576865508, 'lower_bound': 0.829107505070994, 'upper_bound': 0.8595461460446248}, {'samples': 12784, 'accuracy': 0.9255436105476674, 'std': 0.005906443639803634, 'lower_bound': 0.9137931034482759, 'upper_bound': 0.9366125760649088}, {'samples': 13296, 'accuracy': 0.9291450304259634, 'std': 0.0056667971056583565, 'lower_bound': 0.9178498985801217, 'upper_bound': 0.9396551724137931}, {'samples': 13808, 'accuracy': 0.8996774847870184, 'std': 0.006990969834077396, 'lower_bound': 0.8859026369168357, 'upper_bound': 0.9132986815415822}, {'samples': 14320, 'accuracy': 0.9057332657200812, 'std': 0.006422985110350426, 'lower_bound': 0.8924822515212981, 'upper_bound': 0.9188640973630832}, {'samples': 14832, 'accuracy': 0.8943595334685598, 'std': 0.007084646216488441, 'lower_bound': 0.8803245436105477, 'upper_bound': 0.907707910750507}, {'samples': 15344, 'accuracy': 0.9021333671399593, 'std': 0.006552127430456524, 'lower_bound': 0.888932555780933, 'upper_bound': 0.915314401622718}]