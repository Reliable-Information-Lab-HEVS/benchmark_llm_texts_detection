Parameter 'function'=<function DetectorTrainer.load_fact_checking_dataset.<locals>.<lambda> at 0x14fcf6d95260> of the transform datasets.arrow_dataset.Dataset.filter@2.0.1 couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.




Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:01<00:00, 91.22it/s]
Answering fact completion questions...:  80%|████████  | 100/125 [00:01<00:00, 93.74it/s]
Average Loss on fact answering task after 0 samples: 6.0321
Answering fact completion questions...: 100%|██████████| 125/125 [00:01<00:00, 93.99it/s]
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:01<00:00, 93.43it/s]
Answering fact completion questions...:  24%|██▍       | 30/125 [00:00<00:01, 94.00it/s]
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:01<00:00, 93.76it/s]
Answering fact completion questions...:  72%|███████▏  | 90/125 [00:00<00:00, 93.06it/s]
Average Loss on fact answering task after 0 samples: 6.2171
Average Loss on fact answering task after 0 samples: 5.9218

Answering fact completion questions...:  96%|█████████▌| 120/125 [00:01<00:00, 93.53it/s]
Epoch 1/1, Loss after 192 samples: 0.6961
Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Answering fact completion questions...:  80%|████████  | 100/125 [00:01<00:00, 94.48it/s]
Answering fact completion questions...: 100%|██████████| 125/125 [00:01<00:00, 93.85it/s]
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:01<00:00, 93.83it/s]
Answering fact completion questions...:  24%|██▍       | 30/125 [00:00<00:01, 94.17it/s]
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:01<00:00, 93.68it/s]
Answering fact completion questions...:  72%|███████▏  | 90/125 [00:00<00:00, 93.88it/s]
Average Loss on fact answering task after 496 samples: 6.1547
Answering fact completion questions...: 100%|██████████| 125/125 [00:01<00:00, 93.13it/s]
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:01<00:00, 93.30it/s]
Average Loss on fact answering task after 496 samples: 6.0787
Mean accuracy: 0.6816, std: 0.0108, lower bound: 0.6587, upper bound: 0.7023 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 496 samples: 0.6820
Best model with eval accuracy 0.6820486815415822 with 496 samples seen is saved
Epoch 1/1, Loss after 592 samples: 0.6875
Epoch 1/1, Loss after 792 samples: 0.6503
Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Answering fact completion questions...: 100%|██████████| 125/125 [00:01<00:00, 94.48it/s]
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:01<00:00, 94.79it/s]
Answering fact completion questions...:  24%|██▍       | 30/125 [00:00<00:01, 94.70it/s]
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:01<00:00, 94.08it/s]
Answering fact completion questions...:  72%|███████▏  | 90/125 [00:00<00:00, 94.59it/s]
Average Loss on fact answering task after 1000 samples: 6.6708
Answering fact completion questions...: 100%|██████████| 125/125 [00:01<00:00, 93.83it/s]
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:01<00:00, 93.57it/s]
Answering fact completion questions...:  24%|██▍       | 30/125 [00:00<00:01, 94.22it/s]

Answering fact completion questions...:  96%|█████████▌| 120/125 [00:01<00:00, 93.48it/s]
Average Loss on fact answering task after 1000 samples: 6.2836
Mean accuracy: 0.5265, std: 0.0109, lower bound: 0.5046, upper bound: 0.5477 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 1000 samples: 0.5264
Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Epoch 1/1, Loss after 1392 samples: 0.4121
Answering fact completion questions...: 100%|██████████| 125/125 [00:01<00:00, 93.68it/s]
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:01<00:00, 93.59it/s]
Answering fact completion questions...:  40%|████      | 50/125 [00:00<00:00, 93.13it/s]
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:01<00:00, 93.66it/s]
Answering fact completion questions...:  88%|████████▊ | 110/125 [00:01<00:00, 94.45it/s]
Average Loss on fact answering task after 1504 samples: 6.8055
Answering fact completion questions...: 100%|██████████| 125/125 [00:01<00:00, 93.97it/s]
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:01<00:00, 93.75it/s]
Answering fact completion questions...:  40%|████      | 50/125 [00:00<00:00, 94.92it/s]

Answering fact completion questions...:  96%|█████████▌| 120/125 [00:01<00:00, 94.46it/s]
Average Loss on fact answering task after 1504 samples: 6.6951
Mean accuracy: 0.8318, std: 0.0085, lower bound: 0.8154, upper bound: 0.8479 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 1504 samples: 0.8316
Best model with eval accuracy 0.8316430020283976 with 1504 samples seen is saved
Epoch 1/1, Loss after 1592 samples: 0.3237
Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Answering fact completion questions...:  48%|████▊     | 60/125 [00:00<00:00, 93.64it/s]
Answering fact completion questions...: 100%|██████████| 125/125 [00:01<00:00, 94.67it/s]
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:01<00:00, 95.11it/s]
Average Loss on fact answering task after 2008 samples: 7.3519
Answering fact completion questions...: 100%|██████████| 125/125 [00:01<00:00, 94.89it/s]
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:01<00:00, 94.47it/s]
Answering fact completion questions...:  48%|████▊     | 60/125 [00:00<00:00, 95.02it/s]
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:01<00:00, 94.35it/s]
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:01<00:00, 95.03it/s]
Average Loss on fact answering task after 2008 samples: 7.4174
Average Loss on fact answering task after 2008 samples: 7.5199
Mean accuracy: 0.8058, std: 0.0094, lower bound: 0.7875, upper bound: 0.8240 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 2008 samples: 0.8058
Epoch 1/1, Loss after 2192 samples: 0.4621
Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Answering fact completion questions...: 100%|██████████| 125/125 [00:01<00:00, 93.95it/s]
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:01<00:00, 94.03it/s]
Answering fact completion questions...:  16%|█▌        | 20/125 [00:00<00:01, 95.40it/s]
Average Loss on fact answering task after 2512 samples: 7.9284
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:01<00:00, 93.77it/s]
Answering fact completion questions...:  64%|██████▍   | 80/125 [00:00<00:00, 93.84it/s]
Answering fact completion questions...: 100%|██████████| 125/125 [00:01<00:00, 93.43it/s]
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:01<00:00, 93.41it/s]
Average Loss on fact answering task after 2512 samples: 7.9665
Average Loss on fact answering task after 2512 samples: 7.8331
Mean accuracy: 0.7859, std: 0.0094, lower bound: 0.7682, upper bound: 0.8043 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 2512 samples: 0.7865
Epoch 1/1, Loss after 2592 samples: 0.2178
Epoch 1/1, Loss after 2792 samples: 0.3208
Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Answering fact completion questions...:  80%|████████  | 100/125 [00:01<00:00, 94.10it/s]
Answering fact completion questions...: 100%|██████████| 125/125 [00:01<00:00, 94.19it/s]
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:01<00:00, 94.30it/s]
Answering fact completion questions...:  32%|███▏      | 40/125 [00:00<00:00, 94.51it/s]
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:01<00:00, 94.03it/s]
Answering fact completion questions...:  80%|████████  | 100/125 [00:01<00:00, 94.76it/s]
Average Loss on fact answering task after 3016 samples: 8.1502
Answering fact completion questions...: 100%|██████████| 125/125 [00:01<00:00, 94.68it/s]
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:01<00:00, 94.83it/s]
Average Loss on fact answering task after 3016 samples: 8.0120
Mean accuracy: 0.9047, std: 0.0066, lower bound: 0.8915, upper bound: 0.9178 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 3016 samples: 0.9047
Best model with eval accuracy 0.9046653144016227 with 3016 samples seen is saved
Epoch 1/1, Loss after 3192 samples: 0.2492
Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Answering fact completion questions...: 100%|██████████| 125/125 [00:01<00:00, 94.43it/s]
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:01<00:00, 94.64it/s]
Answering fact completion questions...:  32%|███▏      | 40/125 [00:00<00:00, 94.49it/s]
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:01<00:00, 93.57it/s]
Answering fact completion questions...:  80%|████████  | 100/125 [00:01<00:00, 93.95it/s]
Average Loss on fact answering task after 3520 samples: 7.4067
Answering fact completion questions...: 100%|██████████| 125/125 [00:01<00:00, 94.09it/s]
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:01<00:00, 93.87it/s]
Answering fact completion questions...:  24%|██▍       | 30/125 [00:00<00:01, 94.20it/s]

Answering fact completion questions...:  96%|█████████▌| 120/125 [00:01<00:00, 93.93it/s]
Average Loss on fact answering task after 3520 samples: 7.2911
Mean accuracy: 0.9107, std: 0.0066, lower bound: 0.8976, upper bound: 0.9234 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 3520 samples: 0.9108
Best model with eval accuracy 0.9107505070993914 with 3520 samples seen is saved
Epoch 1/1, Loss after 3592 samples: 0.3280
Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Answering fact completion questions...:  40%|████      | 50/125 [00:00<00:00, 93.87it/s]
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:01<00:00, 94.70it/s]
Answering fact completion questions...:  88%|████████▊ | 110/125 [00:01<00:00, 94.05it/s]
Average Loss on fact answering task after 4024 samples: 7.9211
Answering fact completion questions...: 100%|██████████| 125/125 [00:01<00:00, 94.09it/s]
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:01<00:00, 94.16it/s]
Answering fact completion questions...:  32%|███▏      | 40/125 [00:00<00:00, 95.17it/s]
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:01<00:00, 94.25it/s]
Answering fact completion questions...:  80%|████████  | 100/125 [00:01<00:00, 94.64it/s]
Average Loss on fact answering task after 4024 samples: 7.5779

Answering fact completion questions...:  96%|█████████▌| 120/125 [00:01<00:00, 94.76it/s]
Mean accuracy: 0.9146, std: 0.0065, lower bound: 0.9021, upper bound: 0.9270 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 4024 samples: 0.9148
Best model with eval accuracy 0.9148073022312373 with 4024 samples seen is saved
Epoch 1/1, Loss after 4192 samples: 0.3966
Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Answering fact completion questions...: 100%|██████████| 125/125 [00:01<00:00, 93.49it/s]
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:01<00:00, 92.95it/s]
Answering fact completion questions...:  16%|█▌        | 20/125 [00:00<00:01, 94.45it/s]
Average Loss on fact answering task after 4528 samples: 7.5689
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:01<00:00, 94.23it/s]
Answering fact completion questions...:  64%|██████▍   | 80/125 [00:00<00:00, 93.53it/s]
Answering fact completion questions...: 100%|██████████| 125/125 [00:01<00:00, 94.01it/s]
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:01<00:00, 94.05it/s]
Average Loss on fact answering task after 4528 samples: 7.4483
Average Loss on fact answering task after 4528 samples: 7.5666
Mean accuracy: 0.8969, std: 0.0067, lower bound: 0.8839, upper bound: 0.9097 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 4528 samples: 0.8971
Epoch 1/1, Loss after 4592 samples: 0.1712
Epoch 1/1, Loss after 4792 samples: 0.1656
Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Answering fact completion questions...:  88%|████████▊ | 110/125 [00:01<00:00, 94.98it/s]
Answering fact completion questions...: 100%|██████████| 125/125 [00:01<00:00, 94.35it/s]
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:01<00:00, 94.51it/s]
Answering fact completion questions...:  32%|███▏      | 40/125 [00:00<00:00, 94.63it/s]
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:01<00:00, 94.92it/s]
Answering fact completion questions...:  80%|████████  | 100/125 [00:01<00:00, 94.16it/s]
Answering fact completion questions...: 100%|██████████| 125/125 [00:01<00:00, 94.89it/s]
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:01<00:00, 94.90it/s]
Average Loss on fact answering task after 5032 samples: 7.3374
Average Loss on fact answering task after 5032 samples: 7.3458
Mean accuracy: 0.9284, std: 0.0059, lower bound: 0.9173, upper bound: 0.9397 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 5032 samples: 0.9285
Best model with eval accuracy 0.928498985801217 with 5032 samples seen is saved
Epoch 1/1, Loss after 5192 samples: 0.1658
Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Answering fact completion questions...: 100%|██████████| 125/125 [00:01<00:00, 93.62it/s]
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:01<00:00, 93.75it/s]
Answering fact completion questions...:  32%|███▏      | 40/125 [00:00<00:00, 94.70it/s]
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:01<00:00, 94.11it/s]
Answering fact completion questions...:  80%|████████  | 100/125 [00:01<00:00, 94.32it/s]
Average Loss on fact answering task after 5536 samples: 7.4651
Answering fact completion questions...: 100%|██████████| 125/125 [00:01<00:00, 94.07it/s]
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:01<00:00, 94.36it/s]
Answering fact completion questions...:  24%|██▍       | 30/125 [00:00<00:01, 94.43it/s]

Answering fact completion questions...:  96%|█████████▌| 120/125 [00:01<00:00, 95.00it/s]
Average Loss on fact answering task after 5536 samples: 7.3123
Mean accuracy: 0.9101, std: 0.0068, lower bound: 0.8966, upper bound: 0.9234 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 5536 samples: 0.9102
Epoch 1/1, Loss after 5592 samples: 0.2512
Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Answering fact completion questions...:   0%|          | 0/125 [00:00<?, ?it/s]
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:01<00:00, 94.84it/s]
Answering fact completion questions...:  48%|████▊     | 60/125 [00:00<00:00, 94.50it/s]
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:01<00:00, 94.84it/s]
Average Loss on fact answering task after 6040 samples: 7.7059
Answering fact completion questions...: 100%|██████████| 125/125 [00:01<00:00, 94.52it/s]
Answering fact completion questions...: 100%|██████████| 125/125 [00:01<00:00, 94.52it/s]
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:01<00:00, 92.92it/s]
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:01<00:00, 92.92it/s]
Answering fact completion questions...: 100%|██████████| 125/125 [00:01<00:00, 92.81it/s]
Answering fact completion questions...: 100%|██████████| 125/125 [00:01<00:00, 92.81it/s]
Average Loss on fact answering task after 6040 samples: 7.8650
Mean accuracy: 0.8567, std: 0.0079, lower bound: 0.8403, upper bound: 0.8712 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 6040 samples: 0.8565
Epoch 1/1, Loss after 6192 samples: 0.2675
- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Answering fact completion questions...:  64%|██████▍   | 80/125 [00:00<00:00, 93.44it/s] a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Answering fact completion questions...:  64%|██████▍   | 80/125 [00:00<00:00, 93.44it/s] a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Average Loss on fact answering task after 6544 samples: 7.5674
Answering fact completion questions...: 100%|██████████| 125/125 [00:01<00:00, 93.74it/s]a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Answering fact completion questions...: 100%|██████████| 125/125 [00:01<00:00, 93.74it/s]a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Answering fact completion questions...:  56%|█████▌    | 70/125 [00:00<00:00, 94.82it/s]]a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Answering fact completion questions...:  56%|█████▌    | 70/125 [00:00<00:00, 94.82it/s]]a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Average Loss on fact answering task after 6544 samples: 7.6508
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:01<00:00, 94.67it/s]a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:01<00:00, 94.67it/s]a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Mean accuracy: 0.8746, std: 0.0075, lower bound: 0.8590, upper bound: 0.8890 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 6544 samples: 0.8742
Epoch 1/1, Loss after 6592 samples: 0.2552
Epoch 1/1, Loss after 6792 samples: 0.1521
Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).
Answering fact completion questions...: 100%|██████████| 125/125 [00:01<00:00, 93.96it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).
Answering fact completion questions...: 100%|██████████| 125/125 [00:01<00:00, 93.96it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).
Average Loss on fact answering task after 7048 samples: 7.5806
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:01<00:00, 93.55it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:01<00:00, 93.55it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).
Answering fact completion questions...: 100%|██████████| 125/125 [00:01<00:00, 94.77it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).
Answering fact completion questions...: 100%|██████████| 125/125 [00:01<00:00, 94.77it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).
Average Loss on fact answering task after 7048 samples: 7.4226
Average Loss on fact answering task after 7048 samples: 7.5222
Answering fact completion questions...: 100%|██████████| 125/125 [00:01<00:00, 94.77it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).
Mean accuracy: 0.9187, std: 0.0060, lower bound: 0.9072, upper bound: 0.9305 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 7048 samples: 0.9184
Epoch 1/1, Loss after 7192 samples: 0.1996
Answering fact completion questions...:  43%|████▎     | 54/125 [00:00<00:00, 81.22it/s]odel trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Answering fact completion questions...:  94%|█████████▎| 117/125 [00:01<00:00, 81.57it/s]del trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Answering fact completion questions...:  94%|█████████▎| 117/125 [00:01<00:00, 81.57it/s]del trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Average Loss on fact answering task after 7552 samples: 7.7866
Answering fact completion questions...: 100%|██████████| 125/125 [00:01<00:00, 81.11it/s]del trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Answering fact completion questions...: 100%|██████████| 125/125 [00:01<00:00, 81.11it/s]del trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Answering fact completion questions...:  94%|█████████▎| 117/125 [00:01<00:00, 81.14it/s]del trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Answering fact completion questions...:  94%|█████████▎| 117/125 [00:01<00:00, 81.14it/s]del trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Answering fact completion questions...: 100%|██████████| 125/125 [00:01<00:00, 80.61it/s]del trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Answering fact completion questions...: 100%|██████████| 125/125 [00:01<00:00, 80.61it/s]del trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Average Loss on fact answering task after 7552 samples: 7.8800
Mean accuracy: 0.8998, std: 0.0068, lower bound: 0.8869, upper bound: 0.9133 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 7552 samples: 0.8996
Epoch 1/1, Loss after 7592 samples: 0.1884
Epoch 1/1, Loss after 7792 samples: 0.1782
Answering fact completion questions...:   0%|          | 0/125 [00:00<?, ?it/s]kpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
Answering fact completion questions...:   0%|          | 0/125 [00:00<?, ?it/s]kpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
Answering fact completion questions...:  94%|█████████▎| 117/125 [00:01<00:00, 81.48it/s]a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
Answering fact completion questions...:  94%|█████████▎| 117/125 [00:01<00:00, 81.48it/s]a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:01<00:00, 81.26it/s]a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:01<00:00, 81.26it/s]a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
Answering fact completion questions...:  86%|████████▋ | 108/125 [00:01<00:00, 81.04it/s]a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
Answering fact completion questions...:  86%|████████▋ | 108/125 [00:01<00:00, 81.04it/s]a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
Average Loss on fact answering task after 8056 samples: 7.7373
Answering fact completion questions...:  94%|█████████▎| 117/125 [00:01<00:00, 80.95it/s]a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
Answering fact completion questions...:  94%|█████████▎| 117/125 [00:01<00:00, 80.95it/s]a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
Mean accuracy: 0.9197, std: 0.0061, lower bound: 0.9072, upper bound: 0.9315 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 8056 samples: 0.9199
Epoch 1/1, Loss after 8192 samples: 0.1182
Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:01<00:00, 81.40it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:01<00:00, 81.40it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Average Loss on fact answering task after 8560 samples: 7.8952
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:01<00:00, 94.24it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:01<00:00, 94.24it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:01<00:00, 94.00it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:01<00:00, 94.00it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Average Loss on fact answering task after 8560 samples: 7.7846
Average Loss on fact answering task after 8560 samples: 7.9653
Answering fact completion questions...: 100%|██████████| 125/125 [00:01<00:00, 94.00it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Mean accuracy: 0.9065, std: 0.0068, lower bound: 0.8925, upper bound: 0.9199 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 8560 samples: 0.9067
Epoch 1/1, Loss after 8592 samples: 0.1521
Epoch 1/1, Loss after 8792 samples: 0.1616
Answering fact completion questions...:  40%|████      | 50/125 [00:00<00:00, 94.45it/s]odel trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:01<00:00, 94.64it/s]del trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:01<00:00, 94.64it/s]del trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Average Loss on fact answering task after 9064 samples: 7.8301
Answering fact completion questions...:  40%|████      | 50/125 [00:00<00:00, 93.92it/s]]del trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Answering fact completion questions...:  40%|████      | 50/125 [00:00<00:00, 93.92it/s]]del trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:01<00:00, 93.42it/s]del trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:01<00:00, 93.42it/s]del trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Average Loss on fact answering task after 9064 samples: 7.9203
Answering fact completion questions...: 100%|██████████| 125/125 [00:01<00:00, 93.98it/s]del trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Answering fact completion questions...: 100%|██████████| 125/125 [00:01<00:00, 93.98it/s]del trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Mean accuracy: 0.9033, std: 0.0065, lower bound: 0.8905, upper bound: 0.9153 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 9064 samples: 0.9037
Epoch 1/1, Loss after 9192 samples: 0.1558
- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:01<00:00, 93.42it/s]a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:01<00:00, 93.42it/s]a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
Average Loss on fact answering task after 9568 samples: 7.4523
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:01<00:00, 93.42it/s]a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:01<00:00, 94.23it/s]a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:01<00:00, 94.23it/s]a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:01<00:00, 94.40it/s]a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:01<00:00, 94.40it/s]a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
Average Loss on fact answering task after 9568 samples: 7.4082
Mean accuracy: 0.8995, std: 0.0068, lower bound: 0.8859, upper bound: 0.9133 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 9568 samples: 0.8996
Epoch 1/1, Loss after 9592 samples: 0.1728
Epoch 1/1, Loss after 9792 samples: 0.1663
Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...:  32%|███▏      | 40/125 [00:00<00:00, 95.00it/s]]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...:  32%|███▏      | 40/125 [00:00<00:00, 95.00it/s]]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:01<00:00, 95.07it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:01<00:00, 95.07it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Average Loss on fact answering task after 10072 samples: 7.4360
Answering fact completion questions...: 100%|██████████| 125/125 [00:01<00:00, 93.72it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:01<00:00, 93.72it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Average Loss on fact answering task after 10072 samples: 7.2283
Mean accuracy: 0.9335, std: 0.0059, lower bound: 0.9219, upper bound: 0.9447 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 10072 samples: 0.9336
Best model with eval accuracy 0.9335699797160243 with 10072 samples seen is saved
Epoch 1/1, Loss after 10192 samples: 0.1854
- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:01<00:00, 94.53it/s]del trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:01<00:00, 94.53it/s]del trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Answering fact completion questions...: 100%|██████████| 125/125 [00:01<00:00, 93.07it/s]del trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Answering fact completion questions...: 100%|██████████| 125/125 [00:01<00:00, 93.07it/s]del trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:01<00:00, 95.08it/s]del trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:01<00:00, 95.08it/s]del trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Average Loss on fact answering task after 10576 samples: 7.5132
Answering fact completion questions...: 100%|██████████| 125/125 [00:01<00:00, 94.32it/s]del trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Answering fact completion questions...: 100%|██████████| 125/125 [00:01<00:00, 94.32it/s]del trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Average Loss on fact answering task after 10576 samples: 7.5932
Mean accuracy: 0.9298, std: 0.0057, lower bound: 0.9189, upper bound: 0.9407 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 10576 samples: 0.9300
Epoch 1/1, Loss after 10592 samples: 0.1634
- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
Answering fact completion questions...:  56%|█████▌    | 70/125 [00:00<00:00, 93.98it/s] a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
Answering fact completion questions...:  56%|█████▌    | 70/125 [00:00<00:00, 93.98it/s] a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:01<00:00, 94.11it/s]a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:01<00:00, 94.11it/s]a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
Average Loss on fact answering task after 11080 samples: 7.6042
Answering fact completion questions...:  48%|████▊     | 60/125 [00:00<00:00, 94.56it/s]]a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
Answering fact completion questions...:  48%|████▊     | 60/125 [00:00<00:00, 94.56it/s]]a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:01<00:00, 93.55it/s]a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:01<00:00, 93.55it/s]a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
Average Loss on fact answering task after 11080 samples: 7.5384
Mean accuracy: 0.9337, std: 0.0055, lower bound: 0.9229, upper bound: 0.9442 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 11080 samples: 0.9336
Epoch 1/1, Loss after 11192 samples: 0.1090
Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:01<00:00, 93.27it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:01<00:00, 93.27it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:01<00:00, 93.49it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:01<00:00, 93.49it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Average Loss on fact answering task after 11584 samples: 7.6198
Answering fact completion questions...: 100%|██████████| 125/125 [00:01<00:00, 94.45it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:01<00:00, 94.45it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Average Loss on fact answering task after 11584 samples: 7.5628
Answering fact completion questions...: 100%|██████████| 125/125 [00:01<00:00, 94.45it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Average Loss on fact answering task after 11584 samples: 7.7788
Mean accuracy: 0.9487, std: 0.0048, lower bound: 0.9391, upper bound: 0.9579 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 11584 samples: 0.9488
Best model with eval accuracy 0.9487829614604463 with 11584 samples seen is saved
Epoch 1/1, Loss after 11592 samples: 0.1133
Epoch 1/1, Loss after 11792 samples: 0.1534
Answering fact completion questions...:  80%|████████  | 100/125 [00:01<00:00, 94.61it/s]del trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Answering fact completion questions...: 100%|██████████| 125/125 [00:01<00:00, 94.01it/s]del trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Answering fact completion questions...: 100%|██████████| 125/125 [00:01<00:00, 94.01it/s]del trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Average Loss on fact answering task after 12088 samples: 7.6313
Average Loss on fact answering task after 12088 samples: 7.7095
Answering fact completion questions...: 100%|██████████| 125/125 [00:01<00:00, 94.01it/s]del trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Answering fact completion questions...:  80%|████████  | 100/125 [00:01<00:00, 93.96it/s]del trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:01<00:00, 94.15it/s]del trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Answering fact completion questions...:  96%|█████████▌| 120/125 [00:01<00:00, 94.15it/s]del trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Answering fact completion questions...: 100%|██████████| 125/125 [00:01<00:00, 93.78it/s]del trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Answering fact completion questions...: 100%|██████████| 125/125 [00:01<00:00, 93.78it/s]del trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
Average Loss on fact answering task after 12088 samples: 7.6452
Mean accuracy: 0.9419, std: 0.0054, lower bound: 0.9310, upper bound: 0.9518 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 12088 samples: 0.9422
Epoch 1/1, Loss after 12192 samples: 0.1617
- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
Answering fact completion questions...:  72%|███████▏  | 90/125 [00:00<00:00, 94.62it/s] a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
Answering fact completion questions...:  72%|███████▏  | 90/125 [00:00<00:00, 94.62it/s] a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:01<00:00, 93.78it/s]a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:01<00:00, 93.78it/s]a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
Average Loss on fact answering task after 12592 samples: 7.3800
Answering fact completion questions...:  64%|██████▍   | 80/125 [00:00<00:00, 94.53it/s]]a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
Answering fact completion questions...:  64%|██████▍   | 80/125 [00:00<00:00, 94.53it/s]]a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:01<00:00, 93.98it/s]a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:01<00:00, 93.98it/s]a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).l).
Average Loss on fact answering task after 12592 samples: 7.6792
Mean accuracy: 0.8720, std: 0.0076, lower bound: 0.8570, upper bound: 0.8869 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 12592 samples: 0.8717
Epoch 1/1, Loss after 12792 samples: 0.1894
Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:01<00:00, 94.30it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:01<00:00, 94.30it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...:  32%|███▏      | 40/125 [00:00<00:00, 94.16it/s]]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...:  32%|███▏      | 40/125 [00:00<00:00, 94.16it/s]]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Average Loss on fact answering task after 13096 samples: 7.5283
Answering fact completion questions...: 100%|██████████| 125/125 [00:01<00:00, 93.64it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:01<00:00, 93.64it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:01<00:00, 93.91it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:01<00:00, 93.91it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Average Loss on fact answering task after 13096 samples: 7.8397
Mean accuracy: 0.9193, std: 0.0064, lower bound: 0.9072, upper bound: 0.9321 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 13096 samples: 0.9194
Epoch 1/1, Loss after 13192 samples: 0.0833
Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:01<00:00, 93.52it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:01<00:00, 93.52it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...:  32%|███▏      | 40/125 [00:00<00:00, 94.15it/s]]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...:  32%|███▏      | 40/125 [00:00<00:00, 94.15it/s]]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Average Loss on fact answering task after 13600 samples: 7.9506
Answering fact completion questions...: 100%|██████████| 125/125 [00:01<00:00, 93.90it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:01<00:00, 93.90it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:01<00:00, 93.42it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:01<00:00, 93.42it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Average Loss on fact answering task after 13600 samples: 7.6657
Mean accuracy: 0.9154, std: 0.0060, lower bound: 0.9036, upper bound: 0.9270 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 13600 samples: 0.9153
Epoch 1/1, Loss after 13792 samples: 0.1560
Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...:   0%|          | 0/125 [00:00<?, ?it/s]93.84it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...:   0%|          | 0/125 [00:00<?, ?it/s]93.84it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Average Loss on fact answering task after 14104 samples: 7.7707
Answering fact completion questions...: 100%|██████████| 125/125 [00:01<00:00, 93.54it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:01<00:00, 93.54it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:01<00:00, 94.02it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:01<00:00, 94.02it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Average Loss on fact answering task after 14104 samples: 7.7800
Average Loss on fact answering task after 14104 samples: 7.5741
Mean accuracy: 0.9302, std: 0.0057, lower bound: 0.9184, upper bound: 0.9412 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 14104 samples: 0.9300
Epoch 1/1, Loss after 14192 samples: 0.1652
Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...:  16%|█▌        | 20/125 [00:00<00:01, 92.90it/s]]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...:  16%|█▌        | 20/125 [00:00<00:01, 92.90it/s]]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Average Loss on fact answering task after 14608 samples: 7.7528
Answering fact completion questions...: 100%|██████████| 125/125 [00:01<00:00, 93.74it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:01<00:00, 93.74it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:01<00:00, 93.86it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:01<00:00, 93.86it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Average Loss on fact answering task after 14608 samples: 7.7621
Average Loss on fact answering task after 14608 samples: 7.5118
Mean accuracy: 0.9381, std: 0.0059, lower bound: 0.9265, upper bound: 0.9493 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 14608 samples: 0.9381
Epoch 1/1, Loss after 14792 samples: 0.1644
Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:01<00:00, 93.43it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:01<00:00, 93.43it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Average Loss on fact answering task after 15112 samples: 7.4378
Answering fact completion questions...: 100%|██████████| 125/125 [00:01<00:00, 94.50it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:01<00:00, 94.50it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:01<00:00, 94.78it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:01<00:00, 94.78it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:01<00:00, 94.54it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:01<00:00, 94.54it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:01<00:00, 94.59it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:01<00:00, 94.59it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Average Loss on fact answering task after 15112 samples: 7.7027
Mean accuracy: 0.9215, std: 0.0061, lower bound: 0.9092, upper bound: 0.9331 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 15112 samples: 0.9214
Epoch 1/1, Loss after 15192 samples: 0.1324
Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:01<00:00, 93.54it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:01<00:00, 93.54it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...:   8%|▊         | 10/125 [00:00<00:01, 94.46it/s]]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...:   8%|▊         | 10/125 [00:00<00:01, 94.46it/s]]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Average Loss on fact answering task after 15616 samples: 7.8566
Answering fact completion questions...: 100%|██████████| 125/125 [00:01<00:00, 94.62it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:01<00:00, 94.62it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:01<00:00, 94.01it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Answering fact completion questions...: 100%|██████████| 125/125 [00:01<00:00, 94.01it/s]g RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']ssification model from a BertForSequenceClassification model).l).
Average Loss on fact answering task after 15616 samples: 7.5223
Mean accuracy: 0.9134, std: 0.0066, lower bound: 0.9006, upper bound: 0.9260 for 1000 bootstraps
Epoch 1/1, Validation accuracy after 15616 samples: 0.9138
Training signal is False, stopping training
Training finished
Best model: {'eval_acc': 0.9487829614604463, 'nb_samples': 11584}
Training loss logs: [{'samples': 192, 'loss': 0.69607666015625}, {'samples': 392, 'loss': 0.6901513671875}, {'samples': 592, 'loss': 0.68751220703125}, {'samples': 792, 'loss': 0.650281982421875}, {'samples': 992, 'loss': 0.58377685546875}, {'samples': 1192, 'loss': 0.5270517349243165}, {'samples': 1392, 'loss': 0.4121199035644531}, {'samples': 1592, 'loss': 0.3237478446960449}, {'samples': 1792, 'loss': 0.3622766709327698}, {'samples': 1992, 'loss': 0.28462648391723633}, {'samples': 2192, 'loss': 0.46205878257751465}, {'samples': 2392, 'loss': 0.3049456214904785}, {'samples': 2592, 'loss': 0.21780382871627807}, {'samples': 2792, 'loss': 0.3208333683013916}, {'samples': 2992, 'loss': 0.2782395648956299}, {'samples': 3192, 'loss': 0.24917978048324585}, {'samples': 3392, 'loss': 0.2804997968673706}, {'samples': 3592, 'loss': 0.327956907749176}, {'samples': 3792, 'loss': 0.23563751459121704}, {'samples': 3992, 'loss': 0.22834824562072753}, {'samples': 4192, 'loss': 0.396558895111084}, {'samples': 4392, 'loss': 0.3214025187492371}, {'samples': 4592, 'loss': 0.17124777793884277}, {'samples': 4792, 'loss': 0.16560678720474242}, {'samples': 4992, 'loss': 0.2268004059791565}, {'samples': 5192, 'loss': 0.1657634496688843}, {'samples': 5392, 'loss': 0.20766263127326964}, {'samples': 5592, 'loss': 0.2511528277397156}, {'samples': 5792, 'loss': 0.2246366000175476}, {'samples': 5992, 'loss': 0.2225702691078186}, {'samples': 6192, 'loss': 0.2675100266933441}, {'samples': 6392, 'loss': 0.19605665922164917}, {'samples': 6592, 'loss': 0.25516664862632754}, {'samples': 6792, 'loss': 0.1521427857875824}, {'samples': 6992, 'loss': 0.17961270809173585}, {'samples': 7192, 'loss': 0.19955217003822326}, {'samples': 7392, 'loss': 0.1681029224395752}, {'samples': 7592, 'loss': 0.18839139938354493}, {'samples': 7792, 'loss': 0.17817267417907715}, {'samples': 7992, 'loss': 0.1976757514476776}, {'samples': 8192, 'loss': 0.11821120738983154}, {'samples': 8392, 'loss': 0.17489962935447692}, {'samples': 8592, 'loss': 0.15208834052085876}, {'samples': 8792, 'loss': 0.16160499095916747}, {'samples': 8992, 'loss': 0.11914533138275146}, {'samples': 9192, 'loss': 0.15576555132865905}, {'samples': 9392, 'loss': 0.17904795408248902}, {'samples': 9592, 'loss': 0.1727585506439209}, {'samples': 9792, 'loss': 0.16626072645187379}, {'samples': 9992, 'loss': 0.16623758435249328}, {'samples': 10192, 'loss': 0.1853501856327057}, {'samples': 10392, 'loss': 0.1434640669822693}, {'samples': 10592, 'loss': 0.1634056043624878}, {'samples': 10792, 'loss': 0.13467582702636718}, {'samples': 10992, 'loss': 0.1126978850364685}, {'samples': 11192, 'loss': 0.10899518251419067}, {'samples': 11392, 'loss': 0.14075928270816804}, {'samples': 11592, 'loss': 0.11328979015350342}, {'samples': 11792, 'loss': 0.15342432677745818}, {'samples': 11992, 'loss': 0.15268004536628724}, {'samples': 12192, 'loss': 0.1617234480381012}, {'samples': 12392, 'loss': 0.14919209361076355}, {'samples': 12592, 'loss': 0.093397616147995}, {'samples': 12792, 'loss': 0.18938440084457397}, {'samples': 12992, 'loss': 0.14393184185028077}, {'samples': 13192, 'loss': 0.08332505643367767}, {'samples': 13392, 'loss': 0.1355063408613205}, {'samples': 13592, 'loss': 0.09402824640274048}, {'samples': 13792, 'loss': 0.15598482191562651}, {'samples': 13992, 'loss': 0.13202665448188783}, {'samples': 14192, 'loss': 0.1651831352710724}, {'samples': 14392, 'loss': 0.14015021026134492}, {'samples': 14592, 'loss': 0.10553366303443909}, {'samples': 14792, 'loss': 0.1643610966205597}, {'samples': 14992, 'loss': 0.11382662475109101}, {'samples': 15192, 'loss': 0.1324350678920746}, {'samples': 15392, 'loss': 0.12344937205314636}, {'samples': 15592, 'loss': 0.12258045852184296}]
Evaluation accuracy logs: [{'samples': 496, 'accuracy': 0.6816054766734281, 'std': 0.01082153100288413, 'lower_bound': 0.6587094320486815, 'upper_bound': 0.7023326572008114}, {'samples': 1000, 'accuracy': 0.5265212981744422, 'std': 0.010935889664555378, 'lower_bound': 0.5045638945233266, 'upper_bound': 0.5476673427991886}, {'samples': 1504, 'accuracy': 0.8318174442190668, 'std': 0.008505005686325317, 'lower_bound': 0.8154031440162272, 'upper_bound': 0.847882860040568}, {'samples': 2008, 'accuracy': 0.8058093306288032, 'std': 0.009406206610571136, 'lower_bound': 0.787525354969574, 'upper_bound': 0.8240365111561866}, {'samples': 2512, 'accuracy': 0.7858869168356998, 'std': 0.009359901484099173, 'lower_bound': 0.7682429006085192, 'upper_bound': 0.8042596348884381}, {'samples': 3016, 'accuracy': 0.9047089249492901, 'std': 0.006598230357109377, 'lower_bound': 0.8914807302231237, 'upper_bound': 0.9178498985801217}, {'samples': 3520, 'accuracy': 0.9106678498985802, 'std': 0.006568306164905415, 'lower_bound': 0.8975659229208925, 'upper_bound': 0.9234279918864098}, {'samples': 4024, 'accuracy': 0.9145953346855985, 'std': 0.00645024198816689, 'lower_bound': 0.902129817444219, 'upper_bound': 0.9269776876267748}, {'samples': 4528, 'accuracy': 0.8968950304259634, 'std': 0.0067332922054506444, 'lower_bound': 0.8838742393509128, 'upper_bound': 0.90973630831643}, {'samples': 5032, 'accuracy': 0.9283686612576065, 'std': 0.005888359679678091, 'lower_bound': 0.9173427991886409, 'upper_bound': 0.9396551724137931}, {'samples': 5536, 'accuracy': 0.9101140973630831, 'std': 0.006759350994313869, 'lower_bound': 0.896551724137931, 'upper_bound': 0.9234279918864098}, {'samples': 6040, 'accuracy': 0.8566632860040567, 'std': 0.007927220593337257, 'lower_bound': 0.84026369168357, 'upper_bound': 0.8712094320486816}, {'samples': 6544, 'accuracy': 0.8745552738336715, 'std': 0.00748748815783484, 'lower_bound': 0.859026369168357, 'upper_bound': 0.8889579107505071}, {'samples': 7048, 'accuracy': 0.9187119675456389, 'std': 0.005980247870200154, 'lower_bound': 0.9072008113590264, 'upper_bound': 0.930540060851927}, {'samples': 7552, 'accuracy': 0.899843813387424, 'std': 0.006825262735906896, 'lower_bound': 0.8869041582150101, 'upper_bound': 0.9132860040567952}, {'samples': 8056, 'accuracy': 0.919684584178499, 'std': 0.006116679777695222, 'lower_bound': 0.9072008113590264, 'upper_bound': 0.9315415821501014}, {'samples': 8560, 'accuracy': 0.9064538539553753, 'std': 0.006815183227189891, 'lower_bound': 0.8924949290060852, 'upper_bound': 0.9198782961460447}, {'samples': 9064, 'accuracy': 0.9032738336713996, 'std': 0.006527227358937034, 'lower_bound': 0.8904665314401623, 'upper_bound': 0.9153270791075051}, {'samples': 9568, 'accuracy': 0.8995466531440162, 'std': 0.006779999726435095, 'lower_bound': 0.8858899594320486, 'upper_bound': 0.9132860040567952}, {'samples': 10072, 'accuracy': 0.9335456389452332, 'std': 0.00588662885788013, 'lower_bound': 0.9219066937119675, 'upper_bound': 0.9447261663286004}, {'samples': 10576, 'accuracy': 0.9297662271805274, 'std': 0.00572892154795484, 'lower_bound': 0.9188640973630832, 'upper_bound': 0.9406693711967545}, {'samples': 11080, 'accuracy': 0.9336744421906694, 'std': 0.005522998335050615, 'lower_bound': 0.922920892494929, 'upper_bound': 0.9442317444219067}, {'samples': 11584, 'accuracy': 0.9486866125760649, 'std': 0.004844747545156108, 'lower_bound': 0.9391480730223124, 'upper_bound': 0.9579107505070994}, {'samples': 12088, 'accuracy': 0.9418539553752535, 'std': 0.005353372741200828, 'lower_bound': 0.9310218052738336, 'upper_bound': 0.9518382352941177}, {'samples': 12592, 'accuracy': 0.8719802231237322, 'std': 0.007604597042808095, 'lower_bound': 0.8569979716024341, 'upper_bound': 0.8869295131845842}, {'samples': 13096, 'accuracy': 0.9193149087221095, 'std': 0.006415853064386667, 'lower_bound': 0.9072008113590264, 'upper_bound': 0.9320613590263692}, {'samples': 13600, 'accuracy': 0.9154269776876267, 'std': 0.005992891855062559, 'lower_bound': 0.9036384381338742, 'upper_bound': 0.9269776876267748}, {'samples': 14104, 'accuracy': 0.9302261663286003, 'std': 0.005694777282440345, 'lower_bound': 0.9183569979716024, 'upper_bound': 0.9411764705882353}, {'samples': 14608, 'accuracy': 0.9380780933062881, 'std': 0.00592516724677978, 'lower_bound': 0.9264579107505071, 'upper_bound': 0.949302738336714}, {'samples': 15112, 'accuracy': 0.9214974645030426, 'std': 0.006059991574680618, 'lower_bound': 0.9092292089249493, 'upper_bound': 0.9330628803245437}, {'samples': 15616, 'accuracy': 0.913357505070994, 'std': 0.006580913862191252, 'lower_bound': 0.9006085192697769, 'upper_bound': 0.9259761663286005}]