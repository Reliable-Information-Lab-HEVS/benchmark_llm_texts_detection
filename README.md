# LLM detector hide-and-seek :mag:



## Project goal
- train a detector to detect responses generated by an LLM from human responsesÂ¨
- train or use different techniques to make the generator evade the detector


## File structure
<a name="file_structure"></a>

### 0. Model classes
- `generator.py`: wrapper arround any LLM used to generate text from a prompt
- `detector.py`: wrapper around any BERT type (bi-directional encoder) used to classify text as human or LLM generated

### 1. Generating dataset of fake LLM responses and human reponses
- `generate_fake_true_dataset.py`: takes an LLM and generate responses using prompts from a given dataset

### 2. Training an LLM to detect LLM generated responses
- `train_detector.py`: finetune a pretrained detector on the dataset generated using `generate_fake_true_dataset.py` 


## Experiments

### Experiment 1
**Goal:** Test degradation of the base model using different training methods
- Tested training methods: finetuning classification head only, finetuning adapter (PEFT method) and full finetuning
- Tested models: RoBERTa-large and DistilRoBERTa-base
- Dataset used: `fake_true_dataset_mistral_10k` (see below to create this dataset)


**Steps to reproduce:**
- Generate the datasets of fake and true samples by launching ...sh script, the script creates `fake_true_dataset_{dataset_name}_10k` for each dataset
- launch `script_experiment1.sh` to launch the training with all the different models, the results are saved in `saved_training_logs` (see [file structure](#file-structure))
- Use the json files `training_logs.json` to create the plots. (To update)


### Experiment 2

- Full finetuning of the detector
- Detect degradation of MLM task using JudgeLLM (check each 1000 samples for example) and correct degradation

### Experiment 3

- Test transferability + compare
- Create a table for each detector with detector trained on specific geneartor on x-axis and generator on y axis 



                                            
|                                              | generator_1 | generator_2 | generator_3 | ... |   | generator_1_FT_chat | generator_not_seen | generator_not_seen_FT_chat |   |
|----------------------------------------------|-------------|-------------|-------------|-----|---|---------------------|--------------------|----------------------------|---|
| detector_1_trained_on_gen1                   |             |             |             |     |   |                     |                    |                            |   |
| detector_1_trained_on_gen2                   |             |             |             |     |   |                     |                    |                            |   |
| detector_1_trained_on_gen3                   |             |             |             |     |   |                     |                    |                            |   |
| ...                                          |             |             |             |     |   |                     |                    |                            |   |
| detector_1_trained_on_gen1,2,3 (Round Robin) |             |             |             |     |   |                     |                    |                            |   |





- Train until degradation threshold (depending on experience 2)

### Experience 4
- Same but add evasive prompt


## Dataset used

- EMNLP news
- Instruction dataset (dolly), not sure